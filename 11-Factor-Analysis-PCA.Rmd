# Factor Analysis and Principal Component Analysis {#factor-analysis-PCA}

[**Wright et al, 2019 citation in psych assessment**; **Bensch et al, 2019 citation** ;**Tackett et al, 2019 citation on MTMM**; **Simms et al. 2019 citation**; **Clark & Watson, 2019 citation**; **Sellbom & Tellegen, 2019**; **Wright & Zimmerman, 2019 citation**; **Burchett et al, 2019 citation**; **Chandler et al, 2020 citation**]

[**Costa et al, 2019 citation**]

[**Markon, 2019 article on bifactor models**]

## Overview of Factor Analysis {#factorAnalysisOverview}

Factor analysis is a class of latent variable models that is designated to identify the structure of a measure or set of measures. It aims to identify the optimal latent structure for a group of variables. Factor analysis encompasses two general types: confirmatory factor analysis and exploratory factor analysis. *Exploratory factor analysis* (EFA) is a latent variable modeling approach that is used when the researcher has no a priori hypotheses about how a set of variables are structured. EFA seeks to identify the empirically optimal-fitting model in ways that balance accuracy (i.e., variance accounted for) and parsimony (i.e., simplicity). *Confirmatory factor analysis* (CFA) is a latent variable modeling approach that is used when a researcher wants to evaluate how well a hypothesized model fits, and the model can examined in comparison to alternative models. Using a CFA approach, the research can pit models representing two theoretical frameworks against each other to see which better accounts for the observed data.

Factor analysis is considered to be a "pure" data-driven method for identifying the structure of the data, but the “truth” that we get depends heavily on the decisions we make regarding the parameters of our factor analysis. The goal of factor analysis is to identify simple, parsimonious factors that underlie the "junk" (i.e., scores filled with measurement error) that we observe.

It used to take a long time to calculate a factor analysis because factor analysis was computed by hand. Now, it is very fast with computers (e.g., oftentimes less than 30 ms). In the 1920s, Spearman developed factor analysis to understand the factor structure of intelligence. It was a long process—it took Spearman around one year to calculate the first factor analysis! Factor analysis takes a large dimension dataset and simplifies into smaller constructs. If you believe that nature is simple underneath, factor analysis gives nature a chance to display the simplicity that lives beneath the complexity on the surface. Spearman identified a single factor, *g*, that accounted for most of the covariation between the measures of intelligence.

Factor analysis involves observed (manifest) variables and unobserved (latent) factors. In a [reflective model](#reflectiveConstruct), it is assumed that the latent factor influences the manifest variables, and the latent factor therefore reflects the common variance among the variables. A factor model potentially includes factor loadings, residuals (errors or distrurbances), intercepts, covariances, and regression paths. A regression path indicates a hypothesis that one variable (or factor) influences another. The standardized regression coefficient represents the strength of association between the variables or factors. A factor loading is a regression path from a latent factor to an observed (manifest) variable. The standardized factor loading represents the strength of association between the variable and the latent factor. A residual is variance in a variable (or factor) that is unexplained by other variables or factors. An intercept is the expected value of a variable if the mean of the factor is equal to zero. Covariances are the associations between variables (or factors).

**Decisions to be made in factor analysis (in addition to questions about which variables to use, how to scale, etc.); garbage in, garbage out; method factors**

### Example Factor Models from Correlation Matrices

Below, I provide some example factor models from various correlation matrices. Analytical examples of factor analysis are presented in Section \@ref(factorAnalysis).

Consider the example correlation matrix in Figure \@ref(fig:correlationMatrix1). Because all of the correlations are the same ($r = .60$), we expect there is approximately one factor for this pattern of data.

```{r correlationMatrix1, out.width = "100%", fig.align = "center", fig.cap = "Example correlation matrix.", echo = FALSE}
knitr::include_graphics("./Images/correlationMatrix1.png")
```

In a single-factor model fit to these data, the factor loadings are .77 and the residual error terms are .40, as depicted in Figure \@ref(fig:factorAnalysis1). The amount of common variance ($R^2$) is estimated as the square of the standardized loading: $0.60 = .77 \times .77$. The amount of error is estimated as: $\text{error} = 1 - \text{common variance}$, so in this case, the amount of error is: $.40 = 1 - 0.60$. In this model, the altent factor explains the covariance among the variables. If the answer is simple, a small and parsimonious model should be able to obtain the answer.

```{r factorAnalysis1, out.width = "100%", fig.align = "center", fig.cap = "Example confirmatory factor analysis model.", echo = FALSE}
knitr::include_graphics("./Images/FactorAnalysis-01.png")
```

Consider a different correlation matrix in Figure \@ref(fig:correlationMatrix2). There is no common variance (correlations between the variables are zero), so there is no reason to believe there is a common factor that influences all of the variables. Variables that are not correlated cannot be related by a third variable, such as a common factor, so a common factor is not the right model.

```{r correlationMatrix2, out.width = "100%", fig.align = "center", fig.cap = "Example correlation matrix.", echo = FALSE}
knitr::include_graphics("./Images/correlationMatrix2.png")
```

Consider another correlation matrix in Figure \@ref(fig:correlationMatrix3).

```{r correlationMatrix3, out.width = "100%", fig.align = "center", fig.cap = "Example correlation matrix.", echo = FALSE}
knitr::include_graphics("./Images/correlationMatrix3.png")
```

If you try to fit a single factor to this correlation matrix, it generates a factor model depicted in Figure \@ref(fig:factorAnalysis2). In this model, the first three variables have a factor loading of .77, but the remaining three variables have a factor loading of zero. This indicates that three remaining factors likely do not share a common factor with the first three variables.

```{r factorAnalysis2, out.width = "100%", fig.align = "center", fig.cap = "Example confirmatory factor analysis model.", echo = FALSE}
knitr::include_graphics("./Images/FactorAnalysis-02.png")
```

Therefore, a one-factor model is probably not correct; instead, the structure of the data is probably best represented by a two-factor model, as depicted in Figure \@ref(fig:factorAnalysis3). In the model, Factor 1 explains why measures 1, 2, and 3 are correlated, whereas Factor 2 explains why measures 4, 5, and 6 are correlated. A two-factor model thus explains why measures 1, 2, and 3 are not correlated with measures 4, 5, and 6.

```{r factorAnalysis3, out.width = "100%", fig.align = "center", fig.cap = "Example confirmatory factor analysis model.", echo = FALSE}
knitr::include_graphics("./Images/FactorAnalysis-03.png")
```

Consider another correlation matrix in Figure \@ref(fig:correlationMatrix4).

```{r correlationMatrix4, out.width = "100%", fig.align = "center", fig.cap = "Example correlation matrix.", echo = FALSE}
knitr::include_graphics("./Images/correlationMatrix4.png")
```

One way to model these data is depicted in Figure \@ref(fig:factorAnalysis4). In this model, the factor loadings are .77, the residual error terms are .40, and there is a covariance path of .50 for the association between Factor 1 and Factor 2. Going from the model to the correlation matrix is deterministic. If you know the model, you can calculate the correlation matrix. For instance, using path tracing rules (described in Section \@ref(ctt)), the correlation of measures within a factor in this model is calculated as: $0.60 = .77 \times .77$. Using path tracing rules, the correlation of measures across factors in this model is calculated as: $0.30 = .77 \times .50 \times .77$.

```{r factorAnalysis4, out.width = "100%", fig.align = "center", fig.cap = "Example confirmatory factor analysis model.", echo = FALSE}
knitr::include_graphics("./Images/FactorAnalysis-04.png")
```

Although going from the model to the correlation matrix is deterministic, going from the correlation matrix to the model is not deterministic. If you know the correlation matrix, there may be many possible models. For instance, the model could also be the one depicted in Figure \@ref(fig:factorAnalysis5), with factor loadings of .77, residual error terms of .40, and a regression path of .50. This model has the exact same same fit as the previous model, but it has different implications. Unlike the previous model, in this model, there is a "causal" pathway from Factor 1 to Factor 2. However, the causal effect of Factor 1 does not account for all of the variance in Factor 2 because the correlation is only .50.

```{r factorAnalysis5, out.width = "100%", fig.align = "center", fig.cap = "Example confirmatory factor analysis model.", echo = FALSE}
knitr::include_graphics("./Images/FactorAnalysis-05.png")
```

Alternatively, something else (e.g., another factor) could be explaining the data that we have not considered, as depicted in Figure \@ref(fig:factorAnalysis6). This is a hierarchical model, in which there is a higher-order factor ($A_1$) that influences both lower-order factors, Factor 1 ($F_1$) and Factor 2 ($F_2$). This model has the exact same same fit as the previous models. The factor loadings from the lower order factors to the manifest variables are .77, the factor loading from the higher-order factor to the lower-order factors is .71, and the residual error terms are .40. Using path tracing rules, the correlation of measures across factors in this model is calculated as: $0.30 = .77 \times .71 \times .71 \times .77$.

```{r factorAnalysis6, out.width = "100%", fig.align = "center", fig.cap = "Example confirmatory factor analysis model.", echo = FALSE}
knitr::include_graphics("./Images/FactorAnalysis-06.png")
```

Alternatively, there could be a single factor that ties measures 1, 2, and 3 together and measures 4, 5, and 6 together, as depicted in Figure \@ref(fig:factorAnalysis7). In this model, the measures no longer have random error: measures 1, 2, and 3 have correlated residuals—that is, they share error variance; likewise, measures 4, 5, and 6 have correlated residuals. This model has the exact same same fit as the previous models. Using path tracing rules, the correlation of measures within a factor in this model is calculated as: $.60 = (.55 \times .55) + (.70 \times .43 \times .70) + (.70 \times .43 \times .43 \times .70)$. Using path tracing rules, the correlation of measures across factors in this model is calculated as: $0.30 = .50 \times .50$.

```{r factorAnalysis7, out.width = "100%", fig.align = "center", fig.cap = "Example confirmatory factor analysis model.", echo = FALSE}
knitr::include_graphics("./Images/FactorAnalysis-07.png")
```

### Indeterminacy

There could be many more models that have the same fit to the data. Thus, factor analysis has *indeterminancy* because all of these models can explain these data equally well, with all having different theoretical meaning. The goal of factor analysis is for the model to look at the data and induce the model. However, most data matrices in real life are very complicated—much more complicated than in these examples.

This is why we do not calculate our own factor analysis by hand; use a stats program! It is important to think about the possibility of other models to determine how confident you can be in your data model. For every fully specified factor model—i.e., where the relevant paths are all defined, there is one and only one predictive data matrix (correlation matrix). However, each data matrix can produce many different factor models. There is no way to distinguish which of these factor models is correct from the data matrix alone. Any given data matrix can predict an infinite number of factor models that accurately represent the data structure—so we make decisions that determine what type of factor solution our data will yield.

Many models could explain your data, and there are many more models that do not explain the data. For equally good-fitting models, decide based on interpretability. If you have strong theory, decide based on theory and things outside of factor analysis!

### Practical Considerations

There are important considerations for doing factor analysis in real life with complex data. Traditionally, researchers had to consider what kind of data they have, and often assumed interval-level data even though data in psychology are often not interval data. In the past, factor analysis was not good with categorical, dichotomous (e.g., True/False) data because the variance then is completely determined by the mean. So, we need something more complicated for dichotomous data. More solutions are available now for factor analysis with ordinal and dichotomous data, but it is generally best to have at least four ordered categories to perform factor analysis.

The necessary sample size depends on the complexity of the true factor structure. If there is a strong single factor for 30 items, then $N = 50$ is plenty. But, if there are five factors and some correlated errors, then the sample size will need to be closer to ~5,000. Factor Analysis can recover the truth when the world is simple. However, nature is often not simple, and it may end in the distortion of nature instead of nature itself.

### Decisions to Make in Factor Analysis {#factorAnalysisDecisions}

There are many decisions to make in factor analysis. These decisions can have important impacts on the resulting solution. Decisions include things such as:

1. What variables to include in the model and how to scale them
2. Method of factor extraction: factor analysis or PCA
3. If factor analysis, the kind of factor analysis: EFA or CFA
4. How many factors to retain
5. Whether and how to rotate factors (factor rotation)
6. Model selection and interpretation

#### 1. Variables to Include and their Scaling

The first decision when conducting a factor analysis is which variables to include and the scaling of those variables. What factors (or components) you extract can differ widely depending on what variables you include in the analysis. For example, if you include many variables from the same source (e.g., self-report), it is possible that you will extract a factor that represents the common variance among the variables from that source (i.e., the self-reported variables). This would be considered a method factor, which works against the goal of estimating latent factors that represent the constructs of interest (as opposed to the measurement methods used to estimate those constructs). An additional consideration is the scaling of the variables—whether to use the raw scaling or whether to standardize them to be on a more common metric (e.g., z-score metric with a mean of zero and standard deviation of one).

#### 2. Method of Factor Extraction

The second decision is to select the method of factor extraction. This is the algorithm that is going to try to identify factors. There are two main families of factor or component extraction: analytic or principal components. The principal components approach is called principal component analysis (PCA). PCA is not really a form factor analysis; rather, it is useful for data reduction [@Lilienfeld2015]. The analytic family includes approaches such as principal axis factoring and maximum likelihood factor analysis.

##### Principal Component Analysis {#pca}

Principal component analysis (PCA) is used if you want to reduce your data matrix. PCA composites represent the variances of an observed measure in as economical a fashion as possible, with no latent underlying variables. The goal of PCA is to identify a smaller number of components that explain as much variance in a set of variables as possible. It is an atheoretical way to decompose a matrix. PCA involves decomposition of a data matrix into a set of eigenvectors, which are transformations of the old variables.

The eigenvectors attempt to simplify the data in the matrix. PCA takes the data matrix and identifies the weighted sum of all variables that does the best job at explaining variance: these are the principal components, also called eigenvector. Principal components reflect optimally weighted sums. In this way, PCA is a [formative model](#formativeConstruct) (by contrast, factor analysis applies a [reflective model](#reflectiveConstruct)).

PCA decomposes the data matrix into any number of components—as many as the number of variables, which will always account for all variance. Then, after the model is fit, you can look at the results and discard the components which likely reflect error variance. Judgments about which factors to retain are based on empirical criteria in conjunction with theory to select a parsimonious number of components that account for the majority of variance.

The eigenvalue reflects amount of variance explained by the component (eigenvector). PCA then pulls the principle component out and makes a new data matrix: i.e., new correlation matrix. Then the PCA pulls out the component that explains the next most variance—i.e., the eigenvector with the next largest eigenvalue, and it does this for all components, equal to the same number of variables. For instance, if there are six variables, it will iteratively extract an additional component up to six components. You can extract as many eigenvectors as there are variables. If you extract all six components, the data matrix left over will be the same as the correlation matrix in Figure \@ref(fig:correlationMatrix2). That is, the remaining variables will be entirely uncorrelated with the remaining variables, because six components explain 100% of the variance from six variables. In other words, you can can explain (6) variables with (6) new things!

However, it does no good if you have to use all (6) components because there is no data reduction from the original number of variables, but hopefully the first few components will explain most of the variance.

The sum of all eigenvalues is equal to the number of variables in the analysis. PCA does not have same assumptions as factor analysis, which assumes that measures are partly from common variance and error. But, if you estimate (6) eigenvectors and only keep (2), the is a two-component model and whatever left becomes error. Therefore, PCA does not have the same assumptions as factor analysis, but it often ends up in the same place.

Most people who want to conduct a factor analysis use PCA, but PCA is not really factor analysis. PCA is what SPSS can do quickly. But computers are so fast now—just do a real factor analysis! Factor analysis better handles error than PCA—factor analysis assumes that what is in the variable is the combination of common construct variance and error. By contrast, PCA assumes that the measures have no measurement error.

##### Analytic Approach: Principal Axis Factoring or Maximum Likelihood Factor Analysis

Factor analysis is a special case of [structural equation modeling](#sem) (SEM). Factor analysis is an analytic technique that is interested in the factor structure of a measure of set of measures. Factor analysis is a theoretical approach that considers that there are latent theoretical constructs that influence the scores on particular variables. It assumes that part of the explanation for each variable is shared between variables, and that part of it is unique variance. The unique variance is considered error. The common variance is called the communality, which is the factor variance, where $1 - \text{communality}$ is the amount of variance due to error.

Factor analysis can be used to test [measurement/factorial invariance](#measurementInvariance) and for [multi-trait multi-method](#MTMM) designs.

There are several differences between (real) factor analysis versus PCA. Factor analysis has greater sophistication than PCA, but greater sophistication often results in greater assumptions. Factor analysis does not always work; the day may not always fit to a factor analysis model; therefore, use PCA as a second/last option. PCA can decompose any data matrix; it always works. PCA is okay if you are not interested in the factor structure. PCA uses all variance of variables and assumes variables have no error, so it does not account for measurement error. PCA is good if you just want to form a linear composite; if the causal structure is [formative](formativeConstruct) (rather than [reflective](#reflectiveConstruct)). However, if you are interested in the factor structure, use factor analysis, which estimates a latent variable that accounts for the common variance and discards error variance. Factor analysis is useful for the identification of latent constructs—i.e., underlying dimensions or factors that explain (cause) scores on items.

#### 3. EFA or CFA

A third decision is the kind of factor analysis to use: exploratory factor analysis (EFA) or confirmatory factor analysis (CFA). EFA is used if you have no a priori hypotheses about the factor structure of the model, but you would like to understand the latent variables represented by your items. CFA is used to confirm a priori hypotheses about the factor structure of the model.

EFA is partly induced from the data. You feed in the data and let the program build the factor model. You can set some parameters going in, including how to extract or rotate the factors.

CFA is a test of the hypothesis. In a CFA, you specify the model and ask how well does this model represent the data.	In real life, there is not a clear distinction between EFA and CFA. In CFA, it is often that researchers only set half of the constraints, and let the data fill in the rest. In EFA, researchers often set constraints and assumptions. Thus, the line between EFA and CFA is often blurred.

#### 4. How Many Factors to Retain

A goal of factor analysis and PCA is simplification or parsimony, while still explaining as much variance as possible. The hope is that you can have fewer factors that explain the associations between the variables than the number of observed variables. But how do you decide on the number of factors (in factor analysis) or components (in PCA)?

There are a number of criteria that one can use to help determine how many factors to keep:

- Kaiser-Guttman criterion: factors with eigenvalues greater than one
- Cattell's scree test: the "elbow" in a scree plot minus one
- Parallel analysis
- Very simple structure plot

There is not necessarily a "correct" criterion to use in determining how many factors to keep, so it is generally recommended that researchers use multiple criteria in combination with theory and interpretability.

A scree plot from a factor analysis or PCA can provide lots of information. A scree plot has the factor number on the x-axis and the eigenvalue on the y-axis. The eigenvalue is the variance accounted for by a factor. An example of a scree plot is in Figure \@ref(fig:screePlot).

```{r screePlot, out.width = "100%", fig.align = "center", fig.cap = "Example of a scree plot.", echo = FALSE}
knitr::include_graphics("./Images/screePlot.png")
```

The total variance is equal to the number of variables you have, so one eigenvalue is approximately one variable's worth of variance. In a factor analysis and PCA, the first factor (or component) accounts for the most variance, the second factor accounts for the second-most variance, and so on. The more factors you add, the less variance is explained by the additional factor.

One criterion for how many factors to keep is the Kaiser-Guttman criterion. According to the Kaiser-Guttman criterion, you should keep any factors whose eigenvalue is greater than one. That is, for the sake of simplicity, parsimony, and data reduction, you should take any factors that explain more than a single variable would explain. According to the Kaiser-Guttman criterion, we would keep three factors from Figure \@ref(fig:screePlot) that have eigenvalues greater than one. The default in SPSS is to retain factors with eigenvalues greater than 1. However, keeping factors whose eigenvalue is greater than one is not the most correct rule. If you let SPSS do this, you may get many factors with eigenvalues around 1 (e.g., factors with an eigenvalue ~ 1.0001) that are not adding so much that it is worth the added complexity. The Kaiser-Guttman criterion usually results in keeping too many factors. Factors with small eigenvalues around 1 could reflect error shared across variables. For instance, factors with small eigenvalues could reflect method variance (i.e., method factor), such as a self-report factor that turns up as a factor in factor analysis, but that may be useless to you as a conceptual factor of a construct of interest.

Another criterion is Cattell's scree test, which involves selecting the number of factors from looking at the scree plot. "Scree" refers to the rubble of stones at the bottom of a mountain. According to Cattell's scree test, you should keep the factors before the last steep drop in eigenvalues—i.e., the factors before the rubble, where the slope approaches zero. The beginning of the scree (or rubble), where the slope approaches zero, is called the "elbow" of a scree plot. Using Cattell's scree test, you retain the number of factors that explain the most variance prior to the explained variance drop off, because, ultimately, you want to include only as many factors in which you gain substantially more by the inclusion of these factors. That is, you would keep the number of factors at the elbow of the scree plot minus one. If the last steep drop occurs from Factor 4 to Factor 5 and the elbow is at Factor 5, we would keep four factors. In Figure \@ref(fig:screePlot), the last steep drop in eigenvalues occurs from Factor 3 to Factor 4; the elbow of the scree plot occurs at Factor 4. We would keep the number of factors at the elbow minus one. Thus, using Cattell's scree test, we would keep three factors based on Figure \@ref(fig:screePlot).

There are more sophisticated ways of using scree plot, but they usually end up at a similar decision. Examples of more sophisticated tests include parallel analysis and very simple structure (VSS) plots. In a parallel analysis, you examine where the eigenvalues from observed data and random data converge, so you do not retain a factor that explains less variance than would be expected by random chance. A parallel analysis can be helpful when you have many variables and one factor accounts for the majority of the variance such that the elbow is at Factor 2 (which would result in keeping one factor), but you have theoretical reasons to select more than one factor. An example in which parallel analysis may be helpful is with neurophysiological data. For instance, parallel analysis can be helpful when conducting temporo-spatial PCA of event-related potential (ERP) data in which you want to separate multiple time windows and multiple spatial locations despite a predominant signal during a given time window and spatial location. [**Joe Dien citations**]

In general, my recommendation is to use Cattell's scree test, and then test the factor solutions with plus or minus one factor. You should never accept factors with eigenvalues less than one, because they are likely to be error. If you are using maximum likelihood factor analysis, you can compare the fit of various models with model fit criteria to see which model fits best for its parsimony. A model will always fit better when you add additional parameters or factors, so you examine if there is *significant* improvement in model fit when adding the additional factor—that is, we keep adding complexity until additional complexity does not buy us much. Always try a factor solution that is one less and one more than suggested by Cattell's scree test to buffer your final solution because the purpose of factor analysis is to explain things and to have interpretability. Even if all rules or indicators suggest to keep X number of factors, maybe $\pm$ one factor helps clarifies things. Even though factor analysis is empirical, theory and interpretatability should also inform decisions.

#### 5. Factor Rotation

The next step is to rotate the factors to make them more interpretable and simple, which is the whole goal. To interpret the results of a factor analysis, we examine the factor matrix. An example of a factor matrix is in Figure \@ref(fig:factorMatrix). The columns refer to the different factors, the columns refer to the different observed variables. The cells in the table are the factor loadings—they are basically the correlation between the variable and the factor.

```{r factorMatrix, out.width = "100%", fig.align = "center", fig.cap = "Example of a factor matrix.", echo = FALSE}
knitr::include_graphics("./Images/factorMatrix.png")
```

In the example factor matrix in Figure \@ref(fig:factorMatrix), the factor analysis is not very helpful—it tells us very little because it did not distinguish between the two factors. The variables had similar loadings on Factor 1 and Factor 2. An example of a unrotated factor solution is in Figure \@ref(fig:factorSolutionUnrotated). In the figure, all of the variables are in the midst of the quadrants—they are not on the factors' axes. Thus, the factors are not very informative.

```{r factorSolutionUnrotated, out.width = "100%", fig.align = "center", fig.cap = "Example of an unrotated factor solution.", echo = FALSE}
knitr::include_graphics("./Images/factorSolutionUnrotated.png")
```

As a result, to improve the interetability of the factor analysis, we can do what is called rotation. *Rotation* involves changing the orientation of the factors by changing the axes so that variables end up with very high (close to one or negative one) or very low (close to zero) loadings, so that it is clear which factors include which variables. That is, it tries to identify the ideal solution (factor) for each variable. It searches for simple structure and keeps searching until it finds a minimum. After rotation, each factor will have loadings close to one (or negative one) for some variables and close to zero for other variables. The goal of rotation is to achieve *simple structure*. Simple structure means that every variable loads perfectly on one and only one factor, as operationalized by a matrix of factor loadings with values of one and zero and nothing else. However, pure simple structure only occurs in simulations, not in real-life data. Nevertheless, rotations are intended to get closer to this goal of simple structure. To do this, orthogonal rotations are often used. Orthogonal rotations makes the rotated factors uncorrelated. An example of a commonly used orthogonal rotation is varimax rotation.

An example of a factor matrix following an orthogonal rotation is depicted in Figure \@ref(fig:factorMatrixRotated). An example of a factor solution following an orthogonal rotation is depicted in Figure \@ref(fig:factorSolutionRotated).

```{r factorMatrixRotated, out.width = "100%", fig.align = "center", fig.cap = "Example of a rotated factor matrix.", echo = FALSE}
knitr::include_graphics("./Images/factorMatrixRotated.png")
```

```{r factorSolutionUnrotated, out.width = "100%", fig.align = "center", fig.cap = "Example of a rotated factor solution.", echo = FALSE}
knitr::include_graphics("./Images/factorSolutionRotated.png")
```

An example of a factor matrix from SPSS following an orthogonal rotation is depicted in Figure \@ref(fig:rotatedFactorMatrix).

```{r rotatedFactorMatrix, out.width = "100%", fig.align = "center", fig.cap = "Example of a rotated factor matrix from SPSS.", echo = FALSE}
knitr::include_graphics("./Images/rotatedFactorMatrix.png")
```

Sometimes, however, the two factors and their constituent variables may be correlated. Examples of two correlated factors may be depression and anxiety. When the two factors are correlated in reality, if we make them uncorrelated, this would result in an inaccurate model. Oblique rotation allows for factors to be correlated, but if the factors have low correlation (e.g., .2 or less), you can likely continue with orthogonal rotation. Results from an oblique rotation are more complicated than orthogonal rotation—they provide lots of output and are more complicated to interpret. In addition, oblique rotation might not yield a smooth answer if you have a relatively small sample size.

As an example of rotation based on interpretability, consider the Five-Factor Model of Personality (the Big Five), which goes by the acryonym, OCEAN: **o**penness, **c**onscientiousness, **e**xtraversion, **a**greeableness, and **n**euroticism. Although the five factors of personality are somewhat correlated, we can use rotation to ensure they are maximally independent. Upon rotation, extraversion and neuroticism are essentially uncorrelated. The other pole of extraversion is intraversion and the other pole of neuroticism might be emotional stability or calmness. 

Simple structure is achieved when each variable loads highly onto as few factors as possible (i.e., each item will has only one significant or primary loading). Oftentimes this is not the case, so we choose our rotation method in order to decide if the factors can be correlated (an oblique rotation) or if the factors will be uncorrelated (an orthogonal rotation). If the factors are not correlated with each other, use and orthogonal rotation. The correlation between an item and a factor is a factor loading, which is simply a way to ask how much a variable is correlated with the underlying factor. However, its interpretation is more complicated if there are correlated factors!

An orthogonal rotation (e.g., varimax) can help with simplicity of interpretation because it seeks to yield simple structure without cross-loadings. Cross-loadings are instances where a variable loads onto multiple factors. My recommendation would always be to use an orthogonal rotation if you have reason to believe that finding simple structure in your data is possible; otherwise the factors are extremely difficult to interpret—what exactly does a cross-loading even mean? However, you should always try an oblique rotation, too, to see how strongly the factors are correlated. Examples of oblique rotations include oblimin and promax.

#### 6. Model Selection and Interpretation

The next step of factor analysis is selecting and interpreting the model. One data matrix can lead to many different (correct) models—you must choose one based on the factor structure and theory. Use theory to interpret the model and label the factors. When interpreting others' findings, do not rely just on the factor labels—look at the actual items to determine what they assess.

### The Downfall of Factor Analysis

The downfall of factor analysis is cross-validation. Cross-validating a factor structure would mean getting the same factor structure with a new sample. We want factor structures to show good replicability across samples. However, cross-validation often falls apart. They way to attempt to replicate a factor structure in an independent sample is to use CFA to set everything up and test the hypothesized factor structure in the independent sample.

### What to Do with Factors

What can you do with factors once you have them? In SEM, factors have meaning. You can use them as predictors, mediators, moderators, or outcomes. People often want to use factors outside of SEM, but there is confusion here: When researchers find that three variables load onto Factor A, the researchers often combine those three using a sum or average—but this is not accurate. If you just add or average them, this ignores the factor loadings and the error. Another solution is to form a linear composite by adding and weighting the variables by the factor loadings, which retains the differences in correlations (i.e., a weighted sum), but this still ignores the estimated error, so it still may not be generalizable and meaningful. At the same time, weighted sums may be less generalizable than unit-weighted composites where each variable is given equal weight because some variability in factor loadings likely reflects sampling error.

### Missing Data Handling

The PCA default in SPSS is listwise deletion of missing data: if a participant is missing data on any variable, the subject gets excluded from the analysis, so you might end up with too few participants. Instead, use a correlation matrix with pairwise deletion for PCA with missing data. Maximum likelihood factor analysis can make use of all available data points for a participant, even if they are missing some data points. Mplus, which is often used for [SEM](#sem) and factor analysis, will notify you if you are removing many participants in CFA/EFA. The `lavaan` package in `R` also notifies you if you removing participants in CFA/SEM models.

## Getting Started

### Load Libraries

```{r, message = FALSE, warning = FALSE}
library("lavaan")
library("psych")
library("corrplot")
library("nFactors")
library("semPlot")
library("lavaan")
library("semTools")
library("dagitty")
library("tidyverse")
library("here")
library("tinytex")
library("knitr")
library("rmarkdown")
```

### Load Data

The `MTMM` data file comes from W. Joel Schneider: http://my.ilstu.edu/~wjschne/442/MTMM.csv

```{r, message = FALSE}
MTMM <- read_csv(here("Data", "MTMM.csv")) #read_csv("../Data/MTMM.csv")
```

### Prepare Data

#### Add Missing Data

Adding missing data to dataframes helps make examples more realistic to real-life data, and helps you get in the habit of programming to account for missing data. For reproducibility, I set the seed below. Using the same seed will yield the same answer every time. There is nothing special about this particular seed.

```{r}
set.seed(52242)

varNames <- names(HolzingerSwineford1939)
dimensionsDf <- dim(HolzingerSwineford1939)
unlistedDf <- unlist(HolzingerSwineford1939)
unlistedDf[sample(1:length(unlistedDf), size = .15 * length(unlistedDf))] <- NA
HolzingerSwineford1939 <- as.data.frame(matrix(unlistedDf, ncol = dimensionsDf[2]))
names(HolzingerSwineford1939) <- varNames
```

## Descriptive Statistics and Correlations

Before conducting a factor analysis, it is important examine descriptive statistics and correlations among variables.

### Descriptive Statistics

```{r, eval = knitr::is_html_output(excludes = "epub")}
paged_table(psych::describe(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")]))
```

```{r, eval = knitr::is_latex_output()}
kable(psych::describe(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")]),
      booktabs = TRUE)
```

```{r eval = knitr::is_html_output(excludes = c("markdown","html","html4","html5","revealjs","s5","slideous","slidy","gfm"))}
kable(psych::describe(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")]),
      booktabs = TRUE)
```

### Correlations

Pairs panel plots were generated using the `psych` package [@R-psych]. Correlation plots were generated using the `corrplot` package [@R-corrplot].

```{r}
cor(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], use = "pairwise.complete.obs")
```

```{r, out.width = "100%", fig.cap = "Pairs panel plot"}
pairs.panels(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")])
```

```{r, out.width = "100%", fig.cap = "Correlation plot"}
corrplot(cor(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], use = "pairwise.complete.obs"))
```

## Factor Analysis {#factorAnalysis}

### Exploratory Factor Analysis (EFA) {#efa}

#### Determine number of factors

Determine the number of factors to retain using the Scree plot and Very Simple Structure plot.

##### Scree Plot

Scree plots were generated using the `psych` [@R-psych] and `nFactors` [@R-nFactors] packages. The acceleration factor attempts to operationalize the scree test: i.e., the "elbow" of the scree plot. The acceleration factor is quantified using the acceleration of the curve, that is, the second derivative.

```{r, out.width = "100%", fig.cap = "Scree plots in exploratory factor analysis"}
fa.parallel(x = HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], fm = "ml", fa = "fa")
plot(nScree(x = cor(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], use = "pairwise.complete.obs"), model = "factors"))
```

##### Very Simple Structure (VSS) Plot

Very simple structure (VSS) plots were generated using the `psych` package [@R-psych]. [**Nice description of VSS plots: https://it.unt.edu/sites/default/files/vss_l_jds_dec2014.pdf**]

###### Orthogonal (Varimax) rotation

```{r, out.width = "100%", fig.cap = "Very simple structure plot with orthogonal rotation in exploratory factor analysis"}
vss(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], rotate = "varimax", fm = "ml")
nfactors(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], rotate = "varimax", fm = "ml")
```

###### Oblique (Oblimin) rotation

```{r, out.width = "100%", fig.cap = "Very simple structure plot with oblique rotation in exploratory factor analysis"}
vss(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], rotate = "oblimin", fm = "ml")
nfactors(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], rotate = "oblimin", fm = "ml")
```

###### No rotation

```{r, out.width = "100%", fig.cap = "Very simple structure plot with no rotation in exploratory factor analysis"}
vss(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], rotate = "none", fm = "ml")
nfactors(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], rotate = "none", fm = "ml")
```

#### Run factor analysis

Exploratory factor analysis (EFA) models were fit using the `psych` package [@R-psych].

##### Orthogonal (Varimax) rotation

Fit a different model with each number of possible factors:

```{r}
efa1factorOrthogonal <- fa(r = HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 1, rotate = "varimax", fm = "ml")
efa2factorOrthogonal <- fa(r = HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 2, rotate = "varimax", fm = "ml")
efa3factorOrthogonal <- fa(r = HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 3, rotate = "varimax", fm = "ml")
efa4factorOrthogonal <- fa(r = HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 4, rotate = "varimax", fm = "ml")
efa5factorOrthogonal <- fa(r = HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 5, rotate = "varimax", fm = "ml")
efa6factorOrthogonal <- fa(r = HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 6, rotate = "varimax", fm = "ml")
efa7factorOrthogonal <- fa(r = HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 7, rotate = "varimax", fm = "ml")
efa8factorOrthogonal <- fa(r = HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 8, rotate = "varimax", fm = "ml")
efa9factorOrthogonal <- fa(r = HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 9, rotate = "varimax", fm = "ml")
```

##### Oblique (Oblimin) rotation

Fit a different model with each number of possible factors:

```{r}
efa1factorOblique <- fa(r = HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 1, rotate = "oblimin", fm = "ml")
efa2factorOblique <- fa(r = HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 2, rotate = "oblimin", fm = "ml")
efa3factorOblique <- fa(r = HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 3, rotate = "oblimin", fm = "ml")
efa4factorOblique <- fa(r = HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 4, rotate = "oblimin", fm = "ml")
efa5factorOblique <- fa(r = HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 5, rotate = "oblimin", fm = "ml")
efa6factorOblique <- fa(r = HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 6, rotate = "oblimin", fm = "ml")
efa7factorOblique <- fa(r = HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 7, rotate = "oblimin", fm = "ml")
efa8factorOblique <- fa(r = HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 8, rotate = "oblimin", fm = "ml") #no convergence
efa9factorOblique <- fa(r = HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 9, rotate = "oblimin", fm = "ml")
```

#### Factor scores

##### Orthogonal (Varimax) rotation

```{r}
fa3Orthogonal <- efa3factorOrthogonal$scores
```

##### Oblique (Oblimin) rotation

```{r}
fa3Oblique <- efa3factorOblique$scores
```

#### Plots

Pairs panel plots were generated using the `psych` package [@R-psych]. Correlation plots were generated using the `corrplot` package [@R-corrplot].

#### Orthogonal (Varimax) rotation

```{r, out.width = "100%", fig.cap = "Factor plot with orthogonal rotation in exploratory factor analysis"}
factor.plot(efa3factorOrthogonal, cut = 0.5)
```

```{r, out.width = "100%", fig.cap = "Factor diagram with orthogonal rotation in exploratory factor analysis"}
fa.diagram(efa3factorOrthogonal, digits = 2)
```

```{r, out.width = "100%", fig.cap = "Pairs panel plot with orthogonal rotation in exploratory factor analysis"}
pairs.panels(fa3Orthogonal)
```

```{r, out.width = "100%", fig.cap = "Correlation plot with orthogonal rotation in exploratory factor analysis"}
corrplot(cor(fa3Orthogonal, use = "pairwise.complete.obs")) 
```

##### Oblique (Oblimin) rotation

```{r, out.width = "100%", fig.cap = "Factor plot with oblique rotation in exploratory factor analysis"}
factor.plot(efa3factorOblique, cut = 0.5)
```

```{r, out.width = "100%", fig.cap = "Factor diagram with oblique rotation in exploratory factor analysis"}
fa.diagram(efa3factorOblique, digits = 2)
```

```{r, out.width = "100%", fig.cap = "Pairs panel plot with oblique rotation in exploratory factor analysis"}
pairs.panels(fa3Oblique)
```

```{r, out.width = "100%", fig.cap = "Correlation plot with oblique rotation in exploratory factor analysis"}
corrplot(cor(fa3Oblique, use = "pairwise.complete.obs"))
```

### Confirmatory Factor Analysis (CFA) {#cfa}

I introduced confirmatory factor analysis (CFA) models in Section \@ref(cfa-sem) in the chapter on [structural equation models](#sem).

The CFA models were fit in the `lavaan` package [@R-lavaan]. The examples were adapted from the `lavaan` documentation: http://lavaan.ugent.be/tutorial/cfa.html

#### Specify the model

```{r}
cfaModel_syntax <- '
 #Factor loadings
 visual  =~ x1 + x2 + x3
 textual =~ x4 + x5 + x6
 speed   =~ x7 + x8 + x9
'

cfaModel_fullSyntax <- '
 #Factor loadings (free the factor loading of the first indicator)
 visual  =~ NA*x1 + x2 + x3
 textual =~ NA*x4 + x5 + x6
 speed   =~ NA*x7 + x8 + x9
 
 #Fix latent means to zero
 visual ~ 0
 textual ~ 0
 speed ~ 0
 
 #Fix latent variances to one
 visual ~~ 1*visual
 textual ~~ 1*textual
 speed ~~ 1*speed
 
 #Estimate covariances among latent variables
 visual ~~ textual
 visual ~~ speed
 textual ~~ speed
 
 #Estimate residual variances of manifest variables
 x1 ~~ x1
 x2 ~~ x2
 x3 ~~ x3
 x4 ~~ x4
 x5 ~~ x5
 x6 ~~ x6
 x7 ~~ x7
 x8 ~~ x8
 x9 ~~ x9
 
 #Free intercepts of manifest variables
 x1 ~ int1*1
 x2 ~ int2*1
 x3 ~ int3*1
 x4 ~ int4*1
 x5 ~ int5*1
 x6 ~ int6*1
 x7 ~ int7*1
 x8 ~ int8*1
 x9 ~ int9*1
'
```

##### Model syntax in table form:

```{r}
lavaanify(cfaModel_syntax)
lavaanify(cfaModel_fullSyntax)
```

#### Fit the model

```{r}
cfaModelFit <- cfa(cfaModel_syntax,
                   data = HolzingerSwineford1939,
                   missing = "ML",
                   estimator = "MLR",
                   std.lv = TRUE)

cfaModelFit_full <- lavaan(cfaModel_fullSyntax,
                           data = HolzingerSwineford1939,
                           missing = "ML",
                           estimator = "MLR")
```

#### Display summary output

```{r}
summary(cfaModelFit,
        fit.measures = TRUE,
        standardized = TRUE,
        rsquare = TRUE)

summary(cfaModelFit_full,
        fit.measures = TRUE,
        standardized = TRUE,
        rsquare = TRUE)
```

#### Estimates of model fit

```{r}
fitMeasures(cfaModelFit, fit.measures = c("chisq", "df", "pvalue",
                                          "chisq.scaled", "df.scaled", "pvalue.scaled",
                                          "chisq.scaling.factor",
                                          "rmsea", "cfi", "tli", "srmr",
                                          "rmsea.robust", "cfi.robust", "tli.robust"))
```

#### Residuals

```{r}
residuals(cfaModelFit, type = "cor")
```

#### Modification indices

```{r}
modificationindices(cfaModelFit)
```

#### Factor scores

```{r}
cfaFactorScores <- predict(cfaModelFit)
as_tibble(cfaFactorScores)
```

##### Compare CFA factor scores to EFA factor scores

```{r}
cor.test(x = cfaFactorScores[,"visual"], y = fa3Orthogonal[,"ML3"])
cor.test(x = cfaFactorScores[,"textual"], y = fa3Orthogonal[,"ML1"])
cor.test(x = cfaFactorScores[,"speed"], y = fa3Orthogonal[,"ML2"])
```

#### Reliability

Reliability of factor scores for the latent factors, as quantified by Cronbach's alpha ($\alpha$) and omega ($\omega$), was estimated using the `semTools` package [@R-semTools].

```{r}
semTools::reliability(cfaModelFit)
```

#### Path diagram

Below is a path diagram of the model generated using the `semPlot` package [@R-semPlot].

```{r, out.width = "100%", fig.height = 12, fig.cap = "Confirmatory factor analysis model diagram"}
semPaths(cfaModelFit,
         what = "std",
         layout = "tree2",
         optimizeLatRes = FALSE,
         exoVar = FALSE,
         edge.label.cex = 1.3)
```

#### Class Examples

```{r}
standardDeviations <- rep(1, 6)
sampleSize <- 300
```

##### Model 1

###### Create the covariance matrix

```{r}
correlationMatrixModel1 <- matrix(.6, nrow = 6, ncol = 6)
diag(correlationMatrixModel1) <- 1
rownames(correlationMatrixModel1) <- colnames(correlationMatrixModel1) <- paste("V", 1:6, sep = "")

covarianceMatrixModel1 <- psych::cor2cov(correlationMatrixModel1, sigma = standardDeviations)
covarianceMatrixModel1
```

###### Specify the model

```{r}
cfaModel1 <- '
 #Factor loadings
 f1 =~ V1 + V2 + V3 + V4 + V5 + V6
'
```

###### Model syntax in table form:

```{r}
lavaanify(cfaModel1)
```

###### Fit the model

```{r}
cfaModel1Fit <- cfa(cfaModel1,
                    sample.cov = covarianceMatrixModel1,
                    sample.nobs = sampleSize,
                    estimator = "ML",
                    std.lv = TRUE)
```

###### Display summary output

```{r}
summary(cfaModel1Fit,
        fit.measures = TRUE,
        standardized = TRUE,
        rsquare = TRUE)
```

###### Estimates of model fit

```{r}
fitMeasures(cfaModel1Fit, fit.measures = c("chisq", "df", "pvalue", "rmsea", "cfi", "tli", "srmr"))
```

###### Residuals

```{r}
residuals(cfaModel1Fit, type = "cor")
```

###### Modification indices

```{r}
modificationindices(cfaModel1Fit)
```

###### Reliability

Reliability of factor scores for the latent factors, as quantified by Cronbach's alpha ($\alpha$) and omega ($\omega$), was estimated using the `semTools` package [@R-semTools].

```{r}
semTools::reliability(cfaModel1Fit)
```

###### Path diagram

Below is a path diagram of the model generated using the `semPlot` package [@R-semPlot].

```{r, out.width = "100%", fig.height = 12, fig.cap = "Confirmatory factor analysis model 1 diagram"}
semPaths(cfaModel1Fit,
         what = "std",
         layout = "tree2",
         optimizeLatRes = FALSE,
         exoVar = FALSE,
         edge.label.cex = 1.5)
```

##### Model 2

###### Create the covariance matrix

```{r}
correlationMatrixModel2 <- matrix(0, nrow = 6, ncol = 6)
diag(correlationMatrixModel2) <- 1
rownames(correlationMatrixModel2) <- colnames(correlationMatrixModel2) <- paste("V", 1:6, sep = "")

covarianceMatrixModel2 <- psych::cor2cov(correlationMatrixModel2, sigma = standardDeviations)
covarianceMatrixModel2
```

###### Specify the model

```{r}
cfaModel2 <- '
 #Factor loadings
 f1 =~ V1 + V2 + V3 + V4 + V5 + V6
'
```

###### Model syntax in table form:

```{r}
lavaanify(cfaModel2)
```

###### Fit the model

```{r}
cfaModel2Fit <- cfa(cfaModel2,
                    sample.cov = covarianceMatrixModel2,
                    sample.nobs = sampleSize,
                    estimator = "ML",
                    std.lv = TRUE)
```

###### Display summary output

```{r}
summary(cfaModel2Fit,
        fit.measures = TRUE,
        standardized = TRUE,
        rsquare = TRUE)
```

###### Estimates of model fit

```{r}
fitMeasures(cfaModel2Fit, fit.measures = c("chisq", "df", "pvalue", "rmsea", "cfi", "tli", "srmr"))
```

###### Residuals

```{r}
residuals(cfaModel2Fit, type = "cor")
```

###### Modification indices

```{r}
modificationindices(cfaModel1Fit)
```

###### Reliability

Reliability of factor scores for the latent factors, as quantified by Cronbach's alpha ($\alpha$) and omega ($\omega$), was estimated using the `semTools` package [@R-semTools].

```{r}
semTools::reliability(cfaModel2Fit)
```

###### Path diagram

Below is a path diagram of the model generated using the `semPlot` package [@R-semPlot].

```{r, out.width = "100%", fig.height = 12, fig.cap = "Confirmatory factor analysis model 2 diagram"}
semPaths(cfaModel2Fit,
         what = "std",
         layout = "tree2",
         optimizeLatRes = FALSE,
         exoVar = FALSE,
         edge.label.cex = 1.5)
```

##### Model 3

###### Create the covariance matrix

```{r}
correlationMatrixModel3 <- matrix(c(1,.6,.6,0,0,0,
                                    .6,1,.6,0,0,0,
                                    0,0,1,0,0,0,
                                    0,0,0,1,.6,.6,
                                    0,0,0,.6,1,.6,
                                    0,0,0,.6,.6,1), nrow = 6, ncol = 6)
rownames(correlationMatrixModel3) <- colnames(correlationMatrixModel3) <- paste("V", 1:6, sep = "")

covarianceMatrixModel3 <- psych::cor2cov(correlationMatrixModel3, sigma = standardDeviations)
covarianceMatrixModel3
```

###### Specify the model

```{r}
cfaModel3A <- '
 #Factor loadings
 f1 =~ V1 + V2 + V3 + V4 + V5 + V6
'

cfaModel3B <- '
 #Factor loadings
 f1 =~ V1 + V2 + V3
 f2 =~ V4 + V5 + V6
'
```

###### Model syntax in table form:

```{r}
lavaanify(cfaModel3A)
lavaanify(cfaModel3B)
```

###### Fit the model

```{r}
cfaModel3AFit <- cfa(cfaModel3A,
                     sample.cov = covarianceMatrixModel3,
                     sample.nobs = sampleSize,
                     estimator = "ML",
                     std.lv = TRUE)

cfaModel3BFit <- cfa(cfaModel3B,
                     sample.cov = covarianceMatrixModel3,
                     sample.nobs = sampleSize,
                     estimator = "ML",
                     std.lv = TRUE)
```

###### Display summary output

```{r}
summary(cfaModel3AFit,
        fit.measures = TRUE,
        standardized = TRUE,
        rsquare = TRUE)

summary(cfaModel3BFit,
        fit.measures = TRUE,
        standardized = TRUE,
        rsquare = TRUE)
```

###### Estimates of model fit

```{r}
fitMeasures(cfaModel3AFit, fit.measures = c("chisq", "df", "pvalue", "rmsea", "cfi", "tli", "srmr"))
fitMeasures(cfaModel3BFit, fit.measures = c("chisq", "df", "pvalue", "rmsea", "cfi", "tli", "srmr"))
```

###### Compare model fit

Test of whether two-factor model fits better than one-factor model. Significant chi-square difference test indicates that two-factor model fits significantly better than one-factor model.

```{r}
anova(cfaModel3BFit, cfaModel3AFit)
```

###### Residuals

```{r}
residuals(cfaModel3AFit, type = "cor")
residuals(cfaModel3BFit, type = "cor")
```

###### Modification indices

```{r}
modificationindices(cfaModel3AFit)
modificationindices(cfaModel3BFit)
```

###### Reliability

Reliability of factor scores for the latent factors, as quantified by Cronbach's alpha ($\alpha$) and omega ($\omega$), was estimated using the `semTools` package [@R-semTools].

```{r}
semTools::reliability(cfaModel3AFit)
semTools::reliability(cfaModel3BFit)
```

###### Path diagram

Below are path diagrams of the models generated using the `semPlot` package [@R-semPlot].

```{r, out.width = "100%", fig.height = 12, fig.cap = "Confirmatory factor analysis model 3A diagram"}
semPaths(cfaModel3AFit,
         what = "std",
         layout = "tree2",
         optimizeLatRes = FALSE,
         exoVar = FALSE,
         edge.label.cex = 1.5)
```

```{r, out.width = "100%", fig.height = 12, fig.cap = "Confirmatory factor analysis model 3B diagram"}
semPaths(cfaModel3BFit,
         what = "std",
         layout = "tree2",
         optimizeLatRes = FALSE,
         exoVar = FALSE,
         edge.label.cex = 1.5)
```

##### Model 4

##### Create the covariance matrix

```{r}
correlationMatrixModel4 <- matrix(c(1,.6,.6,.3,.3,.3,
                                    .6,1,.6,.3,.3,.3,
                                    .3,.3,1,.3,.3,.3,
                                    .3,.3,.3,1,.6,.6,
                                    .3,.3,.3,.6,1,.6,
                                    .3,.3,.3,.6,.6,1), nrow = 6, ncol = 6)
rownames(correlationMatrixModel4) <- colnames(correlationMatrixModel4) <- paste("V", 1:6, sep = "")

covarianceMatrixModel4 <- psych::cor2cov(correlationMatrixModel4, sigma = standardDeviations)
covarianceMatrixModel4
```

###### Specify the model

```{r}
cfaModel4A <- '
 #Factor loadings
 f1 =~ V1 + V2 + V3
 f2 =~ V4 + V5 + V6
'

cfaModel4B <- '
 #Factor loadings
 f1 =~ V1 + V2 + V3
 f2 =~ V4 + V5 + V6
 
 #Regression path
 f2 ~ f1
'

cfaModel4C <- '
 #Factor loadings
 f1 =~ V1 + V2 + V3
 f2 =~ V4 + V5 + V6
 
 #Higher-order factor
 A1 = ~ fixloading*f1 + fixloading*f2
'

cfaModel4D <- '
 #Factor loadings
 f1 =~ V1 + V2 + V3 + V4 + V5 + V6
 
 #Correlated residuals/errors
 V1 ~~ fixcor*V2
 V2 ~~ fixcor*V3
 V1 ~~ fixcor*V3
 V4 ~~ fixcor*V5
 V5 ~~ fixcor*V6
 V4 ~~ fixcor*V6
'
```

###### Model syntax in table form:

```{r}
lavaanify(cfaModel4A)
lavaanify(cfaModel4B)
lavaanify(cfaModel4C)
lavaanify(cfaModel4D)
```

###### Fit the model

```{r}
cfaModel4AFit <- cfa(cfaModel4A,
                     sample.cov = covarianceMatrixModel4,
                     sample.nobs = sampleSize,
                     estimator = "ML",
                     std.lv = TRUE)

cfaModel4BFit <- cfa(cfaModel4B,
                     sample.cov = covarianceMatrixModel4,
                     sample.nobs = sampleSize,
                     estimator = "ML",
                     std.lv = TRUE)

cfaModel4CFit <- cfa(cfaModel4C,
                     sample.cov = covarianceMatrixModel4,
                     sample.nobs = sampleSize,
                     estimator = "ML",
                     std.lv = TRUE)

cfaModel4DFit <- cfa(cfaModel4D,
                     sample.cov = covarianceMatrixModel4,
                     sample.nobs = sampleSize,
                     estimator = "ML",
                     std.lv = TRUE)
```

###### Display summary output

```{r}
summary(cfaModel4AFit,
        fit.measures = TRUE,
        standardized = TRUE,
        rsquare = TRUE)

summary(cfaModel4BFit,
        fit.measures = TRUE,
        standardized = TRUE,
        rsquare = TRUE)

summary(cfaModel4CFit,
        fit.measures = TRUE,
        standardized = TRUE,
        rsquare = TRUE)

summary(cfaModel4DFit,
        fit.measures = TRUE,
        standardized = TRUE,
        rsquare = TRUE)
```

###### Estimates of model fit

```{r}
fitMeasures(cfaModel4AFit, fit.measures = c("chisq", "df", "pvalue", "rmsea", "cfi", "tli", "srmr"))
fitMeasures(cfaModel4BFit, fit.measures = c("chisq", "df", "pvalue", "rmsea", "cfi", "tli", "srmr"))
fitMeasures(cfaModel4CFit, fit.measures = c("chisq", "df", "pvalue", "rmsea", "cfi", "tli", "srmr"))
fitMeasures(cfaModel4DFit, fit.measures = c("chisq", "df", "pvalue", "rmsea", "cfi", "tli", "srmr"))
```

###### Residuals

```{r}
residuals(cfaModel4AFit, type = "cor")
residuals(cfaModel4BFit, type = "cor")
residuals(cfaModel4CFit, type = "cor")
residuals(cfaModel4DFit, type = "cor")
```

###### Modification indices

```{r}
modificationindices(cfaModel4AFit)
modificationindices(cfaModel4BFit)
modificationindices(cfaModel4CFit)
modificationindices(cfaModel4DFit)
```

###### Reliability

Reliability of factor scores for the latent factors, as quantified by Cronbach's alpha ($\alpha$) and omega ($\omega$), was estimated using the `semTools` package [@R-semTools].

```{r}
semTools::reliability(cfaModel4AFit)
semTools::reliability(cfaModel4BFit)
semTools::reliability(cfaModel4CFit)
semTools::reliabilityL2(cfaModel4CFit, "A1")
semTools::reliability(cfaModel4DFit)
```

###### Path diagram

Below are path diagrams of the models generated using the `semPlot` package [@R-semPlot].

```{r, out.width = "100%", fig.height = 12, fig.cap = "Confirmatory factor analysis model 4A diagram"}
semPaths(cfaModel4AFit,
         what = "std",
         layout = "tree2",
         optimizeLatRes = FALSE,
         exoVar = FALSE,
         edge.label.cex = 1.3)
```

```{r, out.width = "100%", fig.height = 12, fig.cap = "Confirmatory factor analysis model 4B diagram"}
semPaths(cfaModel4BFit,
         what = "std",
         layout = "tree2",
         optimizeLatRes = FALSE,
         exoVar = FALSE,
         edge.label.cex = 1.3)
```

```{r, out.width = "100%", fig.height = 12, fig.cap = "Confirmatory factor analysis model 4C diagram"}
semPaths(cfaModel4CFit,
         what = "std",
         layout = "tree2",
         optimizeLatRes = FALSE,
         exoVar = FALSE,
         edge.label.cex = 1.3)
```

```{r, out.width = "100%", fig.height = 12, fig.cap = "Confirmatory factor analysis model 4D diagram"}
semPaths(cfaModel4DFit,
         what = "std",
         layout = "tree2",
         optimizeLatRes = FALSE,
         exoVar = FALSE,
         edge.label.cex = 1.3)
```

###### Equivalently fitting models

Markov equivalent directed acyclic graphs (DAGs) were depicted using the `dagitty` package [@R-dagitty].

```{r, message = FALSE}
dagModel <- lavaanToGraph(cfaModel4AFit)

par(mfrow = c(4, 4))
```

```{r, out.width = "100%", fig.height = 12, fig.cap = "Equivalently fitting models to confirmatory factor analysis model 4"}
lapply(equivalentDAGs(dagModel, n = 16), plot)
```

#### Bifactor Model

##### Specify the model

```{r}
cfaModelBifactor_syntax <- '
  Verbal =~ 
    VO1 + VO2 + VO3 + 
    VW1 + VW2 + VW3 + 
    VM1 + VM2 + VM3
  Spatial =~ 
    SO1 + SO2 + SO3 + 
    SW1 + SW2 + SW3 + 
    SM1 + SM2 + SM3
  Quant =~ 
    QO1 + QO2 + QO3 + 
    QW1 + QW2 + QW3 + 
    QM1 + QM2 + QM3
  g =~ 
    VO1 + VO2 + VO3 + 
    VW1 + VW2 + VW3 + 
    VM1 + VM2 + VM3 +
    SO1 + SO2 + SO3 + 
    SW1 + SW2 + SW3 + 
    SM1 + SM2 + SM3 +
    QO1 + QO2 + QO3 + 
    QW1 + QW2 + QW3 + 
    QM1 + QM2 + QM3
'

cfaModelBifactor_fullSyntax <- '
  #Factor loadings (free the factor loading of the first indicator)
  Verbal =~ 
    NA*VO1 + VO2 + VO3 + 
    VW1 + VW2 + VW3 + 
    VM1 + VM2 + VM3
  Spatial =~ 
    NA*SO1 + SO2 + SO3 + 
    SW1 + SW2 + SW3 + 
    SM1 + SM2 + SM3
  Quant =~ 
    NA*QO1 + QO2 + QO3 + 
    QW1 + QW2 + QW3 + 
    QM1 + QM2 + QM3
  g =~ 
    NA*VO1 + VO2 + VO3 + 
    VW1 + VW2 + VW3 + 
    VM1 + VM2 + VM3 +
    SO1 + SO2 + SO3 + 
    SW1 + SW2 + SW3 + 
    SM1 + SM2 + SM3 +
    QO1 + QO2 + QO3 + 
    QW1 + QW2 + QW3 + 
    QM1 + QM2 + QM3
    
  #Fix latent means to zero
  Verbal ~ 0
  Spatial ~ 0
  Quant ~ 0
  g ~ 0
  
  #Fix latent variances to one
  Verbal ~~ 1*Verbal
  Spatial ~~ 1*Spatial
  Quant ~~ 1*Quant
  g ~~ 1*g
  
  #Fix covariances among latent variables at zero
  Verbal ~~ 0*Spatial
  Verbal ~~ 0*Quant
  Spatial ~~ 0*Quant
  g ~~ 0*Verbal
  g ~~ 0*Spatial
  g ~~ 0*Quant
  
  #Estimate residual variances of manifest variables
  VO1 ~~ VO1
  VO2 ~~ VO2
  VO3 ~~ VO3
  VW1 ~~ VW1
  VW2 ~~ VW2
  VW3 ~~ VW3
  VM1 ~~ VM1
  VM2 ~~ VM2
  VM3 ~~ VM3
  SO1 ~~ SO1
  SO2 ~~ SO2
  SO3 ~~ SO3
  SW1 ~~ SW1
  SW2 ~~ SW2
  SW3 ~~ SW3
  SM1 ~~ SM1
  SM2 ~~ SM2
  SM3 ~~ SM3
  QO1 ~~ QO1
  QO2 ~~ QO2
  QO3 ~~ QO3
  QW1 ~~ QW1
  QW2 ~~ QW2
  QW3 ~~ QW3
  QM1 ~~ QM1
  QM2 ~~ QM2
  QM3 ~~ QM3
  
  #Free intercepts of manifest variables
  VO1 ~ intVO1*1
  VO2 ~ intVO2*1
  VO3 ~ intVO3*1
  VW1 ~ intVW1*1
  VW2 ~ intVW2*1
  VW3 ~ intVW3*1
  VM1 ~ intVM1*1
  VM2 ~ intVM2*1
  VM3 ~ intVM3*1
  SO1 ~ intSO1*1
  SO2 ~ intSO2*1
  SO3 ~ intSO3*1
  SW1 ~ intSW1*1
  SW2 ~ intSW2*1
  SW3 ~ intSW3*1
  SM1 ~ intSM1*1
  SM2 ~ intSM2*1
  SM3 ~ intSM3*1
  QO1 ~ intQO1*1
  QO2 ~ intQO2*1
  QO3 ~ intQO3*1
  QW1 ~ intQW1*1
  QW2 ~ intQW2*1
  QW3 ~ intQW3*1
  QM1 ~ intQM1*1
  QM2 ~ intQM2*1
  QM3 ~ intQM3*1
'
```

##### Model syntax in table form:

```{r}
lavaanify(cfaModelBifactor_syntax)
lavaanify(cfaModelBifactor_fullSyntax)
```

##### Fit the model

```{r}
cfaModelBifactorFit <- cfa(cfaModelBifactor_syntax, 
                        data = MTMM, 
                        orthogonal = TRUE,
                        std.lv = TRUE,
                        missing = "ML",
                        estimator = "MLR")

cfaModelBifactorFit_full <- lavaan(cfaModelBifactor_fullSyntax,
                                   data = MTMM,
                                   missing = "ML",
                                   estimator = "MLR")
```

##### Display summary output

```{r}
summary(cfaModelBifactorFit,
        fit.measures = TRUE,
        standardized = TRUE,
        rsquare = TRUE)

summary(cfaModelBifactorFit_full,
        fit.measures = TRUE,
        standardized = TRUE,
        rsquare = TRUE)
```

##### Estimates of model fit

```{r}
fitMeasures(cfaModelBifactorFit, fit.measures = c("chisq", "df", "pvalue",
                                                  "chisq.scaled", "df.scaled", "pvalue.scaled",
                                                  "chisq.scaling.factor",
                                                  "rmsea", "cfi", "tli", "srmr",
                                                  "rmsea.robust", "cfi.robust", "tli.robust"))
```

##### Residuals

```{r}
residuals(cfaModelBifactorFit, type = "cor")
```

##### Modification indices

```{r}
modificationindices(cfaModelBifactorFit)
```

##### Reliability

Reliability of factor scores for the latent factors, as quantified by Cronbach's alpha ($\alpha$) and omega ($\omega$), was estimated using the `semTools` package [@R-semTools].

```{r}
semTools::reliability(cfaModelBifactorFit)
```

##### Path diagram

Below is a path diagram of the model generated using the `semPlot` package [@R-semPlot].

```{r, out.width = "100%", fig.height = 12, fig.cap = "Bifactor model"}
semPaths(cfaModelBifactorFit,
         what = "std",
         layout = "tree3",
         optimizeLatRes = FALSE,
         bifactor = c("g"),
         exoVar = FALSE,
         edge.label.cex = 1.3)
```

##### Equivalently fitting models

Markov equivalent directed acyclic graphs (DAGs) were depicted using the `dagitty` package [@R-dagitty].

```{r, message = FALSE}
dagModelBifactor <- lavaanToGraph(cfaModelBifactorFit)

par(mfrow = c(2, 2))
```

```{r, out.width = "100%", fig.height = 12, fig.cap = "Equivalently fitting models to a bifactor model"}
lapply(equivalentDAGs(dagModelBifactor, n = 4), plot)
```

#### Multi-Trait Multi-Method Model (MTMM) {#mtmmCFA}

##### Specify the model

```{r}
cfaModelMTMM_syntax <- '
  g =~ Verbal + Spatial + Quant
  Verbal =~ 
    VO1 + VO2 + VO3 + 
    VW1 + VW2 + VW3 + 
    VM1 + VM2 + VM3
  Spatial =~ 
    SO1 + SO2 + SO3 + 
    SW1 + SW2 + SW3 + 
    SM1 + SM2 + SM3
  Quant =~ 
    QO1 + QO2 + QO3 + 
    QW1 + QW2 + QW3 + 
    QM1 + QM2 + QM3
  Oral =~ 
    VO1 + VO2 + VO3 + 
    SO1 + SO2 + SO3 + 
    QO1 + QO2 + QO3
  Written =~ 
    VW1 + VW2 + VW3 + 
    SW1 + SW2 + SW3 + 
    QW1 + QW2 + QW3
  Manipulative =~ 
    VM1 + VM2 + VM3 + 
    SM1 + SM2 + SM3 + 
    QM1 + QM2 + QM3
'

cfaModelMTMM_fullSyntax <- '
  #Factor loadings (free the factor loading of the first indicator)
  g =~ NA*Verbal + Spatial + Quant
  Verbal =~ 
    NA*VO1 + VO2 + VO3 + 
    VW1 + VW2 + VW3 + 
    VM1 + VM2 + VM3
  Spatial =~ 
    NA*SO1 + SO2 + SO3 + 
    SW1 + SW2 + SW3 + 
    SM1 + SM2 + SM3
  Quant =~ 
    NA*QO1 + QO2 + QO3 + 
    QW1 + QW2 + QW3 + 
    QM1 + QM2 + QM3
  Oral =~ 
    NA*VO1 + VO2 + VO3 + 
    SO1 + SO2 + SO3 + 
    QO1 + QO2 + QO3
  Written =~ 
    NA*VW1 + VW2 + VW3 + 
    SW1 + SW2 + SW3 + 
    QW1 + QW2 + QW3
  Manipulative =~ 
    NA*VM1 + VM2 + VM3 + 
    SM1 + SM2 + SM3 + 
    QM1 + QM2 + QM3
    
  #Fix latent means to zero
  Verbal ~ 0
  Spatial ~ 0
  Quant ~ 0
  Oral ~ 0
  Written ~ 0
  Manipulative ~ 0
  g ~ 0
  
  #Fix latent variances to one
  Verbal ~~ 1*Verbal
  Spatial ~~ 1*Spatial
  Quant ~~ 1*Quant
  Oral ~~ 1*Oral
  Written ~~ 1*Written
  Manipulative ~~ 1*Manipulative
  g ~~ 1*g
  
  #Fix covariances among latent variables at zero
  Verbal ~~ 0*Spatial
  Verbal ~~ 0*Quant
  Verbal ~~ 0*Oral
  Verbal ~~ 0*Written
  Verbal ~~ 0*Manipulative
  Spatial ~~ 0*Quant
  Spatial ~~ 0*Oral
  Spatial ~~ 0*Written
  Spatial ~~ 0*Manipulative
  Quant ~~ 0*Oral
  Quant ~~ 0*Written
  Quant ~~ 0*Manipulative
  Oral ~~ 0*Written
  Oral ~~ 0*Manipulative
  Written ~~ 0*Manipulative
  g ~~ 0*Verbal
  g ~~ 0*Spatial
  g ~~ 0*Quant
  g ~~ 0*Oral
  g ~~ 0*Written
  g ~~ 0*Manipulative
  
  #Estimate residual variances of manifest variables
  VO1 ~~ VO1
  VO2 ~~ VO2
  VO3 ~~ VO3
  VW1 ~~ VW1
  VW2 ~~ VW2
  VW3 ~~ VW3
  VM1 ~~ VM1
  VM2 ~~ VM2
  VM3 ~~ VM3
  SO1 ~~ SO1
  SO2 ~~ SO2
  SO3 ~~ SO3
  SW1 ~~ SW1
  SW2 ~~ SW2
  SW3 ~~ SW3
  SM1 ~~ SM1
  SM2 ~~ SM2
  SM3 ~~ SM3
  QO1 ~~ QO1
  QO2 ~~ QO2
  QO3 ~~ QO3
  QW1 ~~ QW1
  QW2 ~~ QW2
  QW3 ~~ QW3
  QM1 ~~ QM1
  QM2 ~~ QM2
  QM3 ~~ QM3
  
  #Free intercepts of manifest variables
  VO1 ~ intVO1*1
  VO2 ~ intVO2*1
  VO3 ~ intVO3*1
  VW1 ~ intVW1*1
  VW2 ~ intVW2*1
  VW3 ~ intVW3*1
  VM1 ~ intVM1*1
  VM2 ~ intVM2*1
  VM3 ~ intVM3*1
  SO1 ~ intSO1*1
  SO2 ~ intSO2*1
  SO3 ~ intSO3*1
  SW1 ~ intSW1*1
  SW2 ~ intSW2*1
  SW3 ~ intSW3*1
  SM1 ~ intSM1*1
  SM2 ~ intSM2*1
  SM3 ~ intSM3*1
  QO1 ~ intQO1*1
  QO2 ~ intQO2*1
  QO3 ~ intQO3*1
  QW1 ~ intQW1*1
  QW2 ~ intQW2*1
  QW3 ~ intQW3*1
  QM1 ~ intQM1*1
  QM2 ~ intQM2*1
  QM3 ~ intQM3*1
'
```

##### Model syntax in table form:

```{r}
lavaanify(cfaModelMTMM_syntax)
lavaanify(cfaModelMTMM_fullSyntax)
```

##### Fit the model

```{r}
cfaModelMTMMFit <- cfa(cfaModelMTMM_syntax, 
                        data = MTMM, 
                        orthogonal = TRUE,
                        std.lv = TRUE,
                        missing = "ML",
                        estimator = "MLR")

cfaModelMTMMFit_full <- lavaan(cfaModelMTMM_fullSyntax,
                               data = MTMM,
                               missing = "ML",
                               estimator = "MLR")
```

##### Display summary output

```{r}
summary(cfaModelMTMMFit,
        fit.measures = TRUE,
        standardized = TRUE,
        rsquare = TRUE)

summary(cfaModelMTMMFit_full,
        fit.measures = TRUE,
        standardized = TRUE,
        rsquare = TRUE)
```

##### Estimates of model fit

```{r}
fitMeasures(cfaModelMTMMFit, fit.measures = c("chisq", "df", "pvalue",
                                              "chisq.scaled", "df.scaled", "pvalue.scaled",
                                              "chisq.scaling.factor",
                                              "rmsea", "cfi", "tli", "srmr",
                                              "rmsea.robust", "cfi.robust", "tli.robust"))
```

##### Residuals

```{r}
residuals(cfaModelMTMMFit, type = "cor")
```

##### Modification indices

```{r}
modificationindices(cfaModelMTMMFit)
```

##### Reliability

Reliability of factor scores for the latent factors, as quantified by Cronbach's alpha ($\alpha$) and omega ($\omega$), was estimated using the `semTools` package [@R-semTools].

```{r}
semTools::reliability(cfaModelMTMMFit)
semTools::reliabilityL2(cfaModelMTMMFit, "g")
```

##### Path diagram

Below is a path diagram of the model generated using the `semPlot` package [@R-semPlot].

```{r, out.width = "100%", fig.height = 12, fig.cap = "Multi-trait multi-method model in confirmatory factor analysis"}
semPaths(cfaModelMTMMFit,
         what = "std",
         layout = "tree3",
         optimizeLatRes = FALSE,
         bifactor = c("Verbal","Spatial","Quant","g"),
         exoVar = FALSE,
         edge.label.cex = 1.3)
```

##### Equivalently fitting models

Markov equivalent directed acyclic graphs (DAGs) were depicted using the `dagitty` package [@R-dagitty].

```{r, message = FALSE}
dagModelMTMM <- lavaanToGraph(cfaModelMTMMFit)

par(mfrow = c(2, 2))
```

```{r, out.width = "100%", fig.height = 12, fig.cap = "Equivalently fitting models to a multi-trait multi-method model in confirmatory factor analysis"}
lapply(equivalentDAGs(dagModelMTMM, n = 4), plot)
```

## Principal Component Analysis (PCA) {#pcaAnalayis}

Principal component analysis (PCA) is a technique for data reduction. PCA seeks to identify the principal components of the data, so that data can be reduced to that smaller set of components. Although PCA is sometimes referred to as factor analysis, PCA is not factor analysis [@Lilienfeld2015]. PCA uses the total variance of all variables whereas factor analysis uses the common variance among the variables.

### Determine number of components

Determine the number of components to retain using the Scree plot and Very Simple Structure (VSS) plot.

#### Scree Plot

Scree plots were generated using the **psych** [@R-psych] and **nFactors** [@R-nFactors] packages.

```{r, out.width = "100%", fig.cap = "Scree plots in principal component analysis"}
fa.parallel(x = HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], fm = "ml", fa = "pc")
plot(nScree(x = cor(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], use = "pairwise.complete.obs"), model = "components"))
```

#### Very Simple Structure (VSS) Plot

Very simple structure (VSS) plots were generated using the `psych` package [@R-psych].

##### Orthogonal (Varimax) rotation

```{r, out.width = "100%", fig.cap = "Very simple structure plot in principal component analysis"}
vss(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], rotate = "varimax", fm = "pc")
```

##### Oblique (Oblimin) rotation

```{r, out.width = "100%", fig.cap = "Very simple structure plot in principal component analysis"}
vss(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], rotate = "oblimin", fm = "pc")
```

##### No rotation

```{r, out.width = "100%", fig.cap = "Very simple structure plot in principal component analysis"}
vss(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], rotate = "none", fm = "pc")
```

### Run the Principal Component Analysis

Principal component analysis (PCA) models were fit using the `psych` package [@R-psych].

#### Orthogonal (Varimax) rotation

```{r}
pca1factorOrthogonal <- principal(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 1, rotate = "varimax")
pca2factorOrthogonal <- principal(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 2, rotate = "varimax")
pca3factorOrthogonal <- principal(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 3, rotate = "varimax")
pca4factorOrthogonal <- principal(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 4, rotate = "varimax")
pca5factorOrthogonal <- principal(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 5, rotate = "varimax")
pca6factorOrthogonal <- principal(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 6, rotate = "varimax")
pca7factorOrthogonal <- principal(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 7, rotate = "varimax")
pca8factorOrthogonal <- principal(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 8, rotate = "varimax")
pca9factorOrthogonal <- principal(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 9, rotate = "varimax")
```

#### Oblique (Oblimin) rotation

```{r}
pca1factorOblique <- principal(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 1, rotate = "oblimin")
pca2factorOblique <- principal(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 2, rotate = "oblimin")
pca3factorOblique <- principal(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 3, rotate = "oblimin")
pca4factorOblique <- principal(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 4, rotate = "oblimin")
pca5factorOblique <- principal(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 5, rotate = "oblimin")
pca6factorOblique <- principal(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 6, rotate = "oblimin")
pca7factorOblique <- principal(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 7, rotate = "oblimin")
pca8factorOblique <- principal(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 8, rotate = "oblimin")
pca9factorOblique <- principal(HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")], nfactors = 9, rotate = "oblimin")
```

### Component scores

#### Orthogonal (Varimax) rotation

```{r}
pca3Orthogonal <- pca3factorOrthogonal$scores
```

#### Oblique (Oblimin) rotation

```{r}
pca3Oblique <- pca3factorOblique$scores
```

### Plots

Pairs panel plots were generated using the `psych` package [@R-psych]. Correlation plots were generated using the `corrplot` package [@R-corrplot].

#### Orthogonal (Varimax) rotation

```{r, out.width = "100%", fig.cap = "Pairs panel plot using orthogonal rotation in principal component analysis"}
pairs.panels(pca3Orthogonal)
```

```{r, out.width = "100%", fig.cap = "Correlation plot using orthogonal rotation in principal component analysis"}
corrplot(cor(pca3Orthogonal, use = "pairwise.complete.obs")) 
```

#### Oblique (Oblimin) rotation

```{r, out.width = "100%", fig.cap = "Pairs panel plot using oblique rotation in principal component analysis"}
pairs.panels(pca3Oblique)
```

```{r, out.width = "100%", fig.cap = "Correlation plot using oblique rotation in principal component analysis"}
corrplot(cor(pca3Oblique, use = "pairwise.complete.obs"))
```

## Conclusion

## Exercises

```{r, include = FALSE}
library("MOTE")
```

```{r, include = FALSE}
# Load data

cnlsy <- read_csv(here("Data", "cnlsy.csv")) #read_csv(file.path(path, "/GitHub/AssessmentCourse/Data/cnlsy.csv"))

# Prepare Data

# Descriptive Statistics and Correlations

## Descriptive Statistics
describe(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")])

## Correlations
pairs.panels(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], stars = TRUE)
cor(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], use = "pairwise.complete.obs")
corrplot(cor(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], use = "pairwise.complete.obs"))

# Exploratory Factor Analysis (EFA)

## Determine number of factors

### Scree Plot
fa.parallel(x = cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], fm = "ml", fa = "fa")
plot(nScree(x = cor(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], use = "pairwise.complete.obs"), model = "factors"))
plot(nScree(x = na.omit(as.data.frame(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")])), model = "factors"))

### Very Simple Structure (VSS) Plot

#### Orthogonal (Varimax) rotation
vss(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], rotate = "varimax", fm = "ml")
nfactors(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], rotate = "varimax", fm = "ml")


#### Oblique (Oblimin) rotation
vss(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], rotate = "oblimin", fm = "ml")
nfactors(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], rotate = "oblimin", fm = "ml")


#### No rotation
vss(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], rotate = "none", fm = "ml")
nfactors(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], rotate = "none", fm = "ml")


## Run factor analysis

### Orthogonal (Varimax) rotation

efa1factorOrthogonal_ex <- fa(r = cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], nfactors = 1, rotate = "varimax", fm = "ml")
efa2factorOrthogonal_ex <- fa(r = cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], nfactors = 2, rotate = "varimax", fm = "ml")

### Oblique (Oblimin) rotation

efa1factorOblique_ex <- fa(r = cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], nfactors = 1, rotate = "oblimin", fm = "ml")
efa2factorOblique_ex <- fa(r = cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], nfactors = 2, rotate = "oblimin", fm = "ml")

## Factor scores

### Orthogonal (Varimax) rotation
fa2Orthogonal_ex <- efa2factorOrthogonal_ex$scores
as_tibble(fa2Orthogonal_ex)

### Oblique (Oblimin) rotation
fa2Oblique_ex <- efa2factorOblique_ex$scores
as_tibble(fa2Oblique_ex)

## Plots

### Orthogonal (Varimax) rotation
factor.plot(efa2factorOrthogonal_ex, cut = 0.5)
fa.diagram(efa2factorOrthogonal_ex, digits = 2)
pairs.panels(fa2Orthogonal_ex)
corrplot(cor(fa2Orthogonal_ex, use = "pairwise.complete.obs")) 

### Oblique (Oblimin) rotation
factor.plot(efa2factorOblique_ex, cut = 0.5)
fa.diagram(efa2factorOblique_ex, digits = 2)
pairs.panels(fa2Oblique_ex)
corrplot(cor(fa2Oblique_ex, use = "pairwise.complete.obs"))


# CFA ---------------------------------------------------------------------

## Specify the model
measurementModelCorrelatedErrorsT1 <- '
 #Factor loadings
 antisocialT1 =~ bpi_antisocialT1_1 + bpi_antisocialT1_2 + bpi_antisocialT1_3 + bpi_antisocialT1_4 + bpi_antisocialT1_5 + bpi_antisocialT1_6 + bpi_antisocialT1_7
 
 #Correlated errors
 bpi_antisocialT1_5 ~~ bpi_antisocialT1_6
'

## Fit the model
measurementModelCorrelatedErrors1Fit <- cfa(measurementModelCorrelatedErrorsT1,
                                            data = cnlsy,
                                            missing = "ML",
                                            estimator = "MLR",
                                            std.lv = TRUE)

## Display summary output
summary(measurementModelCorrelatedErrors1Fit,
        fit.measures = TRUE,
        standardized = TRUE,
        rsquare = TRUE)

## Modification indices
modificationindices(measurementModelCorrelatedErrors1Fit)

## Factor scores
as_tibble(predict(measurementModelCorrelatedErrors1Fit))

## Equivalently fitting models (DAGs)
dagModel <- lavaanToGraph(measurementModelCorrelatedErrors1Fit)

par(mfrow = c(3, 3))
lapply(equivalentDAGs(dagModel, n = 9), plot)

# Principal Component Analysis (PCA)

## Determine number of components

### Scree Plot

fa.parallel(x = cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], fm = "ml", fa = "pc")
plot(nScree(x = cor(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], use = "pairwise.complete.obs"), model = "components"))

plot(nScree(x = na.omit(as.data.frame(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")])), model = "components"))

### Very Simple Structure (VSS) Plot

#### Orthogonal (Varimax) rotation

vss(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], rotate = "varimax", fm = "pc")

#### Oblique (Oblimin) rotation

vss(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], rotate = "oblimin", fm = "pc")

#### No rotation

vss(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], rotate = "none", fm = "pc")

## Run the Principal Component Analysis

### Orthogonal (Varimax) rotation

pca1factorOrthogonal_ex <- principal(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], nfactors = 1, rotate = "varimax")
pca2factorOrthogonal_ex <- principal(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], nfactors = 2, rotate = "varimax")
pca3factorOrthogonal_ex <- principal(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], nfactors = 3, rotate = "varimax")
pca4factorOrthogonal_ex <- principal(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], nfactors = 4, rotate = "varimax")
pca5factorOrthogonal_ex <- principal(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], nfactors = 5, rotate = "varimax")
pca6factorOrthogonal_ex <- principal(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], nfactors = 6, rotate = "varimax")
pca7factorOrthogonal_ex <- principal(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], nfactors = 7, rotate = "varimax")

### Oblique (Oblimin) rotation

pca1factorOblique_ex <- principal(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], nfactors = 1, rotate = "oblimin")
pca2factorOblique_ex <- principal(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], nfactors = 2, rotate = "oblimin")
pca3factorOblique_ex <- principal(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], nfactors = 3, rotate = "oblimin")
pca4factorOblique_ex <- principal(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], nfactors = 4, rotate = "oblimin")
pca5factorOblique_ex <- principal(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], nfactors = 5, rotate = "oblimin")
pca6factorOblique_ex <- principal(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], nfactors = 6, rotate = "oblimin")
pca7factorOblique_ex <- principal(cnlsy[,c("bpi_antisocialT1_1","bpi_antisocialT1_2","bpi_antisocialT1_3","bpi_antisocialT1_4","bpi_antisocialT1_5","bpi_antisocialT1_6","bpi_antisocialT1_7")], nfactors = 7, rotate = "oblimin")

## Component scores

### Orthogonal (Varimax) rotation

pca3Orthogonal_ex <- pca3factorOrthogonal_ex$scores
as_tibble(pca3Orthogonal_ex)

### Oblique (Oblimin) rotation

pca3Oblique_ex <- pca3factorOblique_ex$scores
as_tibble(pca3Oblique_ex)

## Plots

### Orthogonal (Varimax) rotation

pairs.panels(pca3Orthogonal_ex)
corrplot(cor(pca3Orthogonal_ex, use = "pairwise.complete.obs")) 

### Oblique (Oblimin) rotation

pairs.panels(pca3Oblique_ex)
corrplot(cor(pca3Oblique_ex, use = "pairwise.complete.obs"))

# Factor scores -----------------------------------------------------------

efa1factorOblique_ex$scores

as_tibble(predict(measurementModelCorrelatedErrors1Fit))

pca1factorOblique_ex$scores

cor(cbind(efa1factorOblique_ex$scores, as_tibble(predict(measurementModelCorrelatedErrors1Fit)), pca1factorOblique_ex$scores), use = "pairwise.complete.obs")
```

### Questions

Note: several of the following questions use data from the Children of the National Longitudinal Survey of Youth Survey (CNLSY). The CNLSY is a publicly available longitudinal dataset provided by the Bureau of Labor Statistics (https://www.bls.gov/nls/nlsy79-children.htm#topical-guide). The CNLSY data file for these exercises is located on the course’s page of the Open Science Framework (https://osf.io/3pwza). Children’s behavior problems were rated in 1988 (time 1: T1) and then again in 1990 (time 2: T2) on the Behavior Problems Index (BPI). Below are the items corresponding to the Antisocial subscale of the BPI:
1) cheats or tells lies
2) bullies or is cruel/mean to others
3) does not seem to feel sorry after misbehaving
4) breaks things deliberately
5) is disobedient at school
6) has trouble getting along with teachers
7) has sudden changes in mood or feeling

1. Fit an exploratory factor analysis (EFA) model using maximum likelihood to the seven items of the Antisocial subscale of the Behavior Problems Index at T1.
    a. Create a scree plot using the raw data (not the correlation or covariance matrix).
    b. How many factors should be retained according to the Kaiser–Guttman rule? How many factors should be retained according to a Parallel Test? Based on the available information, how many factors would you retain?
    c. Extract two factors with an oblique (oblimin) rotation. Which items load most strongly on Factor 1? Which items load most strongly on Factor 2? Why do you think these items load onto Factor 2? Which item has the highest cross-loading (on both Factors 1 and 2)?
    d. What is the association between Factors 1 and 2 when using an oblique rotation? What does this indicate about whether you should use an orthogonal or oblique rotation?
2. [For exercises about confirmatory factor analysis, see the exercises on “Structural Equation Modeling”.] -- **INCLUDE CROSS-REFERENCE**
3. Fit a principal component analysis (PCA) model to the seven items of the Antisocial subscale of the Behavior Problems Index at T1. How many components should be kept?
4. Fit a one-factor confirmatory factor analysis (CFA) model to the seven items of the Antisocial subscale of the Behavior Problems Index at T1. Set the scale of the latent factor by standardizing the latent factor—set the mean of the factor to one and the variance of the factor to zero. Freely estimate the factor loadings of all indicators. Allow the residuals of indicators 5 and 6 to be correlated. Use full information maximum likelihood (FIML) to account for missing data. Use robust standard errors to account for non-normally distributed data.
    a. How strongly are the factor scores from the one-factor CFA model associated with the factors from a one-factor EFA model and a one-component PCA model?
5. If your model fits well, why is it important to consider alternative models? What are some possible alternative models?

### Answers

1.	
    a. [**INSERT FIGURE**]
    b. The Kaiser-Guttman rule states that all factors should be retained whose eigenvalues are greater than one. According to the Kaiser-Guttman rule, one factor should be retained in this case. According to the Parallel Test, factors should be retained that explain more variance than randomly generated data. According to the Parallel Test, two factors should be retained. However, it is not conceptually elegant to include factors that explain less than one variable’s worth of variance (i.e., one eigenvalue), so in this case, it would probably make sense to retain one factor.
    c. Items 1 (loading = .52), 2 (loading = .66), 3 (loading = .48), 4 (loading = .52), and 7 (loading = .39) load most strongly on Factor 1. Items 5 (“disobedient at school”; loading = 1.00) and 6 (“trouble getting along with teachers”; loading = .42) load most strongly on Factor 2. It is likely that these two items load onto a separate factor from the other antisocial items because they reflect school-related antisocial behavior. Item 6 has the highest cross-loading—in addition to loading onto Factor 2 (.42), it has a non-trivial loading on Factor 1 (.22).
    d. The association between Factors 1 and 2 is .48. This indicates that the factors are moderately correlated and that using an orthogonal rotation would likely be improper. An oblique rotation would be important to capture the association between the two factors. However, this could come at the expense of having simple structure and, therefore, straightforward interpretation of the factors.
2. [See the exercises on “Structural Equation Modeling”.] [**INSERT CROSS-REFERENCE**]
3. According to the Kaiser-Guttman rule and the Parallel Test, one component should be retained.
4. The factor scores from the CFA model are correlated r = .97 and r = .98 with the factor scores of the EFA model and the component scores of the PCA model, respectively. The factor scores of the EFA model are correlated r = .997 with the component scores of the PCA model.
5. Even if a given model fits well, there are many other models that fit just as well. Any given data matrix can produce an infinite number of factor models that accurately represent the data structure (indeterminacy). For any model, there always exist an infinite number of models that fit exactly the same. There is no way to distinguish which factor model is correct from the data matrix. Although the fit of the structural model may be consistent with the hypothesized model, fit does not demonstrate that a given model is uniquely valid. Thus, it is important to consider alternative models that may better capture the causal processes, even if their fit is mathematically equivalent. For instance, you could consider models with multiple factors, correlated factors, correlated residuals, cross-loading indicators, regression paths, hierarchical (higher-order) factors, bifactors, etc. Remember, an important part of science is specifying and testing plausible alternative explanations!
Below are some alternative, equivalently fitting models:

[**INSERT FIGURE**]
