# Item Response Theory {#irt}

In the chapter on [reliability](#reliability), we introduced [classical test theory](#ctt) (CTT).\index{reliability}\index{classical test theory}
[Classical test theory](#ctt) is a measurement theory of how test scores relate to a construct.\index{classical test theory}
[Classical test theory](#ctt) provides a way to estimate the relation between the measure (or item) and the construct.\index{classical test theory}
For instance, with a [CTT](#ctt) approach, to estimate the relation between an item and the construct, you would compute an [item–total correlation](#itemTotalCorrelation-reliability).\index{classical test theory}\index{reliability!internal consistency!average item–total correlation}
An [item–total correlation](#itemTotalCorrelation-reliability) is the correlation of an item with the total score on the measure (e.g., sum score).\index{reliability!internal consistency!average item–total correlation}
The [item–total correlation](#itemTotalCorrelation-reliability) approximates the relation between an item and the construct.\index{reliability!internal consistency!average item–total correlation}
However, the [item–total correlation](#itemTotalCorrelation-reliability) is a crude estimate of the relation between an item and the construct.\index{reliability!internal consistency!average item–total correlation}
And there are many other ways to characterize the relation between an item and a construct.
One such way is with item response theory (IRT).\index{item response theory}

## Overview {#overview-irt}

Unlike [CTT](#ctt), which is a measurement theory of how test scores relate to a construct, IRT is a measurement theory that describes how an *item* is related to a construct.\index{classical test theory}\index{item response theory}
For instance, given a particular person's level on the construct, what is their chance of answering "TRUE" on a particular item?\index{item response theory}

IRT is an approach to [latent variable modeling](#latentVariableModeling).\index{item response theory}\index{latent variable}
In IRT, we estimate a person's construct score (i.e., level on the construct) based on their item responses.
The construct is estimated as a latent factor that represents the common variance among all items as in [structural equation modeling](#sem) or [confirmatory factor analysis](#cfa).\index{item response theory}
The person's level on the construct is called theta ($\theta$).\index{item response theory}\index{item response theory!theta}
When dealing with performance-based tests, theta is sometimes called "ability."\index{item response theory}\index{item response theory!theta}

### Item Characteristic Curve {#icc}

In IRT, we can plot an *item characteristic curve* (ICC).\index{item response theory}\index{item response theory!item characteristic curve}
The ICC is a plot of the model-derived probability of a symptom being present (or a correct response) as a function of a person's standing on a latent continuum.\index{item response theory!item characteristic curve}
For instance, we can create empirical ICCs that can take any shape (see Figure \@ref(fig:empiricalICC)).\index{item response theory!item characteristic curve}

```{r, include = FALSE}
empiricalICCdata <- data.frame(
  item1 =  c(.40, .63, .73, .85, .93, .95, .97, .99, .99),
  item2 =  c(.20, .53, .56, .76, .90, .94, .95, .98, .99),
  item3 =  c(.10, .28, .39, .58, .75, .82, .88, .94, .99),
  item4 =  c(.05, .24, .30, .50, .68, .73, .84, .92, .99),
  item5 =  c(.03, .19, .27, .38, .49, .53, .79, .97, .99),
  item6 =  c(.02, .15, .25, .43, .57, .65, .75, .85, .95),
  item7 =  c(.01, .07, .15, .29, .42, .59, .70, .82, .90),
  item8 =  c(.01, .05, .10, .26, .35, .46, .60, .80, .85),
  item9 =  c(.01, .02, .05, .16, .23, .35, .46, .55, .70),
  item10 = c(.01, .02, .03, .05, .06, .07, .10, .14, .20),
  itemSum = 1:9
)
```

```{r empiricalICC, echo = FALSE, results = "hide", out.width = "100%", fig.align = "center", fig.height = 8, fig.cap = "Empirical Item Characteristic Curves of the Probability of Endorsement of a Given Item as a Function of the Person's Sum Score."}
plot(empiricalICCdata$itemSum, empiricalICCdata$item10, type = "n", xlim = c(1,9), ylim = c(0,1), xlab = "Person's Sum Score", ylab = "Probability of Item Endorsement", xaxt = "n")
lines(empiricalICCdata$itemSum, empiricalICCdata$item1, type = "b", pch = "1")
lines(empiricalICCdata$itemSum, empiricalICCdata$item2, type = "b", pch = "2")
lines(empiricalICCdata$itemSum, empiricalICCdata$item3, type = "b", pch = "3")
lines(empiricalICCdata$itemSum, empiricalICCdata$item4, type = "b", pch = "4")
lines(empiricalICCdata$itemSum, empiricalICCdata$item5, type = "b", pch = "5")
lines(empiricalICCdata$itemSum, empiricalICCdata$item6, type = "b", pch = "6")
lines(empiricalICCdata$itemSum, empiricalICCdata$item7, type = "b", pch = "7")
lines(empiricalICCdata$itemSum, empiricalICCdata$item8, type = "b", pch = "8")
lines(empiricalICCdata$itemSum, empiricalICCdata$item9, type = "b", pch = "9")
lines(empiricalICCdata$itemSum, empiricalICCdata$item10, type = "l")
points(empiricalICCdata$itemSum, empiricalICCdata$item10, type = "p", pch = 19, col = "white", cex = 3)
text(empiricalICCdata$itemSum, empiricalICCdata$item10, labels = "10")
axis(1, at = 1:9, labels = 1:9)
```

In a model-implied ICC, we fit a logistic (sigmoid) curve to each item's probability of a symptom being present as a function of a person's level on the latent construct.\index{item response theory!item characteristic curve}
The model-implied ICCs for the same 10 items from Figure \@ref(fig:empiricalICC) are depicted in Figure \@ref(fig:modelImpliedICC).\index{item response theory!item characteristic curve}

```{r, include = FALSE}
#https://www.statforbiology.com/nonlinearregression/usefulequations#logistic_curve; archived at https://perma.cc/8WFX-FCEQ
#https://github.com/OnofriAndreaPG/aomisc/blob/1eb698b3bc5f55a718c37bfd2028b9ac73a6fbbe/R/SSL.R; archived at https://perma.cc/94CM-PSGY

library("viridis")

#Log-Logistic Function nlsL.2 (similar to L.3 from drc package)
L2.fun <- function(predictor, a, b) {
  x <- predictor
  1/(1 + exp( - a* (x - b)))
}

L2.Init <- function(mCall, LHS, data, ...) {
  xy <- sortedXyData(mCall[["predictor"]], LHS, data)
  x <- xy[, "x"]; y <- xy[, "y"]
  d <- 1
  ## Linear regression on pseudo y values
  pseudoY <- log((d - y)/(y+0.00001))
  coefs <- coef( lm(pseudoY ~ x))
  k <- coefs[1]; a <- - coefs[2]
  b <- k/a
  value <- c(a, b)
  names(value) <- mCall[c("a", "b")]
  value
}

NLS.L2 <- selfStart(L2.fun, L2.Init, parameters = c("a", "b"))

twoPLitem1 <- nls(item1 ~ NLS.L2(itemSum, a, b), data = empiricalICCdata, control = list(warnOnly = TRUE))
twoPLitem2 <- nls(item2 ~ NLS.L2(itemSum, a, b), data = empiricalICCdata, control = list(warnOnly = TRUE))
twoPLitem3 <- nls(item3 ~ NLS.L2(itemSum, a, b), data = empiricalICCdata, control = list(warnOnly = TRUE))
twoPLitem4 <- nls(item4 ~ NLS.L2(itemSum, a, b), data = empiricalICCdata, control = list(warnOnly = TRUE))
twoPLitem5 <- nls(item5 ~ NLS.L2(itemSum, a, b), data = empiricalICCdata, control = list(warnOnly = TRUE))
twoPLitem6 <- nls(item6 ~ NLS.L2(itemSum, a, b), data = empiricalICCdata, control = list(warnOnly = TRUE))
twoPLitem7 <- nls(item7 ~ NLS.L2(itemSum, a, b), data = empiricalICCdata, control = list(warnOnly = TRUE))
twoPLitem8 <- nls(item8 ~ NLS.L2(itemSum, a, b), data = empiricalICCdata, control = list(warnOnly = TRUE))
twoPLitem9 <- nls(item9 ~ NLS.L2(itemSum, a, b), data = empiricalICCdata, control = list(warnOnly = TRUE))
twoPLitem10 <- nls(item10 ~ NLS.L2(itemSum, a, b), data = empiricalICCdata, control = list(warnOnly = TRUE))

newdata <- data.frame(itemSum = seq(from = 1, to = 9, length.out = 1000))
newdata$item1 <- predict(twoPLitem1, newdata = newdata)
newdata$item2 <- predict(twoPLitem2, newdata = newdata)
newdata$item3 <- predict(twoPLitem3, newdata = newdata)
newdata$item4 <- predict(twoPLitem4, newdata = newdata)
newdata$item5 <- predict(twoPLitem5, newdata = newdata)
newdata$item6 <- predict(twoPLitem6, newdata = newdata)
newdata$item7 <- predict(twoPLitem7, newdata = newdata)
newdata$item8 <- predict(twoPLitem8, newdata = newdata)
newdata$item9 <- predict(twoPLitem9, newdata = newdata)
newdata$item10 <- predict(twoPLitem10, newdata = newdata)
newdata$itemTotal <- rowSums(newdata[,paste("item", 1:10, sep = "")])
```

```{r modelImpliedICC, echo = FALSE, results = "hide", out.width = "100%", fig.align = "center", fig.cap = "Item Characteristic Curves of the Probability of Endorsement of a Given Item as a Function of the Person's Level on the Latent Construct."}
plot(newdata$itemSum, newdata$item1, type = "n", ylim = c(0,1), xlab = expression(paste("Person's Level on the Latent Construct (", theta, ")", sep = "")), ylab = "Probability of Item Endorsement", xaxt = "n")
lines(newdata$itemSum, newdata$item1, type = "l", lwd = 2, col = gray.colors(10)[1])
lines(newdata$itemSum, newdata$item2, type = "l", lwd = 2, col = gray.colors(10)[2])
lines(newdata$itemSum, newdata$item3, type = "l", lwd = 2, col = gray.colors(10)[3])
lines(newdata$itemSum, newdata$item4, type = "l", lwd = 2, col = gray.colors(10)[4])
lines(newdata$itemSum, newdata$item5, type = "l", lwd = 2, col = gray.colors(10)[5])
lines(newdata$itemSum, newdata$item6, type = "l", lwd = 2, col = gray.colors(10)[6])
lines(newdata$itemSum, newdata$item7, type = "l", lwd = 2, col = gray.colors(10)[7])
lines(newdata$itemSum, newdata$item8, type = "l", lwd = 2, col = gray.colors(10)[8])
lines(newdata$itemSum, newdata$item9, type = "l", lwd = 2, col = gray.colors(10)[9])
lines(newdata$itemSum, newdata$item10, type = "l", lwd = 2, col = gray.colors(10)[10])
axis(1, at = seq(from = 1, to = 9, length.out = 9), labels = c(-4:4))
legend("topleft", legend = paste("item", 1:10, sep = " "), col = gray.colors(10), lwd = 2, cex = 0.6)
```

ICCs can be summed across items to get the test characteristic curve (TCC):\index{item response theory!item characteristic curve}\index{item response theory!test characteristic curve}

```{r modelImpliedTCC, echo = FALSE, results = "hide", out.width = "100%", fig.align = "center", fig.cap = "Test Characteristic Curve of the Expected Total Score on the Test as a Function of the Person's Level on the Latent Construct."}
plot(newdata$itemSum, newdata$itemTotal, type = "n", ylim = c(0,10), xlab = expression(paste("Person's Level on the Latent Construct (", theta, ")", sep = "")), ylab = "Expected Total Score on Test", xaxt = "n")
lines(newdata$itemSum, newdata$itemTotal, type = "l", lwd = 2)
axis(1, at = seq(from = 1, to = 9, length.out = 9), labels = c(-4:4))
```

An ICC provides more information than an [item–total correlation](#itemTotalCorrelation-reliability).\index{item response theory!item characteristic curve}\index{reliability!internal consistency!average item–total correlation}
Visually, we can see the utility of various items by looking at the items' ICC plots.\index{item response theory!item characteristic curve}
For instance, consider what might be a useless item for diagnostic purposes.\index{item response theory!item characteristic curve}
For a particular item, among those with a low total score (level on the construct), 90% respond with "TRUE" to the item, whereas among everyone else, 100% respond with "TRUE" (see Figure \@ref(fig:iccCeilingEffect)).\index{item response theory!item characteristic curve}
This item has a ceiling effect and provides only a little information about who would be considered above clinical threshold for a disorder.\index{item response theory!item characteristic curve}\index{ceiling effect}
So, the item is not very clinically useful.\index{item response theory!item characteristic curve}

```{r, include = FALSE}
library("drc")

empiricalICCdata$ceilingEffect <- c(.90, .95, .98, 1, 1, 1, 1, 1, 1)
empiricalICCdata$diagnosticallyUseful <- c(0, 0, 0, 0, 0, 0, .69, .70, .70)

threePL_ceilingeffect <- drm(ceilingEffect ~ itemSum, data = empiricalICCdata, fct = LL.3u(), type = "continuous") #fix upper asymptote at 1
threePL_diagnosticallyUseful <- drm(diagnosticallyUseful ~ itemSum, data = empiricalICCdata, fct = LL.3(), type = "continuous") #fix lower asymptote at 0

newdata$ceilingEffect <- predict(threePL_ceilingeffect, newdata = newdata)
newdata$diagnosticallyUseful <- predict(threePL_diagnosticallyUseful, newdata = newdata)
```

```{r iccCeilingEffect, echo = FALSE, results = "hide", out.width = "100%", fig.align = "center", fig.cap = "Item Characteristic Curve of an Item with a Ceiling Effect That Is Not Diagnostically Useful."}
plot(newdata$itemSum, newdata$ceilingEffect, type = "l", lwd = 2, ylim = c(0,1), xlab = expression(paste("Person's Level on the Latent Construct (", theta, ")", sep = "")), ylab = "Probability of Item Endorsement", xaxt = "n")
axis(1, at = seq(from = 1, to = 9, length.out = 9), labels = c(-4:4))
```

Now, consider a different item.\index{item response theory!item characteristic curve}
For those with a low level on the construct, 0% respond "TRUE", so it has a floor effect and tells us nothing about the lower end of the construct.\index{item response theory!item characteristic curve}
But for those with a higher level on the construct, 70% respond true (see Figure \@ref(fig:iccDiagnosticallyUseful)).\index{item response theory!item characteristic curve}
So, the item tells us something about the higher end of the distribution, and could be diagnostically useful.\index{item response theory!item characteristic curve}
Thus, an ICC allows us to immediately tell the utility of items.\index{item response theory!item characteristic curve}

```{r iccDiagnosticallyUseful, echo = FALSE, results = "hide", out.width = "100%", fig.align = "center", fig.cap = "Item Characteristic Curve of an Item with a Floor Effect That Is Diagnostically Useful."}
plot(newdata$itemSum, newdata$diagnosticallyUseful, type = "l", lwd = 2, ylim = c(0,1), xlab = expression(paste("Person's Level on the Latent Construct (", theta, ")", sep = "")), ylab = "Probability of Item Endorsement", xaxt = "n")
axis(1, at = seq(from = 1, to = 9, length.out = 9), labels = c(-4:4))
```

### Parameters {#parameters-irt}

We can estimate up to four parameters in an IRT model and can glean up to four key pieces of information from an item's ICC:\index{item response theory}\index{item response theory!item characteristic curve}

1. Difficulty (severity)\index{item response theory!item difficulty}
1. Discrimination\index{item response theory!item discrimination}
1. Guessing\index{item response theory!item guessing}
1. Inattention/careless errors\index{item response theory!item careless errors}

#### Difficulty (Severity) {#itemDifficulty}

The item's *difficulty* parameter is the item's location on the latent construct.\index{item response theory!item difficulty}
It is quantified by the intercept, i.e., the location on the x-axis of the inflection point of the [ICC](#icc).\index{item response theory!item difficulty}\index{item response theory!item characteristic curve}
In a 1- or 2-parameter model, the inflection point is where 50% of the sample endorses the item (or gets the item correct), that is, the point on the x-axis where the [ICC](#icc) crosses .5 probability on the y-axis (i.e., the level on the construct at which the probability of endorsing the item is equal to the probability of not endorsing the item).\index{item response theory!item difficulty}\index{item response theory!item characteristic curve}
Item difficulty is similar to item means or intercepts in [structural equation modeling](#sem) or [factor analysis](#factorAnalysis).\index{item response theory!item difficulty}\index{structural equation modeling!intercept}
Some items are more useful at the higher levels of the construct, whereas other items are more useful at the lower levels of the construct.
See Figure \@ref(fig:iccDifficulty) for an example of an item with a low difficulty and an item with a high difficulty.\index{item response theory!item difficulty}

```{r, include = FALSE}
library("tidyverse")
library("psych")

difficultyData <- data.frame(itemSum = 1:length(-4:4))

difficultyData$lowDifficulty <- psych::logistic(-4:4, d = -0.8)
difficultyData$highDifficulty <- psych::logistic(-4:4, d = 0.8)

L1.fun <- function(predictor, b) {
  x <- predictor
  1/(1 + exp( -(x - b)))
}

L1.Init <- function(mCall, LHS, data, ...) {
  xy <- sortedXyData(mCall[["predictor"]], LHS, data)
  x <- xy[, "x"]; y <- xy[, "y"]
  d <- 1
  ## Linear regression on pseudo y values
  pseudoY <- log((d - y)/(y+0.00001))
  coefs <- coef( lm(pseudoY ~ x))
  k <- coefs[1]
  b <- coefs[2]
  value <- c(b)
  names(value) <- mCall[c("b")]
  value
}

NLS.L1 <- selfStart(L1.fun, L1.Init, parameters = c("b"))

onePL_lowDifficulty <- nls(lowDifficulty ~ NLS.L1(itemSum, b), data = difficultyData, control = list(warnOnly = TRUE))
onePL_highDifficulty <- nls(highDifficulty ~ NLS.L1(itemSum, b), data = difficultyData, control = list(warnOnly = TRUE))

newdata$lowDifficulty <- predict(onePL_lowDifficulty, newdata = newdata)
newdata$highDifficulty <- predict(onePL_highDifficulty, newdata = newdata)

newdata$theta <- seq(from = -4, to = 4, length.out = 1000)

midpoint_lowDifficulty <- newdata$theta[which.min(abs(newdata$lowDifficulty - 0.5))]
midpoint_highDifficulty <- newdata$theta[which.min(abs(newdata$highDifficulty - 0.5))]

difficulty_long <- pivot_longer(newdata, cols = lowDifficulty:highDifficulty) %>%
  rename(item = name)

difficulty_long$Difficulty <- NA
difficulty_long$Difficulty[which(difficulty_long$item == "lowDifficulty")] <- "Low"
difficulty_long$Difficulty[which(difficulty_long$item == "highDifficulty")] <- "High"
```

(ref:iccDifficulty) Item Characteristic Curves of an Item with Low Difficulty Versus High Difficulty. The dashed horizontal line indicates a probability of item endorsement of .50. The dashed vertical line is the item difficulty, i.e., the person's level on the construct (the location on the x-axis) at the inflection point of the item characteristic curve. In a two-parameter logistic model, the inflection point corresponds to the probability of item endorsement is 50%. Thus, in a two-parameter logistic model, the difficulty of an item is the person's level on the construct where the probability of endorsing the item is 50%.

```{r iccDifficulty, echo = FALSE, results = "hide", out.width = "100%", fig.align = "center", fig.cap = "(ref:iccDifficulty)", fig.scap = "Item Characteristic Curves of an Item with Low Difficulty Versus High Difficulty."}
ggplot(difficulty_long, aes(theta, value, group = Difficulty, color = Difficulty)) +
  geom_line(linewidth = 1.5) +
  scale_color_grey() +
  geom_hline(yintercept = 0.5, linetype = "dashed") +
  geom_segment(aes(x = midpoint_lowDifficulty, xend = midpoint_lowDifficulty, y = 0, yend = 0.5), linewidth = 0.5, col = "black", linetype = "dashed") +
  geom_segment(aes(x = midpoint_highDifficulty, xend = midpoint_highDifficulty, y = 0, yend = 0.5), linewidth = 0.5, col = "black", linetype = "dashed") +
  scale_x_continuous(name = expression(paste("Person's Level on the Latent Construct (", theta, ")", sep = "")), breaks = -4:4) +
  scale_y_continuous(name = "Probability of Item Endorsement") +
  theme_bw()
```

When dealing with a measure of clinical symptoms (e.g., depression), the difficulty parameter is sometimes called severity, because symptoms that are endorsed less frequently tend to be more severe [e.g., suicidal behavior; @Krueger2004].\index{item response theory!item difficulty}
One way of thinking about the severity parameter of an item is: "How severe does your psychopathology have to be for half of people to endorse the symptom?"\index{item response theory!item difficulty}

When dealing with a measure of performance, aptitude, or intelligence, the parameter would be more likely to be called difficulty: "How high does your ability have to be for half of people to pass the item?"\index{item response theory!item difficulty}
An item with a low difficulty would be considered easy, because even people with a low ability tend to pass the item.\index{item response theory!item difficulty}
An item with a high difficulty would be considered difficult, because only people with a high ability tend to pass the item.\index{item response theory!item difficulty}

#### Discrimination {#itemDiscrimination}

The item's *discrimination* parameter is how well the item can distinguish between those who were higher versus lower on the construct, that is, how strongly the item is correlated with the construct (i.e., the latent factor).\index{item response theory!item discrimination}
It is similar to the factor loading in [structural equation modeling](#sem) or [factor analysis](#factorAnalysis).\index{item response theory!item discrimination}\index{structural equation modeling!factor loading}
It is quantified by the slope of the [ICC](#icc), i.e., the steepness of the line at its steepest point.\index{item response theory!item discrimination}\index{item response theory!item characteristic curve}
The slope reflects the inverse of how much range of construct levels it would take to flip 50/50 whether a person is likely to pass or fail an item.\index{item response theory!item discrimination}

Some items have [ICCs](#icc) that go up fast (have a steep slope).\index{item response theory!item discrimination}\index{item response theory!item characteristic curve}
These items provide a fine distinction between people with lower versus higher levels on the construct and therefore have high discrimination.\index{item response theory!item discrimination}
Some items go up gradually (less steep slope), so it provides less precision and information, and has a low discrimination.\index{item response theory!item discrimination}\index{item response theory!item characteristic curve}\index{item response theory!information}
See Figure \@ref(fig:iccDiscrimination) for an example of an item with a low discrimination and an item with a high discrimination.\index{item response theory!item discrimination}\index{item response theory!item characteristic curve}

```{r, include = FALSE}
discriminationData <- data.frame(itemSum = 1:length(-4:4))

discriminationData$lowDiscrimination <- psych::logistic(-4:4, d = 0, a = 0.7)
discriminationData$highDiscrimination <- psych::logistic(-4:4, d = 0, a = 2)

twoPL_lowDiscrimination <- nls(lowDiscrimination ~ NLS.L2(itemSum, a, b), data = discriminationData, control = list(warnOnly = TRUE))
twoPL_highDiscrimination <- nls(highDiscrimination ~ NLS.L2(itemSum, a, b), data = discriminationData, control = list(warnOnly = TRUE))

newdata$lowDiscrimination <- predict(twoPL_lowDiscrimination, newdata = newdata)
newdata$highDiscrimination <- predict(twoPL_highDiscrimination, newdata = newdata)

newdata$theta <- seq(from = -4, to = 4, length.out = 1000)

discrimination_long <- pivot_longer(newdata, cols = lowDiscrimination:highDiscrimination) %>%
  rename(item = name)

discrimination_long$Discrimination <- NA
discrimination_long$Discrimination[which(discrimination_long$item == "lowDiscrimination")] <- "Low"
discrimination_long$Discrimination[which(discrimination_long$item == "highDiscrimination")] <- "High"
```

```{r iccDiscrimination, echo = FALSE, results = "hide", out.width = "100%", fig.align = "center", fig.cap = "Item Characteristic Curves of an Item with Low Discrimination Versus High Discrimination. The discrimination of an item is the slope of the line at its inflection point.", fig.scap = "Item Characteristic Curves of an Item with Low Discrimination Versus High Discrimination."}
ggplot(discrimination_long, aes(theta, value, group = Discrimination, color = Discrimination)) +
  geom_line(linewidth = 1.5) +
  scale_color_grey() +
  scale_x_continuous(name = expression(paste("Person's Level on the Latent Construct (", theta, ")", sep = "")), breaks = -4:4) +
  scale_y_continuous(name = "Probability of Item Endorsement") +
  theme_bw()
```

#### Guessing {#itemGuessing}

The item's *guessing* parameter is reflected by the lower asymptote of the [ICC](#icc).\index{item response theory!item guessing}
If the item has a lower asymptote above zero, it suggests that the probability of getting the item correct (or endorsing the item) never reaches zero, for any level of the construct.\index{item response theory!item guessing}
On an educational test, this could correspond to the person's likelihood of being able to answer the item correctly by chance just by guessing.\index{item response theory!item guessing}
For example, for a 4-option multiple choice test, a respondent would be expected to get a given item correct 25% of the time just by guessing.\index{item response theory!item guessing}
See Figure \@ref(fig:iccGuessingTF) for an example of an item from a true/false exam and Figure \@ref(fig:iccGuessingMC) for an example of an item from a 4-option multiple choice exam.\index{item response theory!item guessing}

```{r, include = FALSE}
empiricalICCdata$guessingTF <- psych::logistic(-4:4, c = 0.5)
empiricalICCdata$guessingMC <- psych::logistic(-4:4, c = 0.25)

threePL_guessingTF <- drm(guessingTF ~ itemSum, data = empiricalICCdata, fct = LL.3u(), type = "continuous") #fix upper asymptote at 1
threePL_guessingMC <- drm(guessingMC ~ itemSum, data = empiricalICCdata, fct = LL.3u(), type = "continuous") #fix upper asymptote at 1

newdata$guessingTF <- predict(threePL_guessingTF, newdata = newdata)
newdata$guessingMC <- predict(threePL_guessingMC, newdata = newdata)
```

(ref:iccGuessingTFCaption) Item Characteristic Curve of an Item from a True/False Exam, Where Test Takers Get the Item Correct at Least 50% of the Time.

```{r iccGuessingTF, echo = FALSE, results = "hide", out.width = "100%", fig.align = "center", fig.cap = "(ref:iccGuessingTFCaption)"}
plot(newdata$itemSum, newdata$guessingTF, type = "l", lwd = 2, ylim = c(0,1), xlab = expression(paste("Person's Level on the Latent Construct (", theta, ")", sep = "")), ylab = "Probability of Item Endorsement", xaxt = "n", yaxt = "n")
axis(1, at = seq(from = 1, to = 9, length.out = 9), labels = c(-4:4))
axis(2, at = c(0, .25, .5, .75, 1), labels = c(0, .25, .5, .75, 1))
```

(ref:iccGuessingMCCaption) Item Characteristic Curve of an Item from a 4-Option Multiple Choice Exam, Where Test Takers Get the Item Correct at Least 25% of the Time.

```{r iccGuessingMC, echo = FALSE, results = "hide", out.width = "100%", fig.align = "center", fig.cap = "(ref:iccGuessingMCCaption)"}
plot(newdata$itemSum, newdata$guessingMC, type = "l", lwd = 2, ylim = c(0,1), xlab = expression(paste("Person's Level on the Latent Construct (", theta, ")", sep = "")), ylab = "Probability of Item Endorsement", xaxt = "n", yaxt = "n")
axis(1, at = seq(from = 1, to = 9, length.out = 9), labels = c(-4:4))
axis(2, at = c(0, .25, .5, .75, 1), labels = c(0, .25, .5, .75, 1))
```

#### Inattention/Careless Errors {#itemCarelessErrors}

The item's *inattention* (or *careless error*) parameter is the reflected by the upper asymptote of the [ICC](#icc).\index{item response theory!item careless errors}
If the item has an upper asymptote below one, it suggests that the probability of getting the item correct (or endorsing the item) never reaches one, for any level on the construct.\index{item response theory!item careless errors}
See Figure \@ref(fig:iccInattention) for an example of an item whose probability of endorsement (or getting it correct) exceeds .85.\index{item response theory!item careless errors}

```{r, include = FALSE}
empiricalICCdata$inattention <- psych::logistic(-4:4, z = 0.85, a = 2)

threePL_inattention <- drm(inattention ~ itemSum, data = empiricalICCdata, fct = LL.3(), type = "continuous") #fix lower asymptote at 0

newdata$inattention <- predict(threePL_inattention, newdata = newdata)
```

```{r iccInattention, echo = FALSE, results = "hide", out.width = "100%", fig.align = "center", fig.cap = "Item Characteristic Curve of an Item Where the Probability of Getting an Item Correct Never Exceeds .85."}
plot(newdata$itemSum, newdata$inattention, type = "l", lwd = 2, ylim = c(0,1), xlab = expression(paste("Person's Level on the Latent Construct (", theta, ")", sep = "")), ylab = "Probability of Item Endorsement", xaxt = "n")
axis(1, at = seq(from = 1, to = 9, length.out = 9), labels = c(-4:4))
```

### Models {#models-irt}

IRT models can be fit that estimate one or more of these four item parameters.\index{item response theory}

#### One-Parameter and Rasch Models {#onePL}

A Rasch model estimates the [item difficulty](#itemDifficulty) parameter and holds everything else fixed across items.\index{item response theory!one-parameter model}\index{item response theory!item difficulty}
It fixes the [item discrimination](#itemDiscrimination) to be one for each item.\index{item response theory!one-parameter model}\index{item response theory!item discrimination}
In the Rasch model, the probability that a person $j$ with a level on the construct of $\theta$ gets a score of one (instead of zero) on item $i$, based on the difficulty ($b$) of the item, is estimated using Equation \@ref(eq:raschModel):\index{item response theory!one-parameter model}\index{item response theory!item difficulty}\index{item response theory!theta}

$$
\begin{equation}
P(X = 1|\theta_j, b_i) = \frac{e^{\theta_j - b_i}}{1 + e^{\theta_j - b_i}}
(\#eq:raschModel)
\end{equation}
$$

The `petersenlab` package [@R-petersenlab] contains the `fourPL()` function that estimates the probability of item endorsement as function of the item characteristics from the Rasch model and the person's level on the construct (theta).\index{petersenlab package}\index{item response theory!one-parameter model}\index{item response theory!item difficulty}\index{item response theory!theta}
To estimate the probability of endorsement from the Rasch model, specify $b$ and $\theta$, while keeping the defaults for the other parameters.\index{item response theory!theta}

```{r}
library("petersenlab")
```

```{r}
fourPL(b = 1, theta = 0)
```

A one-parameter logistic (1-PL) IRT model, similar to a Rasch model, estimates the [item difficulty](#itemDifficulty) parameter, and holds everything else fixed across items (see Figure \@ref(fig:irt1PL)).\index{item response theory!one-parameter model}\index{item response theory!item difficulty}
The 1-PL model holds the [item discrimination](#itemDiscrimination) fixed across items, but does not fix it to one, unlike the Rasch model.\index{item response theory!one-parameter model}\index{item response theory!item difficulty}

In the 1-PL model, the probability that a person $j$ with a level on the construct of $\theta$ gets a score of one (instead of zero) on item $i$, based on the [difficulty](#itemDifficulty) ($b$) of the item and the items' (fixed) [discrimination](#itemDiscrimination) ($a$), is estimated using Equation \@ref(eq:onePL):\index{item response theory!one-parameter model}\index{item response theory!item difficulty}\index{item response theory!item discrimination}\index{item response theory!theta}

$$
\begin{equation}
P(X = 1|\theta_j, b_i, a) = \frac{e^{a(\theta_j - b_i)}}{1 + e^{a(\theta_j - b_i)}}
(\#eq:onePL)
\end{equation}
$$

To estimate the probability of endorsement from the 1-PL model, specify $a$, $b$, and $\theta$, while keeping the defaults for the other parameters.\index{item response theory!one-parameter model}\index{item response theory!item difficulty}\index{item response theory!item discrimination}\index{item response theory!theta}

```{r, eval = FALSE}
fourPL(a, b, theta)
```

Rasch and 1-PL models are common and are the easiest to fit.\index{item response theory!one-parameter model}\index{item response theory!item difficulty}
However, they make fairly strict assumptions.
They assume that items have the same [discrimination](#itemDiscrimination).\index{item response theory!one-parameter model}\index{item response theory!item discrimination}

```{r, include = FALSE}
library("tidyverse")

onePLitems <- data.frame(
  theta = -4:4,
  item1 = fourPL(b = -2, theta = -4:4),
  item2 = fourPL(b = -1, theta = -4:4),
  item3 = fourPL(b = 0, theta = -4:4),
  item4 = fourPL(b = 1, theta = -4:4),
  item5 = fourPL(b = 2, theta = -4:4))

onePL_item1 <- nls(item1 ~ NLS.L1(theta, b), data = onePLitems, control = list(warnOnly = TRUE))
onePL_item2 <- nls(item2 ~ NLS.L1(theta, b), data = onePLitems, control = list(warnOnly = TRUE))
onePL_item3 <- nls(item3 ~ NLS.L1(theta, b), data = onePLitems, control = list(warnOnly = TRUE))
onePL_item4 <- nls(item4 ~ NLS.L1(theta, b), data = onePLitems, control = list(warnOnly = TRUE))
onePL_item5 <- nls(item5 ~ NLS.L1(theta, b), data = onePLitems, control = list(warnOnly = TRUE))

onePLitems_newdata <- data.frame(theta = seq(from = -4, to = 4, length.out = 1000))

onePLitems_newdata$item1 <- predict(onePL_item1, newdata = onePLitems_newdata)
onePLitems_newdata$item2 <- predict(onePL_item2, newdata = onePLitems_newdata)
onePLitems_newdata$item3 <- predict(onePL_item3, newdata = onePLitems_newdata)
onePLitems_newdata$item4 <- predict(onePL_item4, newdata = onePLitems_newdata)
onePLitems_newdata$item5 <- predict(onePL_item5, newdata = onePLitems_newdata)

onePLitems_long <- pivot_longer(onePLitems_newdata, cols = item1:item5) %>%
  rename(item = name)

onePLitems_long$Item <- NA
onePLitems_long$Item[which(onePLitems_long$item == "item1")] <- 1
onePLitems_long$Item[which(onePLitems_long$item == "item2")] <- 2
onePLitems_long$Item[which(onePLitems_long$item == "item3")] <- 3
onePLitems_long$Item[which(onePLitems_long$item == "item4")] <- 4
onePLitems_long$Item[which(onePLitems_long$item == "item5")] <- 5
```

```{r irt1PL, echo = FALSE, results = "hide", out.width = "100%", fig.align = "center", fig.cap = "One-Parameter Logistic Model in Item Response Theory."}
ggplot(onePLitems_long, aes(theta, value, group = factor(Item), color = factor(Item))) +
  geom_line(linewidth = 1.5) +
  labs(color = "Item") +
  scale_color_grey() +
  scale_x_continuous(name = expression(paste("Person's Level on the Latent Construct (", theta, ")", sep = "")), breaks = -4:4) +
  scale_y_continuous(name = "Probability of Item Endorsement") +
  theme_bw()
```

A 1-PL model is only valid if there is no crossing of lines in empirical [ICCs](#icc) (see Figure \@ref(fig:empiricalICCnoCrossing)).\index{item response theory!one-parameter model}\index{item response theory!item characteristic curve}

```{r, include = FALSE}
empiricalICCdata_noCrossing <- data.frame(
  item1 =  c(.40, .63, .73, .85, .93, .95, .97, .99, .99),
  item2 =  c(.20, .53, .56, .76, .90, .94, .95, .98, .99),
  item3 =  c(.10, .28, .39, .58, .75, .82, .88, .94, .99),
  item4 =  c(.05, .24, .30, .50, .68, .73, .84, .92, .99),
  item5 =  c(.03, .19, .27, .48, .57, .65, .79, .89, .99),
  item6 =  c(.02, .15, .25, .38, .48, .59, .75, .85, .95),
  item7 =  c(.01, .07, .15, .29, .42, .53, .70, .82, .90),
  item8 =  c(.01, .05, .10, .26, .35, .46, .60, .80, .85),
  item9 =  c(.01, .02, .05, .16, .23, .35, .46, .55, .70),
  item10 = c(.01, .02, .03, .05, .06, .07, .10, .14, .20),
  itemSum = 1:9
)
```

```{r empiricalICCnoCrossing, echo = FALSE, results = "hide", out.width = "100%", fig.align = "center", fig.height = 8, fig.cap = "Empirical Item Characteristic Curves of the Probability of Endorsement of a Given Item as a Function of the Person's Sum Score. The empirical item characteristic curves of these items do not cross each other.", fig.scap = "Empirical Item Characteristic Curves of the Probability of Endorsement of a Given Item as a Function of the Person's Sum Score."}
plot(empiricalICCdata_noCrossing$itemSum, empiricalICCdata_noCrossing$item10, type = "n", xlim = c(1,9), ylim = c(0,1), xlab = "Person's Sum Score", ylab = "Probability of Item Endorsement", xaxt = "n")
lines(empiricalICCdata_noCrossing$itemSum, empiricalICCdata_noCrossing$item1, type = "b", pch = "1")
lines(empiricalICCdata_noCrossing$itemSum, empiricalICCdata_noCrossing$item2, type = "b", pch = "2")
lines(empiricalICCdata_noCrossing$itemSum, empiricalICCdata_noCrossing$item3, type = "b", pch = "3")
lines(empiricalICCdata_noCrossing$itemSum, empiricalICCdata_noCrossing$item4, type = "b", pch = "4")
lines(empiricalICCdata_noCrossing$itemSum, empiricalICCdata_noCrossing$item5, type = "b", pch = "5")
lines(empiricalICCdata_noCrossing$itemSum, empiricalICCdata_noCrossing$item6, type = "b", pch = "6")
lines(empiricalICCdata_noCrossing$itemSum, empiricalICCdata_noCrossing$item7, type = "b", pch = "7")
lines(empiricalICCdata_noCrossing$itemSum, empiricalICCdata_noCrossing$item8, type = "b", pch = "8")
lines(empiricalICCdata_noCrossing$itemSum, empiricalICCdata_noCrossing$item9, type = "b", pch = "9")
lines(empiricalICCdata_noCrossing$itemSum, empiricalICCdata_noCrossing$item10, type = "l")
points(empiricalICCdata_noCrossing$itemSum, empiricalICCdata_noCrossing$item10, type = "p", pch = 19, col = "white", cex = 3)
text(empiricalICCdata_noCrossing$itemSum, empiricalICCdata_noCrossing$item10, labels = "10")
axis(1, at = 1:9, labels = 1:9)
```

#### Two-Parameter {#twoPL}

A two-parameter logistic (2-PL) IRT model estimates item [difficulty](#itemDifficulty) and [discrimination](#itemDiscrimination), and it holds the asymptotes fixed across items (see Figure \@ref(fig:irt2PL)).\index{item response theory!two-parameter model}\index{item response theory!item difficulty}\index{item response theory!item discrimination}
Like 1-PL models, 2-PL models are also common.\index{item response theory!two-parameter model}

In the 2-PL model, the probability that a person $j$ with a level on the construct of $\theta$ gets a score of one (instead of zero) on item $i$, based on the [difficulty](#itemDifficulty) ($b$) and [discrimination](#itemDiscrimination) ($a$) of the item, is estimated using Equation \@ref(eq:twoPL):\index{item response theory!two-parameter model}\index{item response theory!item difficulty}\index{item response theory!item discrimination}\index{item response theory!theta}

$$
\begin{equation}
P(X = 1|\theta_j, b_i, a_i) = \frac{e^{a_i(\theta_j - b_i)}}{1 + e^{a_i(\theta_j - b_i)}}
(\#eq:twoPL)
\end{equation}
$$

To estimate the probability of endorsement from the 2-PL model, specify $a$, $b$, and $\theta$, while keeping the defaults for the other parameters.\index{item response theory!two-parameter model}\index{item response theory!item difficulty}\index{item response theory!item discrimination}\index{item response theory!theta}

```{r}
fourPL(a = 0.6, b = 0, theta = -1)
```

```{r, include = FALSE}
twoPLitems <- data.frame(theta = -6:6,
                         item1 = fourPL(a = 1.3, b = -2, theta = -6:6),
                         item2 = fourPL(a = 0.8, b = -1, theta = -6:6),
                         item3 = fourPL(a = 0.6, b = 0, theta = -6:6),
                         item4 = fourPL(a = 1.5, b = 1, theta = -6:6),
                         item5 = fourPL(a = 2.3, b = 2, theta = -6:6))

twoPL_item1 <- nls(item1 ~ NLS.L2(theta, a, b), data = twoPLitems, control = list(warnOnly = TRUE))
twoPL_item2 <- nls(item2 ~ NLS.L2(theta, a, b), data = twoPLitems, control = list(warnOnly = TRUE))
twoPL_item3 <- nls(item3 ~ NLS.L2(theta, a, b), data = twoPLitems, control = list(warnOnly = TRUE))
twoPL_item4 <- nls(item4 ~ NLS.L2(theta, a, b), data = twoPLitems, control = list(warnOnly = TRUE))
twoPL_item5 <- nls(item5 ~ NLS.L2(theta, a, b), data = twoPLitems, control = list(warnOnly = TRUE))

twoPLitems_newdata <- data.frame(theta = seq(from = -6, to = 6, length.out = 1000))

twoPLitems_newdata$item1 <- predict(twoPL_item1, newdata = twoPLitems_newdata)
twoPLitems_newdata$item2 <- predict(twoPL_item2, newdata = twoPLitems_newdata)
twoPLitems_newdata$item3 <- predict(twoPL_item3, newdata = twoPLitems_newdata)
twoPLitems_newdata$item4 <- predict(twoPL_item4, newdata = twoPLitems_newdata)
twoPLitems_newdata$item5 <- predict(twoPL_item5, newdata = twoPLitems_newdata)

twoPLitems_long <- pivot_longer(twoPLitems_newdata, cols = item1:item5) %>%
  rename(item = name)

twoPLitems_long$Item <- NA
twoPLitems_long$Item[which(twoPLitems_long$item == "item1")] <- 1
twoPLitems_long$Item[which(twoPLitems_long$item == "item2")] <- 2
twoPLitems_long$Item[which(twoPLitems_long$item == "item3")] <- 3
twoPLitems_long$Item[which(twoPLitems_long$item == "item4")] <- 4
twoPLitems_long$Item[which(twoPLitems_long$item == "item5")] <- 5
```

```{r irt2PL, echo = FALSE, results = "hide", out.width = "100%", fig.align = "center", fig.cap = "Two-Parameter Logistic Model in Item Response Theory."}
ggplot(twoPLitems_long, aes(theta, value, group = factor(Item), color = factor(Item))) +
  geom_line(linewidth = 1.5) +
  labs(color = "Item") +
  scale_color_grey() +
  scale_x_continuous(name = expression(paste("Person's Level on the Latent Construct (", theta, ")", sep = "")), breaks = seq(from = -6, to = 6, by = 2), limits = c(-6,6)) +
  scale_y_continuous(name = "Probability of Item Endorsement") +
  theme_bw()
```

#### Three-Parameter {#threePL}

A three-parameter logistic (3-PL) IRT model estimates item [difficulty](#itemDifficulty), [discrimination](#itemDiscrimination), and [guessing](#itemGuessing) (lower asymptote), and it holds the upper asymptote fixed across items (see Figure \@ref(fig:irt3PL)).\index{item response theory!three-parameter model}\index{item response theory!item difficulty}\index{item response theory!item discrimination}\index{item response theory!item guessing}
This model would provide information about where an item drops out.\index{item response theory!three-parameter model}\index{item response theory!item difficulty}\index{item response theory!item discrimination}\index{item response theory!item guessing}
Three-parameter logistic models are less common to estimate because this adds considerable computational complexity and requires a large sample size, and the [guessing](#itemGuessing) parameter is often not as important as [difficulty](#itemDifficulty) and [discrimination](#itemDiscrimination).\index{item response theory!three-parameter model}\index{item response theory!item difficulty}\index{item response theory!item discrimination}\index{item response theory!item guessing}
Nevertheless, 3-PL models are sometimes estimated in the education literature to account for getting items correct by random guessing.\index{item response theory!three-parameter model}\index{item response theory!item difficulty}\index{item response theory!item discrimination}\index{item response theory!item guessing}

In the 3-PL model, the probability that a person $j$ with a level on the construct of $\theta$ gets a score of one (instead of zero) on item $i$, based on the [difficulty](#itemDifficulty) ($b$), [discrimination](#itemDiscrimination) ($a$), and [guessing parameter](#itemGuessing) ($c$) of the item, is estimated using Equation \@ref(eq:threePL):\index{item response theory!three-parameter model}\index{item response theory!item difficulty}\index{item response theory!item discrimination}\index{item response theory!item guessing}\index{item response theory!theta}

$$
\begin{equation}
P(X = 1|\theta_j, b_i, a_i, c_i) = c_i + (1 - c_i) \frac{e^{a_i(\theta_j - b_i)}}{1 + e^{a_i(\theta_j - b_i)}}
(\#eq:threePL)
\end{equation}
$$

To estimate the probability of endorsement from the 3-PL model, specify $a$, $b$, $c$, and $\theta$, while keeping the defaults for the other parameters.\index{item response theory!three-parameter model}\index{item response theory!item difficulty}\index{item response theory!item discrimination}\index{item response theory!item guessing}\index{item response theory!theta}

```{r}
fourPL(a = 0.8, b = -1, c = .25, theta = -1)
```

```{r, include = FALSE}
threePLitems <- data.frame(theta = 1:13,
                           item1 = fourPL(a = 1.3, b = -2, c = 0, theta = -6:6),
                           item2 = fourPL(a = 0.8, b = -1, c = .25, theta = -6:6),
                           item3 = fourPL(a = 0.6, b = 0, c = .3, theta = -6:6),
                           item4 = fourPL(a = 1.5, b = 1, c = .15, theta = -6:6),
                           item5 = fourPL(a = 2.3, b = 2, c = 0, theta = -6:6))

threePL_item1 <- drm(item1 ~ theta, data = threePLitems, fct = LL.3u(), type = "continuous")
threePL_item2 <- drm(item2 ~ theta, data = threePLitems, fct = LL.3u(), type = "continuous")
threePL_item3 <- drm(item3 ~ theta, data = threePLitems, fct = LL.3u(), type = "continuous")
threePL_item4 <- drm(item4 ~ theta, data = threePLitems, fct = LL.3u(), type = "continuous")
threePL_item5 <- drm(item5 ~ theta, data = threePLitems, fct = LL.3u(), type = "continuous")

threePLitems_newdata <- data.frame(theta = seq(from = 1, to = 13, length.out = 1000))

threePLitems_newdata$item1 <- predict(threePL_item1, newdata = threePLitems_newdata)
threePLitems_newdata$item2 <- predict(threePL_item2, newdata = threePLitems_newdata)
threePLitems_newdata$item3 <- predict(threePL_item3, newdata = threePLitems_newdata)
threePLitems_newdata$item4 <- predict(threePL_item4, newdata = threePLitems_newdata)
threePLitems_newdata$item5 <- predict(threePL_item5, newdata = threePLitems_newdata)

threePLitems_newdata$theta <- seq(from = -6, to = 6, length.out = 1000)

threePLitems_long <- pivot_longer(threePLitems_newdata, cols = item1:item5) %>%
  rename(item = name)

threePLitems_long$Item <- NA
threePLitems_long$Item[which(threePLitems_long$item == "item1")] <- 1
threePLitems_long$Item[which(threePLitems_long$item == "item2")] <- 2
threePLitems_long$Item[which(threePLitems_long$item == "item3")] <- 3
threePLitems_long$Item[which(threePLitems_long$item == "item4")] <- 4
threePLitems_long$Item[which(threePLitems_long$item == "item5")] <- 5
```

```{r irt3PL, echo = FALSE, results = "hide", out.width = "100%", fig.align = "center", fig.cap = "Three-Parameter Logistic Model in Item Response Theory."}
ggplot(threePLitems_long, aes(theta, value, group = factor(Item), color = factor(Item))) +
  geom_line(linewidth = 1.5) +
  labs(color = "Item") +
  scale_color_grey() +
  scale_x_continuous(name = expression(paste("Person's Level on the Latent Construct (", theta, ")", sep = "")), breaks = seq(from = -6, to = 6, by = 2), limits = c(-6,6)) +
  scale_y_continuous(name = "Probability of Item Endorsement") +
  theme_bw()
```

#### Four-Parameter {#fourPL}

A four-parameter logistic (4-PL) IRT model estimates item [difficulty](#itemDifficulty), [discrimination](#itemDiscrimination), [guessing](#itemGuessing), and [careless errors](#itemCarelessErrors) (see Figure \@ref(fig:irt4PL)).\index{item response theory!four-parameter model}\index{item response theory!item difficulty}\index{item response theory!item discrimination}\index{item response theory!item guessing}\index{item response theory!item careless errors}
The fourth parameter adds considerable computational complexity and is rare to estimate.\index{item response theory!four-parameter model}

In the 4-PL model, the probability that a person $j$ with a level on the construct of $\theta$ gets a score of one (instead of zero) on item $i$, based on the [difficulty](#itemDifficulty) ($b$), [discrimination](#itemDiscrimination) ($a$), [guessing parameter](#itemGuessing) ($c$), and [careless error parameter](#itemCarelessErrors) ($d$) of the item, is estimated using Equation \@ref(eq:fourPL) [@Magis2013]:\index{item response theory!four-parameter model}\index{item response theory!item difficulty}\index{item response theory!item discrimination}\index{item response theory!item guessing}\index{item response theory!item careless errors}\index{item response theory!theta}

$$
\begin{equation}
P(X = 1|\theta_j, b_i, a_i, c_i, d_i) = c_i + (d_i - c_i) \frac{e^{a_i(\theta_j - b_i)}}{1 + e^{a_i(\theta_j - b_i)}}
(\#eq:fourPL)
\end{equation}
$$

To estimate the probability of endorsement from the 4-PL model, specify $a$, $b$, $c$, $d$, and $\theta$.\index{item response theory!four-parameter model}\index{item response theory!item difficulty}\index{item response theory!item discrimination}\index{item response theory!item guessing}\index{item response theory!item careless errors}\index{item response theory!theta}

```{r}
fourPL(a = 1.5, b = 1, c = .15, d = 0.85, theta = 3)
```

```{r, include = FALSE}
fourPLitems <- data.frame(theta = 1:13,
                          item1 = fourPL(a = 1.3, b = -2, c = 0, d = 1, theta = -6:6),
                          item2 = fourPL(a = 0.8, b = -1, c = .25, d = 0.9, theta = -6:6),
                          item3 = fourPL(a = 0.6, b = 0, c = .3, d = 0.95, theta = -6:6),
                          item4 = fourPL(a = 1.5, b = 1, c = .15, d = 0.85, theta = -6:6),
                          item5 = fourPL(a = 2.3, b = 2, c = 0, d = 0.98, theta = -6:6))

fourPL_item1 <- drm(item1 ~ theta, data = fourPLitems, fct = LL.4(), type = "continuous")
fourPL_item2 <- drm(item2 ~ theta, data = fourPLitems, fct = LL.4(), type = "continuous")
fourPL_item3 <- drm(item3 ~ theta, data = fourPLitems, fct = LL.4(), type = "continuous")
fourPL_item4 <- drm(item4 ~ theta, data = fourPLitems, fct = LL.4(), type = "continuous")
fourPL_item5 <- drm(item5 ~ theta, data = fourPLitems, fct = LL.4(), type = "continuous")

fourPLitems_newdata <- data.frame(theta = seq(from = 1, to = 13, length.out = 1000))

fourPLitems_newdata$item1 <- predict(fourPL_item1, newdata = fourPLitems_newdata)
fourPLitems_newdata$item2 <- predict(fourPL_item2, newdata = fourPLitems_newdata)
fourPLitems_newdata$item3 <- predict(fourPL_item3, newdata = fourPLitems_newdata)
fourPLitems_newdata$item4 <- predict(fourPL_item4, newdata = fourPLitems_newdata)
fourPLitems_newdata$item5 <- predict(fourPL_item5, newdata = fourPLitems_newdata)

fourPLitems_newdata$theta <- seq(from = -6, to = 6, length.out = 1000)

fourPLitems_long <- pivot_longer(fourPLitems_newdata, cols = item1:item5) %>%
  rename(item = name)

fourPLitems_long$Item <- NA
fourPLitems_long$Item[which(fourPLitems_long$item == "item1")] <- 1
fourPLitems_long$Item[which(fourPLitems_long$item == "item2")] <- 2
fourPLitems_long$Item[which(fourPLitems_long$item == "item3")] <- 3
fourPLitems_long$Item[which(fourPLitems_long$item == "item4")] <- 4
fourPLitems_long$Item[which(fourPLitems_long$item == "item5")] <- 5
```

```{r irt4PL, echo = FALSE, results = "hide", out.width = "100%", fig.align = "center", fig.cap = "Four-Parameter Logistic Model in Item Response Theory."}
ggplot(fourPLitems_long, aes(theta, value, group = factor(Item), color = factor(Item))) +
  geom_line(linewidth = 1.5) +
  labs(color = "Item") +
  scale_color_grey() +
  scale_x_continuous(name = expression(paste("Person's Level on the Latent Construct (", theta, ")", sep = "")), breaks = seq(from = -6, to = 6, by = 2), limits = c(-6,6)) +
  scale_y_continuous(name = "Probability of Item Endorsement") +
  theme_bw()
```

#### Graded Response Model {#grm}

*Graded response models* and *generalized partial credit models* can be estimated with one, two, three, or four parameters.\index{item response theory!graded response model}\index{item response theory!one-parameter model}\index{item response theory!two-parameter model}\index{item response theory!three-parameter model}\index{item response theory!four-parameter model}
However, they use polytomous data (not dichotomous data), as described in the section below.\index{item response theory!graded response model}

The [two-parameter](#twoPL) graded response model takes the general form of Equation \@ref(eq:gradedResponseModel1):\index{item response theory!graded response model}\index{item response theory!two-parameter model}

$$
\begin{equation}
P(X_{ji} = x_{ji}|\theta_j) = P^*_{x_{ji}}(\theta_j) - P^*_{x_{ji} + 1}(\theta_j)
(\#eq:gradedResponseModel1)
\end{equation}
$$

where:

$$
\begin{equation}
P^*_{x_{ji}}(\theta_j) = P(X_{ji} \geq x_{ji}|\theta_j, b_{ic}, a_i) = \frac{1}{1 + e^{a_i(\theta_j - b_{ic})}}
(\#eq:gradedResponseModel2)
\end{equation}
$$

In the model, $a_i$ an item-specific [discrimination parameter](#itemDiscrimination), $b_{ic}$ is an item- and category-specific [difficulty parameter](#itemDifficulty), and $θ_n$ is an estimate of a person's standing on the latent variable.\index{item response theory!graded response model}\index{item response theory!two-parameter model}\index{item response theory!item difficulty}\index{item response theory!item discrimination}
In the model, $i$ represents unique items, $c$ represents different categories that are rated, and $j$ represents participants.\index{item response theory!graded response model}

### Type of Data {#dataTypes-irt}

IRT models are most commonly estimated with binary or dichotomous data.\index{item response theory}\index{data!dichotomous}
For example, the measures have questions or items that can be considered collapsed into two groups (e.g., true/false, correct/incorrect, endorsed/not endorsed).\index{item response theory}\index{data!dichotomous}
IRT models can also be estimated with polytomous data (e.g., likert scale), which adds computational complexity.\index{data!polytomous}\index{Likert scale}
IRT models with polytomous data can be fit with a graded response model or generalized partial credit model.\index{item response theory}\index{item response theory!graded response model}\index{data!polytomous}

For example, see Figure \@ref(fig:polytomousItemBoundaryCurves) for an example of an item boundary characteristic curve for an item from a 5-level likert scale (based on a cumulative distribution).\index{item response theory}\index{item response theory!item boundary characteristic curve}\index{Likert scale}
If an item has $k$ response categories, it has $k - 1$ thresholds.\index{item response theory}\index{item response theory!item boundary characteristic curve}
For example, an item with 5-level likert scale (1 = strongly disagree; 2 = disagree; 3 = neither agree nor disagree; 4 = agree; 5 = strongly agree) has 4 thresholds: one from 1–2, one from 2–3, one from 3–4, and one from 4–5.\index{item response theory}\index{item response theory!item boundary characteristic curve}\index{Likert scale}
The item boundary characteristic curve is the probability that a person selects a response category higher than $k$ of a polytomous item.\index{item response theory}\index{item response theory!item boundary characteristic curve}\index{data!polytomous}
As depicted, one likert scale item does equivalent work as 4 binary items.\index{item response theory}\index{item response theory!item boundary characteristic curve}\index{data!dichotomous}\index{data!polytomous}\index{Likert scale}
See Figure \@ref(fig:polytomousItemResponseCategoryCurves) for the same 5-level likert scale item plotted with an item response category characteristic curve (based on a static, non-cumulative distribution).\index{item response theory}\index{item response theory!item response category characteristic curve}\index{Likert scale}

```{r, include = FALSE}
polytomousItemBoundary <- data.frame(theta = seq(from = -4, to = 4, length.out= 1000),
                                     itemBoundary1 = psych::logistic(seq(from = -4, to = 4, length.out = 1000), d = -2),
                                     itemBoundary2 = psych::logistic(seq(from = -4, to = 4, length.out = 1000), d = -0.6667),
                                     itemBoundary3 = psych::logistic(seq(from = -4, to = 4, length.out = 1000), d = 0.6667),
                                     itemBoundary4 = psych::logistic(seq(from = -4, to = 4, length.out = 1000), d = 2))

polytomousItemResponseCategory <- data.frame(theta = seq(from = -4, to = 4, length.out = 1000))
polytomousItemResponseCategory$itemResponseCategory1 <- 1 - polytomousItemBoundary$itemBoundary1
polytomousItemResponseCategory$itemResponseCategory5 <- polytomousItemBoundary$itemBoundary4
polytomousItemResponseCategory$itemResponseCategory4 <- polytomousItemBoundary$itemBoundary3 - polytomousItemResponseCategory$itemResponseCategory5
polytomousItemResponseCategory$itemResponseCategory3 <- polytomousItemBoundary$itemBoundary2 - rowSums(polytomousItemResponseCategory[,c("itemResponseCategory4","itemResponseCategory5")])
polytomousItemResponseCategory$itemResponseCategory2 <- polytomousItemBoundary$itemBoundary1 - rowSums(polytomousItemResponseCategory[,c("itemResponseCategory3","itemResponseCategory4","itemResponseCategory5")])

polytomousItemResponseCategory <- polytomousItemResponseCategory %>%
  dplyr::select(theta, itemResponseCategory1, itemResponseCategory2, itemResponseCategory3, itemResponseCategory4, itemResponseCategory5)

polytomousItemBoundary_long <- pivot_longer(polytomousItemBoundary, cols = itemBoundary1:itemBoundary4) %>%
  rename(item = name)

polytomousItemBoundary_long$boundary <- NA
polytomousItemBoundary_long$boundary[which(polytomousItemBoundary_long$item == "itemBoundary1")] <- 1
polytomousItemBoundary_long$boundary[which(polytomousItemBoundary_long$item == "itemBoundary2")] <- 2
polytomousItemBoundary_long$boundary[which(polytomousItemBoundary_long$item == "itemBoundary3")] <- 3
polytomousItemBoundary_long$boundary[which(polytomousItemBoundary_long$item == "itemBoundary4")] <- 4

polytomousItemResponseCategory_long <- pivot_longer(polytomousItemResponseCategory, cols = itemResponseCategory1:itemResponseCategory5) %>%
  rename(item = name)

polytomousItemResponseCategory_long$responseCategory <- NA
polytomousItemResponseCategory_long$responseCategory[which(polytomousItemResponseCategory_long$item == "itemResponseCategory1")] <- 1
polytomousItemResponseCategory_long$responseCategory[which(polytomousItemResponseCategory_long$item == "itemResponseCategory2")] <- 2
polytomousItemResponseCategory_long$responseCategory[which(polytomousItemResponseCategory_long$item == "itemResponseCategory3")] <- 3
polytomousItemResponseCategory_long$responseCategory[which(polytomousItemResponseCategory_long$item == "itemResponseCategory4")] <- 4
polytomousItemResponseCategory_long$responseCategory[which(polytomousItemResponseCategory_long$item == "itemResponseCategory5")] <- 5
```

```{r polytomousItemBoundaryCurves, echo = FALSE, results = "hide", out.width = "100%", fig.align = "center", fig.cap = "Item Boundary Characteristic Curves from Two-Parameter Graded Response Model in Item Response Theory."}
ggplot(polytomousItemBoundary_long, aes(theta, value, group = factor(boundary), color = factor(boundary))) +
  geom_line(linewidth = 1.5) +
  labs(color = "Boundary") +
  scale_color_grey() +
  scale_x_continuous(name = expression(paste("Person's Level on the Latent Construct (", theta, ")", sep = "")), breaks = -4:4) +
  scale_y_continuous(name = "Probability of Endorsing Item Response Category At or Above Boundary") +
  theme_bw() +
  theme(axis.title.y = element_text(size = 9))
```

```{r polytomousItemResponseCategoryCurves, echo = FALSE, results = "hide", out.width = "100%", fig.align = "center", fig.cap = "Item Response Category Characteristic Curves from Two-Parameter Graded Response Model in Item Response Theory."}
ggplot(polytomousItemResponseCategory_long, aes(theta, value, group = factor(responseCategory), color = factor(responseCategory))) +
  geom_line(linewidth = 1.5) +
  labs(color = "Response Category") +
  scale_color_grey() +
  scale_x_continuous(name = expression(paste("Person's Level on the Latent Construct (", theta, ")", sep = "")), breaks = -4:4) +
  scale_y_continuous(name = "Probability of Item Response Category Endorsement") +
  theme_bw()
```

IRT does not handle continuous data well, with some exceptions [@Chen2019] such as in a Bayesian framework [@Buerkner2021].\index{item response theory}
If you want to use continuous data, you might consider moving to a [factor analysis](#factorAnalysis) framework.\index{factor analysis}

### Sample Size {#sampleSize-irt}

Sample size requirements depend on the complexity of the model.
A 1-parameter model often requires ~100 participants.\index{item response theory!one-parameter model}
A 2-parameter model often requires ~1,000 participants.\index{item response theory!two-parameter model}
A 3-parameter model often requires ~10,000 participants.\index{item response theory!three-parameter model}

### Reliability (Information) {#irtReliability}

IRT conceptualizes [reliability](#reliability) in a different way than [classical test theory](#ctt) does.\index{item response theory}\index{classical test theory}\index{item response theory!information}\index{reliability}
Both IRT and [classical test theory](#ctt) conceptualize [reliability](#reliability) as involving the *precision* of a measure's scores.\index{item response theory}\index{classical test theory}\index{item response theory!information}\index{reliability}\index{reliability!precision}
In [classical test theory](#ctt), (im)precision—as operationalized by the [standard error of measurement](#standardErrorofMeasurement)—is estimated with a single index across the whole range of the construct.\index{classical test theory}\index{reliability}\index{reliability!precision}\index{reliability!standard error of measurement}
That is, in [classical test theory](#ctt), the same [standard error of measurement](#standardErrorofMeasurement) applies to all scores in the population [@Embretson1996].\index{classical test theory}\index{reliability}\index{reliability!precision}\index{reliability!standard error of measurement}
However, IRT estimates how much measurement precision (information) or imprecision ([standard error of measurement](#standardErrorofMeasurement)) each item, and the test as a whole, has at different construct levels.\index{item response theory!information}\index{reliability}
This allows IRT to conceptualize [reliability](#reliability) in such a way that precision/[reliability](#reliability) can *differ* at different construct levels, unlike in [classical test theory](#ctt) [@Embretson1996].\index{item response theory}\index{classical test theory}\index{item response theory!information}\index{reliability}
Thus, IRT does not have one index of [reliability](#reliability); rather, its estimate of [reliability](#reliability) differs at different levels on the construct.\index{item response theory}\index{classical test theory}\index{item response theory!information}\index{reliability}

Based on an item's [difficulty](#itemDifficulty) and [discrimination](#itemDiscrimination), we can calculate how much information each item provides.\index{item response theory}\index{item response theory!information}\index{reliability}\index{item response theory!item difficulty}\index{item response theory!item discrimination}
In IRT, *information* is how much measurement precision or consistency an item (or the measure) provides.\index{item response theory}\index{item response theory!information}\index{reliability}\index{reliability!precision}
In other words, information is the degree to which an item (or measure) reduces the [standard error of measurement](#standardErrorofMeasurement), that is, how much it reduces uncertainty of a person's level on the construct.\index{item response theory}\index{item response theory!information}\index{reliability}\index{reliability!standard error of measurement}
As a reminder (from Equation \@ref(eq:standardErrorOfMeasurement)), the [standard error of measurement](#standardErrorofMeasurement) is calculated as:\index{reliability!standard error of measurement}

$$
\text{standard error of measurement (SEM)} = \sigma_x \sqrt{1 - r_{xx}}
$$

where $\sigma_x = \text{standard deviation of observed scores on the item } x$, and $r_{xx} = \text{reliability of the item } x$.\index{reliability!standard error of measurement}
The [standard error of measurement](#standardErrorofMeasurement) is used to generate confidence intervals for people's scores.\index{reliability!standard error of measurement}
In IRT, the [standard error of measurement](#standardErrorofMeasurement) (at a given construct level) can be calculated as the inverse of the square root of the amount of test information at that construct level, as in Equation \@ref(eq:semIRT):\index{item response theory}\index{item response theory!information}\index{reliability}\index{reliability!standard error of measurement}\index{item response theory!standard error of measurement}\index{item response theory!test information curve}

$$
\begin{equation}
\text{SEM}(\theta) = \frac{1}{\sqrt{\text{information}(\theta)}}
(\#eq:semIRT)
\end{equation}
$$

The `petersenlab` package [@R-petersenlab] contains the `standardErrorIRT()` function that estimates the [standard error of measurement](#standardErrorofMeasurement) at a person's level on the construct (theta) from the amount of information that the item (or test) provides.\index{petersenlab package}\index{item response theory}\index{item response theory!information}\index{reliability}\index{reliability!standard error of measurement}\index{item response theory!standard error of measurement}\index{item response theory!theta}

```{r}
standardErrorIRT(0.6)
```

The [standard error of measurement](#standardErrorofMeasurement) tends to be higher (i.e., [reliability](#reliability)/information tends to be lower) at the extreme levels of the construct where there are fewer items.\index{item response theory}\index{item response theory!information}\index{reliability}\index{reliability!standard error of measurement}\index{item response theory!standard error of measurement}

The formula for information for item $i$ at construct level $\theta$ in a Rasch model is in Equation \@ref(eq:itemInformationRasch) [@Baker2017]:\index{item response theory}\index{item response theory!information}\index{reliability}\index{item response theory!theta}

$$
\begin{equation}
\text{information}_i(\theta) = P_i(\theta)Q_i(\theta)
(\#eq:itemInformationRasch)
\end{equation}
$$

where $P_i(\theta)$ is the probability of getting a one instead of a zero on item $i$ at a given level on the latent construct, and $Q_i(\theta) = 1 - P_i(\theta)$.\index{item response theory}\index{item response theory!information}\index{reliability}\index{item response theory!theta}

The `petersenlab` package [@R-petersenlab] contains the `itemInformation()` function that estimates the amount of information an item provides as function of the item characteristics from the Rasch model and the person's level on the construct (theta).\index{petersenlab package}\index{item response theory}\index{item response theory!information}\index{reliability}\index{item response theory!theta}
To estimate the amount of information an item provides in a Rasch model, specify $b$ and $\theta$, while keeping the defaults for the other parameters.\index{item response theory}\index{item response theory!information}\index{reliability}\index{item response theory!theta}\index{item response theory!one-parameter model}\index{item response theory!item difficulty}\index{item response theory!theta}

```{r}
itemInformation(b = 1, theta = 0)
```

The formula for information for item $i$ at construct level $\theta$ in a two-parameter logistic model is in Equation \@ref(eq:itemInformation2PL) [@Baker2017]:\index{item response theory}\index{item response theory!information}\index{reliability}\index{item response theory!theta}\index{item response theory!two-parameter model}

$$
\begin{equation}
\text{information}_i(\theta) = a^2_iP_i(\theta)Q_i(\theta)
(\#eq:itemInformation2PL)
\end{equation}
$$

To estimate the amount of information an item provides in a two-parameter logistic model, specify $a$, $b$, and $\theta$, while keeping the defaults for the other parameters.\index{item response theory}\index{item response theory!information}\index{reliability}\index{item response theory!theta}\index{item response theory!one-parameter model}\index{item response theory!item difficulty}\index{item response theory!item discrimination}\index{item response theory!theta}

```{r}
itemInformation(a = 0.6, b = 0, theta = -1)
```

The formula for information for item $i$ at construct level $\theta$ in a three-parameter logistic model is in Equation \@ref(eq:itemInformation3PL) [@Baker2017]:\index{item response theory}\index{item response theory!information}\index{reliability}\index{item response theory!theta}\index{item response theory!three-parameter model}

$$
\begin{equation}
\text{information}_i(\theta) = a^2_i\bigg[\frac{Q_i(\theta)}{P_i(\theta)}\bigg]\bigg[\frac{(P_i(\theta) - c_i)^2}{(1 - c_i)^2}\bigg]
(\#eq:itemInformation3PL)
\end{equation}
$$

To estimate the amount of information an item provides in a three-parameter logistic model, specify $a$, $b$, $c$, and $\theta$, while keeping the defaults for the other parameters.\index{item response theory}\index{item response theory!information}\index{reliability}\index{item response theory!theta}\index{item response theory!three-parameter model}\index{item response theory!item difficulty}\index{item response theory!item discrimination}\index{item response theory!item guessing}\index{item response theory!theta}

```{r}
itemInformation(a = 0.8, b = -1, c = .25, theta = -1)
```

The formula for information for item $i$ at construct level $\theta$ in a four-parameter logistic model is shown in Equation \@ref(eq:itemInformation4PL) [@Magis2013]:\index{item response theory}\index{item response theory!information}\index{reliability}\index{item response theory!theta}\index{item response theory!four-parameter model}

$$
\begin{equation}
\text{information}_i(\theta) = \frac{a^2_i[P_i(\theta) - c_i]^2[d_i - P_i(\theta)^2]}{(d_i - c_i)^2 P_i(\theta)[1 - P_i(\theta)]}
(\#eq:itemInformation4PL)
\end{equation}
$$

To estimate the amount of information an item provides in a four-parameter logistic model, specify $a$, $b$, $c$, $d$, and $\theta$.\index{item response theory}\index{item response theory!information}\index{reliability}\index{item response theory!theta}\index{item response theory!four-parameter model}\index{item response theory!item difficulty}\index{item response theory!item discrimination}\index{item response theory!item guessing}\index{item response theory!item careless errors}\index{item response theory!theta}

```{r}
itemInformation(a = 1.5, b = 1, c = .15, d = 0.85, theta = 3)
```

[Reliability](#irtReliability) at a given level of the construct ($\theta$) can be estimated as in Equation \@ref(eq:reliabilityIRT):\index{item response theory}\index{item response theory!information}\index{reliability}\index{item response theory!theta}

$$
\begin{aligned}
  \text{reliability}(\theta) &= \frac{\text{information}(\theta)}{\text{information}(\theta) + \sigma^2(\theta)} \\
  &= \frac{\text{information}(\theta)}{\text{information}(\theta) + 1}
\end{aligned}
(\#eq:reliabilityIRT)
$$

where $\sigma^2(\theta)$ is the variance of theta, which is fixed to one in most IRT models.\index{item response theory}\index{item response theory!information}\index{reliability}\index{item response theory!theta}

The `petersenlab` package [@R-petersenlab] contains the `reliabilityIRT()` function that estimates the amount of [reliability](#irtReliability) an item or a measure provides as function of its information and the variance of people's construct levels ($\theta$).\index{petersenlab package}\index{item response theory}\index{item response theory!information}\index{reliability}\index{item response theory!theta}

```{r}
reliabilityIRT(10)
```

Consider some hypothetical items depicted with [ICCs](#icc) in Figure \@ref(fig:reliabilityIRTicc).\index{item response theory!item characteristic curve}

```{r, include = FALSE}
#https://journals.sagepub.com/doi/full/10.1177/0146621613475471
irtReliability <- data.frame(theta = seq(from = -4, to = 4, length.out = 1000))

irtReliability$item1 <- fourPL(b = -1, a = 1, theta = irtReliability$theta)
irtReliability$item2 <- fourPL(b = 0, a = 0.6, theta = irtReliability$theta)
irtReliability$item3 <- fourPL(b = 1, a = 1.5, theta = irtReliability$theta)
irtReliability$item4 <- fourPL(b = 2, a = 2, theta = irtReliability$theta)

irtInformation <- data.frame(theta = seq(from = -4, to = 4, length.out = 1000))

irtInformation$information1 <- itemInformation(b = -1, a = 1, theta = irtInformation$theta)
irtInformation$information2 <- itemInformation(b = 0, a = 0.6, theta = irtInformation$theta)
irtInformation$information3 <- itemInformation(b = 1, a = 1.5, theta = irtInformation$theta)
irtInformation$information4 <- itemInformation(b = 2, a = 2, theta = irtInformation$theta)

midpoint_item1 <- irtReliability$theta[which.min(abs(irtReliability$item1 - 0.5))]
midpoint_item2 <- irtReliability$theta[which.min(abs(irtReliability$item2 - 0.5))]
midpoint_item3 <- irtReliability$theta[which.min(abs(irtReliability$item3 - 0.5))]
midpoint_item4 <- irtReliability$theta[which.min(abs(irtReliability$item4 - 0.5))]

irtInformation$testInformation <- rowSums(irtInformation[,c("information1","information2","information3","information4")])
irtInformation$standardError <- standardErrorIRT(irtInformation$testInformation)
irtInformation$reliability <- reliabilityIRT(irtInformation$testInformation)

irtReliability_long <- pivot_longer(irtReliability, cols = item1:item4) %>%
  rename(item = name)

irtInformation_long <- pivot_longer(irtInformation, cols = information1:information4) %>% 
  rename(item = name)

irtReliability_long$Item <- NA
irtReliability_long$Item[which(irtReliability_long$item == "item1")] <- 1
irtReliability_long$Item[which(irtReliability_long$item == "item2")] <- 2
irtReliability_long$Item[which(irtReliability_long$item == "item3")] <- 3
irtReliability_long$Item[which(irtReliability_long$item == "item4")] <- 4

irtInformation_long$Item <- NA
irtInformation_long$Item[which(irtInformation_long$item == "information1")] <- 1
irtInformation_long$Item[which(irtInformation_long$item == "information2")] <- 2
irtInformation_long$Item[which(irtInformation_long$item == "information3")] <- 3
irtInformation_long$Item[which(irtInformation_long$item == "information4")] <- 4
```

(ref:reliabilityIRTiccCaption) Item Characteristic Curves from Two-Parameter Logistic Model in Item Response Theory. The dashed horizontal line indicates a probability of item endorsement of .50. The dashed vertical line is the item difficulty, i.e., the person's level on the construct (the location on the x-axis) at the inflection point of the item characteristic curve. In a two-parameter logistic model, the inflection point corresponds to the probability of item endorsement is 50%. Thus, in a two-parameter logistic model, the difficulty of an item is the person's level on the construct where the probability of endorsing the item is 50%.

```{r reliabilityIRTicc, echo = FALSE, results = "hide", out.width = "100%", fig.align = "center", fig.cap = "(ref:reliabilityIRTiccCaption)", fig.scap = "Item Characteristic Curves from Two-Parameter Logistic Model in Item Response Theory."}
ggplot(irtReliability_long, aes(theta, value, group = factor(Item), color = factor(Item))) +
  geom_line(linewidth = 1.5) +
  labs(color = "Item") +
  scale_color_grey() +
  geom_hline(yintercept = 0.5, linetype = "dashed") +
  geom_segment(aes(x = midpoint_item1, xend = midpoint_item1, y = 0, yend = 0.5), linewidth = 0.5, col = "black", linetype = "dashed") +
  geom_segment(aes(x = midpoint_item2, xend = midpoint_item2, y = 0, yend = 0.5), linewidth = 0.5, col = "black", linetype = "dashed") +
  geom_segment(aes(x = midpoint_item3, xend = midpoint_item3, y = 0, yend = 0.5), linewidth = 0.5, col = "black", linetype = "dashed") +
  geom_segment(aes(x = midpoint_item4, xend = midpoint_item4, y = 0, yend = 0.5), linewidth = 0.5, col = "black", linetype = "dashed") +
  scale_x_continuous(name = expression(paste("Person's Level on the Latent Construct (", theta, ")", sep = "")), breaks = -4:4) +
  scale_y_continuous(name = "Probability of Item Endorsement") +
  theme_bw()
```

We can present the [ICC](#icc) in terms of an item information curve (see Figure \@ref(fig:reliabilityIRTitemInformation)).\index{item response theory}\index{item response theory!item characteristic curve}\index{item response theory!information}\index{reliability}\index{item response theory!item information curve}
On the x-axis, the information peak is located at the [difficulty/severity](#itemDifficulty) of the item.
The higher the [discrimination](#itemDiscrimination), the higher the information peak on the y-axis.\index{item response theory}\index{item response theory!item characteristic curve}\index{item response theory!information}\index{reliability}\index{item response theory!item difficulty}\index{item response theory!item discrimination}

```{r reliabilityIRTitemInformation, echo = FALSE, results = "hide", out.width = "100%", fig.align = "center", fig.cap = "Item Information from Two-Parameter Logistic Model in Item Response Theory. The dashed vertical line is the item difficulty, which is located at the peak of the item information curve.", fig.scap = "Item Information from Two-Parameter Logistic Model in Item Response Theory."}
ggplot(irtInformation_long, aes(theta, value, group = factor(Item), color = factor(Item))) +
  geom_line(linewidth = 1.5) +
  labs(color = "Item") +
  scale_color_grey() +
  geom_vline(xintercept = midpoint_item1, linetype = "dashed") +
  geom_vline(xintercept = midpoint_item2, linetype = "dashed") +
  geom_vline(xintercept = midpoint_item3, linetype = "dashed") +
  geom_vline(xintercept = midpoint_item4, linetype = "dashed") +
  scale_x_continuous(name = expression(paste("Person's Level on the Latent Construct (", theta, ")", sep = "")), breaks = -4:4) +
  scale_y_continuous(name = "Item Information") +
  theme_bw()
```

We can aggregate (sum) information across items to determine how much information the measure as a whole provides.\index{item response theory}\index{item response theory!item characteristic curve}\index{item response theory!information}\index{reliability}\index{item response theory!test information curve}
This is called the test information curve (see Figure \@ref(fig:reliabilityIRTtestInformation)).\index{item response theory}\index{item response theory!item characteristic curve}\index{item response theory!information}\index{reliability}\index{item response theory!test information curve}
Note that we get more information from likert/multiple response items compared to binary/dichotomous items.\index{item response theory}\index{item response theory!item characteristic curve}\index{item response theory!information}\index{reliability}\index{data!dichotomous}\index{data!polytomous}\index{item response theory!test information curve}\index{Likert scale}
Having 10 items with a 5-level response scale yields as much information as 40 dichotomous items.\index{item response theory}\index{item response theory!item characteristic curve}\index{item response theory!information}\index{reliability}\index{item response theory!test information curve}

```{r reliabilityIRTtestInformation, echo = FALSE, results = "hide", out.width = "100%", fig.align = "center", fig.cap = "Test Information Curve from Two-Parameter Logistic Model in Item Response Theory."}
ggplot(irtInformation, aes(theta, testInformation)) +
  geom_line(linewidth = 1.5) +
  scale_color_grey() +
  scale_x_continuous(name = expression(paste("Person's Level on the Latent Construct (", theta, ")", sep = "")), breaks = -4:4) +
  scale_y_continuous(name = "Test Information", limits = c(0,2)) +
  theme_bw()
```

Based on test information, we can calculate the [standard error of measurement](#standardErrorOfMeasurement) (see Figure \@ref(fig:reliabilityIRTtestSE)).\index{item response theory}\index{item response theory!item characteristic curve}\index{item response theory!information}\index{reliability}\index{reliability!standard error of measurement}\index{item response theory!standard error of measurement}\index{item response theory!test information curve}
Notice how the degree of [(un)reliability](#reliability) differs at different construct levels.\index{item response theory}\index{item response theory!item characteristic curve}\index{item response theory!information}\index{reliability}\index{reliability!standard error of measurement}\index{item response theory!standard error of measurement}\index{item response theory!test information curve}

```{r reliabilityIRTtestSE, echo = FALSE, results = "hide", out.width = "100%", fig.align = "center", fig.cap = "Test Standard Error of Measurement from Two-Parameter Logistic Model in Item Response Theory."}
ggplot(irtInformation, aes(theta, standardError)) +
  geom_line(linewidth = 1.5) +
  scale_color_grey() +
  scale_x_continuous(name = expression(paste("Person's Level on the Latent Construct (", theta, ")", sep = "")), breaks = -4:4) +
  scale_y_continuous(name = "Standard Error of Measurement", limits = c(0,4)) +
  theme_bw()
```

Based on test information, we can estimate the [reliability](#irtReliability) (see Figure \@ref(fig:reliabilityIRTtestReliability)).\index{item response theory}\index{item response theory!item characteristic curve}\index{item response theory!information}\index{reliability}\index{reliability!standard error of measurement}\index{item response theory!standard error of measurement}\index{item response theory!test information curve}
Notice how the degree of [(un)reliability](#reliability) differs at different construct levels.\index{item response theory}\index{item response theory!item characteristic curve}\index{item response theory!information}\index{reliability}\index{reliability!standard error of measurement}\index{item response theory!standard error of measurement}

```{r reliabilityIRTtestReliability, echo = FALSE, results = "hide", out.width = "100%", fig.align = "center", fig.cap = "Test Reliability from Two-Parameter Logistic Model in Item Response Theory."}
ggplot(irtInformation, aes(theta, reliability)) +
  geom_line(linewidth = 1.5) +
  scale_color_grey() +
  scale_x_continuous(name = expression(paste("Person's Level on the Latent Construct (", theta, ")", sep = "")), breaks = -4:4) +
  scale_y_continuous(name = "Reliability", limits = c(0,1)) +
  theme_bw()
```

### Efficient Assessment {#efficientAssessment}

One of the benefits of IRT is for item selection to develop brief assessments.\index{item response theory}\index{brief assessment}
For instance, you could use two items to estimate where the person is on the construct: low, middle, or high (see Figure \@ref(fig:efficientAssessment)).\index{item response theory}\index{brief assessment}
If the responses to the two items do not meet expectations, for instance, the person passes the difficult item but fails the easy item, we would keep assessing additional items to determine their level on the construct.\index{item response theory}\index{brief assessment}
If two items perform similarly, that is, they have the same [difficulty](#itemDifficulty) and [discrimination](#itemDiscrimination), they are redundant, and we can sacrifice one of them.\index{item response theory}\index{brief assessment}\index{item response theory!item difficulty}\index{item response theory!item discrimination}
This leads to greater efficiency and better measurement in terms of [reliability](#reliability) and [validity](#validity).\index{item response theory}\index{brief assessment}\index{reliability}\index{validity}
For more information on designing and evaluating short forms compared to their full-scale counterparts, see @Smith2000.\index{brief assessment}

```{r, include = FALSE}
efficientAssessment <- data.frame(theta = seq(from = -4, to = 4, length.out = 1000))

efficientAssessment$item1 <- fourPL(b = -1.5, a = 2, theta = efficientAssessment$theta)
efficientAssessment$item2 <- fourPL(b = 1.5, a = 2, theta = efficientAssessment$theta)

efficientAssessment_long <- pivot_longer(efficientAssessment, cols = item1:item2) %>%
  rename(item = name)

efficientAssessment_long$Item <- NA
efficientAssessment_long$Item[which(efficientAssessment_long$item == "item1")] <- 1
efficientAssessment_long$Item[which(efficientAssessment_long$item == "item2")] <- 2
```

```{r efficientAssessment, echo = FALSE, results = "hide", out.width = "100%", fig.align = "center", fig.cap = "Visual Representation of an Efficient Assessment Based on Item Characteristic Curves from Two-Parameter Logistic Model in Item Response Theory."}
ggplot(efficientAssessment_long, aes(theta, value, group = factor(Item), color = factor(Item))) +
  geom_line(linewidth = 1.5) +
  labs(color = "Item") +
  scale_color_grey() +
  scale_x_continuous(name = expression(paste("Person's Level on the Latent Construct (", theta, ")", sep = "")), breaks = -4:4) +
  scale_y_continuous(name = "Probability of Item Endorsement") +
  theme_bw()
```

IRT forms the basis of [computerized adaptive testing](#cat), which is discussed in Chapter \@ref(cat).\index{item response theory}\index{brief assessment}\index{adaptive testing}
As discussed earlier, briefer measures can increase [reliability](#reliability) and [validity](#validity) of measurement if the items are tailored to the ability level of the participant.\index{brief assessment}\index{reliability}\index{validity}
The idea of [adaptive testing](#cat) is that, instead of having a standard scale for all participants, the items adapt to each person.\index{brief assessment}\index{adaptive testing}
An example of a measure that has used [computerized adaptive testing](#cat) is the Graduate Record Examination (GRE).\index{adaptive testing}

With [adaptive testing](#cat), it is important to develop a comprehensive item bank that spans the difficulty range of interest.\index{adaptive testing}
The starting construct level is the 50th percentile.
If the respondent gets the first item correct, it moves moves to the next item that would provide the most [information](#irtReliability) for the person, based on a split of the remaining sample (e.g., 75th percentile).\index{adaptive testing}\index{reliability}\index{item response theory!information}
And so on...\index{adaptive testing}
The goal of [adaptive testing](#cat) is to find the construct level where the respondent keeps getting items right and wrong 50% of the time.\index{adaptive testing}
[Adaptive testing](#cat) is a promising approach that saves time because it tailors which items are administered to which person (based on their construct level) to get the most [reliable](#reliability) estimate in the shortest time possible.\index{adaptive testing}\index{reliability}\index{brief assessment}
However, it assumes that if you get a more difficult item correct, that you would have gotten easier items correct, which might not be true in all contexts (especially for constructs that are not unidimensional).\index{adaptive testing}

Although most uses of IRT have been in cognitive and educational testing, IRT may also benefit other domains of assessment including clinical assessment [@Gibbons2016; @Reise2009; @Thomas2019].\index{item response theory}

#### A Good Measure {#goodMeasure}

According to IRT, a good measure should:\index{item response theory}

1. fit your goals of the assessment, in terms of the range of interest regarding levels on the construct,\index{item response theory}
1. have good items that yield lots of [information](#irtReliability), and\index{item response theory}\index{item response theory!information}\index{reliability}
1. have a good set of items that densely cover the construct within the range of interest, without redundancy.\index{item response theory}

First, a good measure should fit your goals of the assessment, in terms of the "range of interest" or the "target range" of levels on the construct.\index{item response theory}
For instance, if your goal is to perform diagnosis, you would only care about the high end of the construct (e.g., 1–3 standard deviations above the mean)—there is no use discriminating between "nothing", "almost nothing", and "a little bit."\index{item response theory}
For secondary prevention, i.e., early identification of risk to prevent something from getting worse, you would be interested in finding people with elevated risk—e.g., you would need to know who is 1 or more standard deviations above the mean, but you would not need to discriminate beyond that.\index{item response theory}
For assessing individual differences, you would want items that discriminate across the full range, including at the lower end.\index{item response theory}
The items' [difficulty](#itemDifficulty) should span the range of interest.\index{item response theory}\index{item response theory!item difficulty}

Second, a good measure should have good items that yield lots of [information](#irtReliability).\index{item response theory}\index{item response theory!information}\index{reliability}
For example, the items should have strong [discrimination](#itemDiscrimination), that is, the items are strongly related to the construct.\index{item response theory}\index{item response theory!item discrimination}
The items should have sufficient variability in responses.\index{item response theory}
This can be achieved by having items with more response options (e.g., likert/multiple choice items, as opposed to binary items), items that differ in difficulty, and (at least some) items that are not too difficult or too easy (to avoid ceiling/floor effects).\index{item response theory}\index{data!dichotomous}\index{data!polytomous}\index{Likert scale}

Third, a good measure should have a good set of items that densely cover the construct within the range of interest, without redundancy.\index{item response theory}
The items should not have the same [difficulty](#itemDifficulty) or they would be considered redundant, and one of the redundant items could be dropped.\index{item response theory}\index{item response theory!item difficulty}
The items' [difficulty](#itemDifficulty) should densely cover the construct within the range of interest.\index{item response theory}\index{item response theory!item difficulty}
For instance, if the construct range of interest is 1–2 standard deviations above the mean, the items should have [difficulty](#itemDifficulty) that densely cover this range (e.g., 1.0, 1.05, 1.10, 1.15, 1.20, 1.25, 1.30, ..., 2.0).\index{item response theory}\index{item response theory!item difficulty}

With items that (1) span the range of interest, (2) have high [discrimination](#itemDiscrimation) and [information](#irtReliability), and (3) densely cover the range of interest without redundancy, the measure should have a high [information](#irtReliability) in the range of interest.\index{item response theory}\index{item response theory!item difficulty}\index{item response theory!item discrimination}\index{item response theory!information}\index{reliability}
This would allow it to efficiently and accurately assess the construct for the intended purpose.\index{item response theory}\index{brief assessment}

An example of a bad measure for assessing the full range of individual differences is depicted in terms of [ICCs](#icc) in Figure \@ref(fig:badMeasureICCs) and in terms of [test information](#irtReliability) in Figure \@ref(fig:badMeasureInfo).\index{item response theory}\index{item response theory!information}\index{reliability}\index{item response theory!test information curve}
The measure performs poorly for the intended purpose, because its items do not (a) span the range of interest (−3 to 3 standard deviations from the mean of the latent construct), (b) have high [discrimination](#itemDiscrimation) and [information](#irtReliability), and (c) densely cover the range of interest without redundancy.\index{item response theory}\index{item response theory!information}\index{item response theory!item discrimination}\index{reliability}

```{r, include = FALSE}
badMeasure <- badMeasureInfo <- data.frame(theta = seq(from = -4, to = 4, length.out = 1000))

badMeasure$item1 <- fourPL(b = -2.5, a = 0.4, theta = badMeasure$theta)
badMeasure$item2 <- fourPL(b = -2.4, a = 1, theta = badMeasure$theta)
badMeasure$item3 <- fourPL(b = -2.3, a = 0.8, theta = badMeasure$theta)
badMeasure$item4 <- fourPL(b = -2.2, a = 1.5, theta = badMeasure$theta)
badMeasure$item5 <- fourPL(b = 2.0, a = 0.6, theta = badMeasure$theta)
badMeasure$item6 <- fourPL(b = 2.1, a = 1.2, theta = badMeasure$theta)
badMeasure$item7 <- fourPL(b = 2.2, a = 0.8, theta = badMeasure$theta)
badMeasure$item8 <- fourPL(b = 2.3, a = 1.3, theta = badMeasure$theta)
badMeasure$item9 <- fourPL(b = 2.4, a = 0.6, theta = badMeasure$theta)
badMeasure$item10 <- fourPL(b = 2.5, a = 0.5, theta = badMeasure$theta)

badMeasureInfo$info1 <- itemInformation(b = -2.5, a = 0.4, theta = badMeasureInfo$theta)
badMeasureInfo$info2 <- itemInformation(b = -2.4, a = 1, theta = badMeasureInfo$theta)
badMeasureInfo$info3 <- itemInformation(b = -2.3, a = 0.8, theta = badMeasureInfo$theta)
badMeasureInfo$info4 <- itemInformation(b = -2.2, a = 1.5, theta = badMeasureInfo$theta)
badMeasureInfo$info5 <- itemInformation(b = 2.0, a = 0.6, theta = badMeasureInfo$theta)
badMeasureInfo$info6 <- itemInformation(b = 2.1, a = 1.2, theta = badMeasureInfo$theta)
badMeasureInfo$info7 <- itemInformation(b = 2.2, a = 0.8, theta = badMeasureInfo$theta)
badMeasureInfo$info8 <- itemInformation(b = 2.3, a = 1.3, theta = badMeasureInfo$theta)
badMeasureInfo$info9 <- itemInformation(b = 2.4, a = 0.6, theta = badMeasureInfo$theta)
badMeasureInfo$info10 <- itemInformation(b = 2.5, a = 0.5, theta = badMeasureInfo$theta)

badMeasureInfo$information <- rowSums(badMeasureInfo[,paste("info", 1:10, sep = "")])

badMeasure_long <- pivot_longer(badMeasure, cols = item1:item10) %>%
  rename(item = name)

badMeasure_long$Item <- NA
badMeasure_long$Item[which(badMeasure_long$item == "item1")] <- 1
badMeasure_long$Item[which(badMeasure_long$item == "item2")] <- 2
badMeasure_long$Item[which(badMeasure_long$item == "item3")] <- 3
badMeasure_long$Item[which(badMeasure_long$item == "item4")] <- 4
badMeasure_long$Item[which(badMeasure_long$item == "item5")] <- 5
badMeasure_long$Item[which(badMeasure_long$item == "item6")] <- 6
badMeasure_long$Item[which(badMeasure_long$item == "item7")] <- 7
badMeasure_long$Item[which(badMeasure_long$item == "item8")] <- 8
badMeasure_long$Item[which(badMeasure_long$item == "item9")] <- 9
badMeasure_long$Item[which(badMeasure_long$item == "item10")] <- 10
```

```{r badMeasureICCs, echo = FALSE, results = "hide", out.width = "100%", fig.align = "center", fig.cap = "Visual Representation of a Bad Measure Based on Item Characteristic Curves of Items from a Bad Measure Estimated from Two-Parameter Logistic Model in Item Response Theory."}
ggplot(badMeasure_long, aes(theta, value, group = factor(Item), color = factor(Item))) +
  geom_line(linewidth = 1.5) +
  labs(color = "Item") +
  scale_color_grey() +
  scale_x_continuous(name = expression(paste("Person's Level on the Latent Construct (", theta, ")", sep = "")), breaks = -4:4) +
  scale_y_continuous(name = "Probability of Item Endorsement") +
  theme_bw()
```

```{r badMeasureInfo, echo = FALSE, results = "hide", out.width = "100%", fig.cap = "Visual Representation of a Bad Measure Based on the Test Information Curve."}
ggplot(badMeasureInfo, aes(theta, information)) +
  geom_line(linewidth = 1.5) +
  scale_color_grey() +
  scale_x_continuous(name = expression(paste("Person's Level on the Latent Construct (", theta, ")", sep = "")), breaks = -4:4) +
  scale_y_continuous(name = "Information") +
  theme_bw()
```

An example of a good measure for distinguishing clinical-range versus sub-clinical range is depicted in terms of [ICCs](#icc) in Figure \@ref(fig:goodMeasureICCs) and in terms of [test information](#irtReliability) in Figure \@ref(fig:goodMeasureInfo).\index{item response theory}\index{item response theory!information}\index{reliability}\index{item response theory!item characteristic curve}\index{item response theory!test information curve}
The measure is good for the intended purpose, in terms of having items that (a) span the range of interest (1–3 standard deviations above the mean of the latent construct), (b) have high [discrimination](#itemDiscrimation) and [information](#irtReliability), and (c) densely cover the range of interest without redundancy.\index{item response theory}\index{item response theory!information}\index{item response theory!item discrimination}\index{reliability}\index{item response theory!test information curve}

```{r, include = FALSE}
goodMeasure <- goodMeasureInfo <- data.frame(theta = seq(from = -4, to = 4, length.out = 1000))

goodMeasure$item1 <-  fourPL(b = 1.0, a = 10, theta = goodMeasure$theta)
goodMeasure$item2 <-  fourPL(b = 1.1, a = 10, theta = goodMeasure$theta)
goodMeasure$item3 <-  fourPL(b = 1.2, a = 10, theta = goodMeasure$theta)
goodMeasure$item4 <-  fourPL(b = 1.3, a = 10, theta = goodMeasure$theta)
goodMeasure$item5 <-  fourPL(b = 1.4, a = 10, theta = goodMeasure$theta)
goodMeasure$item6 <-  fourPL(b = 1.5, a = 10, theta = goodMeasure$theta)
goodMeasure$item7 <-  fourPL(b = 1.6, a = 10, theta = goodMeasure$theta)
goodMeasure$item8 <-  fourPL(b = 1.7, a = 10, theta = goodMeasure$theta)
goodMeasure$item9 <-  fourPL(b = 1.8, a = 10, theta = goodMeasure$theta)
goodMeasure$item10 <- fourPL(b = 1.9, a = 10, theta = goodMeasure$theta)
goodMeasure$item11 <- fourPL(b = 2.0, a = 10, theta = goodMeasure$theta)
goodMeasure$item12 <- fourPL(b = 2.1, a = 10, theta = goodMeasure$theta)
goodMeasure$item13 <- fourPL(b = 2.2, a = 10, theta = goodMeasure$theta)
goodMeasure$item14 <- fourPL(b = 2.3, a = 10, theta = goodMeasure$theta)
goodMeasure$item15 <- fourPL(b = 2.4, a = 10, theta = goodMeasure$theta)
goodMeasure$item16 <- fourPL(b = 2.5, a = 10, theta = goodMeasure$theta)
goodMeasure$item17 <- fourPL(b = 2.6, a = 10, theta = goodMeasure$theta)
goodMeasure$item18 <- fourPL(b = 2.7, a = 10, theta = goodMeasure$theta)
goodMeasure$item19 <- fourPL(b = 2.8, a = 10, theta = goodMeasure$theta)
goodMeasure$item20 <- fourPL(b = 2.9, a = 10, theta = goodMeasure$theta)

goodMeasureInfo$info1 <- itemInformation(b = 1.0, a = 10, theta = goodMeasureInfo$theta)
goodMeasureInfo$info2 <- itemInformation(b = 1.1, a = 10, theta = goodMeasureInfo$theta)
goodMeasureInfo$info3 <- itemInformation(b = 1.2, a = 10, theta = goodMeasureInfo$theta)
goodMeasureInfo$info4 <- itemInformation(b = 1.3, a = 10, theta = goodMeasureInfo$theta)
goodMeasureInfo$info5 <- itemInformation(b = 1.4, a = 10, theta = goodMeasureInfo$theta)
goodMeasureInfo$info6 <- itemInformation(b = 1.5, a = 10, theta = goodMeasureInfo$theta)
goodMeasureInfo$info7 <- itemInformation(b = 1.6, a = 10, theta = goodMeasureInfo$theta)
goodMeasureInfo$info8 <- itemInformation(b = 1.7, a = 10, theta = goodMeasureInfo$theta)
goodMeasureInfo$info9 <- itemInformation(b = 1.8, a = 10, theta = goodMeasureInfo$theta)
goodMeasureInfo$info10 <- itemInformation(b = 1.9, a = 10, theta = goodMeasureInfo$theta)
goodMeasureInfo$info11 <- itemInformation(b = 2.0, a = 10, theta = goodMeasureInfo$theta)
goodMeasureInfo$info12 <- itemInformation(b = 2.1, a = 10, theta = goodMeasureInfo$theta)
goodMeasureInfo$info13 <- itemInformation(b = 2.2, a = 10, theta = goodMeasureInfo$theta)
goodMeasureInfo$info14 <- itemInformation(b = 2.3, a = 10, theta = goodMeasureInfo$theta)
goodMeasureInfo$info15 <- itemInformation(b = 2.4, a = 10, theta = goodMeasureInfo$theta)
goodMeasureInfo$info16 <- itemInformation(b = 2.5, a = 10, theta = goodMeasureInfo$theta)
goodMeasureInfo$info17 <- itemInformation(b = 2.6, a = 10, theta = goodMeasureInfo$theta)
goodMeasureInfo$info18 <- itemInformation(b = 2.7, a = 10, theta = goodMeasureInfo$theta)
goodMeasureInfo$info19 <- itemInformation(b = 2.8, a = 10, theta = goodMeasureInfo$theta)
goodMeasureInfo$info20 <- itemInformation(b = 2.9, a = 10, theta = goodMeasureInfo$theta)

goodMeasureInfo$information <- rowSums(goodMeasureInfo[,paste("info", 1:20, sep = "")])

goodMeasure_long <- pivot_longer(goodMeasure, cols = item1:item20) %>%
  rename(item = name)

goodMeasure_long$Item <- NA
goodMeasure_long$Item[which(goodMeasure_long$item == "item1")] <- 1
goodMeasure_long$Item[which(goodMeasure_long$item == "item2")] <- 2
goodMeasure_long$Item[which(goodMeasure_long$item == "item3")] <- 3
goodMeasure_long$Item[which(goodMeasure_long$item == "item4")] <- 4
goodMeasure_long$Item[which(goodMeasure_long$item == "item5")] <- 5
goodMeasure_long$Item[which(goodMeasure_long$item == "item6")] <- 6
goodMeasure_long$Item[which(goodMeasure_long$item == "item7")] <- 7
goodMeasure_long$Item[which(goodMeasure_long$item == "item8")] <- 8
goodMeasure_long$Item[which(goodMeasure_long$item == "item9")] <- 9
goodMeasure_long$Item[which(goodMeasure_long$item == "item10")] <- 10
goodMeasure_long$Item[which(goodMeasure_long$item == "item11")] <- 11
goodMeasure_long$Item[which(goodMeasure_long$item == "item12")] <- 12
goodMeasure_long$Item[which(goodMeasure_long$item == "item13")] <- 13
goodMeasure_long$Item[which(goodMeasure_long$item == "item14")] <- 14
goodMeasure_long$Item[which(goodMeasure_long$item == "item15")] <- 15
goodMeasure_long$Item[which(goodMeasure_long$item == "item16")] <- 16
goodMeasure_long$Item[which(goodMeasure_long$item == "item17")] <- 17
goodMeasure_long$Item[which(goodMeasure_long$item == "item18")] <- 18
goodMeasure_long$Item[which(goodMeasure_long$item == "item19")] <- 19
goodMeasure_long$Item[which(goodMeasure_long$item == "item20")] <- 20
```

```{r goodMeasureICCs, echo = FALSE, results = "hide", out.width = "100%", fig.align = "center", fig.align = "center", fig.cap = "Visual Representation of a Good Measure Based on Item Characteristic Curves of Items from a Good Measure Estimated from Two-Parameter Logistic Model in Item Response Theory."}
ggplot(goodMeasure_long, aes(theta, value, group = factor(Item), color = factor(Item))) +
  geom_line(linewidth = 1.5) +
  labs(color = "Item") +
  scale_color_grey() +
  scale_x_continuous(name = expression(paste("Person's Level on the Latent Construct (", theta, ")", sep = "")), breaks = -4:4) +
  scale_y_continuous(name = "Probability of Item Endorsement") +
  theme_bw() +
  theme(legend.key.size = unit(0.15, "in"))
```

```{r goodMeasureInfo, echo = FALSE, results = "hide", out.width = "100%", fig.cap = "Visual Representation of a Good Measure (for Distinguishing Clinical-Range Versus Sub-clinical Range) Based on the Test Information Curve."}
ggplot(goodMeasureInfo, aes(theta, information)) +
  geom_line(linewidth = 1.5) +
  scale_color_grey() +
  scale_x_continuous(name = expression(paste("Person's Level on the Latent Construct (", theta, ")", sep = "")), breaks = -4:4) +
  scale_y_continuous(name = "Information") +
  theme_bw()
```

## Getting Started {#gettingStarted-irt}

### Load Libraries {#loadLibraries-irt}

```{r}
library("petersenlab")
library("mirt")
library("lavaan")
library("semTools")
library("semPlot")
library("lme4")
library("MOTE")
library("here")
library("tidyverse")
library("tinytex")
library("knitr")
library("kableExtra")
library("rmarkdown")
library("bookdown")
```

### Load Data {#loadData-irt}

`LSAT7` is a data set from the `mirt` package [@R-mirt] that contains five items from the Law School Admissions Test.

```{r}
mydataIRT <- expand.table(LSAT7)
```

### Descriptive Statistics {#descriptiveStats-IRT}

```{r}
itemstats(mydataIRT, ts.tables = TRUE)
```

## Comparison of Scoring Approaches {#scoringApproaches}

A measure that is a raw symptom count (i.e., a count of how many symptoms a person endorses) is low in precision and has a high standard error of measurement.\index{reliability}\index{reliability!precision}\index{reliability!standard error of measurement}\index{scoring}
Some diagnostic measures provide an ordinal response scale for each symptom.\index{scoring}
For example, the Structured Clinical Interview of Mental Disorders (SCID) provides a response scale from 0 to 2, where 0 = the symptom is absent, 1 = the symptom is sub-threshold, and 2 = the symptom is present.\index{scoring}
If your measure was a raw symptom sum, as opposed to a count of how many symptoms were present, the measure would be slightly more precise and have a somewhat smaller standard error of measurement.\index{reliability}\index{reliability!precision}\index{reliability!standard error of measurement}\index{scoring}

A weighted symptom sum is the [classical test theory](#ctt) analog of IRT.\index{scoring}\index{classical test theory}\index{item response theory}
In [classical test theory](#ctt), proportion correct (or endorsed) would correspond to item [difficulty](#itemDifficulty) and the [item–total correlation](#itemTotalCorrelation-reliability) (i.e., a point-biserial correlation) would correspond to item [discrimination](#itemDiscrimination).\index{scoring}\index{classical test theory}\index{item response theory!item difficulty}\index{reliability!internal consistency!average item–total correlation}
If we were to compute a weighted sum of each item according to its strength of association with the construct (i.e., the [item–total correlation](#itemTotalCorrelation-reliability)), this measure would be somewhat more precise than the raw symptom sum, but it is not a latent variable method.\index{scoring}\index{reliability!internal consistency!average item–total correlation}\index{latent variable}

In IRT analysis, the weight for each item influences the estimate of a person's level on the construct.\index{item response theory}
IRT down-weights the poorly [discriminating](#itemDiscrimination) items and up-weights the strongly [discriminating](#itemDiscrimination) items.\index{item response theory}\index{item response theory!item discrimination}
This leads to greater [precision](#reliability) and a lower [standard error of measurement](#standardErrorOfMeasurement) than non-latent scoring approaches.\index{scoring}\index{item response theory}\index{latent variable}\index{reliability}\index{reliability!precision}\index{reliability!standard error of measurement}

According to @Embretson1996, many perspectives have changed because of IRT.\index{item response theory}
First, according to [classical test theory](#ctt), longer tests are more [reliable](#reliability) than shorter tests, as described in Section \@ref(splitHalf-reliability) in the chapter on [reliability](#reliability).\index{classical test theory}\index{reliability}
However, according to IRT, shorter tests (i.e., tests with fewer items) can be more [reliable](#reliability) than longer tests.\index{item response theory}\index{brief assessment}\index{reliability}
Item selection using IRT can lead to briefer assessments that have greater [reliability](#reliability) than longer scales.\index{item response theory}\index{brief assessment}\index{reliability}
For example, [adaptive tests](#cat) that tailor the [difficulty](#itemDifficulty) of the items to the ability level of the participant.\index{item response theory}\index{brief assessment}\index{reliability}\index{adaptive testing}\index{item response theory!item difficulty}

Second, in [classical test theory](#ctt), a score's meaning is tied to its location in a distribution (i.e., the [norm-referenced](#norm) standard).\index{classical test theory}
In IRT, however, the people and items are calibrated on a common scale.\index{item response theory}
Based on a child's IRT-estimated ability level (i.e., level on the construct), we can have a better sense of what the child knows and does not know, because it indicates the [difficulty](#itemDifficulty) level at which they would tend to get items correct 50% of the time; the person would likely fail items with a higher [difficulty](#itemDifficulty) compared to this level, whereas the person would likely pass items with a lower [difficulty](#itemDifficulty) compared to this level.\index{item response theory}\index{item response theory!item difficulty}\index{item response theory!theta}
Consider Binet's distribution of ability that arranges the items from easiest to most difficult.
Based on the item [difficulty](#itemDifficulty) and content of the items and the child's performance, we can have a better indication that a child can perform items successfully in a particular range (e.g., count to 10) but might not be able to perform more difficult items (e.g., tie their shoes).\index{item response theory}\index{item response theory!item difficulty}
From an intervention perspective, this would allow working in the "window of opportunity" or the zone of proximal development.
Thus, IRT can provide more meaningful understanding of a person's ability compared to traditional [classical test theory](#ctt) interpretations such as the child being at the "63rd percentile" for a child of their age, which lacks conceptual meaning.\index{item response theory}\index{classical test theory}\index{data!percentile rank}

According to @Cooper2009a, our current diagnostic system relies heavily on how many symptoms a person endorses as an index of severity, but this assumes that all that all symptom endorsements have the same overall weight (severity).\index{scoring}
Using IRT, we can determine the relative [severity](#itemDifficulty) of each item (symptom)—and it is clear that some symptoms indicate more severity than others.\index{scoring}\index{item response theory}\index{item response theory!item difficulty}
From this analysis, a respondent can endorse fewer, more severe items, and have overall more severe psychopathology than an individual who endorses more, less [severe](#itemDifficulty) items.\index{scoring}\index{item response theory}\index{item response theory!item difficulty}
Basically, not all items are equally [severe](#itemDifficulty)—know your items!\index{scoring}\index{item response theory}\index{item response theory!item difficulty}

## One-Parameter Logistic (Rasch) Model {#irt-onePL}

A one-parameter logistic (1-PL) item response theory (IRT) model is a model fit to dichotomous data, which estimates a different [difficulty](#itemDifficulty) ($b$) parameter for each item.\index{item response theory!one-parameter model}\index{item response theory!item difficulty}
[Discrimination](#itemDiscrimination) ($a$) is not estimated (i.e., it is fixed at the same value—one—across items).\index{item response theory!one-parameter model}\index{item response theory!item discrimination}
Rasch models were fit using the `mirt` package [@R-mirt].\index{item response theory!one-parameter model}

### Fit Model {#irt-onePLfit}

```{r, results = "hide"}
raschModel <- mirt(
  data = mydataIRT,
  model = 1,
  itemtype = "Rasch",
  SE = TRUE)
```

### Model Summary {#irt-onePLoutput}

```{r}
coef(raschModel, simplify = TRUE, IRTpars = TRUE)
```

### Plots {#irt-onePLplots}

#### Test Curves {#irt-onePLtestCurves}

The test curves suggest that the measure is most [reliable](#reliability) (i.e., provides the most [information](#irtReliability) has the smallest [standard error of measurement](#irtReliability)) at lower levels of the construct.\index{item response theory!information}\index{item response theory!standard error of measurement}\index{reliability!standard error of measurement}

##### Test characteristic curve {#irt-onePLtcc}

A test characteristic curve ([TCC](#icc)) plot of the expected total score as a function of a person's level on the latent construct (theta; $\theta$) is in Figure \@ref(fig:onePLtcc).\index{item response theory!item characteristic curve}

```{r onePLtcc, out.width = "100%", fig.align = "center", fig.cap = "Test Characteristic Curve from Rasch Item Response Theory Model."}
plot(raschModel, type = "score")
```

##### Test information curve {#irt-onePLtif}

A plot of [test information](#irtReliability) as a function of a person's level on the latent construct (theta; $\theta$) is shown in Figure \@ref(fig:onePLtif).\index{item response theory!information}\index{item response theory!test information curve}

```{r onePLtif, out.width = "100%", fig.align = "center", fig.cap = "Test Information Curve from Rasch Item Response Theory Model."}
plot(raschModel, type = "info")
```

##### Test reliability {#irt-onePLreliability}

The estimate of marginal [reliability](#irtReliability) is below:\index{reliability}

```{r}
marginal_rxx(raschModel)
```

A plot of test [reliability](#irtReliability) as a function of a person's level on the latent construct (theta; $\theta$) is depicted in Figure \@ref(fig:onePLreliability).\index{reliability}

```{r onePLreliability, out.width = "100%", fig.align = "center", fig.cap = "Test Reliability from Rasch Item Response Theory Model."}
plot(raschModel, type = "rxx")
```

##### Test standard error of measurement {#irt-onePLsem}

A plot of test [standard error of measurement](#irtReliability) (SEM) as a function of a person's level on the latent construct (theta; $\theta$) is shown in Figure \@ref(fig:onePLsem).\index{reliability!standard error of measurement}

```{r onePLsem, out.width = "100%", fig.align = "center", fig.cap = "Test Standard Error of Measurement from Rasch Item Response Theory Model."}
plot(raschModel, type = "SE")
```

##### Test information curve and test standard error of measurement {#irt-onePLtifSEM}

A plot of [test information](#irtReliability) and [standard error of measurement](#irtReliability) (SEM) as a function of a person's level on the latent construct (theta; $\theta$) is presented in Figure \@ref(fig:onePLtifSEM).\index{item response theory!information}\index{item response theory!standard error of measurement}\index{reliability!standard error of measurement}\index{item response theory!test information curve}

```{r, eval = FALSE}
plot(
  raschModel,
  type = "infoSE")
```

```{r onePLtifSEM, out.width = "100%", fig.align = "center", fig.cap = "Test Information Curve and Standard Error of Measurement from Rasch Item Response Theory Model.", echo = FALSE}
plot(
  raschModel,
  type = "infoSE",
  par.settings = standard.theme("pdf", color = FALSE))
```

#### Item Curves {#irt-onePLitemCurves}

##### Item characteristic curves {#irt-onePLicc}

Item characteristic curve ([ICC](#icc)) plots of the probability of item endorsement (or getting the item correct) as a function of a person's level on the latent construct (theta; $\theta$) are shown in Figure \@ref(fig:onePLicc).\index{item response theory!item characteristic curve}

```{r, eval = FALSE}
plot(
  raschModel,
  type = "itemscore",
  facet_items = FALSE)
```

```{r onePLicc, out.width = "100%", fig.align = "center", fig.cap = "Item Characteristic Curves from Rasch Item Response Theory Model.", echo = FALSE}
plot(
  raschModel,
  type = "itemscore",
  facet_items = FALSE,
  par.settings = standard.theme("pdf", color = FALSE))
```

##### Item information curves {#irt-onePLiif}

Plots of [item information](#irtReliability) as a function of a person's level on the latent construct (theta; $\theta$) are shown in Figure \@ref(fig:onePLiif).\index{item response theory!information}\index{item response theory!item information curve}

```{r, eval = FALSE}
plot(
  raschModel,
  type = "infotrace",
  facet_items = FALSE)
```

```{r onePLiif, out.width = "100%", fig.align = "center", fig.cap = "Item Information Curves from Rasch Item Response Theory Model.", echo = FALSE}
plot(
  raschModel,
  type = "infotrace",
  facet_items = FALSE,
  par.settings = standard.theme("pdf", color = FALSE))
```

### CFA {#irt-onePLcfa}

A one-parameter logistic model can also be fit in a [CFA](#cfa) framework, sometimes called item factor analysis.\index{factor analysis!confirmatory}
The item factor analysis models were fit in the `lavaan` package [@R-lavaan].\index{item response theory!one-parameter model}

```{r}
onePLModel_cfa <- '
# Factor Loadings (i.e., discrimination parameters)
latent =~ loading*Item.1 + loading*Item.2 + loading*Item.3 + 
  loading*Item.4 + loading*Item.5

# Item Thresholds (i.e., difficulty parameters)
Item.1 | threshold1*t1
Item.2 | threshold2*t1
Item.3 | threshold3*t1
Item.4 | threshold4*t1
Item.5 | threshold5*t1
'

onePLModel_cfa_fit = sem(
  model = onePLModel_cfa,
  data = mydataIRT,
  ordered = c("Item.1", "Item.2", "Item.3", "Item.4","Item.5"),
  mimic = "Mplus",
  estimator = "WLSMV",
  std.lv = TRUE,
  parameterization = "theta")
```

### Mixed Model {#irt-onePLmixed}

A Rasch model can also be fit in a mixed model framework.\index{mixed model}\index{item response theory!one-parameter model}
The Rasch model below was fit using the `lme4` package [@R-lme4].\index{mixed model}\index{item response theory!one-parameter model}

First, we convert the data from wide form to long form for the mixed model:

```{r}
mydataIRT_long <- mydataIRT %>% 
  mutate(ID = 1:nrow(mydataIRT)) %>% 
  pivot_longer(cols = Item.1:Item.5) %>% 
  rename(
    item = name,
    response = value)
```

Then, we can estimate the Rasch model using a logit or probit link:

```{r}
raschModel_mixed_logit <- glmer(
  response ~ -1 + item + (1|ID),
  mydataIRT_long, 
  family = binomial(link = "logit"))

raschModel_mixed_probit <- glmer(
  response ~ -1 + item + (1|ID),
  mydataIRT_long, 
  family = binomial(link = "probit"))
```

## Two-Parameter Logistic Model {#irt-twoPL}

A two-parameter logistic (2-PL) IRT model is a model fit to dichotomous data, which estimates a different [difficulty](#itemDifficulty) ($b$) and [discrimination](#itemDiscrimination) ($a$) parameter for each item.\index{item response theory!two-parameter model}\index{item response theory!item difficulty}\index{item response theory!item discrimination}
Two-parameter logistic models were fit using the `mirt` package [@R-mirt].\index{item response theory!two-parameter model}

### Fit Model {#irt-twoPLfit}

```{r, results = "hide"}
twoPLModel <- mirt(
  data = mydataIRT,
  model = 1,
  itemtype = "2PL",
  SE = TRUE)
```
### Convert Discrimination to Factor Loading {#irt-twoPLdiscriminationFactorLoading}

As described by [Aiden Loe](https://aidenloe.github.io/introToIRT.html#(alpha{})_par_to_std_loading) (archived at https://perma.cc/H3QN-JAWW), one can convert a discrimination parameter to a standardized factor loading using Equation \@ref(eq:discriminationFactorLoading):\index{item response theory!item discrimination}\index{structural equation modeling!factor loading}

$$
\begin{equation}
f = \frac{a}{\sqrt{1 + a^2}}
(\#eq:discriminationFactorLoading)
\end{equation}
$$

where $a$ is equal to: $\text{discrimination}/1.702$.\index{item response theory!item discrimination}\index{structural equation modeling!factor loading}

The `petersenlab` package [@R-petersenlab] contains the `discriminationToFactorLoading()` function that converts discrimination parameters to standardized factor loadings.\index{petersenlab package}

```{r}
discriminationParameters <- coef(
  twoPLModel,
  simplify = TRUE)$items[,1]

discriminationParameters
discriminationToFactorLoading(discriminationParameters)
```

### CFA {#irt-twoPLcfa}

A two-parameter logistic model can also be fit in a [CFA](#cfa) framework, sometimes called item factor analysis.\index{factor analysis}
The item factor analysis models were fit in the `lavaan` package [@R-lavaan].\index{factor analysis}

```{r}
twoPLModel_cfa <- '
# Factor Loadings (i.e., discrimination parameters)
latent =~ loading1*Item.1 + loading2*Item.2 + loading3*Item.3 + 
  loading4*Item.4 + loading5*Item.5

# Item Thresholds (i.e., difficulty parameters)
Item.1 | threshold1*t1
Item.2 | threshold2*t1
Item.3 | threshold3*t1
Item.4 | threshold4*t1
Item.5 | threshold5*t1
'

twoPLModel_cfa_fit = sem(
  model = twoPLModel_cfa,
  data = mydataIRT,
  ordered = c("Item.1", "Item.2", "Item.3", "Item.4","Item.5"),
  mimic = "Mplus",
  estimator = "WLSMV",
  std.lv = TRUE,
  parameterization = "theta")
```

## Two-Parameter Multidimensional Logistic Model {#irt-twoPLmultidimensional}

A 2-PL multidimensional IRT model is a model that allows multiple dimensions (latent factors) and is fit to dichotomous data, which estimates a different [difficulty](#itemDifficulty) ($b$) and [discrimination](#itemDiscrimination) ($a$) parameter for each item.\index{item response theory!two-parameter model}\index{item response theory!multidimensional model}\index{item response theory!item difficulty}\index{item response theory!item discrimination}
Multidimensional IRT models were fit using the `mirt` package [@R-mirt].\index{item response theory!multidimensional model}
In this example, we estimate a 2-PL multidimensional IRT model by estimating two factors.\index{item response theory!two-parameter model}\index{item response theory!multidimensional model}

### Fit Model {#irt-twoPLmultidimensionalFit}

```{r, results = "hide"}
twoPL2FactorModel <- mirt(
  data = mydataIRT,
  model = 2,
  itemtype = "2PL",
  SE = TRUE)
```

### Model Summary {#irt-twoPLmultidimensionalOutput}

```{r}
coef(twoPL2FactorModel, simplify = TRUE)
```

### Compare Model Fit {#irt-twoPLmultidimensionalCompare}

The modified model with two factors and the original one-factor model are considered "nested" models.\index{nested model}\index{item response theory!multidimensional model}
The original model is nested within the modified model because the modified model includes all of the terms of the original model along with additional terms.\index{nested model}
Model fit of nested models can be compared with a chi-square difference test.\index{nested model}\index{item response theory!chi-square difference test}\index{chi-square!difference test}

```{r}
anova(twoPLModel, twoPL2FactorModel)
```

Using a chi-square difference test to compare two nested models, the two-factor model fits significantly better than the one-factor model.\index{nested model}\index{item response theory!chi-square difference test}\index{item response theory!multidimensional model}\index{chi-square!difference test}

### CFA {#irt-twoPLmultidimensionalCFA}

A two-parameter multidimensional model can also be fit in a [CFA](#cfa) framework, sometimes called item factor analysis.\index{factor analysis!confirmatory}
The item factor analysis models were fit in the `lavaan` package [@R-lavaan].

```{r}
twoPLModelMultidimensional_cfa <- '
# Factor Loadings (i.e., discrimination parameters)
latent1 =~ loading1*Item.1 + loading4*Item.4 + loading5*Item.5
latent2 =~ loading2*Item.2 + loading3*Item.3

# Item Thresholds (i.e., difficulty parameters)
Item.1 | threshold1*t1
Item.2 | threshold2*t1
Item.3 | threshold3*t1
Item.4 | threshold4*t1
Item.5 | threshold5*t1
'

twoPLModelMultidimensional_cfa_fit = sem(
  model = twoPLModelMultidimensional_cfa,
  data = mydataIRT,
  ordered = c("Item.1", "Item.2", "Item.3", "Item.4","Item.5"),
  mimic = "Mplus",
  estimator = "WLSMV",
  std.lv = TRUE,
  parameterization = "theta")
```

## Three-Parameter Logistic Model {#irt-threePL}

A three-parameter logistic (3-PL) IRT model is a model fit to dichotomous data, which estimates a different [difficulty](#itemDifficulty) ($b$), [discrimination](#itemDiscrimation) ($a$), and [guessing](#itemGuessing) parameter for each item.\index{item response theory!three-parameter model}\index{item response theory!item difficulty}\index{item response theory!item discrimination}\index{item response theory!item guessing}
Three-parameter logistic models were fit using the `mirt` package [@R-mirt].\index{item response theory!three-parameter model}

### Fit Model {#irt-threePLfit}

```{r, results = "hide"}
threePLModel <- mirt(
  data = mydataIRT,
  model = 1,
  itemtype = "3PL",
  SE = TRUE)
```

### Model Summary {#irt-threePLoutput}

```{r}
coef(threePLModel, simplify = TRUE, IRTpars = TRUE)
```

## Four-Parameter Logistic Model {#irt-fourPL}

A four-parameter logistic (4-PL) IRT model is a model fit to dichotomous data, which estimates a different [difficulty](#itemDifficulty) ($b$), [discrimination](#itemDiscrimination) ($a$), [guessing](#itemGuessing), and [careless errors](#itemCarelessErrors) parameter for each item.\index{item response theory!three-parameter model}\index{item response theory!item difficulty}\index{item response theory!item discrimination}\index{item response theory!item guessing}\index{item response theory!item careless errors}
Four-parameter logistic models were fit using the `mirt` package [@R-mirt].\index{item response theory!four-parameter model}

### Fit Model {#irt-fourPLfit}

```{r, results = "hide"}
fourPLModel <- mirt(
  data = mydataIRT,
  model = 1,
  itemtype = "4PL",
  SE = TRUE,
  technical = list(NCYCLES = 2000))
```

### Model Summary {#irt-fourPLoutput}

```{r}
coef(fourPLModel, simplify = TRUE, IRTpars = TRUE)
```

## Graded Response Model {#irt-gradedResponseModel}

A two-parameter graded response model (GRM) is an IRT model fit to polytomous data (in this case, 1–4 likert scale), which estimates a different [difficulty](#itemDifficulty) ($b$) and [discrimination](#itemDiscrimination) ($a$) parameter for each item.\index{item response theory!graded response model}\index{item response theory!item difficulty}\index{item response theory!item discrimination}
It estimates four parameters for each item: [difficulty](#itemDifficulty) [for each of three threshold transitions: 1–2 ($b_1$), 2–3 ($b_2$), and 3–4 ($b_3$)] and [discrimination](#itemDiscrimination) ($a$).\index{item response theory!graded response model}\index{item response theory!item difficulty}\index{item response theory!item discrimination}
GRM models were fit using the `mirt` package [@R-mirt].\index{item response theory!graded response model}

### Fit Model {#irt-gradedResponseModelFit}

`Science` is a data set from the `mirt` package [@R-mirt] that contains four items evaluating people's attitudes to science and technology on a 1–4 Likert scale.\index{Likert scale}
The data are from the Consumer Protection and Perceptions of Science and Technology section of the 1992 Euro-Barometer Survey of people in Great Britain.

```{r, results = "hide"}
gradedResponseModel <- mirt(
  data = Science,
   model = 1,
   itemtype = "graded",
   SE = TRUE)
```

### Model Summary {#irt-gradedResponseModelOutput}

```{r}
coef(gradedResponseModel, simplify = TRUE, IRTpars = TRUE)
```

### Plots {#irt-gradedResponseModelPlots}

#### Test Curves {#irt-gradedResponseModelTestCurves}

The test curves suggest that the measure is most [reliable](#reliability) (i.e., provides the most [information](#irtReliability) and has the smallest [standard error of measurement](#irtReliability)) across a wide range of construct.\index{item response theory!information}\index{reliability}\index{item response theory!standard error of measurement}
In general, this measure with polytomous (Likert-scale) items provides more information than the measure with binary items that were examined above.\index{data!dichotomous}\index{data!polytomous}\index{Likert scale}
This is consistent with the idea that polytomous items tend to provide more information than binary/dichotomous items.\index{data!dichotomous}\index{data!polytomous}\index{item response theory!information}

##### Test characteristic curve {#irt-gradedResponseModelTCC}

A test characteristic curve ([TCC](#icc)) plot of the expected total score as a function of a person's level on the latent construct (theta; $\theta$) is shown in Figure \@ref(fig:gradedResponseModelTCC).\index{item response theory!item characteristic curve}

```{r gradedResponseModelTCC, out.width = "100%", fig.align = "center", fig.cap = "Test Characteristic Curve from Graded Response Model."}
plot(gradedResponseModel, type = "score")
```

##### Test information curve {#irt-gradedResponseModelTIF}

A plot of [test information](#irtReliability) as a function of a person's level on the latent construct (theta; $\theta$) is shown in Figure \@ref(fig:gradedResponseModelTIF).\index{item response theory!information}\index{item response theory!test information curve}

```{r gradedResponseModelTIF, out.width = "100%", fig.align = "center", fig.cap = "Test Information Curve from Graded Response Model."}
plot(gradedResponseModel, type = "info")
```

##### Test reliability {#irt-gradedResponseModelReliability}

The estimate of marginal [reliability](#irtReliability) is below:\index{reliability}

```{r}
marginal_rxx(gradedResponseModel)
```

A plot of test [reliability](#irtReliability) as a function of a person's level on the latent construct (theta; $\theta$) is illustrated in Figure \@ref(fig:gradedResponseModelReliability).\index{reliability}

```{r gradedResponseModelReliability, out.width = "100%", fig.align = "center", fig.cap = "Test Reliability from Graded Response Model."}
plot(gradedResponseModel, type = "rxx")
```

##### Test standard error of measurement {#irt-gradedResponseModelSEM}

A plot of test [standard error of measurement](#irtReliability) (SEM) as a function of a person's level on the latent construct (theta; $\theta$) is depicted in Figure \@ref(fig:gradedResponseModelSEM).\index{item response theory!standard error of measurement}

```{r gradedResponseModelSEM, out.width = "100%", fig.align = "center", fig.cap = "Test Standard Error of Measurement from Graded Response Model."}
plot(gradedResponseModel, type = "SE")
```

##### Test information curve and standard errors {#irt-gradedResponseModelTIFsem}

A plot of [test information](#irtReliability) and [standard error of measurement](#irtReliability) (SEM) as a function of a person's level on the latent construct (theta; $\theta$) is shown in Figure \@ref(fig:gradedResponseModelTIFsem).\index{item response theory!information}\index{reliability}\index{item response theory!standard error of measurement}\index{item response theory!test information curve}

```{r, eval = FALSE}
plot(
  gradedResponseModel,
  type = "infoSE")
```

```{r gradedResponseModelTIFsem, out.width = "100%", fig.align = "center", fig.cap = "Test Information Curve and Standard Error of Measurement from Graded Response Model.", echo = FALSE}
plot(
  gradedResponseModel,
  type = "infoSE",
  par.settings = standard.theme("pdf", color = FALSE))
```

#### Item Curves {#irt-gradedResponseModelItemCurves}

##### Item characteristic curves {#irt-gradedResponseModelICC}

Item characteristic curve ([ICC](#icc)) plots of the expected score on the item as a function of a person's level on the latent construct (theta; $\theta$) are shown in Figure \@ref(fig:gradedResponseModelICC).\index{item response theory!item characteristic curve}

```{r, eval = FALSE}
plot(
  gradedResponseModel,
  type = "itemscore",
  facet_items = FALSE)
```

```{r gradedResponseModelICC, out.width = "100%", fig.align = "center", fig.cap = "Item Characteristic Curves from Graded Response Model.", echo = FALSE}
plot(
  gradedResponseModel,
  type = "itemscore",
  facet_items = FALSE,
  par.settings = standard.theme("pdf", color = FALSE))
```

##### Item information curves {#irt-gradedResponseModelIIF}

Plots of [item information](#irtReliability) as a function of a person's level on the latent construct (theta; $\theta$) are shown in Figure \@ref(fig:gradedResponseModelIIF).\index{item response theory!information}\index{item response theory!item information curve}

```{r, eval = FALSE}
plot(
  gradedResponseModel,
  type = "infotrace",
  facet_items = FALSE)
```

```{r gradedResponseModelIIF, out.width = "100%", fig.align = "center", fig.cap = "Item Information Curves from Graded Response Model.", echo = FALSE}
plot(
  gradedResponseModel,
  type = "infotrace",
  facet_items = FALSE,
  par.settings = standard.theme("pdf", color = FALSE))
```

##### Item response category characteristic curves {#irt-gradedResponseModelIRCCC}

A plot of the probability of item threshold endorsement as a function of a person's level on the latent construct (theta; $\theta$) is illustrated in Figure \@ref(fig:gradedResponseModelIRCCC).\index{item response theory!item response category characteristic curve}

```{r, eval = FALSE}
plot(
  gradedResponseModel,
  type = "trace")
```

```{r gradedResponseModelIRCCC, out.width = "100%", fig.align = "center", fig.cap = "Item Response Category Characteristic Curves from Graded Response Model.", echo = FALSE}
plot(
  gradedResponseModel,
  type = "trace",
  par.settings = standard.theme("pdf", color = FALSE))
```

##### Item boundary characteristic curves (aka item operation characteristic curves) {#irt-gradedResponseModelIOCC}

A plot of item boundary characteristic curves is illustrated in Figure \@ref(fig:gradedResponseModelIOCC).\index{item response theory!item boundary characteristic curve}
The plot of item boundary characteristic curves was adapted from an example by Aiden Loe:\index{item response theory!item boundary characteristic curve} https://aidenloe.github.io/irtplots.html (archived at https://perma.cc/D4YH-RV6N).

```{r gradedResponseModelIOCC, out.width = "100%", fig.align = "center", echo = FALSE, fig.cap = "Item Boundary Category Characteristic Curves from Graded Response Model."}
modelCoefficients <- coef(
  gradedResponseModel,
  IRTpars = TRUE,
  simplify = TRUE)$items

theta <- seq(from = -6, to = 6, by = .1)

difficultyThresholds <- grep(
  "b",
  dimnames(modelCoefficients)[[2]],
  value = TRUE)
numberDifficultyThresholds <- length(difficultyThresholds)
items <- dimnames(modelCoefficients)[[1]]
numberOfItems <- length(items)

lst <- lapply(
  1:numberOfItems,
  function(x) data.frame(
    matrix(ncol = numberDifficultyThresholds + 1,
           nrow = length(theta),
           dimnames = list(NULL, c("theta", difficultyThresholds)))))

for(i in 1:numberOfItems){
  for(j in 1:numberDifficultyThresholds){
    lst[[i]][,1] <- theta
    lst[[i]][,j + 1] <- fourPL(
      a = modelCoefficients[i,1],
      b = modelCoefficients[i,j + 1],
      theta = theta)
  }
}

names(lst) <- items
dat <- bind_rows(lst, .id = "item")
longer_data <- pivot_longer(
  dat,
  cols = all_of(difficultyThresholds))

ggplot(
  data = longer_data,
  aes(
    theta,
    value,
    group = interaction(item, name),
    color = item)) +
  geom_line() +
  scale_color_grey() +
  ylab("Probability of Endorsing Item Response Category At or Above Boundary") +
  theme_bw() +
  theme(axis.title.y = element_text(size = 8))
```

## Conclusion {#conclusion-irt}

Item response theory is a measurement theory and advanced modeling approach that allows estimating latent variables as the common variance from multiple items, and it allows estimating how the items relate to the construct (latent variable).\index{item response theory}\index{latent variable}
IRT holds promise to enable the development of briefer assessments, including short forms and [adaptive assessments](#cat), that have strong [reliability](#reliability) and [validity](#validity).\index{item response theory}\index{brief assessment}\index{adaptive testing}\index{reliability}\index{validity}

## Suggested Readings {#readings-irt}

If you are interested in learning more about IRT, we recommend the book by @Embretson2000.\index{item response theory}
