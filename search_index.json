[["index.html", "Principles of Psychological Assessment: With Applied Examples in R version 1.0.6 Book Cover", " Principles of Psychological Assessment: With Applied Examples in R version 1.0.6 Isaac T. Petersen 2024-04-29 Book Cover book cover "],["preface.html", "Preface 0.1 Open Access 0.2 License 0.3 Citation 0.4 Accessibility 0.5 How to Contribute 0.6 Acknowledgments", " Preface This is a book in progress—it is incomplete. I will continue to add to and update it as I am able. 0.1 Open Access This is an open-access book. This means that it is freely available for anyone to access. 0.2 License Creative Commons License The online version of this book is licensed under the Creative Commons Attribution License. In short, you can use my work as long as you cite it. 0.3 Citation The APA-style citation for the book is: Petersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. Version 1.0.6. University of Iowa Libraries. https://isaactpetersen.github.io/Principles-Psychological-Assessment. https://doi.org/10.5281/zenodo.6466589 The BibTeX citation for the book is: @book{petersenPrinciplesPsychologicalAssessment, title = {Principles of psychological assessment: With applied examples in {R}}, author = {Petersen, Isaac T.}, year = {2024}, publisher = {{University of Iowa Libraries}}, note = {Version 1.0.6}, doi = {10.5281/zenodo.6466589}, isbn = {9780984037827}, url = {https://isaactpetersen.github.io/Principles-Psychological-Assessment} } A print version is available for purchase: https://www.amazon.com/Principles-Psychological-Assessment-Statistics-Behavioral/dp/1032413069. The APA-style citation for the print version of the book is: Petersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. Chapman and Hall/CRC. https://www.routledge.com/9781032413068; https://doi.org/10.1201/9781003357421 0.4 Accessibility I strive to follow principles of accessibility (archived at https://perma.cc/8XJ9-Q6QJ) to make the book content accessible to people with visual impairments and physical disabilities. If there are additional ways I can make the content more accessible, please let me know. 0.5 How to Contribute This is an open-access textbook. My goal is to share principles of psychological assessment with other researchers and clinicians that they can access for free! Anyone is welcome to contribute to the project. If you would like to contribute, feel free to open an issue or create a pull request on GitHub. The GitHub repository for the book is located here: https://github.com/isaactpetersen/Principles-Psychological-Assessment. If you have data or analysis examples that are you willing to share and include in the book, feel free to contact me. 0.6 Acknowledgments This book was supported by an OpenHawks Open Educational Resources grants program from the University of Iowa Libraries. This book would not be possible without the help of others. Much of the content of this book was inspired by Richard Viken’s course in psychological assessment that I took as a graduate student. I thank W. Joel Schneider who provided several examples that were adapted for this book. I thank Danielle Szabreath, Samar Haddad, and Michele Dumont for help in copyediting. I acknowledge my wife, Alyssa Varner, who helped design several of the graphics used in this book, in addition to all of her support throughout the process. "],["intro.html", "Chapter 1 Introduction 1.1 About This Book 1.2 Why R? 1.3 About the Author 1.4 What Is Assessment? 1.5 Why Should We Care About Assessment (and Science)? 1.6 Prerequisites", " Chapter 1 Introduction 1.1 About This Book First, let us discuss what this book is not. This book is not a guide on how to assess each psychological construct or disorder. This book is also not a comparative summary of the psychometrics of different measures. There already exist many resources that summarize and compare the reliability and validity of measures in psychology (Buros Center for Testing, 2021). Instead, this book is about the principles of psychological assessment. This book was originally written for a graduate-level course on psychological assessment. The chapters provide an overview of topics that each could have its own class and textbook, such as structural equation modeling, item response theory, generalizability theory, factor analysis, prediction, cognitive assessment, psychophysiological assessment, etc. The book gives readers an overview of the breadth of the field of assessment and various assessment approaches. As a consequence, the book does not cover any one assessment device or method in great depth. The goal of this book is to help researchers and clinicians learn to think critically about assessments so they can better develop, evaluate, administer, score, integrate, and interpret psychological assessments. Learning important principles of assessment will put you in a better position to learn any assessment device and to develop better ones. This book applies a scientific perspective to the principles of psychological assessment. The assessments used in a given situation—whether in research or practice—should be supported by the strongest available science, or they should be used cautiously while undergoing development and study. In addition to discussing principles, however, analysis scripts in the software R (R Core Team, 2022) are provided, so that you are able to apply the principles discussed in this book. Some of the chapters includes analysis exercises (at the end of the chapter) for you to test your comprehension. 1.2 Why R? R is free, open source, open platform, and widely used. Unlike proprietary software used for data analysis, R is not a black box. You can examine the code for any function or computation you perform. You can even modify and improve these functions by changing the code, and you can create your own functions. R also has advanced capabilities for data wrangling and has many packages available for advanced statistical analysis and graphing. In addition, there are strong resources available for creating your analyses in R so they are reproducible by others (Gandrud, 2020). 1.3 About the Author Regarding my background, I am a licensed clinical psychologist. My research examines how children develop behavior problems. I am also a trained clinician, and I supervise training clinicians in assessment and therapy, particularly assessment and treatment of children’s disruptive behavior. Given my expertise, many of the examples in the book deal with topics in clinical psychology, but many of the assessment principles discussed are relevant to all areas of psychology—and science more broadly—and are often overlooked in research and practice. As a card-carrying clinical scientist, my perspective is that the scientific epistemology is the strongest approach to knowledge and that assessment should be guided first and foremost by the epistemology of science, regardless of whether one is doing research or practice. 1.4 What Is Assessment? Assessment is the gathering of information about a person, group, setting, or context. In psychological assessment, we are interested in gathering information about people’s psychological functioning, including their thoughts, emotions, and behaviors. Psychological assessment can also consider biological and physiological processes that are linked to people’s thoughts, emotions, and behaviors. Many assessment approaches can be used to assess people’s thoughts, emotions, and behaviors, including self-report questionnaires, questionnaires reported by others (e.g., spouse, parent, teacher, or friend), interviews, observations, biopsychological assessments (e.g., cortisol, heart rate, brain imaging), performance-based assessments, archival approaches (e.g., chart review), and combinations of these. 1.5 Why Should We Care About Assessment (and Science)? In research, assessments are conducted to advance knowledge, such as improved prediction or understanding. For example, in my research, I use assessments to understand what processes influence children’s development of disruptive behavior. In society, assessments are conducted to improve decision-making. For instance, assessments are conducted to determine whether to hire a job candidate or promote an employee. In a clinical context, assessments are conducted to improve treatment and the client’s outcomes. As an example, assessments are conducted to determine which treatment would be most effective for a person suffering from depression. Assessments can be valuable to understanding current functioning as well as making predictions. To best answer these questions and address these goals, we need to have confidence that our devices yield accurate answers for these purposes for the assessed individuals. Science is crucial for knowing how much (or how little) confidence we have in a given assessment for a given purpose and population. Effective treatment often depends on accurate assessment. Thus, knowing how to conduct and critically evaluate science will make you more effective at selecting, administering, and interpreting assessments. Decisions resulting from assessments can have important life-altering consequences. High-stakes decisions based on assessments include decisions about whether a person is hospitalized, whether a child is removed from their abusive home, whether a person is deemed competent to stand trial, whether a prisoner is released on parole, and whether an applicant is admitted to graduate school. These important assessment-related decisions should be made using the best available science. The problem is that there has been a proliferation of pseudoscience in assessment and treatment. There are widely used psychological assessments and treatments that we know are inaccurate, do not work, or in some cases, that we know to be harmful. Lists of harmful psychological treatments (e.g., Lilienfeld, 2007) and inaccurate assessments (e.g., Hunsley et al., 2015) have been published, but these treatments and assessments are still used by professional providers to this day. Practice using such techniques violates the aphorism, “First, do no harm.” This would be inconceivable for other applied sciences, such as chemistry, engineering, and medicine. For instance, the prescription of a particular medication for a particular purpose requires approval by the U.S. Food and Drug Administration (FDA). Psychological assessments and treatments do not have the same level of oversight. The gap between what we know based on science and what is implemented in practice (the science–practice gap) motivated McFall’s (1991) “Manifesto for a Science of Clinical Psychology,” which he later expanded (McFall, 2000). The Manifesto has one cardinal principle and four corollaries: Cardinal Principle: Scientific clinical psychology is the only legitimate and acceptable form of clinical psychology. First Corollary: Psychological services should not be administered to the public (except under strict experimental control) until they have satisfied these four minimal criteria: The exact nature of the service must be described clearly. The claimed benefits of the service must be stated explicitly. These claimed benefits must be validated scientifically. Possible negative side effects that might outweigh any benefits must be ruled out empirically. Second Corollary: The primary and overriding objective of doctoral training programs in clinical psychology must be to produce the most competent clinical scientists possible. Third Corollary: A scientific epistemology differentiates science from pseudoscience. Fourth Corollary: The most caring and humane psychological services are those that have been shown empirically to be the most effective, efficient, and safe. The Manifesto orients you to the scientific perspective from which we will be examining psychological assessment techniques in this book. 1.5.1 Assessment and the Replication Crisis in Science Assessment is also crucial to advancing knowledge in research, as summarized in the maxim, “What we know depends on how we know it.” Findings from studies boil down to the methods that were used to obtain them—thus, everything we know comes down to methods. Many domains of science, particularly social science, have struggled with a replication crisis, such that a large proportion of findings fail to replicate when independent investigators attempt to replicate the original findings (Duncan et al., 2014; Freese &amp; Peterson, 2017; Larson &amp; Carbine, 2017; Lilienfeld, 2017; Open Science Collaboration, 2015; Shrout &amp; Rodgers, 2018; Tackett, Brandes, King, et al., 2019). There is considerable speculation on what factors account for the replication crisis. For instance, one possible factor is the researcher degrees of freedom, which are unacknowledged choices in how researchers prepare, analyze, and report their data that can lead to detecting significance in the absence of real effects (Loken &amp; Gelman, 2017). This is similar to Gelman &amp; Loken (2013)’s description of research as the garden of forking paths, where different decisions along the way can lead to different outcomes (see Figure 1.1). A second possibility for the replication crisis is that some replication studies have had limited statistical power (e.g., insufficiently large sample sizes). A third possibility may be that there is publication bias such that researchers tend to publish only significant findings, which is known as the file-drawer effect. A fourth possibility is that researchers may engage in ethically questionable research practices, such as multiple testing and selective reporting. Figure 1.1: Garden of Forking Paths. (Adapted from https://www.si.umich.edu/about-umsi/news/ditch-stale-pdf-making-research-papers-interactive-and-more-transparent [archived at https://perma.cc/R2V9-CP3F].) However, difficulties with replication could exist even if researchers have the best of intentions, engage in ethical research practices, and are transparent about all of the methods they used and decisions they made. The replication crisis could owe, in part, to noisy (imprecise and inaccurate) measures. The field has paid insufficient attention to measurement unreliability as a key culprit in the replication crisis. As Loken &amp; Gelman (2017) demonstrated, when measures are less noisy, measurement error weakens the association between the measures. But when using noisy measures and selecting what to publish based on statistical significance, measurement error can make the association appear stronger than it is. This is what Loken &amp; Gelman (2017) describe as the statistical significance filter: In a study with noisy measures and a small or moderate sample size, statistically significant estimates are likely to have a stronger effect size than the actual effect size—the “true” underlying effects could be small or nonexistent. The statistical significance filter exists because, with a small sample size, the effect size will need to be larger in order to detect it as statistically significant due to larger standard errors. That is, when researchers publish a statistically significant effect with a small or moderate sample size and noisy measures, the effect size will necessarily be large enough to detect it (and likely larger than the true effect). However, the effect of noise (measurement error) diminishes as the sample size increases. So, the goal should be to use less noisy measures with larger sample sizes. And, as discussed in Chapter 13 on ethical considerations in psychological assessment, the use of pre-registration could be useful to control researcher degrees of freedom. The lack of replicability of findings has the potential to negatively impact the people we study through misinformed assessment, treatment, and policy decisions. Therefore, it is crucial to use assessments with strong psychometric properties and/or to develop better assessments. Psychometrics refer to the reliability and validity of measures. These concepts are described in greater detail in Chapters 4 and 5, but for now, think about reliability as consistency of measurement and validity as accuracy of measurement. 1.5.2 Science Versus Pseudoscience in Assessment Science is the best system of epistemology we have to pursue truth. Science is a process, not a set of facts. It helps us overcome blind spots. The system is revisionary and self-correcting. Science is the epistemology that is the least susceptible to error due to authority, belief, intuition, bias, preference, etc. Clients are in a vulnerable position and deserve to receive services consistent with the strongest available evidence. By providing a client a service, you are implicitly making a claim and prediction. As a psychologist, you are claiming to have expert knowledge and competence. You are making a prediction that the client will improve because of your services. Ethically, you should be making these predictions based on science and a risk-benefit analysis. It is also important to make sure the client knows when services are unproven so they can provide fully informed consent. Otherwise, because of your position as a psychologist, they may believe that you are using an evidence-based approach when you are not. We will be examining psychological assessment from a scientific perspective. Here are characteristics of science that distinguish it from pseudoscience: Risky hypotheses are posed that are falsifiable. The hypotheses can be shown to be wrong. Findings can be replicated independently by different research groups and different methods. Evidence converges across studies and methods. Potential alternative explanations for findings are specified and examined empirically (with data). Steps are taken to guard against the undue influence of personal beliefs and biases. The strength of claims reflects the strength of evidence. Findings and the ability to make judgments or predictions are not overstated. For instance, it is important to present the degree of uncertainty from assessments with error bars or confidence intervals. Scientifically supported measurement strategies are used based on their psychometrics, including reliability and validity. Science does not progress without advances in measurement, including more efficient measurement (see Chapters 8 and 21) more precise measurement (i.e., reliability; see Chapter 4) more accurate measurement (i.e., validity; see Chapter 5) more sophisticated modeling (see Chapter 24) more sophisticated biopsychological (e.g., cognitive neuroscience) techniques, as opposed to self-report and neuropsychological techniques (see Chapter 20) considerations of cultural and individual diversity (see Chapter 25) ethical considerations (see Chapter 13) These considerations serve as the focus of this book. 1.6 Prerequisites This book was written in Markdown using the bookdown package (Xie, 2022a) in R (R Core Team, 2022). The bookdown package was built on top of the rmarkdown (Allaire et al., 2022) and knitr (Xie, 2022b) packages (Xie, 2015). Applied examples in R are provided throughout the book. Each chapter that has R examples has a section on “Getting Started,” which provides the code to load relevant libraries, load data files, simulate data, add missing data (for realism), perform calculations, and more. The data files used for the examples are available on the Open Science Framework (OSF): https://osf.io/3pwza. Most of the R packages used in this book can be installed from the Comprehensive R Archive Network (CRAN) using the following command: Code install.packages(&quot;INSERT_PACKAGE_NAME_HERE&quot;) Several of the packages are hosted on GitHub repositories, including uroc (Gneiting &amp; Walz, 2021), dmacs (Dueber, 2019), and petersenlab (Petersen, 2024b). You can install the uroc and dmacs packages using the following code: Code install.packages(&quot;remotes&quot;) remotes::install_github(&quot;evwalz/uroc&quot;) remotes::install_github(&quot;ddueber/dmacs&quot;) Many of the R functions used in this book are available from the petersenlab package (Petersen, 2024b): https://github.com/DevPsyLab/petersenlab. You can install the petersenlab package (Petersen, 2024b) using the following code: Code install.packages(&quot;remotes&quot;) remotes::install_github(&quot;DevPsyLab/petersenlab&quot;) The code that generates this book is located on GitHub: https://github.com/isaactpetersen/Principles-Psychological-Assessment. References Allaire, J., Xie, Y., McPherson, J., Luraschi, J., Ushey, K., Atkins, A., Wickham, H., Cheng, J., Chang, W., &amp; Iannone, R. (2022). rmarkdown: Dynamic documents for R. https://CRAN.R-project.org/package=rmarkdown Buros Center for Testing. (2021). The twenty-first mental measurements yearbook. Buros Center for Testing. Dueber, D. (2019). dmacs: Measurement nonequivalence effect size calculator. https://github.com/ddueber/dmacs Duncan, G. J., Engel, M., Claessens, A., &amp; Dowsett, C. J. (2014). Replication and robustness in developmental research. Developmental Psychology, 50(11), 2417–2425. https://doi.org/10.1037/a0037996 Freese, J., &amp; Peterson, D. (2017). Replication in social science. Annual Review of Sociology. Gandrud, C. (2020). Reproducible research with R and R studio (3rd ed.). CRC Press. https://www.routledge.com/Reproducible-Research-with-R-and-RStudio/Gandrud/p/book/9780367143985 Gelman, A., &amp; Loken, E. (2013). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time. Department of Statistics, Columbia University. Gneiting, T., &amp; Walz, E.-M. (2021). Receiver operating characteristic (ROC) movies, universal ROC (UROC) curves, and coefficient of predictive ability (CPA). Machine Learning. https://doi.org/10.1007/s10994-021-06114-3 Hunsley, J., Lee, C. M., Wood, J. M., &amp; Taylor, W. (2015). Controversial and questionable assessment techniques. In S. O. Lilienfeld, S. J. Lynn, &amp; J. M. Lohr (Eds.), Science and pseudoscience in clinical psychology (2nd ed., pp. 42–82). The Guilford Press. Larson, M. J., &amp; Carbine, K. A. (2017). Sample size calculations in human electrophysiology (EEG and ERP) studies: A systematic review and recommendations for increased rigor. International Journal of Psychophysiology, 111, 33–41. https://doi.org/10.1016/j.ijpsycho.2016.06.015 Lilienfeld, S. O. (2007). Psychological treatments that cause harm. Perspectives on Psychological Science, 2(1), 53–70. https://doi.org/10.1111/j.1745-6916.2007.00029.x Lilienfeld, S. O. (2017). Psychology’s replication crisis and the grant culture: Righting the ship. Perspectives on Psychological Science, 12(4), 660–664. https://doi.org/10.1177/1745691616687745 Loken, E., &amp; Gelman, A. (2017). Measurement error and the replication crisis. Science, 355(6325), 584–585. https://doi.org/10.1126/science.aal3618 McFall, R. M. (1991). Manifesto for a science of clinical psychology. The Clinical Psychologist, 44(6), 75–91. McFall, R. M. (2000). Elaborate reflections on a simple manifesto. Applied &amp; Preventive Psychology, 9(1), 5–21. https://doi.org/10.1016/s0962-1849(05)80035-6 Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251). https://doi.org/10.1126/science.aac4716 Petersen, I. T. (2024b). petersenlab: Package of R functions for the Petersen Lab. https://doi.org/10.5281/zenodo.7602890 R Core Team. (2022). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/ Shrout, P. E., &amp; Rodgers, J. L. (2018). Psychology, science, and knowledge construction: Broadening perspectives from the replication crisis. Annual Review of Psychology, 69(1), 487–510. https://doi.org/10.1146/annurev-psych-122216-011845 Tackett, J. L., Brandes, C. M., King, K. M., &amp; Markon, K. E. (2019). Psychology’s replication crisis and clinical psychological science. Annual Review of Clinical Psychology, 15(1), 579–604. https://doi.org/10.1146/annurev-clinpsy-050718-095710 Xie, Y. (2015). Dynamic documents with R and knitr (2nd ed.). Chapman; Hall/CRC. Xie, Y. (2022a). bookdown: Authoring books and technical documents with R Markdown. https://CRAN.R-project.org/package=bookdown Xie, Y. (2022b). knitr: A general-purpose package for dynamic report generation in R. https://yihui.org/knitr/ "],["scoresScales.html", "Chapter 2 Scores and Scales 2.1 Getting Started 2.2 Data Types 2.3 Score Transformation 2.4 Conclusion 2.5 Suggested Readings 2.6 Exercises", " Chapter 2 Scores and Scales Assessments yield information. The information is encoded in scores or in other types of data. It is important to consider the different types of data because the types of data restrict what options are available to analyze the data. 2.1 Getting Started 2.1.1 Load Libraries First, we load the R packages used for this chapter: Code library(&quot;petersenlab&quot;) #to install: install.packages(&quot;remotes&quot;); remotes::install_github(&quot;DevPsyLab/petersenlab&quot;) library(&quot;dplyr&quot;) library(&quot;tidyverse&quot;) library(&quot;tinytex&quot;) library(&quot;knitr&quot;) library(&quot;rmarkdown&quot;) library(&quot;bookdown&quot;) 2.1.2 Prepare Data 2.1.2.1 Simulate Data For this example, we simulate data below. For reproducibility, we set the seed below. Using the same seed will yield the same answer every time. There is nothing special about this particular seed. Code set.seed(52242) rawData &lt;- rnorm( n = 1000, mean = 200, sd = 30) scores &lt;- data.frame( rawData = rawData, rawDataNoMissing = rawData) 2.1.2.2 Add Missing Data Adding missing data to dataframes helps make examples more realistic to real-life data and helps you get in the habit of programming to account for missing data. Code scores$rawData[c(5,10)] &lt;- NA 2.2 Data Types There are four general data types: nominal, ordinal, interval, and ratio. Depending on the use of the variable, the data could fall into more than one category. The type of data influences what kinds of data analysis you can do. For instance, parametric statistical analysis (e.g., t test, analysis of variance [ANOVA], and linear regression) assumes that data are interval or ratio. 2.2.1 Nominal Nominal data are distinct categories. They are categorical and unordered. Nominal data make no quantitative claims. Nominal data represent things that we can name (e.g., cat and dog). Nominal data can be represented with numbers. For example, zip codes are nominal. Numbers that represent a participant’s sex, race, or ethnicity are also nominal. Categorical data are often dummy-coded into binary (i.e., dichotomous) variables, which represent nominal data. Higher numbers of nominal data do not reflect higher (or lower) levels of the construct because the numbers represent categories that do not have an order. 2.2.2 Ordinal Ordinal data are ordered categories: they have a name and an order. They make no claim about the conceptual distance between the ranks, only that higher values represent higher (or lower) levels of the construct. For example, ranks following a race are ordinal—that is, the person with rank 1 finished before the person with rank 2, who finished before the person with rank 3 (1 &gt; 2 &gt; 3 &gt; 4). Ordinal data make a limited claim because the conceptual distance between adjacent numbers is not the same. For instance, the person who finished the race first might have finished 10 minutes before the second-place finisher; whereas the third-place finisher might have finished 1 second after the second-place finisher. That is, just because the numbers have the same mathematical distance does not mean that they represent the same conceptual distance on the construct. For example, if the respondent is asked how many drinks they had in the past day, and the options are 0 = 0 drinks; 1 = 1–2 drinks; 2 = 3 or more drinks, the scale is ordinal. Even though the numbers have the same mathematical distance (1, 2, 3), they do not represent the same conceptual distance. Most data in psychology are ordinal data even though they are often treated as if they were interval data. 2.2.3 Interval Interval data are ordered and have meaningful distances (i.e., equal spacing between intervals). You can sum interval data (e.g., 2 is 2 away from 4), but you cannot multiply interval data (\\(2 \\times 2 \\ne 4\\)). Examples of interval data are temperatures in Fahrenheit and Celsius—100 degrees Fahrenheit is not twice as hot as 50 degrees Fahrenheit. A person’s number of years of education is interval, whereas educational attainment (e.g., high school degree, college degree, graduate degree) is only ordinal. Although much data in psychology involve numbers that have the same mathematical distance between intervals, the intervals likely do not represent the same conceptual distance. For example, the difference in severity of two people who have two symptoms and four symptoms of depression, respectively, may not be the same difference in depression severity as two people who have four symptoms and six symptoms, respectively. 2.2.4 Ratio Ratio data are ordered, have meaningful distances, and have a true (absolute) zero that represents absence of the construct. With ratio data, multiplicative relationships are true. An example of ratio data is temperature in Kelvin—100 degrees Kelvin is twice as hot as 50 degrees Kelvin. There is a dream of having ratio scales in psychology, but we still do not have a true zero with psychological constructs—what does total absence of depression mean (apart from a dead person)? 2.3 Score Transformation There are a number of score transformations, depending on the goal. Some score transformations (e.g., log transform) seek to make data more normally distributed to meet assumptions of particular analysis approaches. Score transformations alter the original (raw) data. If you change the data, it can change the results. Score transformations are not neutral. 2.3.1 Raw Scores Raw scores are the original data, or they may be aggregations (e.g., sums or means) of multiple items. Raw scores are the purest because they are closest to the original operation (e.g., behavior). A disadvantage of raw scores is that they are scale dependent, and therefore they may not be comparable across different measures with different scales. An example histogram of raw scores is in Figure 2.1. Code hist(scores$rawData, xlab = &quot;Raw Scores&quot;, main = &quot;Histogram of Raw Scores&quot;) Figure 2.1: Histogram of Raw Scores. 2.3.2 Norm-Referenced Scores Norm-referenced scores are scores that are referenced to some norm. A norm is a standard of comparison. For instance, you may be interested in how well a participant performed relative to other children of the same sex, age, grade, or ethnicity. However, interpretation of norm-referenced scores depends on the measure and on the normative sample. A person’s norm-referenced score can vary widely depending on which norms are used. Which reference group should you use? Age? Sex? Age and sex? Grade? Ethnicity? The optimal reference group depends on the purpose of the assessment. The quality of the norms also depends on the representativeness of the reference group compared to the population of interest. Pros and cons of group-based norms are discussed in Section 16.5.2.2.2. A standard normal distribution on various norm-referenced scales is depicted in Figure 2.2, as adapted from Bandalos (2018). Figure 2.2: Various Norm-Referenced Scales. 2.3.2.1 Percentile Ranks A percentile rank reflects what percent of people a person scored higher than, in a given group (i.e., norm). Percentile ranks are frequently used for tests of intellectual/cognitive ability, academic achievement, academic aptitude, and grant funding. They seem like interval data, but they are not intervals because the conceptual spacing between the numbers is not equal. The difference in ability for two people who scored at the 99th and 98th percentile, respectively, is not the same as the difference in ability for two people who scored at the 49th and 50th percentile, respectively. Percentile ranks are only judged against a baseline; there is no subtraction. Percentile ranks have unusual effects. There are lots of people in the middle of a distribution, so a very small difference in raw scores gets expanded out in percentiles. For instance, a raw score of 20 may have a percentile rank of 50, but a raw score of 24 may have a percentile rank of 68. However, a larger raw score change at the ends of the distribution may have a smaller percentile change. For example, a raw score of 120 may have a percentile rank of 97, whereas a raw score of 140 may have a percentile rank of 99. Thus, percentile ranks stretch out differences for some people but constrict differences for others. Here is an example of how to calculate percentile ranks using the dplyr package from the tidyverse (Wickham et al., 2019; Wickham, 2021): Code scores$percentileRank &lt;- percent_rank(scores$rawDataNoMissing) * 100 A histogram of percentile rank scores is in Figure 2.3. Code hist(scores$percentileRank, xlab = &quot;Percentile Ranks&quot;, main = &quot;Histogram of Percentile Ranks&quot;) Figure 2.3: Histogram of Percentile Ranks. 2.3.2.2 Deviation (Standardized) Scores Deviation or standardized scores are the transformation of raw scores to a normal distribution using some norm. The norm could be a comparison group, or it could be the sample itself. With deviation scores, you have similar challenges as with percentile ranks, including which reference group to use, but there are additional challenges. Deviation scores are more informative when the scores are normally distributed compared to when the scores are skewed. If scores are skewed, it can lead to two z scores on the opposite side of the mean having different probabilities. Many constructs we study in psychology are not normally distributed. For example, the frequency of hallucinations among people would show a positively skewed distribution with a truncation at zero, representing a floor effect—i.e., most people do not show hallucinations. For instance, consider a hypothetical distribution of hallucinations. It might follow a folded distribution, as depicted in Figure 2.4: Code hist(rbinom(100000, 300, .01), breaks = 8, xlab = &quot;Hallucinations (Raw Score)&quot;, main = &quot;Histogram of Hallucinations (Raw Score)&quot;) Figure 2.4: Histogram of Hallucinations (Raw Score). Now consider the same distribution converted to a standardized (z) score, as depicted in Figure 2.5: Code hist(scale(rbinom(100000, 300, .01)), breaks = 8, xlab = &quot;Hallucinations (z Score)&quot;, main = &quot;Histogram of Hallucinations (z Score)&quot;) Figure 2.5: Histogram of Hallucinations (z Score). Thus, you can compute a deviation score, but it may not be meaningful if the data and underlying construct are not normally distributed. 2.3.2.2.1 z scores The z score is the most common standardized score, and it can help put scores from different measures with different scales onto the same playing field. z scores have a mean of zero and a standard deviation of one. To get a z score that uses the sample as its own norm, subtract the mean from all scores and divide by the standard deviation. Every z score represents how far that person’s score is from the (normed) average, represented in standard deviation units. With a standard normal curve, 68% of scores fall within one standard deviation of the mean. 95% of scores fall within two standard deviations of the mean. 99.7% of scores fall within three standard deviations of the mean. The area under a normal curve within one standard deviation of the mean is calculated below using the pnorm() function, which calculates the cumulative density function for a normal curve. Code stdDeviations &lt;- 1 pnorm(stdDeviations) - pnorm(stdDeviations * -1) [1] 0.6826895 The area under a normal curve within one standard deviation of the mean is depicted in Figure 2.6. Code x &lt;- seq(-4, 4, length = 200) y &lt;- dnorm(x, mean = 0, sd = 1) plot(x, y, type = &quot;l&quot;, xlab = &quot;z Score&quot;, ylab = &quot;Normal Density&quot;) x &lt;- seq(stdDeviations * -1, stdDeviations, length = 100) y &lt;- dnorm(x, mean = 0, sd = 1) polygon(c(stdDeviations * -1, x, stdDeviations), c(0, y, 0), col = &quot;blue&quot;) Figure 2.6: Density of Standard Normal Distribution. The blue region represents the area within one standard deviation of the mean. The area under a normal curve within two standard deviations of the mean is calculated below: Code stdDeviations &lt;- 2 pnorm(stdDeviations) - pnorm(stdDeviations * -1) [1] 0.9544997 The area under a normal curve within two standard deviations of the mean is depicted in Figure 2.7. Code x &lt;- seq(-4, 4, length = 200) y &lt;- dnorm(x, mean = 0, sd = 1) plot(x, y, type = &quot;l&quot;, xlab = &quot;z Score&quot;, ylab = &quot;Normal Density&quot;) x &lt;- seq(stdDeviations * -1, stdDeviations, length = 100) y &lt;- dnorm(x, mean = 0, sd = 1) polygon(c(stdDeviations * -1, x, stdDeviations), c(0, y, 0), col = &quot;blue&quot;) Figure 2.7: Density of Standard Normal Distribution. The blue region represents the area within two standard deviations of the mean. The area under a normal curve within three standard deviations of the mean is calculated below: Code stdDeviations &lt;- 3 pnorm(stdDeviations) - pnorm(stdDeviations * -1) [1] 0.9973002 The area under a normal curve within three standard deviations of the mean is depicted in Figure 2.8. Code x &lt;- seq(-4, 4, length = 200) y &lt;- dnorm(x, mean = 0, sd = 1) plot(x, y, type = &quot;l&quot;, xlab = &quot;z Score&quot;, ylab = &quot;Normal Density&quot;) x &lt;- seq(stdDeviations * -1, stdDeviations, length = 100) y &lt;- dnorm(x, mean = 0, sd = 1) polygon(c(stdDeviations * -1, x, stdDeviations), c(0, y, 0), col = &quot;blue&quot;) Figure 2.8: Density of Standard Normal Distribution. The blue region represents the area within three standard deviations of the mean. Alternatively, if you want to determine the z score associated with a particular percentile in a normal distribution, you can use the qnorm() function. For instance, the z score associated with the 37th percentile is: Code qnorm(.37) [1] -0.3318533 z scores are calculated using Equation (2.1): \\[\\begin{equation} z = \\frac{x - \\mu}{\\sigma} \\tag{2.1} \\end{equation}\\] where \\(x\\) is the observed score, \\(\\mu\\) is the mean observed score, and \\(\\sigma\\) is the standard deviation of observed scores. Code scores$zScore &lt;- scale(scores$rawData) all.equal( as.vector(scores$zScore), (scores$rawData - mean(scores$rawData, na.rm = TRUE)) / sd(scores$rawData, na.rm = TRUE)) [1] TRUE An example histogram of z scores is in Figure 2.9. Code hist(scores$zScore, xlab = &quot;z Scores&quot;, main = &quot;Histogram of z Scores&quot;) Figure 2.9: Histogram of z Scores. 2.3.2.2.2 T scores T scores have a mean of 50 and a standard deviation of 10. T scores are frequently used with personality and symptom measures, where clinical cutoffs are often set at 70 (i.e., two standard deviations above the mean). For the Minnesota Multiphasic Personality Inventory (MMPI), you would examine peaks (elevations \\(\\ge\\) 70) and absences (\\(\\le\\) 30). T scores are calculated using Equation (2.2): \\[\\begin{equation} T = 50 + 10z \\tag{2.2} \\end{equation}\\] where \\(z\\) are z scores. Code scores$tScore &lt;- 50 + 10*scale(scores$rawData) An example histogram of T scores is in Figure 2.10. Code hist(scores$tScore, xlab = &quot;T Scores&quot;, main = &quot;Histogram of T Scores&quot;) Figure 2.10: Histogram of T Scores. 2.3.2.2.3 Standard scores Standard scores have a mean of 100 and a standard deviation of 15. Standard scores are frequently used for tests of intellectual ability, academic achievement, and cognitive ability. Intellectual disability is generally considered an I.Q. standard score less than 70 (two standard deviations below the mean), whereas giftedness is at 130 (two standard deviations above the mean). Standard scores with a mean of 100 and standard deviation of 15 are calculated using Equation (2.3): \\[\\begin{equation} \\text{standard score} = 100 + 15z \\tag{2.3} \\end{equation}\\] where \\(z\\) are z scores. Code scores$standardScore &lt;- 100 + 15*scale(scores$rawData) An example histogram of standard scores is in Figure 2.11. Code hist(scores$standardScore, xlab = &quot;Standard Scores&quot;, main = &quot;Histogram of Standard Scores&quot;) Figure 2.11: Histogram of Standard Scores. 2.3.2.2.4 Scaled scores Scaled scores are raw scores that have been converted to a standardized metric. The particular metric of the scaled score depends on the measure. On tests of intellectual or cognitive ability, scaled scores commonly have a mean of 10 and a standard deviation of 3. Using this metric, scaled scores can be calculated using Equation (2.4): \\[\\begin{equation} \\text{scale score} = 10 + 3z \\tag{2.4} \\end{equation}\\] where \\(z\\) are z scores. Code scores$scaledScore &lt;- 10 + 3*scale(scores$rawData) An example histogram of scaled scores is in Figure 2.12. Code hist(scores$scaledScore, xlab = &quot;Scale Scores&quot;, main = &quot;Histogram of Scaled Scores&quot;) Figure 2.12: Histogram of Scaled Scores. 2.3.2.2.5 Stanine scores Stanine scores (short for STANdard Nine) have a mean of 5 and a standard deviation of 2. The scores range from 1–9. Stanine scores are calculated using the bracketed proportions in Table 2.1. The lowest 4% receive a stanine score of 1, the next 7% receive a stanine score of 2, etc. Table 2.1: Calculating Stanine Scores Stanine Bracketed Percent Cumulative Percent z Score Standard Score 1 4% 4% below −1.75 below 74 2 7% 11% −1.75 to −1.25 74 to 81 3 12% 23% −1.25 to −0.75 81 to 89 4 17% 40% −0.75 to −0.25 89 to 96 5 20% 60% −0.25 to +0.25 96 to 104 6 17% 77% +0.25 to +0.75 104 to 111 7 12% 89% +0.75 to +1.25 111 to 119 8 7% 96% +1.25 to +1.75 119 to 126 9 4% 100% above +1.75 above 126 Code scores$stanineScore &lt;- NA scores$stanineScore[which(scores$percentileRank &lt;= 4)] &lt;- 1 scores$stanineScore[ which(scores$percentileRank &gt; 4 &amp; scores$percentileRank &lt;= 11)] &lt;- 2 scores$stanineScore[ which(scores$percentileRank &gt; 11 &amp; scores$percentileRank &lt;= 23)] &lt;- 3 scores$stanineScore[ which(scores$percentileRank &gt; 23 &amp; scores$percentileRank &lt;= 40)] &lt;- 4 scores$stanineScore[ which(scores$percentileRank &gt; 40 &amp; scores$percentileRank &lt;= 60)] &lt;- 5 scores$stanineScore[ which(scores$percentileRank &gt; 60 &amp; scores$percentileRank &lt;= 77)] &lt;- 6 scores$stanineScore[ which(scores$percentileRank &gt; 77 &amp; scores$percentileRank &lt;= 89)] &lt;- 7 scores$stanineScore[ which(scores$percentileRank &gt; 89 &amp; scores$percentileRank &lt;= 96)] &lt;- 8 scores$stanineScore[which(scores$percentileRank &gt; 96)] &lt;- 9 A histogram of stanine scores is in Figure 2.13. Code hist(scores$stanineScore, xlab = &quot;Stanine Scores&quot;, main = &quot;Histogram of Stanine Scores&quot;, breaks = seq(from = 0.5, to = 9.5, by = 1), xlim = c(0,10), xaxp = c(1,9,8)) Figure 2.13: Histogram of Stanine Scores. 2.4 Conclusion It is important to consider the types of data your data are because the types of data restrict what options are available to analyze the data. It is also important to consider whether the data were transformed because score transformations are not neutral—they can impact the results. 2.5 Suggested Readings Childs et al. (2021): https://dzchilds.github.io/stats-for-bio/data-transformations.html (archived at https://perma.cc/3YEB-QW2V) 2.6 Exercises References Bandalos, D. L. (2018). Measurement theory and applications for the social sciences. Guilford Publications. Childs, D. Z., Hindle, B. J., &amp; Warren, P. H. (2021). APS 240: Data analysis and statistics with R. https://dzchilds.github.io/stats-for-bio/ Wickham, H. (2021). tidyverse: Easily install and load the tidyverse. https://CRAN.R-project.org/package=tidyverse Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686. https://doi.org/10.21105/joss.01686 "],["constructs.html", "Chapter 3 Constructs 3.1 Types of Constructs 3.2 Differences in Measurement Expectations 3.3 Practical Issues 3.4 How To Estimate 3.5 Latent Variable Modeling: IRT, SEM, and CFA 3.6 Alternative Conceptualizations of Phenomena: A Network 3.7 Conclusion 3.8 Suggested Readings", " Chapter 3 Constructs 3.1 Types of Constructs A construct is a concept. A construct is generally considered a latent idea or phenomenon that is not directly observable. For example, depression could be considered a construct because we cannot directly measure someone’s level of depression. Rather, we infer a person’s level on the construct of depression by taking measurements of many indirect proxies that we think are indicators of the construct. An indicator is a measurement, and it could be thought of as a behavior or questionnaire item. Potential indicators of depression could include behaviors/items such as whether the person has low mood, low energy, sleep difficulties, loss of interest in formerly enjoyable activities, changes in weight or appetite, etc. Because constructs are latent, unobservable phenomena, they are often estimated using latent variable models, as described in Section 3.5. There are two main types of constructs: reflective constructs and formative constructs. 3.1.1 Reflective Construct With a reflective construct (or reflective model), the construct is the cause of the measures, and the construct is reflected by the indicators, see Figure 3.1 (Bollen &amp; Lennox, 1991). Figure 3.1: Reflective and Formative Constructs in Structural Equation Modeling. An example of a reflective construct is extraversion; see Figure 3.2. We conceptualize extraversion as a latent concept that, although not directly observable, influences the measurements of indicators of extraversion, such as whether the person enjoys talking to strangers, goes to lots of parties, is energetic, and is happy. In a reflective model, the answers that people give on the items are thought to reflect a certain disposition—in this case, extraversion. None of these indicators, in isolation, is considered a direct measure of extraversion. Nevertheless, we feel that we can get a better estimate of the concept of extraversion by capturing the variance that covaries across these indicators, i.e., the common (systematic) variance among the indicators. Thus, the latent factor is estimated to reflect the common variance among the indicators. Because a reflective construct influences the indicators, the indicators are called effect indicators. What we observe, theoretically, are the effects of the underlying construct, which influences scores on each individual measure. Figure 3.2: Extraversion as a Reflective Construct. With a reflective construct, we would expect that the items would show strong internal consistency reliability. That is, we would expect that the indicators would all be correlated with each other, because all indicators are thought to be a reflection of the underlying construct. But in reality, there is often still residual correlation between items even after accounting for the latent factor. In a measurement model of a reflective construct (see Figure 3.1), the lambda (\\(\\lambda\\)) values are called factor loadings that represent regression coefficients from observable indicators to latent variables. A factor loading is the relative weight that the item is given in the estimation of the latent factor, similar to the correlation between the construct and the measure. The stronger an item’s factor loading on the construct, the more relative weight a given item has in the estimation of the latent factor. The epsilon (\\(\\epsilon\\)) term reflects measurement error. Structural equation modeling is an application of the same way we think about measurement according to classical test theory, but in a latent variable framework. According to classical test theory, the observed score is thought to be some combination of the true score and error (\\(\\text{observed score} = \\text{true score} + \\text{error}\\); \\(X = T + e\\)). Or, in a multiple regression framework: \\(\\text{extraversion} = \\lambda_1 \\cdot \\text{strangers} + \\lambda_2 \\cdot \\text{parties} + \\lambda_3 \\cdot \\text{happy} + \\text{errors}\\). That is, the true score influences the observed score on each measure. If a person’s level on the construct (extraversion) changes, their score on each measure/indicator/item is expected to change (J. R. Edwards &amp; Bagozzi, 2000). 3.1.2 Formative Construct Formative constructs are the other fundamental way to understand the relation between a construct and a measure. With a formative construct (or formative model), the measures cause the construct; see Figure 3.1 (Bollen &amp; Lennox, 1991). That is, the construct is created by the measures. Because the indicators influence the formative construct, the indicators are called causal indicators. Examples of formative constructs include constructs such as socioeconomic status (SES), stress, and risk for cardiovascular disease. For instance, we may define SES such that it is the linear combination of a person’s educational attainment, occupational prestige, and income, as in Figure 3.3. With a formative construct, if a person changes in their level on the construct, we might not expect that their scores on all of the measures/indicators/items would change (J. R. Edwards &amp; Bagozzi, 2000). If a person’s SES changes, you would not necessarily expect the person’s score on all of these measures/items to change. For instance, if a person’s SES increases from T1 to T2, you would not necessarily expect that their educational attainment increased; it could be instead that their occupational prestige or income increased. Figure 3.3: Socioeconomic Status as a Formative Construct. In a structural equation model, the gamma (\\(\\gamma\\)) values are regression coefficients from exogenous variables to endogenous variables. Endogenous variables are variables whose values are determined by the model (i.e., they are influenced by other variables), whereas exogenous variables are variables whose values are not determined by the model (i.e., they are not influenced by other variables). In a measurement model of a formative construct (see Figure 3.1), the gamma values reflect the relative weight that the item is given in the estimation of the latent factor, similar to the correlation between the construct and the measure. The stronger an item’s factor loading on the construct, the more relative weight a given item has in the estimation of the latent factor. With a formative construct, the disturbance term, zeta (\\(\\zeta\\)), represents the part of the construct that is not explained by the measures, i.e., the measurement error. However, the measures are thought (or at least assumed) to be error-free causes of the construct. The covariances between the indicators are permissive of item correlations, but unlike the reflective model, the formative model does not require that the items are correlated. In other words, for a formative model, it is not necessary that the items are correlated, even though the items could be correlated. A formative construct is determined by the measures, so if you change the measures, you change the construct. A formative model requires that you include all facets of the construct. This is not true in a reflective model because all items of a unidimensional reflective construct are interchangeable as long as the construct is unidimensional and the items are equally reliable because the construct influences the measure scores. With a reflective construct, all you need to do is find the items that reflect the underlying construct, and you could create parallel measures of the construct. This is impossible to do in a formative model. Although formative conceptualizations of constructs may be useful in some cases, formative constructs can be controversial (J. R. Edwards, 2011; Hardin et al., 2011; Howell et al., 2007; Markus, 2018; Bollen &amp; Bauldry, 2011; Bollen &amp; Diamantopoulos, 2017; but see Bollen &amp; Lennox, 1991; Diamantopoulos et al., 2008). It is important to think carefully about the construct of interest, as guided by theory, and how best to assess and estimate it. 3.2 Differences in Measurement Expectations There are several differences in our measurement expectations based on the type of construct (formative or reflective) or indicator (causal or effect), as described by Bollen and Lennox (1991). One difference we would expect between effect versus causal indicators is the actual correlations between the indicators. Causal indicators may not be correlated, whereas effect indicators must be correlated. So, when we are assessing internal consistency reliability of a measure, and we assume that higher correlations between items suggests higher internal consistency reliability, we are assuming that indicators of the construct have an effects relation to the construct. A scale assessing causal indicators would likely have very low internal consistency reliability. A second difference we would expect between effect versus causal indicators is whether to use a sample or a census of indicators. The suggestions that we sample all facets of a construct when determining indicators is necessary only for causal indicators, for which a failure to assess a certain formative indicator changes the overall meaning of the construct. For effect indicators of a unidimensional construct, equally reliable measures are interchangeable. A third difference we would expect between effect versus causal indicators is the optimal correlations between the indicators. For effect indicators, high correlations are desirable. For causal indicators, too high of a correlation between indicators is likely to introduce the problem of multicollinearity, so lower correlations are optimal. Multicollinearity is when two or more predictors are correlated such that their regression coefficients with the outcome variable (in this case, the factor loadings on the latent factor, i.e., \\(\\lambda\\)) are erratic. 3.3 Practical Issues There are important practical issues to consider with both reflective and formative models. As described in Section 7.4 in the chapter on structural equation modeling, it is challenging to use formative constructs in a latent variable framework. In a structural equation modeling (SEM) framework, formative constructs must be used in the context of a model that allows some constraints. But one can easily estimate formative constructs outside of a SEM context, for example, with an average or weighted average composite, similar to principal component analysis. However, unlike latent variable approaches, linear composite approaches (e.g., mean or sum scores) do not account for measurement error. That is, linear composite approaches do not include a disturbance/error term. Therefore, a linear composite is not the same as a latent variable. If no disturbance term is included in a structural equation model, the model is equivalent to a linear composite approach (e.g., sum score, weighted average). Linear composite approaches are likely more defensible for formative constructs than for reflective constructs. 3.4 How To Estimate To estimate a reflective construct, a latent variable modeling approach is necessary, such as structural equation modeling, factor analysis, or item response theory. To estimate a formative construct, one may estimate an average or sum score across items (which gives each item equal weight), estimate a weighted average, use principal component analysis, use confirmatory composite analysis, or use structural equation modeling. Thus, it is important to think about the nature of the construct before deciding how to estimate it. The nature of a construct is a theoretical question. Data alone cannot determine whether a construct is reflective or formative. Just because items are highly correlated does not mean that scientists think of the construct as reflective. By contrast, if items are not highly correlated, it suggests that a single unidimensional reflective construct may not adequately capture reality—the variance may be better captured by a formative or by multiple reflective constructs (i.e., a multidimensional model). In sum, although empiricism is relevant, theory is necessary to inform our understanding of the nature of constructs. 3.5 Latent Variable Modeling: IRT, SEM, and CFA Item response theory (IRT), structural equation modeling (SEM), confirmatory composite analysis (CCA), and confirmatory factor analysis (CFA) are all approaches to latent variable modeling. CFA and IRT most commonly estimate reflective latent variables rather than formative latent variables. CCA estimates formative latent variables. SEM can estimate both reflective and formative latent variables. IRT is essentially a form of CFA with categorical, binary, or ordinal data. As we discuss in Chapter 7, SEM is CFA with regression paths that specify hypothesized causal relations between the latent variables (the structural component of the model). 3.6 Alternative Conceptualizations of Phenomena: A Network Reflective and formative constructs are considered latent variables. However, there are alternative conceptualizations of phenomena and psychopathology. For instance, one alternative approach is to conceptualize psychopathology as a characteristic that emerges out of the local influences of symptoms on other symptoms. This would be consistent with a network analysis approach to conceptualization of psychopathology. In the network analysis conceptualization, “depression” as a latent construct does not exist; what we call “depression” merely represents the influences of various symptoms on one another. In this approach, if only one item is used to assess each symptom, then the symptoms are observed (manifest) variables that are assumed to be measured without error. Note that the existence of latent constructs and local influences is not mutually exclusive. It could be for a given phenonemon, for instance, that there are underlying processes (representing a latent factor) that lead to a wide range of symptoms; and, simultaneously, that behaviors/symptoms/processes influence one another in a network. 3.7 Conclusion It is important to think about the nature of a construct before deciding how to estimate it. For a formative construct, the indicators influence the construct, and a linear composite such as a weighted mean or sum score is justifiable. For a reflective construct, the construct influences the indicators, and it is more appropriate to estimate it with a reflective latent variable that reflects the common variance among the indicators using structural equation modeling, factor analysis, or item response theory. 3.8 Suggested Readings Bollen &amp; Lennox (1991); Bollen &amp; Bauldry (2011); Bollen &amp; Diamantopoulos (2017) References Bollen, K. A., &amp; Bauldry, S. (2011). Three Cs in measurement models: Causal indicators, composite indicators, and covariates. Psychological Methods, 16(3), 265–284. https://doi.org/10.1037/a0024448 Bollen, K. A., &amp; Diamantopoulos, A. (2017). In defense of causal-formative indicators: A minority report. Psychological Methods, 22(3), 581–596. https://doi.org/10.1037/met0000056 Bollen, K. A., &amp; Lennox, R. D. (1991). Conventional wisdom on measurement: A structural equation perspective. Psychological Bulletin, 110(2), 305–314. https://doi.org/10.1037/0033-2909.110.2.305 Diamantopoulos, A., Riefler, P., &amp; Roth, K. P. (2008). Advancing formative measurement models. Journal of Business Research, 61(12), 1203–1218. https://doi.org/10.1016/j.jbusres.2008.01.009 Edwards, J. R. (2011). The fallacy of formative measurement. Organizational Research Methods, 14(2), 370–388. https://doi.org/10.1177/1094428110378369 Edwards, J. R., &amp; Bagozzi, R. P. (2000). On the nature and direction of relationships between constructs and measures. Psychological Methods, 5(2), 155–174. https://doi.org/10.1037/1082-989X.5.2.155 Hardin, A. M., Chang, J. C.-J., Fuller, M. A., &amp; Torkzadeh, G. (2011). Formative measurement and academic research: In search of measurement theory. Educational and Psychological Measurement, 71(2), 281–305. https://doi.org/10.1177/0013164410370208 Howell, R. D., Breivik, E., &amp; Wilcox, J. B. (2007). Reconsidering formative measurement. Psychological Methods, 12(2), 205–218. https://doi.org/10.1037/1082-989X.12.2.205 Markus, K. A. (2018). Three conceptual impediments to developing scale theory for formative scales. Methodology, 14(4), 156–164. https://doi.org/10.1027/1614-2241/a000154 "],["reliability.html", "Chapter 4 Reliability 4.1 Classical Test Theory 4.2 Measurement Error 4.3 Overview of Reliability 4.4 Getting Started 4.5 Types of Reliability 4.6 Applied Examples 4.7 Standard Error of Measurement 4.8 Influences of Measurement Error on Test–Retest Reliability 4.9 Effect of Measurement Error on Associations 4.10 Method Bias 4.11 Generalizability Theory (G-Theory) 4.12 Item Response Theory 4.13 The Problem of Low Reliability 4.14 Ways to Increase Reliability 4.15 Conclusion 4.16 Suggested Readings 4.17 Exercises", " Chapter 4 Reliability 4.1 Classical Test Theory 4.1.1 Overview To understand the concept of reliability, it is helpful to understand classical test theory (CTT), which is also known as “true score theory.” Classical test theory is one of various measurement theories, in addition to item response theory and generalizability theory. CTT has been the predominant measurement theory through the history of psychology. CTT is a theory of how test scores relate to a construct. A construct is the concept or characteristic that a measure is intended to assess. Assume you take a measurement of the same object multiple times (i.e., repeated measure). For example, you assess the mass of the same rock multiple times. However, you obtain different estimates of the rock’s mass each time. There are multiple possible explanations for this observation. One possible explanation could be that the rock is changing in its mass, which would be consistent with the idea proposed by the Greek philosopher Heraclitus that nothing is stable and the world is in flux. An alternative possibility, however, is that the rock is stable in its mass but the measurements are jittery—that is, they include error. Based on the possibility that the rock is stable and that differences in scores across time reflect measurement error, CTT proposes the true score formula in Equation (4.1): \\[ \\begin{aligned} X &amp;= T + e \\\\ \\text{observed score} &amp;= \\text{true score} + \\text{measurement error} \\end{aligned} \\tag{4.1} \\] \\(X\\) is the observed measurement or score, \\(T\\) is the classical (or psychometric) “true score,” and \\(e\\) is the measurement error (i.e., error score). This formula is depicted visually in the form of a path diagram in Figure 4.1. Figure 4.1: Classical Test Theory Formula in a Path Diagram. It is important to distinguish between the classical true score and the Platonic true score. The Platonic true score is the truth, and it does not depend on measurement. The Platonic true score is an abstract notion because it is not directly observable and is based on Platonic ideals and theories of the construct and what a person’s “true” level is on the construct. In CTT, we attempt to approximate the Platonic true score with the classical true score, (\\(T\\)). If we took infinite repeated observations (and the measurements had no carryover effect), the average score approaches the classical true score, \\(T\\). That is, \\(\\overline{X} = T \\text{ as number of observations} \\rightarrow \\infty\\). CTT attempts to partition variance into different sources. Variance is an index of scores’ variability, i.e., the degree to which scores differ. Variance is defined as the average squared deviation from the mean, as in Equation (4.2): \\[\\begin{equation} \\sigma^2_X = E[(X - \\mu)^2] \\tag{4.2} \\end{equation}\\] According to CTT, any observed measurement includes both true score (construct) variance (\\(\\sigma^2_T\\)) and error variance (\\(\\sigma^2_e\\)). Given the true score formula (\\(X = T + e\\)), this means that their variance is as follows (see Equation (4.3)): \\[ \\begin{aligned} \\sigma^2_X &amp;= \\sigma^2_T + \\sigma^2_e \\\\ \\text{observed score variance} &amp;= \\text{true score variance} + \\text{error variance} \\end{aligned} \\tag{4.3} \\] Nevertheless, the classical true score, \\(T\\), is the expected value and is not necessarily the same thing as the Platonic true score (Borsboom, 2003; Klein &amp; Cleary, 1969) because the expected value would need to be entirely valid/accurate (i.e., it would need to be the construct score) for it to be the Platonic true score. The expected score could also be influenced by systematic sources of error such as other constructs, which would not fall into the error portion of the CTT formula because, as described below, CTT assumes that error is random (not systematic). The distinctions between construct score, (classical) true score, and observed score, in addition to validity, reliability, systematic error, and random error are depicted in Figure 4.2. Figure 4.2: Distinctions Between Construct Score, True Score, and Observed Score, in Addition to Reliability, Validity, Systematic Error, and Random Error. (Adapted from W. Joel Schneider.) The true score formula is theoretically useful, but it is not practically useful because it is an under-identified equation and we do not know the values of \\(T\\) or \\(e\\) based on knowing the observed score (\\(X\\)). For instance, if we obtain an observed score of 10, our formula is \\(10 = T + e\\), and we do not know what the true score or error is. As a result, CTT makes several simplifying assumptions so we can estimate how stable (reliable) or noisy the measure is and what proportion of the observed score reflects true score versus measurement error. \\(E(e) = 0\\) The first assumption of CTT is that the expected value of the error (i.e., error scores) is zero. Basically, the error component of the observed scores is expected to be random with a mean of zero. The likelihood that the observed score is an overestimate of \\(T\\) is assumed to be the same as the likelihood that the observed score is an underestimate of \\(T\\). In other words, the distribution of error scores above \\(T\\) is assumed to be the same as the distribution of error scores below \\(T\\). In reality, this assumption is likely false in many situations. For instance, social desirability bias is a systematic error where people rate themselves as better than they actually are; thus, social desirability results in biasing scores in a particular direction across respondents, so such an error would not be entirely random. But using the assumption that the expected value of \\(e\\) is zero also informs that the expected value of the observed score (\\(X\\)) equals the expected value of the true score (\\(T\\)), as in Equation (4.4): \\[ \\begin{aligned} E(X) &amp;= E(T + e) \\\\ &amp;= E(T) + E(e) \\\\ &amp;= E(T) + 0 \\\\ &amp;= E(T) \\end{aligned} \\tag{4.4} \\] \\(r_{T,e} = 0\\) The second assumption of CTT is that the correlation between \\(T\\) and \\(e\\) is zero—that is, people’s true scores are uncorrelated with the error around their measurement (i.e., people’s error scores). However, this assumption is likely false in many situations. For instance, one can imagine that, on a paper-and-pencil intelligence test, scores may have greater error for respondents with lower true scores and may have less error for respondents with higher true scores. \\(r_{e_1, e_2} = 0\\) The third assumption of CTT is that the error is uncorrelated across time—that is, people’s error scores at time 1 (\\(e_1\\)) are not associated with their error scores at time 2 (\\(e_2\\)). However, this assumption is also likely false in many situations. For example, if some people have a high social desirability bias at time 1, they are likely to also have a high social desirability bias at time 2. That is, the error around measurements of participants is likely to be related across time. These three assumptions are implicit in the path analytic diagram in Figure 4.3, which depicts the CTT approach to understanding reliability of a measure across two time points. Figure 4.3: Reliability of a Measure Across Two Time Points, as Depicted in a Path Diagram. In path analytic (and structural equation modeling) language, rectangles represent variables we observe, and circles represent latent (i.e., unobserved) variables. The observed scores at time 1 (\\(X_1\\)) and time 2 (\\(X_2\\)) are entities we observe, so they are represented by rectangles. We do not directly observe the true scores (\\(T_1, T_2\\)) and error scores (\\(e_1, e_2\\)), so they are considered latent entities and are represented by circles. Single-headed arrows indicate regression paths, where conceptually, one variable is thought to influence another variable. As the model depicts, the observed scores are thought to be influenced both by true scores and by error scores. We also expect the true scores at time 1 (\\(T_1\\)) and time 2 (\\(T_2\\)) to be correlated, so we have a covariance path, as indicated by a double-headed arrow. A covariance is an unstandardized index of the strength of association between two variables. Because a covariance is unstandardized, its scale depends on the scale of the variables. The covariance between two variables is the average product of their deviations from their respective means, as in Equation (4.5): \\[\\begin{equation} \\sigma_{XY} = E[(X - \\mu_X)(X - \\mu_Y)] \\tag{4.5} \\end{equation}\\] The covariance of a variable with itself is equivalent to its variance, as in Equation (4.6): \\[ \\begin{aligned} \\sigma_{XX} &amp;= E[(X - \\mu_X)(X - \\mu_X)] \\\\ &amp;= E[(X - \\mu_X)^2] \\\\ &amp;= \\sigma^2_X \\end{aligned} \\tag{4.6} \\] By contrast, a correlation is a standardized index of the strength of association between two variables. Because a correlation is standardized (fixed between [−1,1]), its scale does not depend on the scales of the variables. In this figure, no other parameters (regressions, covariances, or means) are specified, so the following are implicit in the diagram: \\(E(e) = 0\\) \\(r_{T,e} = 0\\) \\(r_{e_1, e_2} = 0\\) The factor loadings reflect the magnitude that the latent factor influences the observed variable. In this case, the true scores influence the observed scores with a magnitude of \\(\\sqrt{r_{xx}}\\), which is known as the index of reliability. The index of reliability is the theoretical estimate of the correlation between the true scores and the observed scores. This is depicted in Figure 4.4. Figure 4.4: Reliability of a Measure Across Two Time Points, as Depicted in a Path Diagram; Includes the Index of Reliability. We can use path tracing rules to estimate the reliability of the measure, where the reliability of the measure, i.e., the coefficient of reliability (\\(r_{xx}\\)), is estimated as the correlation between the observed score at time 1 (\\(x_1\\)) and the observed score at time 2 (\\(x_2\\)). According to path tracing rules (Pearl, 2013), the correlation between \\(x_1\\) and \\(x_2\\) is equal to the sum of the standardized coefficients of all the routes through which \\(x_1\\) and \\(x_2\\) are connected. The contribution of a given route to the correlation between \\(x_1\\) and \\(x_2\\) is equal to the product of all standardized coefficients on that route that link \\(x_1\\) and \\(x_2\\) that move in the following directions: (a) forward (e.g., \\(T_1\\) to \\(x_1\\)) or (b) backward once and then forward (e.g., \\(x_1\\) to \\(T_1\\) to \\(T_2\\) to \\(x_2\\)). Path tracing does not allow moving forward and then backward—that is, it does not allow retracing (e.g., \\(e\\) to \\(x_1\\) to \\(T_1\\)) in the same route. It also does not allow passing more than one curved arrow (covariance path) or through the same variable twice in the same route. Once you know the contribution of each route to the correlation, you can calculate the total correlation between the two variables as the sum of the contribution of each route. Therefore, using one route, we can calculate the association between \\(x_1\\) and \\(x_2\\) as in Equation (4.7): \\[ \\begin{aligned} r_{x_1,x_2} &amp;= \\sqrt{r_{xx}} \\times r_{T_1,T_2} \\times \\sqrt{r_{xx}} \\\\ &amp;= r_{T_1,T_2} \\times r_{xx} \\\\ &amp;= \\text{correlation of true scores across time} \\times \\text{reliability} \\end{aligned} \\tag{4.7} \\] When dealing with a stable construct, we would assume that the correlation between true scores across time is 1.0: \\(r_{T_1,T_2} = 1.0\\), as depicted in Figure 4.5. Figure 4.5: Reliability of a Measure of a Stable Construct Across Two Time Points, as Depicted in a Path Diagram. Then, to calculate the association between \\(x_1\\) and \\(x_2\\) of a stable construct, we can use path tracing rules as in Equation (4.8): \\[ \\begin{aligned} r_{x_1,x_2} &amp;= \\sqrt{r_{xx}} \\times r_{T_1,T_2} \\times \\sqrt{r_{xx}} \\\\ &amp;= \\sqrt{r_{xx}} \\times 1 \\times \\sqrt{r_{xx}} \\\\ &amp;= r_{xx} \\\\ &amp;= \\text{coefficient of reliability} \\end{aligned} \\tag{4.8} \\] That is, for a stable construct (i.e., whose true scores are perfectly correlated across time; \\(r_{T_1,T_2} = 1.0\\)), we estimate reliability as the correlation between the observed scores at time 1 (\\(x_1\\)) and the observed scores at time 2 (\\(x_2\\)). This is known as test–retest reliability. We therefore assume that the extent to which the correlation between \\(x_1\\) and \\(x_2\\) is less than one reflects measurement error (an unstable measure), rather than people’s changes in their true score on the construct (an unstable construct). As described above, the reliability coefficient (\\(r_{xx}\\)) is the association between a measure and itself over time or with another measure in the domain. By contrast, the reliability index (\\(r_{xT}\\)) is the correlation between observed scores on a measure and the true scores (Nunnally &amp; Bernstein, 1994). The reliability index is the square root of the reliability coefficient, as in Equation (4.9). \\[ \\begin{aligned} r_{xT} &amp;= \\\\ &amp;= \\sqrt{r_{xx}} \\\\ &amp;= \\text{index of reliability} \\end{aligned} \\tag{4.9} \\] 4.1.2 Four CTT Measurement Models There are four primary measurement models in CTT (J. M. Graham, 2006): parallel tau-equivalent essentially tau-equivalent congeneric 4.1.2.1 Parallel The parallel measurement model is the most stringent measurement model for use in estimating reliability. In CTT, a measure is considered parallel if the true scores and error scores are equal across items. That is, the items must be unidimensional and assess the same construct, on the same scale, with the same degree of precision, and with the same amount of error (J. M. Graham, 2006). Items are expected to have the same strength of association (i.e., factor loading or discrimination) with the construct. 4.1.2.2 Tau-Equivalent The tau (\\(\\tau\\))-equivalent measurement model is the same as the parallel measurement model, except error scores are allowed to differ across items. That is, a measure is tau-equivalent if the items are unidimensional and assess the same construct, on the same scale, with the same degree of precision, but with possibly different amounts of error (J. M. Graham, 2006). In other words, true scores are equal across items but each item is allowed to have unique error scores. Items are expected to have the same strength of association with the construct. Variance that is unique to a specific item is assumed to be error variance. 4.1.2.3 Essentially Tau-Equivalent The essentially tau (\\(\\tau\\))-equivalent model is the same as the tau-equivalent measurement model, except items are allowed to differ in their precision. That is, a measure is essentially tau-equivalent if the items are unidimensional and assess the same construct, on the same scale, but with possibly different degrees of precision, and with possibly different amounts of error (J. M. Graham, 2006). The essentially tau-equivalent model allows item true scores to differ by a constant that is unique to each pair of variables. The magnitude of the constant reflects the degree of imprecision and influences the mean of the item scores but not its variance or covariances with other items. Items are expected to have the same strength of association with the construct. 4.1.2.4 Congeneric The congeneric measurement model is the least restrictive measurement model. The congeneric measurement model is the same as the essentially tau (\\(\\tau\\))-equivalent model, except items are allowed to differ in their scale. That is, a measure is congeneric if the items are unidimensional and assess the same construct, but possibly on a different scale, with possibly different degrees of precision, and with possibly different amounts of error (J. M. Graham, 2006). Items are not expected to have the same strength of association with the construct. 4.2 Measurement Error Measurement error is the difference between the measured (observed) value and the true value. All measurements come with uncertainty and measurement error. Even a measure of something as simple as whether someone is dead has error. There are two main types of measurement error: systematic (nonrandom) error and unsystematic (random) error. In addition, measurement error can be within-person, between-person, or both. 4.2.1 Systematic (Nonrandom) Error An example of systematic error is depicted in Figure 4.6. Systematic error is error that influences consistently for a person or across the sample. An error is systematic if the error always occurs, with the same value, when using the measure in the same way and in the same case. An example of a systematic error is a measure that consistently assesses constructs other than the construct the measure was designed to assess. For instance, if a test written in English to assess math skills is administered in a nonnative English-speaking country, some portion of the scores will reflect variance attributable to English reading skills rather than the construct of interest (math skills). Other examples of systematic error include response styles or subjective, idiosyncratic judgments by a rater—for instance, if the rater’s judgments are systematically harsh or lenient. A systematic error affects the average score (i.e., resulting in bias), which makes the group-level estimates less accurate and makes the measurements for an individual less accurate. As depicted in Figure 4.6, systematic error does not affect the variability of the scores but it does affect the mean of the scores, so the person-level mean and group-level mean are less accurate. In other words, a systematic error leads to a biased estimate of the average. However, multiple systematic errors may simultaneously coexist and can operate in the same direction (exacerbating the effects of bias) or in opposite directions (hiding the extent of bias). Figure 4.6: Systematic Error. 4.2.2 Unsystematic (Random) Error An example of unsystematic (random) error is depicted in Figure 4.7. Random error occurs due to chance. For instance, a random error could arise from a participant being fatigued on a particular testing day or from a participant getting lucky in guessing the correct answer. Random error does not have consistent effects for a person or across the sample, and it may vary from one observation to another. Random error does not (systematically) affect the average, i.e., the group-level estimate—random error affects only the variability around the average (noise). However, random error makes measurements for an individual less accurate. A large number of observations of the same construct cancels out random error but does not cancel out systematic error. As depicted in Figure 4.7, random error does not affect the mean of the scores, but it does increase the variability of the scores. In other words, the group-level mean is still accurate, but individuals’ scores are less precise. Figure 4.7: Random Error. 4.2.3 Within-Person Error Consider two data columns, one column for participants’ scores at time 1 and another column for participants’ scores at time 2. Adding within-person error would mean adding noise (\\(e\\)) within the given row (or rows) for the relevant participant(s). Adding between-person error would mean adding noise (\\(e\\)) across the rows within the column. Within-person error occurs within a particular person. For instance, you could add within-person error to a data set by adding error to the given row (or rows) for the relevant participant(s). 4.2.4 Between-Person Error Between-person error occurs across the sample. You could add between-person random error to a variable by adding error across the rows, within a column. 4.2.5 Types of Measurement Error There are four nonmutually exclusive types of measurement error: within-person random error, within-person systematic error, between-person random error, and between-person systematic error. The four types of measurement error are depicted in Figure 4.8, as adapted from Willett (2012). Figure 4.8: Types of Measurement Error. 4.2.5.1 Within-Person Random Error Adding within-person random error would involve adding random noise (\\(e\\)) to the given row (or rows) for the relevant participant(s). This could reflect momentary fluctuations in the assessment for a specific person. When adding within-person random error, the person’s and group’s measurements show no bias, i.e., there is no consistent increase or decrease in the scores from time 1 to time 2 (at least with a sample size large enough to cancel out the random error, according to the law of large numbers). A person’s average approximates their true score if many repeated measurements are taken. A group’s average approximates the sample mean’s true score, especially when averaging the repeated measures across time. The influence of within-person random error is depicted in Figure 4.9. Figure 4.9: Within-Person Random Error. 4.2.5.2 Within-Person Systematic Error Adding within-person systematic error would involve adding systematic noise (\\(e\\)) (the same variance across columns) to the given row (or rows), reflecting the relevant participant(s). These are within-person effects that are consistent across time. For example, social desirability bias is high for some people and low for others. Another instance in which within-person systematic error could exist is when one or more people consistently misinterpret a particular question. Within-person systematic error increases person-level bias because the person’s mean shows a greater difference from their true score. The influence of within-person systematic error is depicted in Figure 4.10. Figure 4.10: Within-Person Systematic Error. 4.2.5.3 Between-Person Random Error Adding between-person random error at time 2 would involve adding random noise (\\(e\\)) across the rows, within the column. Between-person random error would result in less accurate scores at the person level but would not result in bias at the group level. At a given timepoint, it results in over-estimates of the person’s true score for some people and under-estimates for other people, i.e., there is no consistent pattern across the sample. Thus, the group average approximates the sample’s mean true score (at least with a sample size large enough to cancel out the random error, according to the law of large numbers). In addition, the average of repeated measurements of the person’s score would approximate the person’s true score. However, the group’s variance is inflated. The influence of between-person random error is depicted in Figure 4.11. Figure 4.11: Between-Person Random Error. 4.2.5.4 Between-Person Systematic Error Adding between-person systematic error at time 2 would involve adding systematic noise (\\(e\\)) across the rows, within the column. Between-person systematic error results from within-person error that tends to be negative or positive across participants. For instance, this could reflect an influence with a shared effect across subjects. For example, social desirability leads to a positive group-level bias for rating their socially desirable attributes. Another example would be when a research assistant enters values wrong at time 2 (e.g., adding 10 to all participants’ scores). Between-person systematic error increases bias because it results in a greater group mean difference from the group’s mean true score. The influence of between-person systematic error is depicted in Figure 4.12. Figure 4.12: Between-Person Systematic Error. 4.2.6 Summary In sum, all types of measurement error (whether systematic or random) lead to less accurate scores for an individual. But different kinds of error have different implications. Systematic and random error have different effects on accuracy at the group-level. Systematic error leads to less accurate estimates at the group-level, whereas random error does not. CTT assumes that all error is random. According to CTT, as the number of measurements approaches infinity, the mean of the measurements gets closer to the true score, because the random errors cancel each other out. With more measurements, we reduce our uncertainty and increase our precision. According to CTT, if we take many measurements and the average of the measurements is 10, we have some confidence that the true score \\((T) \\approx 10\\). In reality, however, error for a given measure likely includes both systematic and random error. 4.3 Overview of Reliability The “Standards for Educational and Psychological Testing” set the standard for educational and psychological assessment and are jointly published by the American Educational Research Association, American Psychological Association, and National Council on Measurement in Education. According to the “Standards” (American Educational Research Association et al., 2014, p. 33), reliability is the “consistency of the scores across instances of the testing procedure.” In this book, we define reliability as how much repeatability, consistency, and precision the scores from a measure have. Reliability (\\(r_{xx}\\)) has been defined mathematically as the proportion of observed score variance (\\(\\sigma^2_X\\)) that is attributable to true score variance (\\(\\sigma^2_T\\)), as in Equation (4.10): \\[ \\begin{aligned} r_{xx} &amp;= \\frac{\\sigma^2_T}{\\sigma^2_T + \\sigma^2_e} \\\\ &amp;= \\frac{\\sigma^2_T}{\\sigma^2_X} \\\\ &amp;= \\frac{\\text{true score variance}}{\\text{observed score variance}} \\\\ \\end{aligned} \\tag{4.10} \\] An alternative formulation is that reliability (\\(r_{xx}\\)) is the lack of error variance or the degree to which observed scores are correlated with true scores or uncorrelated with error scores. In CTT, reliability can be conceptualized in four primary ways, as depicted in Figure 4.13 (Furr, 2017). Figure 4.13: Four Different Ways of Conceptualizing Reliability. However, we cannot calculate reliability because we cannot measure the true score component of an observation. Therefore, we estimate reliability (the coefficient of reliability) based on the relation between two observations of the same measure (for test–retest reliability) or using other various estimates of reliability. The coefficient of reliability can depend on several factors. Reliability is inversely related to the amount of measurement error. The coefficient of reliability, like correlation, depends on the degree of spread (variability) of the scores. If the scores at one or both time points show restricted range, the scores will show a weaker association and coefficient of reliability, as shown in Figure 4.19. An ideal way to estimate the reliability of a measure would be to take a person and repeatedly measure them many times to get an estimate of their true score and each measurement’s deviation from their true score, and to do this for many people. However, this is rarely done in practice. Instead of taking one person and repeatedly measuring them many times, we typically estimate reliability by taking many people and doing repeated measures twice. This is a shortcut to estimate reliability, but even this shorter approach is not often done. In short, researchers rarely estimate the test–retest reliability of the measures they use. Reliability can also be related to the number of items in the measure. In general, the greater the number of items, the more reliable the measure (assuming the items assess the same construct) because we are averaging out random error. We never see the true scores or the error scores, so we cannot compute reliability—we can only estimate it from the observed scores. This estimate of reliability gives a probabilistic answer of the reliability of the measure, rather than an absolute answer. 4.3.1 How Reliable Is Reliable Enough? As described by Nunnally &amp; Bernstein (1994), how reliable a measure should be depends on the proposed uses. If it is early in the research process, and the focus is on group-level inferences (e.g., associations or group differences), modest reliability (e.g., .70) may be sufficient and save time and money. Then, the researcher can see what the associations would be when disattenuated for unreliability, as described in Section 5.6 of the chapter on validity. If the disattenuated associations are promising, it may be worth increasing the reliability of the measure. Associations are only weakly attenuated above a reliability of .80, so achieving a reliability coefficient of .80 may be an appropriate target for basic research. However, when making decisions about individual people from their score on a measure, reliability and precision are more important (than when making group-level inferences) because small differences in scores can lead to different decisions. Nunnally &amp; Bernstein (1994) recommend that measures have at least a reliability of .90 and—when making important decisions about individual people—that measures preferably have a reliability of .95 or higher. Nevertheless, they also note that one should not switch to a less valid measure merely because it is more reliable. 4.3.2 Standard Error of Measurement (SEM) The estimate of reliability gives a general idea of the degree of uncertainty you have of a person’s true score given their observed score. From this, we can estimate the standard error of measurement, which estimates the extent to which an observed score deviates from a true score. The standard error of measurement indicates the typical distance of the observed score from the true score. The formula for the standard error of measurement is in Equation (4.11): \\[\\begin{equation} \\text{standard error of measurement (SEM)} = \\sigma_x \\sqrt{1 - r_{xx}} \\tag{4.11} \\end{equation}\\] where \\(\\sigma_x\\) represents the standard deviation of scores. Thus, the standard error of measurement is directly related to the reliability of the measure. The higher the reliability, the lower the standard error of measurement. The standard error of measurement as a function of reliability of the measure and the standard deviation of scores is depicted in Figure 4.14. Figure 4.14: Standard Error of Measurement as a Function of Reliability. The derivation of the SEM (from W. Joel Schneider) is in Equation (4.12): \\[ \\begin{aligned} \\text{Remember, based on } X = T + e: &amp;&amp; \\sigma^2_X &amp;= \\sigma^2_T + \\sigma^2_e \\\\ \\text{Solve for }\\sigma^2_T: &amp;&amp; \\sigma^2_T &amp;= \\sigma^2_X - \\sigma^2_e \\\\ \\text{Remember:} &amp;&amp; r_{xx} &amp;= \\frac{\\sigma^2_T}{\\sigma^2_X} \\\\ \\text{Substitute for } \\sigma^2_T: &amp;&amp; &amp;= \\frac{\\sigma^2_X - \\sigma^2_e}{\\sigma^2_X} \\\\ \\text{Multiply by } \\sigma^2_X: &amp;&amp; \\sigma^2_X \\cdot r_{xx} &amp;= \\sigma^2_X - \\sigma^2_e \\\\ \\text{Solve for } \\sigma^2_e: &amp;&amp; \\sigma^2_e &amp;= \\sigma^2_X - \\sigma^2_X \\cdot r_{xx} \\\\ \\text{Factor out } \\sigma^2_X: &amp;&amp; \\sigma^2_e &amp;= \\sigma^2_X (1 - r_{xx}) \\\\ \\text{Take the square root:} &amp;&amp; \\sigma_e &amp;= \\sigma_X \\sqrt{1 - r_{xx}} \\end{aligned} \\tag{4.12} \\] The SEM is equivalent to the standard deviation of measurement error (\\(e\\)) (Lek &amp; Van De Schoot, 2018), as in Equation (4.13): \\[ \\begin{aligned} \\text{standard error of measurement (SEM)} &amp;= \\sigma_x \\sqrt{1 - r_{xx}} \\\\ &amp;= \\sqrt{\\sigma_x^2} \\sqrt{1 - r_{xx}} \\\\ &amp;= \\sqrt{\\sigma_x^2(1 - r_{xx})} \\\\ &amp;= \\sqrt{\\sigma_x^2 - \\sigma_x^2 \\cdot r_{xx}} \\\\ &amp;= \\sqrt{\\sigma_x^2 - \\sigma_x^2 \\frac{\\sigma^2_T}{\\sigma_x^2}} \\\\ &amp;= \\sqrt{\\sigma_x^2 - \\sigma^2_T} \\\\ &amp;= \\sqrt{\\sigma^2_e} \\\\ &amp;= \\sigma_e \\\\ \\end{aligned} \\tag{4.13} \\] Around 95% of scores would be expected to fall within \\(\\pm 2\\) SEMs of the true score (or, more precisely, within \\(\\pm 1.959964\\) SEMs of the true score). In other words, 95% of the time, the true score is expected to fall within \\(\\pm 2\\) SEMs of the observed score. Given an observed score of \\(X = 15\\) and \\(\\text{SEM} = 2\\), the 95% confidence interval of the true score is [11, 19]. So if a person gets a score of 15 on the measure, 95% of the time, their true score is expected to fall within 11–19. We provide an empirical example of estimating the SEM in Section 4.7. Based on the preceding discussion, consider the characteristics of measures that make them more useful from a reliability perspective. A useful measure would show wide variation across people (individual differences), so we can more accurately estimate its reliability. And we would expect a useful measure to show consistency, stability, precision, and reliability of scores. 4.4 Getting Started 4.4.1 Load Libraries Code library(&quot;petersenlab&quot;) #to install: install.packages(&quot;remotes&quot;); remotes::install_github(&quot;DevPsyLab/petersenlab&quot;) library(&quot;psych&quot;) library(&quot;blandr&quot;) library(&quot;MBESS&quot;) library(&quot;lavaan&quot;) library(&quot;semTools&quot;) library(&quot;psychmeta&quot;) library(&quot;irrCAC&quot;) library(&quot;irrICC&quot;) library(&quot;irrNA&quot;) library(&quot;gtheory&quot;) library(&quot;performance&quot;) library(&quot;MOTE&quot;) library(&quot;tidyverse&quot;) library(&quot;tinytex&quot;) library(&quot;knitr&quot;) library(&quot;rmarkdown&quot;) library(&quot;bookdown&quot;) 4.4.2 Prepare Data 4.4.2.1 Simulate Data For reproducibility, we set the seed below. Using the same seed will yield the same answer every time. There is nothing special about this particular seed. Code sampleSize &lt;- 100 set.seed(52242) rater1continuous &lt;- rnorm(n = sampleSize, mean = 50, sd = 10) rater2continuous &lt;- rater1continuous + rnorm( n = sampleSize, mean = 0, sd = 4) rater3continuous &lt;- rater2continuous + rnorm( n = sampleSize, mean = 0, sd = 8) rater1categorical &lt;- sample( c(0,1), size = sampleSize, replace = TRUE) rater2categorical &lt;- rater1categorical rater3categorical &lt;- rater1categorical rater2categorical[ sample(1:length(rater2categorical), size = 10, replace = FALSE)] &lt;- 0 rater3categorical[ sample(1:length(rater3categorical), size = 10, replace = FALSE)] &lt;- 1 time1 &lt;- rnorm(n = sampleSize, mean = 50, sd = 10) time2 &lt;- time1 + rnorm(n = sampleSize, mean = 0, sd = 4) time3 &lt;- time2 + rnorm(n = sampleSize, mean = 0, sd = 8) item1 &lt;- rnorm(n = sampleSize, mean = 50, sd = 10) item2 &lt;- item1 + rnorm(n = sampleSize, mean = 0, sd = 4) item3 &lt;- item2 + rnorm(n = sampleSize, mean = 0, sd = 8) item4 &lt;- item3 + rnorm(n = sampleSize, mean = 0, sd = 12) Person &lt;- as.factor(rep(1:6, each = 8)) Occasion &lt;- Rater &lt;- as.factor(rep(1:2, each = 4, times = 6)) Item &lt;- as.factor(rep(1:4, times = 12)) Score &lt;- c( 9,9,7,4,9,8,5,5,9,8,4,6, 6,5,3,3,8,8,6,2,8,7,3,2, 9,8,6,3,9,6,6,2,10,9,8,7, 8,8,9,7,6,4,5,1,3,2,3,2) 4.4.2.2 Add Missing Data Adding missing data to dataframes helps make examples more realistic to real-life data and helps you get in the habit of programming to account for missing data. Code rater1continuous[c(5,10)] &lt;- NA rater2continuous[c(10,15)] &lt;- NA rater3continuous[c(10)] &lt;- NA rater1categorical[c(5,10)] &lt;- NA rater2categorical[c(10,15)] &lt;- NA rater3categorical[c(10)] &lt;- NA time1[c(5,10)] &lt;- NA time2[c(10,15)] &lt;- NA time3[c(10)] &lt;- NA item1[c(5,10)] &lt;- NA item2[c(10,15)] &lt;- NA item3[c(10)] &lt;- NA item4[c(10)] &lt;- NA 4.4.2.3 Combine Data Into Dataframe Code mydata &lt;- data.frame( subid = 1:sampleSize, rater1continuous, rater2continuous, rater3continuous, rater1categorical, rater2categorical, rater3categorical, time1, time2, time3, item1, item2, item3, item4) pio_cross_dat &lt;- data.frame(Person, Item, Score, Occasion) 4.5 Types of Reliability Reliability is not one thing. There are several types of reliability. In this book, we focus on test–retest, inter-rater, intra-rater, parallel-forms, and internal consistency reliability. 4.5.1 Test–Retest Reliability Test–retest reliability is defined as the consistency of scores across time. Typically, this is based on a two-week retest interval. The intent of a two-week interval between the original testing and the retest is to provide adequate time to pass to reduce any carryover effects from the original testing while not allowing too much time to pass such that the person’s level on the construct (i.e., true scores) would change. A carryover effect is an effect of the experimental condition that affects the participant’s behavior at a later time. Examples of carryover effects resulting from repeated measurement can include fatigue, boredom, learning (practice effects), etc. Another potential issue is that measurement error can be correlated across the two measurements. Test–retest reliability controls for transient error and random response error. If the construct is not stable across time (i.e., people’s true scores change), test–retest reliability is not relevant because the CTT approach to estimating reliability assumes that the true scores are perfectly correlated across time (see Section 4.1). The length of the optimal retest interval depends on the construct of interest. For a construct in which people’s levels change rapidly, a shorter retest interval may be appropriate. But one should pay attention to ways to reduce potential carryover effects. By contrast, if the retest interval is too long, people’s levels on the construct may change during that span. If people’s levels on the construct change from test to retest, we can no longer assume that the true scores are perfectly correlated across time, which would violate CTT assumptions for estimating test–retest reliability of a measure. The longer the retest interval, the smaller the observed association between scores across time will tend to be. For weak associations obtained from a lengthy retest interval, it can be difficult to determine how much of this weak association reflects measurement unreliability versus people’s change in their levels on the construct. Thus, when conducting studies to evaluate test–retest reliability, it is important to consider the length of the retest interval and ways to reduce carryover effects. 4.5.1.1 Coefficient of Stability (and Coefficient of Dependability) The coefficient of stability is the most widely used index when reporting the test–retest reliability of a measure. It is estimated using a Pearson correlation of the scores at time 1 with the score at time 2. That is, the coefficient of stability assesses the stability of individual differences (i.e., rank-order stability). The Pearson correlation is called the coefficient of stability when the length of the retest interval (the delay between test and retest) is on the order of days or weeks. If the retest occurs almost at the same time as the original test (e.g., a 45-minute delay), the Pearson correlation is called the coefficient of dependability (Revelle &amp; Condon, 2019). We estimate the coefficient of stability below: Code cor.test(x = mydata$time1, y = mydata$time2) Pearson&#39;s product-moment correlation data: mydata$time1 and mydata$time2 t = 25.009, df = 95, p-value &lt; 2.2e-16 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.8994368 0.9539169 sample estimates: cor 0.9317389 Code cor(mydata[,c(&quot;time1&quot;,&quot;time2&quot;,&quot;time3&quot;)], use = &quot;pairwise.complete.obs&quot;) time1 time2 time3 time1 1.0000000 0.9317389 0.8290914 time2 0.9317389 1.0000000 0.8425997 time3 0.8290914 0.8425997 1.0000000 The r value of \\(.93\\) indicates a strong positive association. Figure 4.15 depicts a scatterplot of the time 1 scores on the x-axis and time 2 scores on the y-axis. Code plot(mydata$time1, mydata$time2, main = substitute( paste(italic(r), &quot; = &quot;, x, sep = &quot;&quot;), list(x = apa(corValue, 2, leading = FALSE)))) abline(lm(time2 ~ time1, data = mydata), col = &quot;black&quot;) mydataNoMissing &lt;- na.omit(mydata[,c(&quot;time1&quot;,&quot;time2&quot;)]) lines(lowess(mydataNoMissing$time1, mydataNoMissing$time2), col = &quot;red&quot;) Figure 4.15: Test–Retest Reliability Scatterplot. The black line is the best-fitting linear line. The red line is a locally estimated scatterplot smoothing (LOESS) line, which uses nonparametric estimation of the best fit. 4.5.1.1.1 Considerations About the Correlation Coefficient The correlation coefficient ranges from −1.0 to +1.0. The correlation coefficient (\\(r\\)) tells you two things: (1) the direction of the association (positive or negative) and (2) the magnitude of the association. If the correlation coefficient is positive, the association is positive. If the correlation coefficient is negative, the association is negative. If the association is positive, as X increases, Y increases (or conversely, as X decreases, Y decreases). If the association is negative, as X increases, Y decreases (or conversely, as X decreases, Y increases). The smaller the absolute value of the correlation coefficient (i.e., the closer the \\(r\\) value is to zero), the weaker the association and the flatter the slope of the best-fit line in a scatterplot. The larger the absolute value of the correlation coefficient (i.e., the closer the absolute value of the \\(r\\) value is to one), the stronger the association and the steeper the slope of the best-fit line in a scatterplot. See Figure 4.16 for a range of different correlation coefficients and what some example data may look like for each direction and strength of association. Figure 4.16: Correlation Coefficients. Keep in mind that the Pearson correlation examines the strength of the linear association between two variables. If the association between two variables is nonlinear, the Pearson correlation provides the strength of the linear trend and may not provide a meaningful index of the strength of the association between the variables. For instance, Anscombe’s quartet includes four sets of data that have nearly identical basic descriptive statistics (see Tables 4.1 and 4.2), including the same bivariate correlation, yet have very different distributions and whose association takes very different forms (see Figure 4.17). Table 4.1: Anscombe’s Quartet. x1 y1 x2 y2 x3 y3 x4 y4 10 8.04 10 9.14 10 7.46 8 6.58 8 6.95 8 8.14 8 6.77 8 5.76 13 7.58 13 8.74 13 12.74 8 7.71 9 8.81 9 8.77 9 7.11 8 8.84 11 8.33 11 9.26 11 7.81 8 8.47 14 9.96 14 8.1 14 8.84 8 7.04 6 7.24 6 6.13 6 6.08 8 5.25 4 4.26 4 3.1 4 5.39 19 12.5 12 10.84 12 9.13 12 8.15 8 5.56 7 4.82 7 7.26 7 6.42 8 7.91 5 5.68 5 4.74 5 5.73 8 6.89 Table 4.2: Descriptive Statistics of Anscombe’s Quartet. Property Value Sample size 11 Mean of X 9.0 Mean of Y ~7.5 Variance of X 11.0 Variance of Y ~4.1 Equation of regression line Y = 3 + 0.5X Standard error of slope 0.118 One-sample t-statistic 4.24 Sum of squares of X 110.0 Regression sum of squares 27.50 Residual sum of squares of Y 13.75 Correlation coefficient .816 Coefficient of determination .67 Figure 4.17: Anscombe’s Quartet. As an index of correlation, the coefficient of stability (and dependability) has important weaknesses. It is a form of relative reliability rather than absolute reliability: It examines the consistency of scores across time relative to variation across people. Higher stability coefficients reflect greater stability of individual differences—not greater stability in people’s absolute level on the measure. This is a major limitation. Figure 4.18 depicts two example data sets that show strong relative reliability (a strong coefficient of stability) but poor absolute reliability based on inconsistency in people’s absolute level across time. Figure 4.18: Hypothetical Data Demonstrating Good Relative Reliability Despite Poor Absolute Reliability. The figure depicts two fictional data sets (black and red circles), which both exhibit a similar linear association. The line of best fit is the solid line in the graph and is the same for both data sets, but the black circles sit much closer to the line than the red circles, leading to a much higher coefficient of stability (\\(r = .99\\) and \\(.84\\), respectively). However, neither sets of circles are on the line of complete agreement represented by the dashed line in the graph. If the circles fall on the line of complete agreement, it indicates that the measure’s scores show consistency in absolute scores across time. Thus, although the measures show strong relative reliability, they show poor absolute reliability. (Figure reprinted from Vaz et al. (2013), Figure 1, p. 3. Vaz, S., Falkmer, T., Passmore, A. E., Parsons, R., &amp; Andreou, P. (2013). The case for using the repeatability coefficient when calculating test–retest reliability. PLoS ONE, 8(9), e73990. https://doi.org/10.1371/journal.pone.0073990) Another limitation of the coefficient of stability (and dependability) is that it is sensitive to outliers. Additionally, if there are little or no individual differences in scores on a given measure, the coefficient of stability will be low relative to the true reliability because correlation coefficients tend to be attenuated in the presence of a restricted range, and it may not be a useful index of reliability depending on the purpose. For more information see Section 24.3.4.3.2 on the reliability paradox in the chapter on cognitive assessment. See Figure 4.19 for an example of how a correlation coefficient tends to be attenuated when the range of one or both of the variables has restriction of range. Here is the correlation with and without restriction of range on \\(x\\) (i.e., \\(x\\) is restricted to values between 55 and 65): Figure 4.19: Example of Correlation With (Right Panel) and Without (Left Panel) Range Restriction. Filled black points represent the points in common across the two scatterplots. The observed correlation became much weaker due to restriction of range. Thus, when developing measures, it is important to ensure there is adequate variability of scores (i.e., individual differences) and that scores are not truncated due to ceiling or floor effects. Moreover, when interpreting associations with truncated variability, it is important to keep in mind that the true association is likely to be even stronger than what was observed if the measures did not have restricted range. To address these limitations of the coefficient of stability, it is important to consider additional indices of test–retest reliability, such as the coefficient of repeatability and bias that are depicted in Bland-Altman plots, as described later. 4.5.1.2 Coefficient of Agreement The intraclass correlation (ICC) can be used to evaluate the extent of absolute agreement of scores within a person across time. ICC ranges from 0–1, with higher scores indicating greater agreement. ICC was estimated using the psych package (Revelle, 2022). Code ICC(mydata[,c(&quot;time1&quot;,&quot;time2&quot;)], missing = FALSE) Call: ICC(x = mydata[, c(&quot;time1&quot;, &quot;time2&quot;)], missing = FALSE) Intraclass correlation coefficients type ICC F df1 df2 p lower bound upper bound Single_raters_absolute ICC1 0.93 28 99 100 1.1e-45 0.90 0.95 Single_random_raters ICC2 0.93 28 99 99 2.6e-45 0.90 0.95 Single_fixed_raters ICC3 0.93 28 99 99 2.6e-45 0.90 0.95 Average_raters_absolute ICC1k 0.96 28 99 100 1.1e-45 0.95 0.98 Average_random_raters ICC2k 0.96 28 99 99 2.6e-45 0.95 0.98 Average_fixed_raters ICC3k 0.96 28 99 99 2.6e-45 0.95 0.98 Number of subjects = 100 Number of Judges = 2 See the help file for a discussion of the other 4 McGraw and Wong estimates, There are various kinds of ICC coefficients. To read more about the various types of ICC coefficients, type the following command: Code ?ICC 4.5.1.3 Coefficient of Repeatability The coefficient of repeatability (\\(CR\\)), also known as the “smallest real difference,” is a form of absolute reliability. That is, it reflects the consistency of scores across time within a person. It is calculated as ~1.96 times the standard deviation of the mean differences between the two measurements (\\(d_1\\) and \\(d_2\\)), as in Equation (4.14): https://www.medcalc.org/manual/bland-altman-plot.php (archived at https://perma.cc/MEH3-3TGN) (Bland &amp; Altman, 1986, 1999). The \\(CR\\) is used for determining the lower and upper limits of agreement because it defines the range within which 95% of differences between the two measurement methods are expected to be. The precise value is \\(1.959964\\) (not 1.96) times the standard deviation of the within-subject differences between the two measurements. This precise value is based on the fact that 95% of the area under a standard normal distribution lies within \\(\\pm 1.959964\\) standard deviations of the mean. This is based on the quantile of the 2.5th (\\(-1.959964\\)) and 97.5th (\\(1.959964\\)) percentile of the standard normal distribution (the 95% confidence interval spans the 92.5th percentile to the 97.5th percentile). However, some researchers have rounded \\(1.959964\\) to 1.96 or to 2 for simplicity. The minimum possible coefficient of repeatability is zero, but there is no theoretical limit on the maximum possible coefficient of repeatability. The smaller the coefficient of repeatability (i.e., the closer to zero), the more consistent the scores are across time within a person. The coefficient of repeatability is tied to the units of the measure, so how small is considered “good” absolute reliability depends on the measure. In general, however, given the same units of measurement, a smaller coefficient of repeatability indicates stronger absolute test–retest reliability. \\[\\begin{equation} CR = 1.959964 \\times \\sqrt{\\frac{\\sum(d_1 - d_2)^2}{n}} \\tag{4.14} \\end{equation}\\] Code coefficientOfRepeatability &lt;- function(measure1, measure2){ value &lt;- qnorm(.975) * sd((measure1 - measure2), na.rm = TRUE) return(value) } Code coefficientOfRepeatability( measure1 = mydata$time1, measure2 = mydata$time2) [1] 7.694052 The petersenlab package (Petersen, 2024b) contains the repeatability() function that estimates the repeatability of a measure’s scores and generates a Bland-Altman plot in Figure 4.20. The coefficient of repeatability is labeled cr in the output. Code repeatability &lt;- function(measure1, measure2){ cr &lt;- qnorm(.975) * sd((measure1 - measure2), na.rm = TRUE) bias &lt;- mean((measure2 - measure1), na.rm = TRUE) lowerLOA &lt;- bias - cr upperLOA &lt;- bias + cr means &lt;- rowMeans(cbind(measure1, measure2), na.rm = TRUE) differences &lt;- measure2 - measure1 output &lt;- data.frame( cr = cr, bias = bias, lowerLOA = lowerLOA, upperLOA = upperLOA ) plot(means, differences, xlab = &quot;Mean of measurements (measure2 and measure1)&quot;, ylab = &quot;Difference of measurements (measure2 − measure1)&quot;) abline(h = 0) abline(h = bias, lty = 2, col = &quot;red&quot;) abline(h = lowerLOA, lty = 2, col = &quot;blue&quot;) abline(h = upperLOA, lty = 2, col = &quot;blue&quot;) return(output) } Code repeatability( measure1 = mydata$time1, measure2 = mydata$time2) Figure 4.20: Bland-Altman Plot. 4.5.1.3.1 Bland-Altman Plot The Bland-Altman plot can be useful for visualizing the degree of (in)consistency of people’s absolute scores on a measure across two time points (Bland &amp; Altman, 1986, 1999). Figure 4.21: Example of a Bland-Altman Plot. (Figure reprinted from Vaz et al. (2013), Figure 2, p. 4. Vaz, S., Falkmer, T., Passmore, A. E., Parsons, R., &amp; Andreou, P. (2013). The case for using the repeatability coefficient when calculating test–retest reliability. PLoS ONE, 8(9), e73990. https://doi.org/10.1371/journal.pone.0073990) In a Bland-Altman plot, the x-axis represents the mean of a person’s score across the two time points, and the y-axis represents the within-subject differences across time points. The horizontal line at \\(y = 0\\) is called the line of identity. Points would fall on the line of identity if people’s scores showed perfect consistency in their absolute level across time points (i.e., not relative level as in a cross-time correlation). Another line reflects the bias estimate (i.e., mean difference from the measure across the two points). The bias plus or minus the coefficient of repeatability is the upper or lower limit of agreement (LOA), respectively, as in Equations (4.15) and (4.16). The top dashed line reflects the upper LOA. The bottom dashed line reflects the lower LOA. The Bland-Altman plot was created using the blandr package (Datta, 2018). Code BAstats &lt;- blandr.statistics( method1 = mydata$time1, method2 = mydata$time2) BAplotlimits &lt;- blandr.plot.limits(BAstats) Code blandr.plot.rplot( BAstats, plot.limits = BAplotlimits, annotate = TRUE, method1name = &quot;T1&quot;, method2name = &quot;T2&quot;, ciDisplay = FALSE) Figure 4.22: Bland-Altman Plot. 4.5.1.3.2 Bias Bias is the degree of systematic measurement error. Bias is also called “mean error” in the context of assessing accuracy against a criterion (see Section 9.4.1.1.1). In a Bland-Altman plot, (test–retest) bias is estimated as the average of the within-subject differences between the two measurements. Estimated (test–retest) bias values that are closer to zero reflect stronger absolute test–retest reliability at the group-level. However, (estimated) bias values can still be close to zero while individuals show large differences because large positive and negative differences could cancel each other out. As a result, it is also important to examine other indices of test–retest reliability, including the coefficient of stability and the coefficient of repeatability. Code bias &lt;- function(measure1, measure2){ value &lt;- mean((measure2 - measure1), na.rm = TRUE) return(value) } Code bias( measure1 = mydata$time1, measure2 = mydata$time2) [1] 0.3956087 The bias of 0.40 indicates that the scores at time 2 are somewhat larger, on average, than the scores at time 1. 4.5.1.3.3 95% Limits of Agreement \\[\\begin{equation} \\text{Lower Limit of Agreement} = \\text{Bias} - \\text{Coefficient of Repeatability} \\tag{4.15} \\end{equation}\\] \\[\\begin{equation} \\text{Upper Limit of Agreement} = \\text{Bias} + \\text{Coefficient of Repeatability} \\tag{4.16} \\end{equation}\\] It is expected that the 95% limits of agreement (LOA) include 95% of differences between the two measurement methods (assuming the scores are normally distributed), based on how the coefficient of repeatability is calculated. Code lowerLOA &lt;- function(measure1, measure2){ value &lt;- bias(measure1, measure2) - coefficientOfRepeatability(measure1, measure2) return(value) } upperLOA &lt;- function(measure1, measure2){ value &lt;- bias(measure1, measure2) + coefficientOfRepeatability(measure1, measure2) return(value) } Code lowerLOA( measure1 = mydata$time1, measure2 = mydata$time2) [1] -7.298443 Code upperLOA( measure1 = mydata$time1, measure2 = mydata$time2) [1] 8.08966 The limits of agreement indicate that 95% of scores at time 2 are expected to fall within \\([-7.30, 8.09]\\) of scores at time 1. 4.5.1.3.4 Regression Slope Code BAstats$regression.fixed.slope means -0.029 4.5.1.3.5 Bland-Altman Statistics Code BAstats $means [1] 45.41616 29.38352 52.83203 59.09428 40.81583 33.92801 44.04557 52.70680 [9] 50.69188 61.13508 70.10419 30.61419 51.06184 44.05256 56.42619 64.08919 [17] 58.65712 44.11402 54.50209 49.15502 30.89027 41.80044 41.81681 47.21863 [25] 73.31753 50.41762 48.49486 60.69893 48.78386 49.15547 43.90476 42.19101 [33] 51.70235 62.21811 56.93890 41.84110 38.67391 50.27279 49.08995 53.13674 [41] 60.21733 46.72160 45.54500 41.76403 64.76012 47.89258 53.22552 41.87576 [49] 63.96757 82.95511 46.35012 42.05866 43.52036 29.87431 43.55178 58.83918 [57] 73.75502 39.63434 53.81664 56.06827 63.31167 46.01927 45.15406 49.08199 [65] 31.79205 62.95346 52.53782 57.54961 57.71031 50.58533 50.30490 52.04691 [73] 62.87564 59.25578 37.97333 46.80365 54.84389 44.47204 59.39434 49.33871 [81] 38.02790 30.65278 58.71295 45.42564 47.57603 49.65851 53.79797 53.24392 [89] 36.97624 63.91611 67.82623 61.59119 61.52289 61.41113 60.83455 65.03694 [97] 51.83620 $differences [1] 1.245020032 -2.377867165 -2.785315000 -1.289014150 4.301847437 [6] -5.731637566 6.149992448 -6.494610948 2.457557157 0.774819695 [11] -3.554082677 0.186244917 -2.111806973 1.588909002 2.384188410 [16] -5.057180121 -0.647170814 -0.006229196 -5.598211508 0.281610645 [21] -1.694406962 -1.470900094 -5.036628171 -0.153792899 1.456101018 [26] 2.990401115 5.388719805 0.816794054 6.119865945 -5.546951849 [31] 6.373022139 -2.715330615 1.231221443 2.246139472 0.496967958 [36] -3.038138268 10.000195202 3.217102906 -7.473901982 3.011984913 [41] -1.555445860 -2.709198795 -4.827130509 -4.955453611 1.197794395 [46] 5.118462434 -2.936569523 2.879365916 2.801251412 3.102180087 [51] -5.835451910 3.689356630 4.931280445 5.197238390 3.656180485 [56] -1.026930818 -3.197722284 -3.820555994 -3.729365286 5.206846348 [61] 3.801191058 -3.376888132 1.035927654 -1.390297678 -3.174774919 [66] -9.122742915 -0.583252760 -11.644788772 -3.201798975 -5.349772139 [71] 5.415966552 -2.719797446 -0.613921086 -3.850482165 4.375526613 [76] 2.683634845 -2.072588509 -0.516727657 -1.558234634 -7.103018399 [81] -2.581420413 3.801986536 -2.187599244 0.509391773 -3.629211508 [86] -1.979410597 4.614920216 0.158893719 -3.296571501 4.697019503 [91] -6.197827372 -0.522025764 -0.949234099 2.507778726 1.593524369 [96] 0.983768603 -0.052845246 $method1 [1] 46.03867 28.19459 51.43937 58.44978 NA 42.96676 31.06219 47.12057 [9] 49.45949 NA 51.92066 61.52249 68.32715 30.70731 57.12481 50.00594 [17] 44.84701 57.61828 61.56060 58.33353 44.11091 51.70299 49.29583 30.04307 [25] 41.06499 39.29850 47.14174 74.04558 51.91283 51.18922 61.10732 51.84379 [33] 46.38200 47.09127 40.83335 52.31796 63.34118 57.18739 40.32203 43.67401 [41] 51.88135 45.35300 54.64273 59.43961 45.36700 43.13144 39.28630 65.35902 [49] 50.45181 51.75724 43.31545 65.36819 84.50620 43.43240 43.90334 45.98600 [57] 32.47293 45.37987 58.32571 72.15616 37.72407 51.95196 58.67169 65.21227 [65] 44.33083 45.67203 48.38684 30.20466 58.39209 52.24619 51.72721 56.10941 [73] 47.91044 53.01289 50.68701 62.56868 57.33054 40.16109 48.14547 53.80759 [81] 44.21368 58.61522 45.78720 36.73719 32.55377 57.61915 45.68033 45.76143 [89] 48.66880 56.10543 53.32337 35.32796 66.26462 64.72732 61.33018 61.04828 [97] 62.66501 61.63131 65.52882 51.80978 $method2 [1] 44.79365 30.57245 54.22468 59.73879 58.54114 38.66491 36.79383 40.97057 [9] 55.95410 NA 49.46310 60.74767 71.88123 30.52107 NA 52.11775 [17] 43.25810 55.23409 66.61778 58.98071 44.11714 57.30120 49.01422 31.73748 [25] 42.53589 44.33512 47.29553 72.58948 48.92242 45.80050 60.29053 45.72393 [33] 51.92895 40.71825 43.54868 51.08674 61.09504 56.69042 43.36017 33.67381 [41] 48.66424 52.82690 51.63074 60.99505 48.07620 47.95857 44.24175 64.16122 [49] 45.33335 54.69381 40.43608 62.56694 81.40402 49.26785 40.21398 41.05472 [57] 27.27569 41.72369 59.35264 75.35388 41.54462 55.68133 53.46485 61.41108 [65] 47.70771 44.63610 49.77713 33.37944 67.51483 52.82944 63.37200 59.31121 [73] 53.26021 47.59692 53.40681 63.18260 61.18103 35.78557 45.46184 55.88018 [81] 44.73041 60.17346 52.89022 39.31861 28.75178 59.80675 45.17094 49.39064 [89] 50.64821 51.49051 53.16448 38.62453 61.56760 70.92515 61.85220 61.99751 [97] 60.15724 60.03778 64.54505 51.86262 $sig.level [1] 0.95 $sig.level.convert.to.z [1] 1.959964 $bias [1] -0.3956087 $biasUpperCI [1] 0.3955767 $biasLowerCI [1] -1.186794 $biasStdDev [1] 3.925609 $biasSEM [1] 0.3985852 $LOA_SEM [1] 0.6835169 $upperLOA [1] 7.298585 $upperLOA_upperCI [1] 8.655355 $upperLOA_lowerCI [1] 5.941814 $lowerLOA [1] -8.089802 $lowerLOA_upperCI [1] -6.733032 $lowerLOA_lowerCI [1] -9.446572 $proportion [1] 2.74135917 -8.09251988 -5.27202009 -2.18128396 10.53965340 [6] -16.89352667 13.96279479 -12.32215096 4.84802942 1.26738969 [11] -5.06971501 0.60836136 -4.13578293 3.60684842 4.22532252 [16] -7.89084742 -1.10331161 -0.01412067 -10.27155292 0.57290308 [21] -5.48524462 -3.51886261 -12.04450589 -0.32570384 1.98602032 [26] 5.93126139 11.11194116 1.34564825 12.54485846 -11.28450470 [31] 14.51556067 -6.43580309 2.38136471 3.61010561 0.87280915 [36] -7.26113318 25.85772864 6.39929192 -15.22491193 5.66836633 [41] -2.58305350 -5.79860030 -10.59859539 -11.86536345 1.84958638 [46] 10.68738101 -5.51722061 6.87597227 4.37917439 3.73958912 [51] -12.58993884 8.77193161 11.33097459 17.39701432 8.39502053 [56] -1.74531816 -4.33559963 -9.63950875 -6.92976203 9.28661851 [61] 6.00393402 -7.33798726 2.29420701 -2.83260277 -9.98606605 [66] -14.49124950 -1.11015795 -20.23434918 -5.54805396 -10.57573923 [71] 10.76627940 -5.22566581 -0.97640533 -6.49806969 11.52263059 [76] 5.73381499 -3.77906932 -1.16191572 -2.62354062 -14.39644225 [81] -6.78822746 12.40339952 -3.72592285 1.12137513 -7.62823504 [86] -3.98604547 8.57824215 0.29842602 -8.91537739 7.34872606 [91] -9.13780276 -0.84756563 -1.54289575 4.08359024 2.61943989 [96] 1.51263059 -0.10194660 $no.of.observations [1] 97 $regression.equation [1] &quot;y(differences) = -0.029 x(means) + 1.1&quot; $regression.fixed.slope means -0.029 $regression.fixed.intercept (Intercept) 1.1 4.5.2 Inter-Rater Reliability Inter-rater reliability is the consistency of scores across raters. For instance, two different raters may make a diagnostic decision for the same client, two different people may provide observational ratings of the same participant, or two different researchers may process psychophysiological data (e.g., electroencephalography, electrocardiography) from the same person. Consistency of scores can also be assessed within the same rater, which is known as intra-rater reliability and is described in Section 4.5.3. For assessing inter-rater reliability, intraclass correlation (ICC) is recommended for continuous data, such as ratings of a participant’s mood. By contrast, Cohen’s kappa (\\(\\kappa\\)) is recommended for categorical variables, such as making a diagnostic decision whether to give someone a diagnosis of depression or not. 4.5.2.1 Continuous Data Use intraclass correlation (ICC) for estimating inter-rater reliability of continuous data. ICC ranges from 0–1, with higher scores indicating greater agreement. Below, ICC was estimated using the psych package (Revelle, 2022). Code ICC(mydata[,c( &quot;rater1continuous&quot;,&quot;rater2continuous&quot;,&quot;rater3continuous&quot;)], missing = FALSE) Call: ICC(x = mydata[, c(&quot;rater1continuous&quot;, &quot;rater2continuous&quot;, &quot;rater3continuous&quot;)], missing = FALSE) Intraclass correlation coefficients type ICC F df1 df2 p lower bound upper bound Single_raters_absolute ICC1 0.81 14 99 200 2.7e-54 0.75 0.86 Single_random_raters ICC2 0.81 14 99 198 6.4e-54 0.75 0.86 Single_fixed_raters ICC3 0.81 14 99 198 6.4e-54 0.75 0.86 Average_raters_absolute ICC1k 0.93 14 99 200 2.7e-54 0.90 0.95 Average_random_raters ICC2k 0.93 14 99 198 6.4e-54 0.90 0.95 Average_fixed_raters ICC3k 0.93 14 99 198 6.4e-54 0.90 0.95 Number of subjects = 100 Number of Judges = 3 See the help file for a discussion of the other 4 McGraw and Wong estimates, Below, ICC was estimated using the irrNA package: Code iccNA(mydata[,c( &quot;rater1continuous&quot;,&quot;rater2continuous&quot;,&quot;rater3continuous&quot;)]) $ICCs ICC p-value lower CI limit upper CI limit ICC(1) 0.8134569 0 0.7515773 0.8641151 ICC(k) 0.9285392 0 0.9001485 0.9498712 ICC(A,1) 0.8133116 0 0.7512169 0.8640806 ICC(A,k) 0.9284757 0 0.8999754 0.9498570 ICC(C,1) 0.8118998 0 0.7494578 0.8629974 ICC(C,k) 0.9278575 0 0.8991264 0.9494176 $n_iter [1] 0 $amk [1] 2.979798 $k_0 [1] 2.97973 Below, ICC was estimated using the irrICC package: Code icc1a.fn(mydata[,c(&quot;subid&quot;, &quot;rater1continuous&quot;,&quot;rater2continuous&quot;,&quot;rater3continuous&quot;)]) Code icc1b.fn(dropRowsWithAllNA(mydata[,c(&quot;subid&quot;, &quot;rater1continuous&quot;,&quot;rater2continuous&quot;,&quot;rater3continuous&quot;)], ignore = &quot;subid&quot;)) Code icc2.inter.fn(dropRowsWithAllNA(mydata[,c(&quot;subid&quot;, &quot;rater1continuous&quot;,&quot;rater2continuous&quot;,&quot;rater3continuous&quot;)], ignore = &quot;subid&quot;)) Code icc2.nointer.fn(dropRowsWithAllNA(mydata[,c(&quot;subid&quot;, &quot;rater1continuous&quot;,&quot;rater2continuous&quot;,&quot;rater3continuous&quot;)], ignore = &quot;subid&quot;)) Code icc3.inter.fn(dropRowsWithAllNA(mydata[,c(&quot;subid&quot;, &quot;rater1continuous&quot;,&quot;rater2continuous&quot;,&quot;rater3continuous&quot;)], ignore = &quot;subid&quot;)) Code icc3.nointer.fn(dropRowsWithAllNA(mydata[,c(&quot;subid&quot;, &quot;rater1continuous&quot;,&quot;rater2continuous&quot;,&quot;rater3continuous&quot;)], ignore = &quot;subid&quot;)) There are various kinds of ICC coefficients. To read more about the various types of ICC coefficients, type the following commands: Code ?ICC ?iccNA 4.5.2.2 Categorical Data For estimating the inter-rater reliability of categorical data, use Cohen’s kappa (Bakeman &amp; Goodman, 2020), Fleiss’s kappa, the S index (Falotico &amp; Quatto, 2010), or Gwet’s AC1 statistic (Gwet, 2008; Gwet, 2021a, 2021b). Cohen’s kappa was estimated using the psych package (Revelle, 2022). Fleiss’s kappa and Gwet’s AC1 statistic were estimated using the irrCAC package. These estimates of inter-rater reliability range from −1 to +1, with −1 indicating perfect disagreement, 0 indicating chance agreement, and 1 indicating perfect agreement. Larger, more positive scores indicate greater agreement. Code cohen.kappa(mydata[,c( &quot;rater1categorical&quot;,&quot;rater2categorical&quot;,&quot;rater3categorical&quot;)]) Cohen Kappa (below the diagonal) and Weighted Kappa (above the diagonal) For confidence intervals and detail print with all=TRUE rater1categorical rater2categorical rater3categorical rater1categorical 1.00 0.86 0.90 rater2categorical 0.86 1.00 0.76 rater3categorical 0.90 0.76 1.00 Average Cohen kappa for all raters 0.84 Average weighted kappa for all raters 0.84 Code fleiss.kappa.raw(na.omit(mydata[,c( &quot;rater1categorical&quot;,&quot;rater2categorical&quot;,&quot;rater3categorical&quot;)])) $est coeff.name pa pe coeff.val coeff.se conf.int p.value 1 Fleiss&#39; Kappa 0.9175258 0.5001476 0.835 0.04483 (0.746,0.924) 0 w.name 1 unweighted $weights [,1] [,2] [1,] 1 0 [2,] 0 1 $categories [1] 0 1 Code gwet.ac1.raw(na.omit(mydata[,c( &quot;rater1categorical&quot;,&quot;rater2categorical&quot;,&quot;rater3categorical&quot;)])) $est coeff.name pa pe coeff.val coeff.se conf.int p.value 1 AC1 0.9175258 0.4998524 0.8351 0.04479 (0.746,0.924) 0 w.name 1 unweighted $weights [,1] [,2] [1,] 1 0 [2,] 0 1 $categories [1] 0 1 4.5.3 Intra-Rater Reliability Principles of intra-rater reliability are similar to principles of inter-rater reliability (described in Section 4.5.2), except the same rater provides ratings on two occasions, rather than using multiple raters. 4.5.4 Parallel- (or Alternate-) Forms Reliability Parallel-forms reliability (also known as alternate-forms reliability) is the consistency of scores across two parallel forms. Parallel forms are two equivalent measures of the same construct that differ in content or format. The two measures can be administered in the same occasion, known as immediate parallel-forms reliability, or they can be administered separated by a delay, known as delayed parallel-forms reliability. Similar considerations of length of delay duration and ways to reduce carryover effects are relevant to parallel-forms reliability as they are to test–retest reliability. Parallel-forms reliability is often estimated with the Pearson correlation, which is known as the coefficient of equivalence. The coefficient of equivalence is the consistency of scores across two parallel forms relative to variation across people. It is interpreted as the ratio of true score variance to observed score variance, consistent with the formal definition of reliability. An example of parallel-forms reliability would be if we want to assess a participant twice with different measures of the same construct to avoid practice effects. This approach is often conducted in academic achievement testing, and it could involve retesting that is immediate or delayed. For instance, we could administer item set A at time 1 and item set B at time 2 for the same participant, to get rid of any possibility that the person’s score differed across time because of improved performance that resulted merely from prior exposure to the same items. Many standardized academic achievement and aptitude tests have developed parallel forms, including the SAT, ACT, and GRE. Parallel-forms reliability controls for specific error, i.e., error that is particular to a specific measure. However, parallel-forms reliability has the limitation that it assumes that the parallel forms are equivalent, such that it makes no difference which test you use. Two forms are considered equivalent when they assess the same construct, have the same mean and variance of scores, and have the same inter-correlations with external criteria. Technically, two instruments are only truly equivalent if they have the same true scores and variability of error scores, though this is difficult to establish empirically. Given these constraints, it can be difficult to assume that different forms are equivalent. Nevertheless, integrative data analysis and measurement harmonization may help make the assumption more tenable (Hussong et al., 2013, 2020). In general, parallel forms can be difficult to create and they can more than double the time it takes to develop a single measure, often with limited benefit, depending on the intended use. 4.5.5 Internal Consistency Reliability Internal consistency reliability is the consistency of scores across items, that is, their interrelatedness. Internal consistency is necessary, but insufficient, for establishing the homogeneity or unidimensionality of the measure, i.e., that the items on the measure assess the same construct. There are many examples of internal consistency estimates, including Cronbach’s coefficient alpha (\\(\\alpha\\)), Kuder-Richardson Formula 20 (which is a special case of Cronbach’s alpha with dichotomous items), omega coefficients, average variance extracted, greatest lower bound, coefficient H, split-half reliability, average inter-item correlation, and average item–total correlation. In general, internal consistency ranges from 0–1, with higher scores indicating greater consistency. There is no magic cutoff for determining adequate internal consistency; however, values below .70 suggest that the items may not assess the same construct. Internal consistency can be affected by many factors, such as wording of the item a participant’s comprehension of the item, which could be impacted if using items with dated language or double negatives interpretation attention of the participant compliance of the participant Although establishing internal consistency can help ensure that items in the measure are interrelated, you do not want to select items just based on internal consistency. If you did, you would end up with items that are too similar. For example, you could end up only with items that have similar item stems and are coded in the same direction. Therefore, selecting items solely based on internal consistency would likely not assess the breadth of the construct and would reflect strong method variance of the wording of the particular items. In other words, internal consistency can be too high. Internal consistency is helpful to a point, but you do not want items to be too homogeneous or redundant. 4.5.5.1 Average Inter-Item Correlation One estimate of internal consistency is the average (mean or median) inter-item correlation. Code interItemCorrelations &lt;- cor( mydata[,c(&quot;item1&quot;,&quot;item2&quot;,&quot;item3&quot;,&quot;item4&quot;)], use = &quot;pairwise.complete.obs&quot;) diag(interItemCorrelations) &lt;- NA interItemCorrelations_nodiag &lt;- interItemCorrelations interItemCorrelations_nodiag[ upper.tri(interItemCorrelations_nodiag)] &lt;- NA Here is the mean inter-item correlation for each item: Code meanInterItemCorrelations &lt;- colMeans( interItemCorrelations, na.rm = TRUE) meanInterItemCorrelations item1 item2 item3 item4 0.7748087 0.7787348 0.7538928 0.6594609 Here is the mean inter-item correlation across all items: Code mean(meanInterItemCorrelations) [1] 0.7417243 Code mean(interItemCorrelations_nodiag, na.rm = TRUE) [1] 0.7417243 Code psych::alpha( mydata[,c(&quot;item1&quot;,&quot;item2&quot;,&quot;item3&quot;,&quot;item4&quot;)])$total$average_r [1] 0.7417243 Figure 4.23 is a histogram of the inter-item correlations. Code hist(meanInterItemCorrelations, xlab = &quot;Inter-Item Correlations&quot;, main = &quot;Histogram of Inter-Item Correlations&quot;) abline(v = mean(meanInterItemCorrelations), lwd = 2) Figure 4.23: Histogram of Inter-Item Correlations. The vertical black line reflects the mean inter-item correlation. Here is the median inter-item correlation for each item: Code medianInterItemCorrelations &lt;- apply( interItemCorrelations, 2, function(x) median(x, na.rm = TRUE)) medianInterItemCorrelations item1 item2 item3 item4 0.7577984 0.7791957 0.7577984 0.6316587 Here is the median inter-item correlation across all items: Code median(interItemCorrelations[lower.tri(interItemCorrelations)]) [1] 0.7412414 Code psych::alpha(mydata[,c( &quot;item1&quot;,&quot;item2&quot;,&quot;item3&quot;,&quot;item4&quot;)])$total$median_r [1] 0.7412414 4.5.5.2 Average Item–Total Correlation Another estimate of internal consistency is the average item–total correlation. Code itemTotalCorrelations &lt;- psych::alpha( mydata[,c(&quot;item1&quot;,&quot;item2&quot;,&quot;item3&quot;,&quot;item4&quot;)])$item.stats[&quot;raw.r&quot;] mean(itemTotalCorrelations$raw.r) [1] 0.8940236 When examining the item–total correlation for a given item, it is preferable to exclude the item of interest from the total score, so that the item is associated with the total score that is calculated from the remaining items. This reduces the extent to which the association reflects that the item is associated with itself. Code itemTotalCorrelationsDrop &lt;- psych::alpha( mydata[,c(&quot;item1&quot;,&quot;item2&quot;,&quot;item3&quot;,&quot;item4&quot;)])$item.stats[&quot;r.drop&quot;] mean(itemTotalCorrelationsDrop$r.drop) [1] 0.8053408 4.5.5.3 Average Variance Extracted The equivalent of the average item–total (squared) correlation when dealing with a reflective latent construct is the average variance extracted (AVE). AVE is estimated as the mean of the squared loadings of the indicators of a latent factor (i.e., the sum of the squared loadings divided by the number of indicators). When using unstandardized estimates, AVE is the item-variance weighted average of item reliabilities (Rönkkö &amp; Cho, 2020). When using standardized estimates, AVE is the average indicator reliability (Rönkkö &amp; Cho, 2020). AVE is equivalent to the communality of a latent factor. AVE was estimated using a confirmatory factor analysis model using the lavaan (Rosseel et al., 2022) and semTools (Jorgensen et al., 2021) packages. Code latentFactorModel_syntax &lt;- &#39; #Factor loadings latentFactor =~ item1 + item2 + item3 + item4 &#39; latentFactorModel_fit &lt;- cfa( latentFactorModel_syntax, data = mydata, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, std.lv = TRUE) summary( latentFactorModel_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 37 iterations Estimator ML Optimization method NLMINB Number of model parameters 12 Used Total Number of observations 99 100 Number of missing patterns 3 Model Test User Model: Standard Scaled Test Statistic 22.906 26.095 Degrees of freedom 2 2 P-value (Chi-square) 0.000 0.000 Scaling correction factor 0.878 Yuan-Bentler correction (Mplus variant) Model Test Baseline Model: Test statistic 372.128 416.821 Degrees of freedom 6 6 P-value 0.000 0.000 Scaling correction factor 0.893 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.943 0.941 Tucker-Lewis Index (TLI) 0.829 0.824 Robust Comparative Fit Index (CFI) 0.944 Robust Tucker-Lewis Index (TLI) 0.833 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -1377.808 -1377.808 Scaling correction factor 1.040 for the MLR correction Loglikelihood unrestricted model (H1) -1366.355 -1366.355 Scaling correction factor 1.017 for the MLR correction Akaike (AIC) 2779.616 2779.616 Bayesian (BIC) 2810.757 2810.757 Sample-size adjusted Bayesian (SABIC) 2772.861 2772.861 Root Mean Square Error of Approximation: RMSEA 0.325 0.349 90 Percent confidence interval - lower 0.214 0.231 90 Percent confidence interval - upper 0.451 0.483 P-value H_0: RMSEA &lt;= 0.050 0.000 0.000 P-value H_0: RMSEA &gt;= 0.080 1.000 1.000 Robust RMSEA 0.322 90 Percent confidence interval - lower 0.201 90 Percent confidence interval - upper 0.461 P-value H_0: Robust RMSEA &lt;= 0.050 0.000 P-value H_0: Robust RMSEA &gt;= 0.080 0.999 Standardized Root Mean Square Residual: SRMR 0.052 0.052 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all latentFactor =~ item1 9.473 0.881 10.756 0.000 9.473 0.960 item2 10.227 0.907 11.281 0.000 10.227 0.970 item3 10.387 1.013 10.256 0.000 10.387 0.804 item4 11.690 1.656 7.059 0.000 11.690 0.663 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .item1 49.424 0.992 49.836 0.000 49.424 5.011 .item2 49.688 1.061 46.836 0.000 49.688 4.711 .item3 49.918 1.298 38.459 0.000 49.918 3.865 .item4 45.822 1.771 25.876 0.000 45.822 2.601 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .item1 7.542 2.056 3.667 0.000 7.542 0.078 .item2 6.668 2.293 2.908 0.004 6.668 0.060 .item3 58.890 11.594 5.079 0.000 58.890 0.353 .item4 173.784 23.265 7.470 0.000 173.784 0.560 latentFactor 1.000 1.000 1.000 R-Square: Estimate item1 0.922 item2 0.940 item3 0.647 item4 0.440 Code AVE(latentFactorModel_fit) latentFactor 0.64 4.5.5.4 Split-Half Reliability When estimating split-half reliability, we randomly take half of the items on the measure and relate scores on these items to the score on the other half of items. One challenge to split-half reliability is that, according to classical test theory, shorter tests tend to be less reliable than longer tests, so this would be an underestimate of the test’s reliability. In general, the greater the number of items, the more reliable the test. To overcome this, the Spearman-Brown prediction (or prophecy) formula predicts the reliability of a test after changing the test length. Thus, the Spearman-Brown prediction formula accounts for the shorter test in predicting what the reliability of the full-length test would be when estimating split-half reliability. The Spearman-Brown prediction formula is in Equation (4.17): \\[\\begin{equation} \\text{predicted test reliability, } r_{xx} = \\frac{n \\cdot r_{xx&#39;}}{1+(n-1)r_{xx&#39;}} \\tag{4.17} \\end{equation}\\] where \\(n =\\) the number of tests combined (i.e., the ratio of the number of new items to the number of old items) and \\(r_{xx&#39;} =\\) the reliability of the original test. So, according to Equation (4.18), doubling the length of a test whose reliability is .80 would be estimated to have a reliability of \\[ \\begin{aligned} \\text{predicted test reliability, } r_{xx} &amp;= \\frac{2 \\cdot .80}{1+(2-1).80} \\\\ &amp;= .89 \\end{aligned} \\tag{4.18} \\] Another challenge to split-half reliability is that the reliability differs depending on which items go into which half. One option is to calculate the split-half reliability of odd and even trials, while correcting for test length, as is performed with the item_split_half() function of the performance package (Lüdecke et al., 2021): Code item_split_half(mydata[,c( &quot;item1&quot;,&quot;item2&quot;,&quot;item3&quot;,&quot;item4&quot;)]) $splithalf [1] 0.8734005 $spearmanbrown [1] 0.9324226 Modern computational software allows for estimating the mean reliability estimate of all possible split-halves (Revelle &amp; Condon, 2019), while correcting for test length, as seen below. Split-half reliability was estimated using the psych package (Revelle, 2022). Code splitHalf(mydata[,c( &quot;item1&quot;,&quot;item2&quot;,&quot;item3&quot;,&quot;item4&quot;)], brute = TRUE) Split half reliabilities Call: splitHalf(r = mydata[, c(&quot;item1&quot;, &quot;item2&quot;, &quot;item3&quot;, &quot;item4&quot;)], brute = TRUE) Maximum split half reliability (lambda 4) = 0.95 Guttman lambda 6 = 0.93 Average split half reliability = 0.92 Guttman lambda 3 (alpha) = 0.92 Guttman lambda 2 = 0.92 Minimum split half reliability (beta) = 0.87 Average interitem r = 0.75 with median = 0.74 4.5.5.5 Cronbach’s Alpha (Coefficient Alpha) Cronbach’s alpha (\\(\\alpha\\)), also known as coefficient alpha, is approximately equal to the mean reliability estimate of all split-halves, assuming that item standard deviations are equal. Alpha considers the interrelatedness of a total set of items—giving us information about the extent to which each item in a set of items correlates with all other items. Alpha is the most commonly used estimate for internal consistency reliability, but it has many problems. Cronbach’s alpha assumes that the items are unidimensional: that there is one predominant dimension that explains the variance among the items essentially tau (\\(\\tau\\)) equivalent: that the items are equally related to the construct Cronbach’s alpha is affected by the number of items: The more items, the more inflated alpha is, even if correlations among the items are low. variance of the item scores: If the variances of item scores are low (e.g., dichotomous scores), alphas tend to be an underestimate of internal consistency reliability. violations of assumptions: Cronbach’s alpha assumes items are essentially tau equivalent—that the items are equally related to the construct. However, this is an unrealistic assumption. Cronbach’s alpha depends on the dimensionality of the scale: Alpha is only valid when the scale is unidimensional. If you use it with multidimensional scales, you know the extent to which items correlate with another but nothing about whether they are actually measuring the same constructs. Cronbach’s alpha is appropriate only in situations where a scale is unidimensional, where the items equally contribute to the scale, and you would like to determine whether its items are interrelated. However, Cronbach’s alpha does not assess the dimensionality of the scale or any type of construct validity—it can only tell us how strongly the items hang together. Cronbach’s alpha (\\(\\alpha\\)) was estimated using the psych package (Revelle, 2022). Code psych::alpha(mydata[,c( &quot;item1&quot;,&quot;item2&quot;,&quot;item3&quot;,&quot;item4&quot;)])$total$raw_alpha [1] 0.8923861 Below is an estimate of standardized alpha (if the items are standardized before computing the scale score). Code psych::alpha(mydata[,c( &quot;item1&quot;,&quot;item2&quot;,&quot;item3&quot;,&quot;item4&quot;)])$total$std.alpha [1] 0.9199188 Below are item statistics: Code psych::alpha(mydata[,c( &quot;item1&quot;,&quot;item2&quot;,&quot;item3&quot;,&quot;item4&quot;)]) Reliability analysis Call: psych::alpha(x = mydata[, c(&quot;item1&quot;, &quot;item2&quot;, &quot;item3&quot;, &quot;item4&quot;)]) raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r 0.89 0.92 0.92 0.74 11 0.017 49 11 0.74 95% confidence boundaries lower alpha upper Feldt 0.85 0.89 0.92 Duhachek 0.86 0.89 0.93 Reliability if an item is dropped: raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r item1 0.85 0.88 0.84 0.71 7.3 0.024 0.0064 0.72 item2 0.85 0.88 0.83 0.70 7.2 0.024 0.0043 0.72 item3 0.83 0.89 0.89 0.73 8.1 0.029 0.0317 0.63 item4 0.93 0.93 0.92 0.82 14.0 0.014 0.0094 0.78 Item statistics n raw.r std.r r.cor r.drop mean sd item1 98 0.89 0.93 0.93 0.84 49 10 item2 98 0.90 0.93 0.93 0.84 50 11 item3 99 0.91 0.91 0.86 0.83 50 13 item4 99 0.87 0.83 0.73 0.71 46 18 However, because of the many weaknesses of Cronbach’s alpha (Cortina, 1993; Dunn et al., 2014; Flora, 2020; Green &amp; Yang, 2015; A. F. Hayes &amp; Coutts, 2020; Kelley &amp; Pornprasertmanit, 2016; McNeish, 2018; Peters, 2014; Raykov, 2001; Revelle &amp; Condon, 2019; Sijtsma, 2008; but see Raykov &amp; Marcoulides, 2019), current recommendations are to use coefficient omega (\\(\\omega\\)) instead, as described next. 4.5.5.6 Coefficient Omega (\\(\\omega\\)) Current recommendations are to use McDonald’s coefficient omega (\\(\\omega\\)) instead of Cronbach’s alpha (\\(\\alpha\\)) for internal consistency reliability. Specifically, it is recommended to use omega total (\\(\\omega_t\\)) for continuous items that are unidimensional, omega hierarchical (\\(\\omega_h\\)) for continuous items that are multidimensional, and omega categorical (\\(\\omega_C\\)) for categorical items (i.e., items with fewer than five ordinal categories), with confidence intervals calculated from a bias-corrected bootstrap (Flora, 2020; Kelley &amp; Pornprasertmanit, 2016). 4.5.5.6.1 Omega Total (\\(\\omega_t\\)) Omega total (\\(\\omega_t\\)) (or simply omega) is the proportion of the total variance that is accounted for by the common factor (Flora, 2020). Omega total, like Cronbach’s alpha, assumes that the items are unidimensional. Omega total (\\(\\omega_t\\)) was calculated using the MBESS package (Kelley, 2020). This code quickly calculates a point estimate for omega total: Code ci.reliability( mydata[,c(&quot;item1&quot;,&quot;item2&quot;,&quot;item3&quot;,&quot;item4&quot;)], type = &quot;omega&quot;, interval.type = &quot;none&quot;)$est [1] 0.8760799 If you want to calculate a confidence interval for omega, you will need to specify an interval.type and specify how many bootstrap replications. Code bootstrapReplications &lt;- 1000 The following code calculates a confidence interval from a bias-corrected bootstrap. For reproducibility, we set the seed below. Using the same seed will yield the same answer every time. There is nothing special about this particular seed. Warning: This code takes a while to run based on \\(1000\\) bootstrap replications. You can reduce the number of replications to be faster. However, the number of bootstrap replications must be larger than the number of rows in the data. Code set.seed(52242) ci.reliability( mydata[,c(&quot;item1&quot;,&quot;item2&quot;,&quot;item3&quot;,&quot;item4&quot;)], type = &quot;omega&quot;, interval.type = &quot;bca&quot;, B = bootstrapReplications) $est [1] 0.8760799 $se [1] 0.02461448 $ci.lower [1] 0.81431 $ci.upper [1] 0.9149732 $conf.level [1] 0.95 $type [1] &quot;omega&quot; $interval.type [1] &quot;bca bootstrap&quot; 4.5.5.6.2 Omega Hierarchical (\\(\\omega_h\\)) If the items are multidimensional and follow a bifactor structure, omega hierarchical (\\(\\omega_h\\)) is preferred over omega total. Omega hierarchical is the proportion of the total variance that is accounted for by the general factor in a set of multidimensional items (Flora, 2020). Use omega hierarchical for estimating internal consistency of continuous items that are multidimensional and that follow a bifactor structure. If the multidimensional items follow a higher-order structure, use omega higher-order [\\(\\omega_{ho}\\); Flora (2020)]. Omega hierarchical was calculated using the MBESS package (Kelley, 2020). Code ci.reliability( mydata[,c(&quot;item1&quot;,&quot;item2&quot;,&quot;item3&quot;,&quot;item4&quot;)], type = &quot;hierarchical&quot;, interval.type = &quot;none&quot;)$est [1] 0.8453203 The following code calculates a confidence interval from a bias-corrected bootstrap. For reproducibility, we set the seed below. Using the same seed will yield the same answer every time. There is nothing special about this particular seed. Warning: This code takes a while to run based on \\(1000\\) bootstrap replications. You can reduce the number of replications to be faster. However, the number of bootstrap replications must be larger than the number of rows in the data. Code set.seed(52242) ci.reliability( mydata[,c(&quot;item1&quot;,&quot;item2&quot;,&quot;item3&quot;,&quot;item4&quot;)], type = &quot;hierarchical&quot;, interval.type = &quot;bca&quot;, B = bootstrapReplications) $est [1] 0.8453203 $se [1] 0.03381731 $ci.lower [1] 0.764192 $ci.upper [1] 0.8972227 $conf.level [1] 0.95 $type [1] &quot;hierarchical omega&quot; $interval.type [1] &quot;bca bootstrap&quot; 4.5.5.6.3 Omega Categorical (\\(\\omega_C\\)) Use omega categorical (\\(\\omega_C\\)) for categorical items that are unidimensional. To handle categorical data, omega categorical is calculated using a polychoric correlation matrix instead of a Pearson correlation matrix. If the categorical items are multidimensional items and they follow a bifactor structure, use omega hierarchical categorical [\\(\\omega_{h\\text{-cat}}\\); Flora (2020)]. If the categorical items are multidimensional and they follow a higher-order structure, use omega higher-order categorical [\\(\\omega_{ho\\text{-cat}}\\); Flora (2020)]. Omega categorical was calculated using the MBESS package (Kelley, 2020). Code ci.reliability( mydata[,c(&quot;rater1categorical&quot;,&quot;rater2categorical&quot;,&quot;rater3categorical&quot;)], type = &quot;categorical&quot;, interval.type = &quot;none&quot;)$est [,1] [,2] [,3] [1,] 1.0073439 0.9852044 0.9903989 [2,] 0.9852044 0.9635514 0.9686318 [3,] 0.9903989 0.9686318 0.9737390 [1] 0.9600709 The following code calculates a confidence interval from a bias-corrected bootstrap. For reproducibility, we set the seed below. Using the same seed will yield the same answer every time. There is nothing special about this particular seed. Warning: This code takes a while to run based on \\(1000\\) bootstrap replications. You can reduce the number of replications to be faster. However, the number of bootstrap replications must be larger than the number of rows in the data. Code set.seed(52242) ci.reliability( mydata[,c(&quot;rater1categorical&quot;,&quot;rater2categorical&quot;,&quot;rater3categorical&quot;)], type = &quot;categorical&quot;, interval.type = &quot;bca&quot;, B = bootstrapReplications) 4.5.6 Comparison Even though the types of reliability described in Section 4.5 are all called “reliability,” they are very different from each other. Test–retest reliability and inter-rater reliability tend to be lower than immediate parallel-forms reliability and internal consistency reliability because they involve administering measures at different times or with different raters. However, delayed parallel-forms reliability tends to be lower than test–retest reliability because it involves assessing the construct at a different time point and with different items. Thus, if we put the different types of reliability in order from low to high, they tend to be (but not always) arranged like this (Equation (4.19)): \\[ \\begin{aligned} \\text{delayed parallel-forms} &amp;&lt; \\\\ \\text{test–retest}, \\text{inter-rater} &amp;&lt; \\\\ \\text{immediate parallel-forms}, \\text{internal consistency}, \\text{intra-rater} \\end{aligned} \\tag{4.19} \\] 4.5.7 Reliability of a Linear Composite Score A linear composite score is a linear combination of variables, each of which has a given weight in the linear composite. Unit weights (i.e., weights of 1) would reflect an unweighted composite. We first specify the variables used to generate the correlation matrix, and the reliability, standard deviation, and weight of each variable. We specify items 2 and 3 to have twice the weight as item 1 in the composite, and item 4 to have 1.5 times the weight as item 1 in the composite. Code rxx &lt;- c(0.70, 0.75, 0.80, 0.85) #reliability coefficients rho &lt;- cor(mydata[,c(&quot;item1&quot;,&quot;item2&quot;,&quot;item3&quot;,&quot;item4&quot;)], use = &quot;pairwise.complete.obs&quot;) sigma &lt;- apply( mydata[,c(&quot;item1&quot;,&quot;item2&quot;,&quot;item3&quot;,&quot;item4&quot;)], 2, function(x) sd(x, na.rm = TRUE)) #standard deviations weights &lt;- c(1, 2, 2, 1.5) #weights of each variable in the linear combination Here is the reliability of an unweighted composite: Code composite_rel_matrix(rel_vec = rxx, r_mat = rho, sd_vec = sigma) [1] 0.9337237 Here is the reliability of a weighted composite: Code composite_rel_matrix(rel_vec = rxx, r_mat = rho, sd_vec = sigma, wt_vec = weights) [1] 0.931709 4.5.8 Reliability of a Difference Score Difference scores tend to be lower in reliability than the individual indices that compose the difference score. This is especially the case when the two indices are correlated. If the correlation between the indices is equal to the reliability of the indices, the reliability of the difference score will be zero (Revelle &amp; Condon, 2019). If the correlation between the two indices is large, it requires a very high reliability for the difference score to show a strong reliability (Trafimow, 2015). To the extent that the two indices have unequal variances (e.g., if one index has a standard deviation that is four times the standard deviation of the second index), the reliabilities do not need to be quite as high for the difference score to be reliable (Trafimow, 2015). The reliability of a difference score can be estimated using Equation (4.20) (Trafimow, 2015): \\[\\begin{equation} r_{x-y, x-y} = \\frac{\\sigma^2_x r_{xx} + \\sigma^2_y r_{yy} - 2r_{xy} \\sigma_{x}\\sigma_{y}}{\\sigma^2_x + \\sigma^2_y - 2r_{xy} \\sigma_{x}\\sigma_{y}} \\tag{4.20} \\end{equation}\\] The petersenlab package (Petersen, 2024b) contains the reliabilityOfDifferenceScore() function to estimate the reliability of a difference score: Code reliabilityOfDifferenceScore &lt;- function(x, y, reliabilityX, reliabilityY){ rxy &lt;- as.numeric(cor.test(x = x, y = y)$estimate) sigmaX &lt;- sd(x, na.rm = TRUE) sigmaY &lt;- sd(y, na.rm = TRUE) reliability &lt;- (sigmaX^2 * reliabilityX + sigmaY^2 * reliabilityY - (2 * rxy * sigmaX * sigmaY)) / (sigmaX^2 + sigmaY^2 - (2 * rxy * sigmaX * sigmaY)) return(reliability) } Code reliabilityOfDifferenceScore( x = mydata$item1, y = mydata$item2, reliabilityX = .95, reliabilityY = .95) [1] 0.2552898 For instance, in this case, the reliability of the difference score is .26, even though the reliability of both indices is .95. In Figures 4.24 and 4.25, the reliability of a difference score is depicted as a function of (a) the reliability of the indices that compose the difference score and (b) the correlation between them, assuming that both indices have equal variances. Figure 4.24: Reliability of Difference Score as a Function of Reliability of Indices and the Correlation Between Them. Figure 4.25: Reliability of Difference Score as a Function of Correlation Between Indices and Reliability of Indices. 4.6 Applied Examples Although reliability is an important consideration for measures, stronger reliability is not always better. Reliability is not one thing; it is not monolithic. There are multiple aspects of reliability, and which aspect(s) of reliability are most important depend on the construct of interest. That is, we want different kinds of reliability for measures of different constructs. In general, we want our measures to show strong inter-rater, intra-rater, immediate parallel-forms, and internal consistency reliability. However, we do not want internal consistency reliability to be too high so as to have redundant items that are correlated for reasons of shared method biases. Moreover, for some constructs (i.e., formative constructs such as stressful life events), we would not expect internal consistency reliability. And the extent to which our measures should show test–retest and delayed parallel-forms reliability depends on the stability of the construct and the time lag. Consider the construct of reactive dysphoria. Assume that we assess people’s reactive dysphoria with observations and several questionnaires today and one month from now. However, test–retest reliability is not meaningful because the phenomenon is not meant to be stable across time—it is supposed to be reactive to the situation, not a trait. Instead, we would expect that a strong measure of reactive dysphoria has high internal consistency reliability, a high coefficient of equivalence from immediate parallel-forms reliability, and high inter-/intra-rater reliability. Consider the example of personality and intelligence. Personality is defined such that personality “traits” are thought to be stable across time and situation. Likewise, individual differences in intelligence are thought to be relatively stable in terms of individual differences across time (not in absolute level). Thus, strong measures of personality and intelligence should show high test–retest reliability, parallel-forms reliability, internal consistency reliability, and inter-/intra-rater reliability. Just because one aspect of reliability is strong does not necessarily mean that other aspects of reliability will be strong. Measures of some constructs (e.g., depression) could be expected to show low test–retest reliability but high internal consistency reliability. You can also have measures that have low internal consistency reliability but that have high test–retest reliability (e.g., if our nonsensical measure averaged together \\(z\\)-scores of height and blood type). Consider a questionnaire assessing people’s extraversion. We would expect high stability of construct, and therefore a high coefficient of stability, but test and retest scores might not be equal. That is, the measures may show a somewhat lower repeatability coefficient because of random error—on a given day, the participant may be sick, stressed, or tired; they may have recently gotten into an argument; or they may be impacted by the season, the weather, or differences in the context (e.g., lab or online), just to name a few potential sources of random error. 4.7 Standard Error of Measurement We presented and discussed the formula for estimating the standard error of measurement earlier in Equation (4.11). It is also below: \\[ \\text{standard error of measurement (SEM)} = \\sigma_x \\sqrt{1 - r_{xx}} \\] where \\(\\sigma_x = \\text{standard deviation of scores on measure } x\\) and \\(r_{xx} = \\text{reliability of measure } x\\) The reliability of the measure was estimated here based on the test–retest reliability of the measure at T1 with the measure at T2. Code reliabilityOfMeasure &lt;- as.numeric( cor.test( x = mydata$time1, y = mydata$time2)$estimate) sem &lt;- sd(mydata$time1, na.rm = TRUE) * sqrt(1 - reliabilityOfMeasure) sem [1] 2.720493 We can use the standard error of measurement to estimate the confidence interval around a person’s score. A confidence interval indicates a range within which, with some degree of confidence (e.g., 95%), the person’s true score is expected to be. For a 95% confidence interval, the true score is expected to fall within \\(\\pm 1.959964 \\ \\text{SEMs}\\) of the observed score 95% of the time. Code lower95CI &lt;- mean(mydata$time1, na.rm = TRUE) - qnorm(.975)*sem upper95CI &lt;- mean(mydata$time1, na.rm = TRUE) + qnorm(.975)*sem lower95CI [1] 45.60439 Code upper95CI [1] 56.26852 The 95% CI is \\([45.6, 56.27]\\). 4.8 Influences of Measurement Error on Test–Retest Reliability This section describes the influence of measurement error on reliability, specifically test–retest reliability. There are different types of measurement error, and each can have different impacts on reliability and, ultimately, your findings. As discussed in Section 4.2, a given error can be either systematic or random. In addition, error can be within-person, between-person, or both. However, error scores can include multiple errors simultaneously. 4.8.1 No Error This section provides the coefficients of stability, repeatability, and bias that are used as a comparison when measurement error is added. For reproducibility, we set the seed below. Using the same seed will yield the same answer every time. There is nothing special about this particular seed. Code # Simulate Data set.seed(52242) time1 &lt;- rnorm(n = sampleSize, mean = 50, sd = 10) time2 &lt;- time1 + rnorm( n = sampleSize, mean = 0, sd = 4) time3 &lt;- time2 + rnorm( n = sampleSize, mean = 0, sd = 8) # Add Missing Data time1[c(5,10)] &lt;- NA time2[c(10,15)] &lt;- NA time3[c(10)] &lt;- NA # Combine data into data frame mydata &lt;- data.frame( rater1continuous, rater2continuous, rater3continuous, rater1categorical, rater2categorical, rater3categorical, time1, time2, time3, item1, item2, item3, item4) 4.8.1.1 Coefficient of stability Code cor.test(x = mydata$time1, y = mydata$time2) Pearson&#39;s product-moment correlation data: mydata$time1 and mydata$time2 t = 28.184, df = 95, p-value &lt; 2.2e-16 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.9188254 0.9630063 sample estimates: cor 0.9450797 4.8.1.2 Coefficient of repeatability Code qnorm(.975) * sd( blandr.statistics( method1 = mydata$time1, method2 = mydata$time2)$differences, na.rm = TRUE) [1] 6.588758 4.8.1.3 Bias Code blandr.statistics( method1 = mydata$time1, method2 = mydata$time2)$bias [1] 0.335823 4.8.2 Add within-person random error In the example below, we added within-person random error. Specifically, we added error that is randomly distributed around a mean of 0 and a standard deviation of 3 to all participants’ scores at all time points. For reproducibility, we set the seed below. Using the same seed will yield the same answer every time. There is nothing special about this particular seed. Code # Simulate Data set.seed(52242) time1 &lt;- rnorm(n = sampleSize, mean = 50, sd = 10) time2 &lt;- time1 + rnorm(n = sampleSize, mean = 0, sd = 4) time3 &lt;- time2 + rnorm(n = sampleSize, mean = 0, sd = 8) # Add Missing Data time1[c(5,10)] &lt;- NA time2[c(10,15)] &lt;- NA time3[c(10)] &lt;- NA # Combine data into data frame mydata &lt;- data.frame( rater1continuous, rater2continuous, rater3continuous, rater1categorical, rater2categorical, rater3categorical, time1, time2, time3, item1, item2, item3, item4) # Add error set.seed(12345) for(i in 1:nrow(mydata)){ mydata[i, c(&quot;time1&quot;,&quot;time2&quot;,&quot;time3&quot;)] &lt;- mydata[i, c( &quot;time1&quot;,&quot;time2&quot;,&quot;time3&quot;)] + rnorm(3, mean = 0, sd = 3) } 4.8.2.1 Coefficient of stability Code cor.test(x = mydata$time1, y = mydata$time2) Pearson&#39;s product-moment correlation data: mydata$time1 and mydata$time2 t = 18.782, df = 95, p-value &lt; 2.2e-16 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.8361773 0.9235489 sample estimates: cor 0.8875967 4.8.2.2 Coefficient of repeatability Code qnorm(.975) * sd( blandr.statistics( method1 = mydata$time1, method2 = mydata$time2)$differences, na.rm = TRUE) [1] 10.03618 4.8.2.3 Bias Code blandr.statistics( method1 = mydata$time1, method2 = mydata$time2)$bias [1] 0.5901701 4.8.2.4 Findings Adding within-person random error led to a weaker (lower) stability coefficient a weaker (higher) repeatability coefficient 4.8.3 Add within-person systematic error In the example below, we added within-person systematic error. Specifically, to all participants, we added error that is consistent across time for a given participant: pulled from a distribution with a mean of 5 and a standard deviation of 3. For reproducibility, we set the seed below. Using the same seed will yield the same answer every time. There is nothing special about this particular seed. Code # Simulate Data set.seed(52242) time1 &lt;- rnorm(n = sampleSize, mean = 50, sd = 10) time2 &lt;- time1 + rnorm(n = sampleSize, mean = 0, sd = 4) time3 &lt;- time2 + rnorm(n = sampleSize, mean = 0, sd = 8) # Add Missing Data time1[c(5,10)] &lt;- NA time2[c(10,15)] &lt;- NA time3[c(10)] &lt;- NA # Combine data into data frame mydata &lt;- data.frame( rater1continuous, rater2continuous, rater3continuous, rater1categorical, rater2categorical, rater3categorical, time1, time2, time3, item1, item2, item3, item4) # Add error set.seed(12345) for(i in 1:nrow(mydata)){ mydata[i, c(&quot;time1&quot;,&quot;time2&quot;,&quot;time3&quot;)] &lt;- mydata[i, c( &quot;time1&quot;,&quot;time2&quot;,&quot;time3&quot;)] + rnorm(1, mean = 5, sd = 3) } 4.8.3.1 Coefficient of stability Code cor.test(x = mydata$time1, y = mydata$time2) Pearson&#39;s product-moment correlation data: mydata$time1 and mydata$time2 t = 29.005, df = 95, p-value &lt; 2.2e-16 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.9229555 0.9649296 sample estimates: cor 0.9479103 4.8.3.2 Coefficient of repeatability Code qnorm(.975) * sd( blandr.statistics( method1 = mydata$time1, method2 = mydata$time2)$differences, na.rm = TRUE) [1] 6.588758 4.8.3.3 Bias Code blandr.statistics( method1 = mydata$time1, method2 = mydata$time2)$bias [1] 0.335823 4.8.3.4 Findings Adding within-person systematic error led to an artificially stronger (higher) stability coefficient no effect on the repeatability coefficient bias at the person level, but this bias would go undetected by the estimate of group-level bias in the Bland-Altman plot 4.8.4 Add between-person random error at T2 In the example below, we added between-person random error. Specifically, we added error that is randomly distributed around a mean of 0 and a standard deviation of 3 to all participants’ scores at time 2 (see below). For reproducibility, we set the seed below. Using the same seed will yield the same answer every time. There is nothing special about this particular seed. Code # Simulate Data set.seed(52242) time1 &lt;- rnorm(n = sampleSize, mean = 50, sd = 10) time2 &lt;- time1 + rnorm(n = sampleSize, mean = 0, sd = 4) time3 &lt;- time2 + rnorm(n = sampleSize, mean = 0, sd = 8) # Add Missing Data time1[c(5,10)] &lt;- NA time2[c(10,15)] &lt;- NA time3[c(10)] &lt;- NA # Combine data into data frame mydata &lt;- data.frame( rater1continuous, rater2continuous, rater3continuous, rater1categorical, rater2categorical, rater3categorical, time1, time2, time3, item1, item2, item3, item4) # Add error set.seed(12345) mydata$time2 &lt;- mydata$time2 + rnorm(sampleSize, mean = 0, sd = 3) 4.8.4.1 Coefficient of stability Code cor.test(x = mydata$time1, y = mydata$time2) Pearson&#39;s product-moment correlation data: mydata$time1 and mydata$time2 t = 19.367, df = 95, p-value &lt; 2.2e-16 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.8442114 0.9274674 sample estimates: cor 0.8932557 4.8.4.2 Coefficient of repeatability Code qnorm(.975) * sd( blandr.statistics( method1 = mydata$time1, method2 = mydata$time2)$differences, na.rm = TRUE) [1] 9.288859 4.8.4.3 Bias Code blandr.statistics( method1 = mydata$time1, method2 = mydata$time2)$bias [1] -0.4554251 4.8.4.4 Findings Adding between-person random error led to a weaker (lower) stability coefficient a weaker (higher) repeatability coefficient 4.8.5 Add between-person systematic error at T2 In the example below, we added between-person systematic error. Specifically, we added constant variance of 10 to all participants’ scores at T2. For reproducibility, we set the seed below. Using the same seed will yield the same answer every time. There is nothing special about this particular seed. Code # Simulate Data set.seed(52242) time1 &lt;- rnorm(n = sampleSize, mean = 50, sd = 10) time2 &lt;- time1 + rnorm(n = sampleSize, mean = 0, sd = 4) time3 &lt;- time2 + rnorm(n = sampleSize, mean = 0, sd = 8) # Add Missing Data time1[c(5,10)] &lt;- NA time2[c(10,15)] &lt;- NA time3[c(10)] &lt;- NA # Combine data into data frame mydata &lt;- data.frame( rater1continuous, rater2continuous, rater3continuous, rater1categorical, rater2categorical, rater3categorical, time1, time2, time3, item1, item2, item3, item4) # Add error mydata$time2 &lt;- mydata$time2 + 10 4.8.5.1 Coefficient of stability Code cor.test( x = mydata$time1, y = mydata$time2) Pearson&#39;s product-moment correlation data: mydata$time1 and mydata$time2 t = 28.184, df = 95, p-value &lt; 2.2e-16 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.9188254 0.9630063 sample estimates: cor 0.9450797 4.8.5.2 Coefficient of repeatability Code qnorm(.975) * sd( blandr.statistics( method1 = mydata$time1, method2 = mydata$time2)$differences, na.rm = TRUE) [1] 6.588758 4.8.5.3 Bias Code blandr.statistics( method1 = mydata$time1, method2 = mydata$time2)$bias [1] -9.664177 4.8.5.4 Findings Adding between-person systematic error led to no effect on the stability coefficient no effect on the repeatability coefficient bias at the person level and at the group level (detected by group-level estimate of bias in Bland-Altman plot) 4.8.6 Add constant variance of 10 to all participants at all time points In the example below, we added constant variance to all participants’ scores at all time points. Specifically, we added 10 to all participants’ scores at all time points. For reproducibility, we set the seed below. Using the same seed will yield the same answer every time. There is nothing special about this particular seed. Code # Simulate Data set.seed(52242) time1 &lt;- rnorm(n = sampleSize, mean = 50, sd = 10) time2 &lt;- time1 + rnorm(n = sampleSize, mean = 0, sd = 4) time3 &lt;- time2 + rnorm(n = sampleSize, mean = 0, sd = 8) # Add Missing Data time1[c(5,10)] &lt;- NA time2[c(10,15)] &lt;- NA time3[c(10)] &lt;- NA # Combine data into data frame mydata &lt;- data.frame( rater1continuous, rater2continuous, rater3continuous, rater1categorical, rater2categorical, rater3categorical, time1, time2, time3, item1, item2, item3, item4) # Add error mydata$time1 &lt;- mydata$time1 + 10 mydata$time2 &lt;- mydata$time2 + 10 mydata$time3 &lt;- mydata$time3 + 10 4.8.6.1 Coefficient of stability Code cor.test(x = mydata$time1, y = mydata$time2) Pearson&#39;s product-moment correlation data: mydata$time1 and mydata$time2 t = 28.184, df = 95, p-value &lt; 2.2e-16 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.9188254 0.9630063 sample estimates: cor 0.9450797 4.8.6.2 Coefficient of repeatability Code qnorm(.975) * sd( blandr.statistics( method1 = mydata$time1, method2 = mydata$time2)$differences, na.rm = TRUE) [1] 6.588758 4.8.6.3 Bias Code blandr.statistics( method1 = mydata$time1, method2 = mydata$time2)$bias [1] 0.335823 4.8.6.4 Findings Adding constant variance to all participants’ scores at all time points led to no effect on the stability coefficient no effect on the repeatability coefficient bias at the person level and at the group level, but this bias would go undetected by the estimate of group-level bias in the Bland-Altman plot 4.8.7 Summary To summarize the effects of measurement error, random error (both within- and between-person) results in (a) a worse (lower) stability coefficient because a smaller proportion of the observed score variance is attributable to true score variance, and (b) a worse (higher) repeatability coefficient because the standard deviation of the difference between the two measurements is greater. Neither type of random error leads to bias at the person or group level. In terms of systematic error, all types of systematic error lead to increased bias, i.e., reduced accuracy for the person-level average and/or group-level average. Though multiple biases can coexist and can work in opposite directions, which can obscure bias even though it exists. In terms of within-person systematic error, it (a) artificially increases the stability coefficient, even though the measure is not actually more reliable, (b) has no effect on the repeatability coefficient because it is not changing the difference between the two measurements for a person, and (c) results in bias at the person level because the person’s mean systematically differs from their true score. In terms of between-person systematic error at time 2 (an example of adding constant variance to all participants), it (a) had no effect on the stability coefficient because the rank order of individual differences stays the same, (b) had no effect on the repeatability coefficient because the standard deviation of the difference between the two measurements is not changed, (c) resulted in bias at the person level and at the group level that would be detected in a Bland-Altman plot because the group-level average deviates from the group’s average true score. In sum, just because a measure has high estimates of test–retest reliability does not mean that the measure is actually reliable because systematic error does not reduce the stability and repeatability coefficients. Indeed, within-person systematic error actually (and artificially) increases the coefficient of stability. The coefficient of stability index of test–retest reliability is affected by the measure and variance across people, and it can be thrown off by outliers or by range restriction because the Pearson correlation strongly depends on these, as discussed in Section 4.5.1.1.1. And sample variation also has a strong influence, so it is important to cross-validate estimates of reliability in independent samples. 4.9 Effect of Measurement Error on Associations As we describe in greater detail in the chapter on validity in Section 5.6, (random) measurement error weakens (or attenuates) the association between variables. The greater the random measurement error, the weaker the association. 4.9.1 Attenuation of True Test–Retest Reliability Due to Measurement Error In the context of examining the association between test and retest (i.e., test–retest reliability), the estimate of the stability of the construct (i.e., true test–retest association) is attenuated to the extent that the measure is unreliable, as described in Equation (4.21): \\[ \\small \\begin{aligned} r_{x_1x_2} &amp;= r_{x_{1_t}x_{2_t}} \\sqrt{r_{xx} r_{xx}} \\\\ &amp;= r_{x_{1_t}x_{2_t}} \\sqrt{{r_{xx}}^2} \\\\ &amp;= r_{x_{1_t}x_{2_t}} {r_{xx}} \\\\ \\text{test–retest correlation of measure} &amp;= \\text{(true correlation of construct across two time points)} \\times \\\\ &amp; \\;\\;\\;\\;\\; \\text{(reliability of measure)} \\end{aligned} \\tag{4.21} \\] Find the observed test–retest association if the true test–retest association (i.e., the stability of the construct) is .7 and the reliability of the measure is .9: Code trueAssociation &lt;- 0.7 reliabilityOfMeasure &lt;- 0.9 trueAssociation * reliabilityOfMeasure [1] 0.63 The petersenlab package (Petersen, 2024b) contains the attenuationCorrelation() function that estimates the observed association given the true association and the reliability of the predictor and criterion. When dealing with test–retest reliability, the predictor and criterion are the same, so we use the same reliability estimate for each: Code attenuationCorrelation( trueAssociation = 0.7, reliabilityOfPredictor = 0.9, reliabilityOfCriterion = 0.9) [1] 0.63 4.9.2 Disattenuation of Observed Test–Retest Reliability Due to Measurement Error Then, to disattenuate the observed test–retest reliability due to random measurement error, we can rearrange the terms in the formula above to estimate what the true stability of the construct is, according to Equation (4.22): \\[ \\small \\begin{aligned} r_{x_{1_t}x_{2_t}} &amp;= \\frac{r_{x_1x_2}}{r_{xx}} \\\\ \\text{true correlation of construct across two time points} &amp;= \\frac{\\text{test–retest correlation of measure}}{\\text{reliability of measure}} \\end{aligned} \\tag{4.22} \\] Find the true correlation of the construct across two time points given an observed correlation if the observed reliability of the measure is .9: Code reliabilityOfMeasure &lt;- 0.9 observedAssociation &lt;- as.numeric(cor.test( x = mydata$time2, y = mydata$time3)$estimate) observedAssociation / reliabilityOfMeasure [1] 0.8974584 The petersenlab package (Petersen, 2024b) contains the disattenuationCorrelation() function that estimates the observed association given the true association and the reliability of the predictor and criterion. When dealing with test–retest reliability, the predictor and criterion are the same, so we use the same reliability estimate for each: Code disattenuationCorrelation( observedAssociation = observedAssociation, reliabilityOfPredictor = 0.9, reliabilityOfCriterion = 0.9) [1] 0.8974584 The next section discusses method biases, including what they are, why they are important, and how to account for them. 4.10 Method Bias 4.10.1 What Method Biases Are and Why They Are Important Method bias is the influence of measurement on a person’s score that is not due to the person’s level on the construct. Method bias is a form of systematic, nonrandom measurement error because it involves errors/biases that are shared/common to/correlated for two measures. Method bias is problematic because it can bias estimates of a measure’s reliability and validity, and its association with other measures/constructs. There is a biasing effect of assessing two or more constructs with the same method on estimates of the association between them—that is, some of the observed covariance may be due to the fact that they share the same method of measurement. Although estimates vary widely from study to study, construct to construct, and measure to measure, estimates of method bias tend to be substantial. Meta-analyses suggest that around 20–40% of a given measure’s variance reflects method variance (for a review, see Podsakoff et al., 2012). According to Podsakoff et al. (2012), response styles inflate the magnitude of observed correlations by 54% if there is a positive correlation between the constructs and a positive correlation between the response styles. Response styles inflate the magnitude of observed correlations by even more (67%) if there is a negative correlation between the constructs and a negative correlation between the response styles. Moreover, the method biases can also weaken the association between measures. If the direction of correlation between the constructs (e.g., positive) is incongruent with the direction of the correlation between the response styles (e.g., negative), the association between the measures is attenuated. 4.10.2 Types of Method Biases There are many different types of method biases, including the effects of same source: Whether ratings or information comes from the same source can lead to systematic method variance. response styles (response biases): Response styles are systematic ways of responding to items or questions that are not due to the construct of interest. Different types of response styles can lead to systematic method variance across constructs. Examples of response styles include acquiescence: the tendency to agree with items (both positively and negatively worded), regardless of content disacquiescence: the tendency to disagree with items (both positively and negatively worded), regardless of content extreme: the tendency to endorse items on the extremes (positive or negative) of the response scale, regardless of content midpoint/central tendency: the tendency to endorse items on the middle scale category, regardless of content noncontingent: the tendency to respond to items carelessly, randomly, or nonpurposefully social desirability: the tendency to respond to items in a way that others will believe the respondent is better than they actually are. Social desirability includes multiple components, including self-deception and impression management. Impression management is the conscious effort to control the way people view oneself or the way one presents oneself. proximity: Items (of unrelated constructs) that are in closer proximity to one another in a measure are more likely to be correlated than items that are further apart. The associations of items assessing unrelated constructs are weakest when the pair of items are six or more items apart from each other. There is an exception for reversed items. For reversed items (e.g., reverse scored), the further apart the reversed item is placed from another item, the higher the negative correlation. item wording: Whether items are positively or negatively worded can influence the association between items (i.e., like-worded items go with like). item context: Manipulating the context of a measure can have an effect on the perceived association. You can observe a different association between constructs when you put the measure in a negative context versus in a positive context. item ambiguity: When respondents are less certain of how to accurately answer a question, it increases the likelihood that they will rely on response styles or succumb to the effect of context when responding, which biases scores. motivational factors: Researchers need to motivate participants to put in the cognitive effort to answer questions correctly, to decrease the likelihood that they will succumb to responding based on response style only. Also, researchers should decrease pressures for “optimal responding” on respondents. That is, researchers should attempt to reduce social desirability effects. the ability of the respondent: Respondents with low cognitive abilities, less education, more fatigue (after lots of questionnaires), less experience with the topic being assessed, and more uncertainty about how to respond are more likely to succumb to biasing factors. 4.10.3 Potential Remedies There are numerous potential remedies to help reduce the effects of method bias, including obtain measures of the predictor and criterion from multiple or different sources create temporal, physical, and/or psychological separation between the predictor and criterion eliminate common scale properties; vary the scale types and anchor labels remove ambiguity in items—keep questions simple, (behaviorally) specific, and concise reduce social desirability in item wording provide anonymity to respondents for estimating the prevalence of a sensitive behavior, use the randomized response model (S. J. Clark &amp; Desharnais, 1998), as described below provide incentives (e.g., compensation) to respondents reverse score some items to balance positively and negatively worded (i.e., reverse-scored) items separate items on the questionnaire to eliminate proximity effects maximize respondent motivation to give an accurate response develop a good cover story and clear instructions, with reminders as necessary match the measure difficulty to the respondent’s ability; minimize frustration use confirmatory factor analysis or structural equation modeling, as described in Section 7.12.1 use mixed models, as described below 4.10.3.1 Randomized Response Model The randomized response model (S. J. Clark &amp; Desharnais, 1998) is a technique to increase people’s honesty in responding, especially around sensitive topics. The randomized response model sought to answer the question of how much information people withhold on surveys. It is no surprise that people lie on surveys, especially if the survey asks questions about sensitive topics, such as suicidality, criminal behavior (e.g., drug use), and child abuse or neglect. The randomized response model is based on the premise that respondents would give more honest answers if they could be certain that their survey answers would not reveal any sensitive information about themselves, and it is designed to reduce evasive-answer bias by guaranteeing privacy. The randomized response model works by exploiting statistical probabilities. Consider a survey that asks respondents whether they have used various illegal drugs (e.g., “Have you used heroin?” or “Have you used cocaine?”). Each participant is given a coin. They are told to flip the coin before they answer each question. If the coin flip is “heads,” they answer the question truthfully. If the coin flip is “tails,” they answer the questions with a “yes” (i.e., the more sensitive answer) no matter what. That way, a yes could be due to the coin flip or could be the truth—so respondents feel as if they are divulging less. Then, researchers can estimate the actual “no” rate by doubling the observed “no” rate for the sample. For instance, if 20% of people say they have never gambled and 80% say they have gambled, the actual “no” rate of gambling is 40%. That is, the actual gambling rate is 60%. Findings using the randomized response model indicate that there are a lot of things that people are not sharing. The randomized response model is useful for estimating the true prevalence rate of sensitive behaviors. However, it is not great for individual differences research because you do not know whether any given individual truly engages in a behavior (because a “yes” response may have occurred merely because the coin flip was “heads”). Thus, the randomized response model is only helpful for knowing “true” overall rate of a given behavior in the sample. 4.10.3.2 Mixed Models One approach to handle method biases is in mixed models, which are also called mixed-effects models, multilevel models, and hierarchical linear models (HLMs). Mixed models are similar to multiple regression in that they allow fitting a model that examines multiple predictor variables in relation to one outcome variable. One of the assumptions of multiple regression is that the data observations are independent from each other, as evidenced by the residuals being uncorrelated with each other. Mixed models are designed to handle nonindependent observations, that is, data that were collected from nonindependent units. Thus, mixed models can be fit to data that do not meet the nonindependence assumption of multiple regression. Examples of data from nonindependent units include nested data, multiple membership data, and cross-classified data. One example of nonindependent data is if your data are nested. When data are collected from multiple lower-level units (e.g., people) in an upper-level unit (e.g., married couple, family, classroom, school, neighborhood), the data from the lower-level units are considered nested within the upper-level unit. For example, multiple participants in your sample may come from the same classroom. Data from multiple people can be nested within the same family, classroom, school, etc. Longitudinal data can also be considered nested data, in which time points are nested within the participant (i.e., the same participant provides an observation across multiple time points). And if you have multiple informants of a child’s behavior (e.g., parent-, teacher-, friend-, and self-report), the ratings could also be nested within the participant (i.e., there are ratings from multiple informants of the same child). Another form of nonindependent data is when data involve multiple membership. Data involve multiple membership when the lower-level units belong to more than one upper-level unit simultaneously. As an example of multiple membership, children may have more than one teacher, and therefore, in a modeling sense, children “belong” to more than one teacher cluster. Another form of nonindependent data is when data are cross-classified (also called crossed). Data are cross-classified when the lower-level units are classified by two or more upper-level units, but the upper-level units are not hierarchical or nested within one another. For instance, children may be nested within the crossing of schools and neighborhoods. That is, children are nested within schools, and children are also nested within neighborhoods; however, children attending the same school may not necessarily be from the same neighborhood. The data would still have a two-level hierarchy, but the data would be nested in specific school-neighborhood combinations. That is, children are nested within the cross-classification of schools and neighborhoods. The general form of multiple regression is (see Equation (4.23)): \\[\\begin{equation} y = b_0 + b_1x + e \\tag{4.23} \\end{equation}\\] where \\(b_0\\) is the intercept, \\(e\\) is the error term, and \\(b_1\\) is the slope of the predictor, \\(x\\), that states the relation between the predictor (\\(x\\)) and the outcome (\\(y\\)). However, the slope can exist at multiple levels when data are nested within groups or when time points are nested within people. For example, you can observe different associations of a predictor with an outcome at the between-individual level compared to the within-individual level. That is, the association between the predictor and outcome can differ when comparing across individuals versus when looking at the association between the predictor and outcome based on fluctuations within the individual. When the associations differ at multiple levels, this is known as Simpson’s paradox. For an example of Simpson’s paradox, see Figure 4.26 (R. Kievit et al., 2013). Thus, it is important to consider that the association between variables may differ at different levels (person level, group level, population level, etc.). Figure 4.26: Example of Simpson’s paradox, where the association between the predictor and outcome differs at different levels. In this case, the association between dosage and recovery probability is positive at the population-level (upper-level unit), but the association is negative among men and women separately (lower-level unit). (Figure reprinted from R. Kievit et al. (2013), Figure 1, p. 2. Kievit, R., Frankenhuis, W., Waldorp, L., &amp; Borsboom, D. (2013). Simpson’s paradox in psychological science: a practical guide. Frontiers in Psychology, 4(513). https://doi.org/10.3389/fpsyg.2013.00513) To account for method biases in a mixed model, you could use scores from multiple measures and methods as the outcome variable. For instance, your mixed model could examine the association between income and optimism, where optimism is assessed by self-report, parent-report, and a performance-based measure. In the model, you could include, as a fixed effect, a factor that indicates the method of the measure, in addition to its interaction with income, to account for systematic ways in which the measurement method influences scores on the measures across the participants as a whole. You could also include a random effect of measurement method if you wanted to allow the effect of measurement method on the scores to differ from person to person. [INCLUDE EXAMPLE] 4.11 Generalizability Theory (G-Theory) Up to this point, we have discussed reliability from the perspective of CTT. However, as we discussed, CTT makes several assumptions that are unrealistic (e.g., that all error is random). There are other measurement theories that conceptualize reliability differently than the way that CTT conceptualizes reliability. One such measurement theory is generalizability theory (Brennan, 1992), also known as G-theory and domain sampling theory. G-theory is described in detail in Chapter 6 on generalizability theory and is also discussed in the chapters on validity (Chapter 5, Section 5.7), and structural equation modeling (Chapter 7, Section 7.14). 4.12 Item Response Theory In addition to CTT and G-theory, item response theory (IRT) is another measurement theory. IRT also estimates reliability in a different way compared to CTT, as described in Section 8.1.6. We discuss IRT in Chapter 8. 4.13 The Problem of Low Reliability Using measures that have low reliability can lead to several problems, including over- or under-estimation of the effects of interest. When sample sizes are large, measurement error tends to weaken effect sizes. However, when sample sizes are small to modest, measurement error can actually lead to over-estimation of effect sizes as well as inaccurate estimates of the sign of the effect (positive or negative), due to the statistical significance filter (Loken &amp; Gelman, 2017), as described in Section 1.5.1. reduced statistical power to detect the effects of interest failure to replicate findings invalid measurement for the proposed uses. As we described in the chapter on validity in Section 5.5, reliability constrains validity. These concerns are especially important for individual differences research (e.g., correlational designs) or when making decisions about individual people based on their scores on measure. However, these concerns are also relevant for experimental designs. 4.14 Ways to Increase Reliability Here are several potential ways to increase reliability of measurement: Improve the clarity and homogeneity of items. Use a structured response set and scoring system rather than unstructured ones. Make sure participants know how to complete the task or items. For example, make sure they have appropriate instructions that are easily understood. Adhere closely to the standardized procedures for administering and scoring a measure. Make scoring rules as explicit as possible. Train raters to some criterion of inter-rater reliability. Continue to monitor inter-rater reliability over time to reduce coder drift. Use a more precise response scale. The smaller the number of response options, the greater the attenuation of a correlation due to measurement error (Rigdon, 2010). Have a sample with a full range of variation in the items and measure. If possible, do not use a difference score. This is discussed in greater detail in Section 4.5.8 and in Chapter 24 (Section 24.3.4.3.1) on cognitive assessment. Remove items that reduce reliability. This may include items that have low variability because they are too easy or too difficult (i.e., items that show ceiling or floor effects), in addition to items that have a low item–total correlation. Collect more information across people and time. Aggregate: Add items, measures, observers, raters, and time points. Having more items that assess the same construct in the measure tends to increase reliability unless the additional items cause fatigue because the instrument is too long. Aggregation of like things reduces measurement error, because averaging together more items helps cancel out random error. Having multiple measures that assess the same construct can be beneficial, especially measures from different methods that do not share the same systematic errors (e.g., observation in addition to parent report). Items that aggregate a large amount of information tend to have strong predictive validity. For example, when hot dog sales are high, there are more wins at Yankees games. Hot dog sales have no theoretical influence on the Yankees, but it predicts well. In general, for predictive accuracy, it is often better to assess more things less well than to assess one thing extraordinarily well. Aggregation is a double-edged sword: It is your friend as a researcher, because of increased predictive utility, but it is your enemy as an interpreter because it is hard to interpret the theoretical meaning of predictive associations based on an aggregated variable because the measure is more general and nonspecific. If aggregation increases error, test–retest reliability will decrease. If error stays the same when aggregating, test–retest reliability will increase. Use a latent variable approach to aggregate the multiple items, measures, observers, raters, or time points. Latent variable approaches include structural equation modeling and item response theory. Estimating a latent variable allows identifying the variance that is common to all of the measures (i.e., a better approximate of true score variance), and it discards variance that is not shared by all measures (i.e., error variance). Thus, latent variables have the potential to get purer estimates of the constructs by reducing measurement error. 4.15 Conclusion Reliability is how much repeatability, consistency, and precision a measure’s scores have. Reliability is not one thing. There are multiple aspects of reliability, and the extent to which a given aspect of reliability is important depends on the construct of interest. In general, we want our measures to show strong inter-rater, intra-rater, immediate parallel-forms, and internal consistency reliability. However, the extent to which our measures should show test–retest and delayed parallel-forms reliability depends on the stability of the construct and the time lag. Moreover, although we want our measures to show strong internal consistency reliability, the internal consistency reliability of a measure can be too high, and for some constructs (i.e., formative constructs), internal consistency would not be expected. In addition, reliability is not a characteristic that resides in a test. The reliability of a measure’s scores reflects an interaction of the properties of the test with the population for whom it is designed and the sample, situation, and context in which it is administered. Thus, when reporting reliability in papers, it is important to adequately describe the aspects of reliability that have been considered and the population, sample, and context in which the measure is assessed. However, it is not enough for measures to be consistent or reliable; our interpretation of the measures’ scores should also be accurate, meaningful, and useful for the intended uses. In other words, our interpretation of the measures’ scores should be valid for the intended uses. In the next chapter (Chapter 5), we discuss validity of measurement. 4.16 Suggested Readings For more information on inter-rater reliability, we suggest reading Gwet (2021a) and Gwet (2021b), for categorical and continuous ratings, respectively. 4.17 Exercises 4.17.1 Questions Note: Several of the following questions use data from the Children of the National Longitudinal Survey of Youth Survey (CNLSY). The CNLSY is a publicly available longitudinal data set provided by the Bureau of Labor Statistics (https://www.bls.gov/nls/nlsy79-children.htm#topical-guide; archived at https://perma.cc/EH38-HDRN). The CNLSY data file for these exercises is located on the book’s page of the Open Science Framework (https://osf.io/3pwza). Children’s behavior problems were rated in 1988 (time 1: T1) and then again in 1990 (time 2: T2) on the Behavior Problems Index (BPI). What is the test–retest reliability of the Antisocial Behavior subscale of the BPI? Provide the coefficient of stability and coefficient of repeatability, and interpret what they indicate about the Antisocial Behavior subscale of the BPI. What was the estimate of bias of the Antisocial Behavior subscale of the BPI comparing the measurements at T1 and T2, and what does this indicate about the Antisocial Behavior subscale of the BPI and the construct? If a person had a score of 6 on the Antisocial Behavior subscale of the BPI at T1, within what range would we expect their score to be at T2? What interval is often considered the “standard” for evaluating test–retest reliability of measures in psychology? What is a potential downside of having shorter intervals? What is a potential downside of having longer intervals? How would you expect the test–retest reliability and bias to differ had the BPI been re-assessed at that (“standard”) interval? For each of the following, what would the coefficient of stability, coefficient of repeatability, and bias be if the following measurement error were added? Also summarize the effects of measurement error in each case: 3 points to all scores at only T2 3 points to all scores at both T1 and T2 3 points to the scores for the first 500 participants at only T1 3 points to the scores for the first 500 participants at both T1 and T2 You and a colleague both conduct diagnostic interviews on the same five clients. The data are in Table 4.3 below. What is your inter-rater reliability with your colleague? Table 4.3: Exercise 12: Table of Diagnostic Decisions for Assessing Inter-Rater Reliability. Client Your Diagnosis of the Client Your Colleague’s Diagnosis of the Client 1 Yes Yes 2 No No 3 Yes No 4 No Yes 5 No No You and a colleague both rate the same five clients in the extent to which they are likely to be suicidal, where \\(1 =\\) very low likelihood and \\(5 =\\) very high likelihood. The data are in Table 4.4 below. What is your inter-rater reliability with your colleague? Table 4.4: Exercise 12: Table of Ratings for Assessing Inter-Rater Reliability. Client Your Rating of the Client Your Colleague’s Rating of the Client 1 5 4 2 3 2 3 4 3 4 2 2 5 1 2 What was the internal consistency of the items on the Antisocial Behavior subscale of the BPI at T1? Provide Cronbach’s alpha (\\(\\alpha\\)), omega total (\\(\\omega_t\\)), omega hierarchical (\\(\\omega_h\\)), and the average of all possible split-half reliabilities. What do the estimates of internal consistency indicate about the BPI subscale? What is the standard error of measurement of the Antisocial Behavior subscale of the BPI? If a child’s observed score is 6, within what range would we expect their true score to be? How confident are we about a child’s true score on the construct (antisocial behavior) if we know their observed score on the measure (Antisocial Behavior subscale of the BPI)? For many ways of estimating test–retest reliability, we assume that the true association between the construct at T1 and the construct at T2 is 1.0 (that is, the association of the true scores is 1.0). According to the attenuation formula (that attenuates the association due to measurement error), if the true association between the construct at T1 and the construct at T2 was 1.0 and the reliability of the measure was .8, the observed test–retest correlation would be \\(r = .80\\). According to the attenuation formula, what would the observed test–retest correlation be if the true association was .9 and the reliability of the measure was .8? According to the disattenuation formula (to correct for attenuation of the association due to measurement error), what would be the true test–retest association of the construct of antisocial behavior if the reliability of the Antisocial Behavior subscale was \\(r = .6299494\\)? 4.17.2 Answers The coefficient of stability (\\(r = .50\\)) indicates that the Antisocial subscale of the BPI shows moderate rank-order stability over a two-year interval at these ages in this sample. The coefficient of repeatability (\\(4.45\\)) indicates that 95% of repeated assessments on the BPI over a two-year interval would be expected to fall within \\(4.45\\) points of the score at the first measurement occasion (after accounting for any bias). Given that the total score ranges from 0–14, a repeatability coefficient of \\(4.45\\) indicates that individual children showed considerable differences from T1 to T2. The estimate of bias was \\(0.10\\), indicating that children’s scores were slightly (\\(0.10\\) points) higher, on average, at T1 compared to T2. This may indicate developmental change such that children tend to decrease in antisocial behavior as they get older. Their score at T2 is expected to be within the range of \\([1.65, 10.55]\\). A two-week interval is typically considered standard for evaluating test–retest reliability. Shorter intervals have the potential problem that there may be carryover effects from the original testing due to the participant’s memory of their answers from the first testing (thus artificially increasing the estimate of test–retest reliability). Longer intervals have the potential problem that real developmental change could occur in between the measurement occasions, thus artificially decreasing the estimate of test–retest reliability. The BPI was assessed at a two-year interval. Had it been assessed at a two-week interval, we would expect test–retest reliability to be higher (higher coefficient of stability and lower coefficient of repeatability) and bias to be lower. The estimates are as follows: stability coefficient: \\(r = .50\\); repeatability coefficient \\(= 4.45\\); \\(\\text{bias} = -2.90\\) increased bias (greater absolute value); no detectable effect on stability or repeatability coefficient stability coefficient: \\(r = .50\\); repeatability coefficient \\(= 4.45\\); \\(\\text{bias} = 0.10\\) no detectable effect on stability coefficient, repeatability coefficient, or bias stability coefficient: \\(r = .48\\); repeatability coefficient \\(= 4.64\\); \\(\\text{bias} = -0.01\\) lower/worse stability coefficient; higher/worse repeatability coefficient; contributed bias (but in the opposite direction of the bias before the score adjustment) stability coefficient: \\(r = .54\\); repeatability coefficient \\(= 4.45\\); \\(\\text{bias} = 0.10\\) artificially higher/stronger stability coefficient; no detectable effect on repeatability coefficient or bias Your inter-rater reliability was very poor (Cohen’s kappa [\\(\\kappa] = .17\\)). Your inter-rater reliability was good. If you used only one judge’s ratings in analysis, the scores would have an intra-class correlation of \\(\\text{ICC}(2,1) = .76\\). If, however, you used the average of the judge’s ratings in analysis, the scores would have an intra-class correlation of \\(\\text{ICC}(2,k) = .86\\). The estimates of internal consistency (Cronbach’s alpha \\([\\alpha] = .70\\), omega total \\([\\omega_t] = .70\\), omega hierarchical \\([\\omega_h] = .71\\), average split-half reliability \\(= .70\\)) indicate that the ratings show moderate consistency across items on the Antisocial subscale (and that the items are somewhat similar to one another), but that the items on the Antisocial subscale are also somewhat distinct from one another. The standard error of measurement is estimated based on the reliability of the measure. Reliability can be evaluated in different ways, and there is no single index of reliability. If we were to take infinite observations of a child’s score and average them, that average would approximate the child’s true score. Then, the test–retest reliability of the measure would help tell us about how (im)precise our estimate was at any given occasion compared to the true score. For the Antisocial Behavior subscale of the BPI, the test–retest reliability was \\(.50\\) as indexed by the coefficient of stability. If we used this as our estimate of the measure’s reliability (though the stability coefficient would likely be higher if we used a retest interval of two weeks instead of two years), the standard error of measurement would be \\(1.64\\). Using this as our estimate of the standard error of measurement, we would expect the child’s true score to fall within the range of \\([2.79, 9.21]\\). That’s a pretty wide range of uncertainty for a scale that ranges from 0–14. So, we are not very confident of a child’s true score on the construct from their observed score on this measure. The observed test–retest correlation (coefficient of stability) would be \\(r = .72\\). The true test–retest association would be \\(r = .80\\). References American Educational Research Association, American Psychological Association, &amp; National Council on Measurement in Education. (2014). Standards for educational and psychological testing. American Educational Research Association. Bakeman, R., &amp; Goodman, S. H. (2020). Interobserver reliability in clinical research: Current issues and discussion of how to establish best practices. Journal of Abnormal Psychology, 129(1), 5–13. https://doi.org/10.1037/abn0000487 Bland, J. M., &amp; Altman, D. G. (1986). Statistical methods for assessing agreement between two methods of clinical measurement. The Lancet, 327(8476), 307–310. https://doi.org/10.1016/S0140-6736(86)90837-8 Bland, J. M., &amp; Altman, D. G. (1999). Measuring agreement in method comparison studies. Statistical Methods in Medical Research, 8(2), 135–160. https://doi.org/10.1177/096228029900800204 Borsboom, D. (2003). Conceptual issues in psychological measurement. Universiteit van Amsterdam. Brennan, R. L. (1992). Generalizability theory. Educational Measurement: Issues and Practice, 11(4), 27–34. https://doi.org/10.1111/j.1745-3992.1992.tb00260.x Clark, S. J., &amp; Desharnais, R. A. (1998). Honest answers to embarrassing questions: Detecting cheating in the randomized response model. Psychological Methods, 3(2), 160–168. https://doi.org/10.1037/1082-989X.3.2.160 Cortina, J. M. (1993). What is coefficient alpha? An examination of theory and applications. Journal of Applied Psychology, 78, 98–104. https://doi.org/10.1037/0021-9010.78.1.98 Datta, D. (2018). blandr: Bland-Altman method comparison. https://github.com/deepankardatta/blandr/ Dunn, T. J., Baguley, T., &amp; Brunsden, V. (2014). From alpha to omega: A practical solution to the pervasive problem of internal consistency estimation. British Journal of Psychology, 105(3), 399–412. https://doi.org/10.1111/bjop.12046 Falotico, R., &amp; Quatto, P. (2010). On avoiding paradoxes in assessing inter-rater agreement. Italian Journal of Applied Statistics, 22, 151–160. Flora, D. B. (2020). Your coefficient alpha is probably wrong, but which coefficient omega is right? A tutorial on using R to obtain better reliability estimates. Advances in Methods and Practices in Psychological Science, 3(4), 484–501. https://doi.org/10.1177/2515245920951747 Furr, R. M. (2017). Psychometrics: An introduction. SAGE publications. Graham, J. M. (2006). Congeneric and (essentially) tau-equivalent estimates of score reliability: What they are and how to use them. Educational and Psychological Measurement, 66(6), 930–944. https://doi.org/10.1177/0013164406288165 Green, S. B., &amp; Yang, Y. (2015). Evaluation of dimensionality in the assessment of internal consistency reliability: Coefficient alpha and omega coefficients. Educational Measurement: Issues and Practice, 34(4), 14–20. https://doi.org/10.1111/emip.12100 Gwet, K. L. (2008). Computing inter-rater reliability and its variance in the presence of high agreement. British Journal of Mathematical and Statistical Psychology, 61(1), 29–48. https://doi.org/10.1348/000711006X126600 Gwet, K. L. (2021a). Handbook of inter-rater reliability: The definitive guide to measuring the extent of agreement among raters, Vol. 1: Analysis of categorical ratings (5th ed.). AgreeStat Analytics. Gwet, K. L. (2021b). Handbook of inter-rater reliability: The definitive guide to measuring the extent of agreement among raters, Vol. 2: Analysis of quantitative ratings (5th ed.). AgreeStat Analytics. Hayes, A. F., &amp; Coutts, J. J. (2020). Use omega rather than cronbach’s alpha for estimating reliability. but…. Communication Methods and Measures, 14(1), 1–24. https://doi.org/10.1080/19312458.2020.1718629 Hussong, A. M., Bauer, D. J., Giordano, M. L., &amp; Curran, P. J. (2020). Harmonizing altered measures in integrative data analysis: A methods analogue study. Behavior Research Methods. https://doi.org/10.3758/s13428-020-01472-7 Hussong, A. M., Curran, P. J., &amp; Bauer, D. J. (2013). Integrative data analysis in clinical psychology research. Annual Review of Clinical Psychology, 9(1), 61–89. https://doi.org/10.1146/annurev-clinpsy-050212-185522 Jorgensen, T. D., Pornprasertmanit, S., Schoemann, A. M., &amp; Rosseel, Y. (2021). semTools: Useful tools for structural equation modeling. https://github.com/simsem/semTools/wiki Kelley, K. (2020). MBESS: The MBESS R package. http://nd.edu/~kkelley/site/MBESS.html Kelley, K., &amp; Pornprasertmanit, S. (2016). Confidence intervals for population reliability coefficients: Evaluation of methods, recommendations, and software for composite measures. Psychological Methods, 21(1), 69–92. https://doi.org/10.1037/a0040086 Kievit, R., Frankenhuis, W., Waldorp, L., &amp; Borsboom, D. (2013). Simpson’s paradox in psychological science: A practical guide. Frontiers in Psychology, 4(513). https://doi.org/10.3389/fpsyg.2013.00513 Klein, D. F., &amp; Cleary, T. A. (1969). Platonic true scores: Further comment. Psychological Bulletin, 71(4), 278–280. https://doi.org/10.1037/h0026852 Lek, K. M., &amp; Van De Schoot, R. (2018). A comparison of the single, conditional and person-specific standard error of measurement: What do they measure and when to use them? Frontiers in Applied Mathematics and Statistics, 4(40). https://doi.org/10.3389/fams.2018.00040 Loken, E., &amp; Gelman, A. (2017). Measurement error and the replication crisis. Science, 355(6325), 584–585. https://doi.org/10.1126/science.aal3618 Lüdecke, D., Ben-Shachar, M. S., Patil, I., Waggoner, P., &amp; Makowski, D. (2021). performance: An R package for assessment, comparison and testing of statistical models. Journal of Open Source Software, 6(60), 3139. https://doi.org/10.21105/joss.03139 McNeish, D. (2018). Thanks coefficient alpha, we’ll take it from here. Psychological Methods, 23(3), 412–433. https://doi.org/10.1037/met0000144 Nunnally, J. C., &amp; Bernstein, I. H. (1994). Psychometric theory (3rd ed.). McGraw-Hill. Pearl, J. (2013). Linear models: A useful “microscope\" for causal analysis. Journal of Causal Inference, 1(1), 155–170. https://doi.org/10.1515/jci-2013-0003 Peters, G.-J. (2014). The alpha and the omega of scale reliability and validity: Why and how to abandon Cronbach’s alpha and the route towards more comprehensive assessment of scale quality. European Health Psychologist, 16(2), 56–69. Petersen, I. T. (2024b). petersenlab: Package of R functions for the Petersen Lab. https://doi.org/10.5281/zenodo.7602890 Podsakoff, P. M., MacKenzie, S. B., &amp; Podsakoff, N. P. (2012). Sources of method bias in social science research and recommendations on how to control it. Annual Review of Psychology, 63(1), 539–569. https://doi.org/10.1146/annurev-psych-120710-100452 Raykov, T. (2001). Bias of coefficient \\(\\alpha\\) for fixed congeneric measures with correlated errors. 25(1), 69–76. https://doi.org/10.1177/01466216010251005 Raykov, T., &amp; Marcoulides, G. A. (2019). Thanks coefficient alpha, we still need you! Educational and Psychological Measurement, 79(1), 200–210. https://doi.org/10.1177/0013164417725127 Revelle, W. (2022). psych: Procedures for psychological, psychometric, and personality research. https://personality-project.org/r/psych/ Revelle, W., &amp; Condon, D. M. (2019). Reliability from \\(\\alpha\\) to \\(\\omega\\): A tutorial. Psychological Assessment, 31(12), 1395–1411. https://doi.org/10.1037/pas0000754 Rigdon, E. E. (2010). Polychoric correlation coefficient. In N. J. Salkind (Ed.), Encyclopedia of research design. SAGE Publications. https://doi.org/10.4135/9781412961288 Rönkkö, M., &amp; Cho, E. (2020). An updated guideline for assessing discriminant validity. Organizational Research Methods, 1094428120968614. https://doi.org/10.1177/1094428120968614 Rosseel, Y., Jorgensen, T. D., &amp; Rockwood, N. (2022). lavaan: Latent variable analysis. https://lavaan.ugent.be Sijtsma, K. (2008). On the use, the misuse, and the very limited usefulness of cronbach’s alpha. Psychometrika, 74(1), 107. https://doi.org/10.1007/s11336-008-9101-0 Trafimow, D. (2015). A defense against the alleged unreliability of difference scores. Cogent Mathematics, 2(1), 1064626. https://doi.org/10.1080/23311835.2015.1064626 Vaz, S., Falkmer, T., Passmore, A. E., Parsons, R., &amp; Andreou, P. (2013). The case for using the repeatability coefficient when calculating test–retest reliability. PLOS ONE, 8(9), e73990. https://doi.org/10.1371/journal.pone.0073990 Willett, W. (2012). Correction for the effects of measurement error. In W. Willett (Ed.), Nutritional epidemiology (3rd ed., pp. 287–304). Oxford University Press. "],["validity.html", "Chapter 5 Validity 5.1 Overview of Validity 5.2 Getting Started 5.3 Types of Validity 5.4 Validity Is a Process, Not an Outcome 5.5 Reliability Versus Validity 5.6 Effect of Measurement Error on Associations 5.7 Generalizability Theory (G-Theory) 5.8 Ways to Increase Validity 5.9 Conclusion 5.10 Suggested Readings 5.11 Exercises", " Chapter 5 Validity “What we know depends on how we know it.” 5.1 Overview of Validity According to the Standards for Educational and Psychological Testing (American Educational Research Association et al., 2014, p. 11), measurement validity is “the degree to which evidence and theory support the interpretations of test scores for proposed uses of tests.” We summarized reliability with three words: repeatability, consistency, and precision. A summary of validity in three words is accuracy, utility, and meaningfulness. Validity is tied to the interpretation of a measure’s scores for the proposed uses, not (just) to the measure itself. The same set of scores can have different degrees of validity for different purposes. For instance, a measure’s scores may have stronger validity for making a diagnostic decision than for making a prediction about future behavior. Thus, as the Standards indicate, it is incorrect to use the unqualified phrase “the validity of the measure” or “the measure is (in)valid,” because these phrases do not specify which scores were used from the test, what the use is (e.g., predicting whether a person will succeed in a given job), and what interpretation was made of the test scores for this purpose.1 Below, we prepare the data to provide some validity-related examples throughout the rest of the chapter. 5.2 Getting Started 5.2.1 Load Libraries Code library(&quot;petersenlab&quot;) #to install: install.packages(&quot;remotes&quot;); remotes::install_github(&quot;DevPsyLab/petersenlab&quot;) library(&quot;lavaan&quot;) library(&quot;semPlot&quot;) library(&quot;rockchalk&quot;) library(&quot;semTools&quot;) library(&quot;semPlot&quot;) library(&quot;kableExtra&quot;) library(&quot;MASS&quot;) library(&quot;psych&quot;) library(&quot;simstandard&quot;) library(&quot;MOTE&quot;) library(&quot;tidyverse&quot;) library(&quot;tinytex&quot;) library(&quot;knitr&quot;) library(&quot;rmarkdown&quot;) library(&quot;bookdown&quot;) library(&quot;here&quot;) library(&quot;DT&quot;) 5.2.2 Prepare Data 5.2.2.1 Simulate Data For reproducibility, we set the seed below. Using the same seed will yield the same answer every time. There is nothing special about this particular seed. Code sampleSize &lt;- 1000 set.seed(52242) means &lt;- c(50, 100) standardDeviations &lt;- c(10, 15) correlationMatrix &lt;- matrix(.7, nrow = 2, ncol = 2) diag(correlationMatrix) &lt;- 1 rownames(correlationMatrix) &lt;- colnames(correlationMatrix) &lt;- c(&quot;predictor&quot;,&quot;criterion&quot;) covarianceMatrix &lt;- psych::cor2cov( correlationMatrix, sigma = standardDeviations) mydataValidity &lt;- as.data.frame(mvrnorm( n = sampleSize, mu = means, Sigma = covarianceMatrix, empirical = TRUE)) errorToAddToPredictor &lt;- 3.20 errorToAddToCriterion &lt;- 6.15 mydataValidity$predictorWithMeasurementErrorT1 &lt;- mydataValidity$predictor + rnorm(n = sampleSize, mean = 0, sd = errorToAddToPredictor) mydataValidity$predictorWithMeasurementErrorT2 &lt;- mydataValidity$predictor + rnorm(n = sampleSize, mean = 0, sd = errorToAddToPredictor) mydataValidity$criterionWithMeasurementErrorT1 &lt;- mydataValidity$criterion + rnorm(n = sampleSize, mean = 0, sd = errorToAddToCriterion) mydataValidity$criterionWithMeasurementErrorT2 &lt;- mydataValidity$criterion + rnorm(n = sampleSize, mean = 0, sd = errorToAddToCriterion) mydataValidity$oldpredictor &lt;- mydataValidity$criterion + rnorm(n = sampleSize, mean = 0, sd = 7.5) latentCorrelation &lt;- .8 reliabilityPredictor &lt;- .9 reliabilityCriterion &lt;- .85 mydataValidity$predictorLatentSEM &lt;- rnorm(sampleSize, 0 , 1) mydataValidity$criterionLatentSEM &lt;- latentCorrelation * mydataValidity$predictorLatentSEM + rnorm( sampleSize, 0, sqrt(1 - latentCorrelation ^ 2)) mydataValidity$predictorObservedSEM &lt;- reliabilityPredictor * mydataValidity$predictorLatentSEM + rnorm( sampleSize, 0, sqrt(1 - reliabilityPredictor ^ 2)) mydataValidity$criterionObservedSEM &lt;- reliabilityCriterion * mydataValidity$criterionLatentSEM + rnorm( sampleSize, 0, sqrt(1 - reliabilityCriterion ^ 2)) 5.2.2.2 Add Missing Data Adding missing data to dataframes helps make examples more realistic to real-life data and helps you get in the habit of programming to account for missing data. Code missingValuesPredictor &lt;- sample( 1:sampleSize, size = 50, replace = FALSE) missingValuesCriterion &lt;- sample( 1:sampleSize, size = 50, replace = FALSE) mydataValidity$predictor[ missingValuesPredictor] &lt;- NA mydataValidity$predictorWithMeasurementErrorT1[ missingValuesPredictor] &lt;- NA mydataValidity$predictorWithMeasurementErrorT2[ missingValuesPredictor] &lt;- NA mydataValidity$predictorObservedSEM[ missingValuesPredictor] &lt;- NA mydataValidity$criterion[ missingValuesCriterion] &lt;- NA mydataValidity$criterionWithMeasurementErrorT1[ missingValuesCriterion] &lt;- NA mydataValidity$criterionWithMeasurementErrorT2[ missingValuesCriterion] &lt;- NA mydataValidity$criterionObservedSEM[ missingValuesCriterion] &lt;- NA mydataValidity$oldpredictor[ missingValuesPredictor] &lt;- NA 5.3 Types of Validity Like reliability, validity is not one thing. There are many types of validity. In this book, we discuss the following types of validity: face validity content validity criterion-related validity concurrent validity predictive validity construct validity convergent validity discriminant (divergent) validity incremental validity treatment utility of assessment discriminative validity elaborative validity consequential validity representational validity factorial (structural) validity ecological validity process-focused validity diagnostic validity social validity cultural validity internal validity external validity (statistical) conclusion validity We arrange these types of validity into two broader categories: measurement validity and research design validity. 5.3.1 Measurement Validity Aspects of measurement validity involve the validity of a particular measure, or more specifically, the validity of interpretations of scores from that measure for the proposed uses. Aspects of measurement validity include: face validity content validity criterion-related validity concurrent validity predictive validity construct validity convergent validity discriminant (divergent) validity incremental validity treatment utility of assessment discriminative validity elaborative validity consequential validity representational validity factorial (structural) validity ecological validity process-focused validity diagnostic validity social validity cultural validity 5.3.1.1 Face Validity The interpretation of a measure’s scores has face validity (for a given construct and a given use) if a typical person—a nonexpert—who looks at the content of each item will believe that the item belongs in the scale for this construct and for this use. The measure, and each item, looks “on its face” like it assesses the target construct. There are several advantages of a measure having face validity. First, outside groups will be less likely to be critical of the measure because it is intuitive. Second, use of a face valid measure is rarely objected to on ethical and bias charges for selling the measure to the public or clinicians. Third, face validity can be helpful for dissemination because more people may be receptive to it. However, face validity also has important disadvantages. First, judgments of face validity are not based on theory. Second, face validity is based on subjective judgment, which can be inaccurate. Third, these subjective judgments are made by laypeople whose judgments may be inaccurate because of biases and lack of awareness of scientific knowledge. Fourth, a face valid measure may be too simple because anybody can understand the questions and what the questions are intended to assess, so (presumably) respondents can easily fake responses to achieve their goals. Faking of responses may be more of a concern in situations when there is an incentive for the respondent to achieve a particular outcome (e.g., be deemed competent to stand trial, be judged competent to obtain a job or custody of child, be judged to have a disorder to receive accommodations or disability benefits). It is disputed whether having face validity is good or bad. Whether face validity is important to a given measure depends on the construct that is intended to be assessed, the context in which the assessment will occur, who will be paying for and/or administering the assessment, whether the respondents have incentives to achieve particular scores, and the goals of the assessment (i.e., how the assessment will be used). There is also controversy about whether face validity is a true form of validity; many researchers have argued that it is not a true psychometric form of validity, because the appearance of validity is not validity (Royal, 2016). 5.3.1.2 Content Validity Content validity involves a judgment about whether or not the content (items) of the measure theoretically matches the construct that is intended to be assessed—that is, whether the operationalization accurately reflects the construct. Content validity is developed based on items generated and selected by experts of the construct and based on the subjective determination that the measure adequately assesses and covers the construct of interest. Content validity differs from face validity in that, for face validity, a layperson determines whether or not the measure seems to assess the construct of interest. By contrast, for content validity, an expert determines whether or not the measure adheres to the construct of interest. For a measure to have content validity, its items should span the breadth of the construct. For instance, the construct of depression has many facets, such as sleep disturbances, weight/appetite changes, low mood, suicidality, etc., as depicted in Figure 5.1. Figure 5.1: Content Facets of the Construct of Depression. For a measure to have content validity, there should be no gaps—facets of the construct that are not assessed by the measure—and there should be no intrusions—facets of different constructs that are assessed by the measure. Consider the construct of depression. If theory states the construct includes various facets such as sadness, loss of interest in activities, sleep disturbances, lack of energy, weight/appetite change, and suicidal thoughts, then a content-valid measure should assess all of these facets. If the measure does not assess sleep disturbances (a gap), the measure would lack content validity. If the measure assessed facets of other constructs, such as impulsivity (an intrusion), the measure would lack content validity. With content validity, it is important to consider the population of interest. The same construct may look different in different populations and may require different content to assess it. For instance, it is important to consider the cultural relativity of constructs. The content of a construct may depend on the culture, such as in the case of culture-bound syndromes. Culture-bound syndromes are syndromes that are limited to particular cultures. An example of a culture-bound syndrome among Korean women is hwa-byung, which is the feeling of an uncomfortable abdominal mass in response to emotional distress. Another important dimension to consider is development. Constructs can manifest differently at different points in development, known as heterotypic continuity, which is discussed in Section 23.9 of Chapter 23 on repeated assessments across time. When considering the different dimensions of your population, it can be helpful to remember the acronym ADDRESSING, which is described in Section 25.2.1 of Chapter 25 on cultural and individual diversity. However, like face validity, content validity is based on subjective judgment, which can be inaccurate. 5.3.1.3 Criterion-Related Validity Criterion-related validity examines whether a measure behaves the way it should given your theory of the construct. This is quantified by the correlation between a measure’s scores and some (hopefully universally accepted) criterion we select. For instance, a criterion could be a diagnosis, a child’s achievement in school, an employee’s performance in a job, etc. Below, we provide an example of criterion-related validity by examining the Pearson correlation between a predictor and a criterion. Code cor.test(x = mydataValidity$predictor, y = mydataValidity$criterion) Pearson&#39;s product-moment correlation data: mydataValidity$predictor and mydataValidity$criterion t = 30.07, df = 900, p-value &lt; 2.2e-16 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.6737883 0.7390515 sample estimates: cor 0.7079278 In this case, the estimate of criterion-related validity is \\(r = .71\\). There are two types of criterion-related validity: concurrent validity and predictive validity. 5.3.1.3.1 Concurrent Validity Concurrent validity considers the concurrent association between the chosen measure and the criterion. That is, both the measure and the criterion are assessed at the same point in time. An example of concurrent validity would be examining self-report of court involvement in relation to current court records. 5.3.1.3.2 Predictive Validity Predictive validity considers the association between the chosen measure and the criterion at a later time point. An example of predictive validity would be examining the predictive association between children’s scores on an academic achievement test in first grade and their eventual academic outcomes five years later. 5.3.1.3.3 Empiricism and Theory Criterion-related validity arose out of a movement known as radical operationalism. Radical operationalism was a pushback against psychoanalysis. Psychoanalysis focused on grand theoretical accounts for how constructs relate. The goal of radical operationalism was to clarify concepts from a behavioristic perspective to allow predicting and changing behavior more successfully. An “operation” in radical operationalism refers to a fully described measurement. Proponents of radical operationalism argued that all constructs in psychology that could not be operationally defined should be excluded from the field as “nonscientific.” They asserted that operations should be well-defined enough to be able to replicate the findings. So, constructs had to be defined precisely according to this perspective, but how precisely? You could go on forever trying to more precisely describe a behavior in terms of its form, frequency, duration, intensity, situation, antecedents, consequences, biological substrates, etc. So, radical operationalists asserted that we should use theory of the construct to determine what is essential and what is not. Radical operationalism was also related to radical behavioralism, which was espoused by B.F. Skinner. Skinner famously used a box (the “Skinner Box”) to more directly control, describe, and assess behaviors. Skinner noted the major role that the environment played in influencing behavior. Skinner proposed a theory of implicit learning about a behavior or stimulus based on its consequences, known as operant conditioning. According to operant conditioning, something that increases the frequency of a given behavior is called a reinforcer (e.g., praise), and something that decreases the frequency of a behavior is called a punisher (e.g., loss of a privilege). Through this work, Skinner came to view everything an organism does (e.g., action, thought, feeling) as a behavior. Related to these historical perspectives was a perspective known as dustbowl empiricism. Dustbowl empiricism focused on the empirical connections between things—how things were associated using data. It was a completely atheoretical perspective in which interpretation was entirely data driven. An example of dustbowl empiricism is the approach that was used to develop the first version of the Minnesota Multiphasic Personality Inventory (MMPI). The MMPI was developed using an approach known as empirical-criterion keying, where items were selected for the scale for no reason other than the items demonstrate an association with the criterion. That is, an item was selected if it showed a strong ability to discriminate (differentiate) between clinical and control groups. Using this method with hundreds of items (and thousands of inter-item correlations), the MMPI developed 10 clinical scales, which involved operational rules based on previously collected empirical evidence. But what do you know with this abundance of correlations? You can use data reduction methods to reduce the many variables, based on their inter-correlations, down to a more parsimonious set of factors. But how do you name each factor, which is composed of many items? The developers originally numbered the MMPI clinical scales from 1 to 10. But numbered scales are not useful for other people, so the factors were eventually given labels (e.g., Paranoia). And if a client received an elevated score on a factor, many people would label the clients as _____ [the name of the factor], such as “paranoid.” The MMPI is discussed in further detail in Chapter 18 on objective personality testing. The idea of dustbowl empiricism was to develop a strong empirical base that would provide a strong foundation to help build up to a broad understanding that was integrated, coherent, and systematic. However, this process was unclear when there was only a table of correlations. Radical operationalists were opposed to content validity because it allows intrusion of our flawed thinking. According to operationalists, there are no experts. According to this perspective, the content does not matter; we just need enough data to bootstrap ourselves to a better understanding of the constructs. Although the atheoretical approach can perform reasonably well, it can be improved by making better use of theory. An empirical result (e.g., a correlation) might not necessarily have a lot of meaning associated with it. As the maxim goes, correlation does not imply causation. 5.3.1.3.3.1 Correlation Does Not Imply Causation Just because \\(X\\) is associated with \\(Y\\) does not mean that \\(X\\) causes \\(Y\\). Consider that you find an association between variables \\(X\\) and \\(Y\\), consistent with your hypothesis, as depicted in Figure 5.2. Figure 5.2: Hypothesized Causal Effect Based on an Observed Association Between \\(X\\) and \\(Y\\), Such That \\(X\\) Causes \\(Y\\). There are three primary reasons that an observed association between \\(X\\) and \\(Y\\) does not necessarily mean that \\(X\\) causes \\(Y\\). First, the association could reflect the opposite direction of effect, where \\(Y\\) actually causes \\(X\\), as depicted in Figure 5.3. Figure 5.3: Reverse (Opposite) Direction of Effect From the Hypothesized Effect, Where \\(Y\\) Causes \\(X\\). Second, the association could reflect the influence of a third variable. If a third variable is a common cause of each and accounts for their association, it is a confound. An observed association between \\(X\\) and \\(Y\\) could reflect a confound—i.e., a cause (\\(Z\\)) that influences both \\(X\\) and \\(Y\\), which explains why \\(X\\) and \\(Y\\) are correlated even though they are not causally related. A third variable confound that is a common cause of both \\(X\\) and \\(Y\\) is depicted in Figure 5.4. Figure 5.4: Confounded Association Between \\(X\\) and \\(Y\\) due to a Common Cause, \\(Z\\). Third, the association might be spurious. It might just reflect random variation (i.e., chance), and that when tested on an independent sample, what appeared as an association may not hold when testing whether the association generalizes. However, even if the association between \\(X\\) and \\(Y\\) reflects a causal effect and that \\(X\\) causes \\(Y\\), it does not necessarily mean that the effect is clinically actionable or useful. An association may reflect a static or unmodifiable predictor that is not practically useful as a treatment target. 5.3.1.3.3.2 Understanding the Causal System As Silver (2012) notes, “The numbers have no way of speaking for themselves. We speak for them. We imbue them with meaning.” (p. 9). If we understand the variables in the system and how they influence each other, we can predict things more accurately than predicting for the sake of predicting. For instance, we have made great strides in the last decades when it comes to more accurate weather forecasts (archived at https://perma.cc/PF8P-BT3D), including extreme weather events like hurricanes. These great strides have more to do with a better causal understanding of the weather system and the ability to conduct simulations of the atmosphere than merely because of big data (Silver, 2012). By contrast, other events are still incredibly difficult to predict, including earthquakes, in large part because we do not have a strong understanding of the system (and because we do not have ways of precisely measuring those causes because they occur at a depth below which we are realistically able to drill) (Silver, 2012). 5.3.1.3.3.3 Model Over-Fitting Statistical models applied to big data (i.e., lots of variables and lots of samples) can over-fit the data, which means that the statistical model accounts for error variance (an overly specific prediction), which will not generalize to future samples. So, even though an over-fitting statistical model appears to be accurate, it is not actually that accurate—it will predict new data less accurately than how accurately it accounts for the data with which the model was built. In Figure 5.5, the blue line represents the true distribution of the data, and the red line is an over-fitting model: Figure 5.5: Over-fitting Model in Red Relative to the True Distribution of the Data in Blue. 5.3.1.3.3.4 Criterion Contamination An important issue in predictive validity is the criterion problem—finding the right criterion. It is important to avoid criterion contamination, which is artificial commonality between the measure and the criterion. The criterion is not always a well-measured clear criterion to predict (like predicting death in the medical field). And you may not have access to a predictive criterion until a long time from now. So, what researchers often do is adopt intermediate assessments, which are not actually what they are interested in, but it is related to the criterion of interest, and it is in a window of time that allows for some meaningful prediction. For instance, intermediate graduate school markers of whether a graduate student will go on to have a successful career could include their grades in graduate school, whether they completed the dissertation, their performance in comprehensive/qualifying exams, etc. However, these intermediate assessments do not always indicate whether or not a student will go on to have a successful career (i.e., they are not always correlated with the real criterion of interest). 5.3.1.3.3.5 Using Theory as a Guide So, empiricism is often not enough. It is important to use theory to guide selection of an intermediate criterion that will relate to the real criterion of interest. In psychology, even our long-term criteria are not well defined relative to other sciences. In clinical psychology, for example, we are often predicting a diagnosis, which is not that much more valid compared to our measure/predictor. At the same time, in psychology, our theories of the causal processes that influence outcomes are not yet very strong. Indeed, I have misgivings calling them theories because they do not meet the traditional scientific standard for a theory. A scientific theory is an explanation of the natural world that is testable and falsifiable, and that has withstood rigorous scientific testing and scrutiny. In psychology, our “theories” are more like conceptual frameworks. And these conceptual frameworks are often vague, do not make specific predictions of effects and noneffects, and do not hold up consistently when rigorously tested. As described by Meehl (1978): I consider it unnecessary to persuade you that most so-called “theories” in the soft areas of psychology (clinical, counseling, social, personality, community, and school psychology) are scientifically unimpressive and technologically worthless … Perhaps the easiest way to convince yourself is by scanning the literature of soft psychology over the last 30 years and noticing what happens to theories. Most of them suffer the fate that General MacArthur ascribed to old generals—They never die, they just slowly fade away. In the developed sciences, theories tend either to become widely accepted and built into the larger edifice of well-tested human knowledge or else they suffer destruction in the face of recalcitrant facts and are abandoned, perhaps regretfully as a “nice try.” But in fields like personology and social psychology, this seems not to happen. There is a period of enthusiasm about a new theory, a period of attempted application to several fact domains, a period of disillusionment as the negative data come in, a growing bafflement about inconsistent and unreplicable empirical results, multiple resort to ad hoc excuses, and then finally people just sort of lose interest in the thing and pursue other endeavors. (pp. 806–807). Even if we had strong theoretical understanding of the causal system that influences behavior, we would likely still have difficulty making accurate predictions because the field has largely relied on relatively crude instruments. According to one philosophical perspective known as LaPlace’s demon, if we were able to know the exact conditions of everything in the universe, we would be able to know how the conditions would be in the future. This is an example of scientific determinism, where if you know the initial conditions, you also know the future. Other perspectives, such as quantum mechanics and chaos theory, would say that, even if we knew the initial conditions with 100% certainty, there would still be uncertainty in our understanding of the future. But assume, for a moment, that LaPlace’s demon is true. The challenge in psychology is that we have a relatively poor understanding of the initial conditions of the universe. Thus, our predictions would necessarily be probabilistic, similar to weather forecasts. Despite having a strong understanding of how weather systems behave, we have imperfect understanding of the initial conditions (e.g., the position and movement of all molecules) (Silver, 2012). 5.3.1.3.3.6 Psychoanalysis Versus Empiricism We can consider the difference between psychoanalysts and empiricists in cultural references. If we consider a scene from The Hitchhiker’s Guide to the Galaxy where a man extrapolates a simulation of the universe from a single piece of cake, we find similarities to how psychoanalysts connect everything to everything else through grand theories. Psychoanalysts try to reconstruct the entire universe from a sparse bit of data with supposedly strong theoretical understanding (when, in reality, our theories are not so strong). Their “theories” make grand conceptual claims. Let us contrast psychoanalysts with empiricists/radical operationalism. Figure 5.6 presents a depiction of empiricism. Empiricists evaluate how an observed predictor relates to an observed outcome. The rectangles in the figure represent entities that can be observed. For instance, an empiricist might examine the extent to which a person’s blood pressure is related to the number of words they speak per minute. Figure 5.6: Conceptual Depiction of Empiricism. Contrast the empirical approach with psychoanalysis, as depicted in Figure 5.7. Figure 5.7: Conceptual Depiction of Psychoanalysis. Circles represent unobserved, latent entities. For instance a psychoanalyst may make a conceptual claim that one observed variable influences another observed variable through a complex chain of intervening processes that are unobservable. There is a classic and hilarious Monty Python video (see below) that is an absurd example akin to radical operationalism taken to the extreme. Monty Python video on penguin research Monty Python video on penguin research In the video, the researchers make lots of measurements and predictions. The researchers identify interspecies differences in intelligence where humans show better performance on the English-based intelligence test (who got an IQ score of 100) than the penguins, who got an IQ score of 2. But the penguins did not speak English and were unable to provide answers to the English-based intelligence test. So, the researchers also assessed a group of non-English speaking humans as an attempt to control for language ability. They found that the penguins’ scores were equal to the scores of the non-English speaking humans. They argued that, based on their smaller brain and equal IQ when controlling for language ability, that penguins are smarter than humans. However, the researchers clearly made mistakes about confounding variables. And inclusion of a control group of non-English speaking humans does not solve the problem of validity or bias; it just obscures the problem. In summary, radical operationalism provides rich lower-level information, but lacks the broader picture. So, it seems, that we need both theory and empiricism. Theory and empiricism can—and should—inform each other. 5.3.1.4 Construct Validity Construct validity is the extent to which a measure accurately assesses a target construct (Cronbach &amp; Meehl, 1955). Construct validity is not quantified by a single index but rather consists of a “web of evidence” (the totality of evidence), which reflect the sum of inferences from multiple aspects of validity. That is, construct validity is the extent to which a measure assesses a target construct. Construct validity deals with the association between a measure and an unobservable criterion, i.e., a latent construct. By contrast, criterion-related validity deals with an observable criterion. Construct validity encompasses all other types of measurement validity (e.g., content and criterion-related validity), in addition to scores on the measure show homogeneity—i.e., scores on the measure assess a single construct scores on the measure show theoretically expected developmental changes scores on the measure show theoretically expected group differences scores on the measure show theoretically expected intervention effects establishing the nomological network of a construct 5.3.1.4.1 Nomological Network A nomological network is the interlocking system of laws that constitute a theory. It describes how the concepts (constructs) of interest are causally linked, including their observable manifestations and the causal relations among and between them. An example of a nomological network is depicted in Figure 5.8. Figure 5.8: Example of a Nomological Network. O.M. = Observable Manifestation. With construct validity, we can judge the quality of a measure by how well or how sensibly it fits in a nomological network. Latent constructs and observed measures improve each other step by step. But there is no established way to evaluate the process. Approaches such as network analysis may be useful toward this aim. Historically, construct validity became a way for some researchers to skip out on other types of validity. People found a correlation between a measure’s scores and some group membership and argued, therefore, that the measure has construct validity because there was a theoretically expected correlation between some measure and ______ (insert whatever measure). People started finding a correlation of a measure with other measures, asserting that it provides evidence of construct validity, and saying “that’s my nomological network.” But a theoretically expected association is not enough! For example, consider a measure of how quickly someone can count backward by seven. Performance is impaired in those with schizophrenia, anxiety (due to greater distractibility), and depression (due to concentration difficulties and cognitive slowing). Therefore, it is a low-quality claim that counting backward is part of the phenomenology of these disorders because it lacks differential deficit or discriminant validity (D. T. Campbell &amp; Fiske, 1959). This is related to the “glop problem,” which asserts that every bad thing is associated with every other bad thing—there is high comorbidity. Therefore, researchers needed some way to distinguish method variance from construct variance. This led to the development of the multitrait-multimethod matrix. 5.3.1.4.2 Multitrait-Multimethod Matrix (MTMM) The multitrait-multimethod matrix (MTMM), as proposed by Campbell and Fiske (1959), is a concrete way to evaluate the validity of a measure. The MTMM allows you to split the variance of measures’ scores into variance that is due to the method (i.e., method variance or method bias) and variance that is due to the construct (trait) of interest (i.e., construct variance). To create an MTMM, you need at least two methods and at least two constructs. For example, an MTMM could include self-report and observation of depression and introversion. You would then examine the correlations across combinations of construct and method. For an example of an MTMM, see Figure 5.9. Several aspects of psychometrics can be evaluated with an MTMM, including reliability, convergent validity, and discriminant validity. The reliability diagonal of an MTMM is the correlation of a variable with itself, i.e., the test–retest reliability or monotrait-monomethod correlations. The reliability coefficients should be the highest values in the matrix because each measure should be more correlated with itself than with anything else. Figure 5.9: Multitrait-Multimethod Matrix. (Figure reprinted from Campbell and Fiske (1959), Table 1, p. 82. Campbell, D. T., &amp; Fiske, D. W. (1959). Convergent and discriminant validation by the multitrait-multimethod matrix. Psychological Bulletin, 56, 81-105. https://doi.org/10.1037/h0046016) A multitrait-multimethod matrix can be organized by method and then by construct or vice versa. An MTMM organized by method then by construct is depicted in Figure 5.10. Figure 5.10: Multitrait-Multimethod Matrix Organized by Method Then by Construct. An MTMM organized by construct then by method is depicted in Figure 5.11. Figure 5.11: Multitrait-Multimethod Matrix Organized by Construct Then by Method. 5.3.1.4.2.1 Convergent Validity Convergent validity is the extent to which a measure is associated with other measures of the same target construct. In an MTMM, convergent validity evaluates whether measures targeting the same construct, but using different methods, converge upon the same construct. These are observed in the validity diagonals, also known as the convergent correlations or the monotrait-heteromethod correlations. For strong convergent validity, we would expect the values in the validity diagonals to be significant and high-ish. 5.3.1.4.2.2 Discriminant (Divergent) Validity You can also evaluate the discriminant validity, also called divergent validity, of a measure in the context of an MTMM. Discriminant validity is the extent to which a measure is not associated (or less strongly associated) with measures of different constructs that are not theoretically expected to be related (compared to associations with measures of the same construct). In an MTMM, discriminant validity determines the extent to which a measure does not correlate with measures that share a method but assess different constructs. For strong discriminant validity, we would expect the discriminant correlations (heterotrait monomethod) to be low. According to Campbell and Fiske (1959), discriminant validity of measures can be established when three criteria are met: The convergent (monotrait-heteromethod) correlations are stronger than heterotrait-heteromethod correlations. This provides the weakest evidence for discriminant validity. That is, the convergent correlations are higher than the values in the same column or row in the same heteromethod block. This can be evaluated using the heterotrait-monotrait ratio (described below). The convergent (monotrait-heteromethod) correlations are stronger than discriminant correlations (monomethod-heterotrait). This provides stronger evidence of discriminant validity. The patterns of intercorrelations between constructs are the same, regardless of which measurement method is used. That is, the pattern of inter-trait associations is the same in all triangles. For example, if extraversion and anxiety are moderately correlated with each other but uncorrelated with achievement, we would expect this pattern of interrelations between constructs would hold, regardless of which method was used to assess the construct. You can estimate a measure’s degree of discriminant validity based on the heterotrait-monotrait ratio [HTMT; Henseler et al. (2015); Roemer et al. (2021)]. HTMT is the average of the heterotrait-heteromethod correlations (i.e., the correlations of measures from different measurement methods that assess different constructs), relative to the average of the monotrait-heteromethod correlations (i.e., the correlations of measures from different measurement methods that assess the same construct). As described here (https://www.henseler.com/htmt.html) (archived at https://perma.cc/A9DU-WZWQ) based on evidence from Voorhees et al. (2016), If the HTMT is clearly smaller than one, discriminant validity can be regarded as established. In many practical situations, a threshold of 0.85 reliably distinguishes between those pairs of latent variables that are discriminant valid and those that are not. The authors updated the HTMT to use the geometric mean of the correlations rather than the arithmetic mean of the correlations to relax the assumption of tau-equivalence (Roemer et al., 2021). They called the updated index HTMT2. HTMT2 was calculated below using the semTools package (Jorgensen et al., 2021). In this case, the HTMT values are less than .85, providing evidence of discriminant validity. Code modelCFA &lt;- &#39; visual =~ x1 + x2 + x3 textual =~ x4 + x5 + x6 speed =~ x7 + x8 + x9 &#39; htmt( modelCFA, data = HolzingerSwineford1939, missing = &quot;ml&quot;) visual textul speed visual 1.000 textual 0.384 1.000 speed 0.387 0.280 1.000 Other indexes of discriminant validity include the \\(\\text{CI}_\\text{CFA}\\text{(sys)}\\) (Rönkkö &amp; Cho, 2020), \\(\\chi^2\\text{(sys)}\\) (Rönkkö &amp; Cho, 2020), and the Fornell-Larcker Ratio [FLR; Fornell &amp; Larcker (1981)]. The FLR is based on the idea that a construct should be more strongly associated with its indicators than with other constructs, and is computed as the ratio of (a) the highest squared inter-correlation the construct has with other constructs to (b) the average variance extracted [AVE] by the construct. A criterion of FLR &lt; 1 evaluates whether the AVE of a factor is greater than the highest squared inter-correlation between the factor and other factors in the model (Fornell &amp; Larcker, 1981). Another index estimates the degree of correspondence of convergent and discriminant validity correlations with a priori hypotheses as an index of construct validity (Furr &amp; Heuckeroth, 2019). Using an MTMM, a researcher can learn a lot about the quality of a measure and build a nomological network. For instance, we can estimate the extent of method variance, or variance that is attributable to the measurement method rather than the construct of interest. Method variance is estimated by the difference between monomethod versus heteromethod blocks. A poor example of an MTMM would be having two constructs such as height and depression and two methods such as Likert and true/false. This would be a poor MTMM for two reasons: (1) there are trivial differences in the methods, which would lead to obvious convergence, and (2) the differences between the constructs are not important—they are obviously discriminant. Using such an MTMM would find strong convergent associations because the methods are maximally similar and weak discriminant associations because the constructs are maximally different. It would be better to use maximally different measurement methods (e.g., self-report and performance-based measures) and to use constructs that are important to distinguish (e.g., depression and anxiey). The paper by Campbell and Fiske (1959) that introduced the MTMM is one of the most widely cited papers in psychology of all time. Psychological Bulletin published a more recent paper by Fiske and Campbell (1992), entitled “Citations Do Not Solve Problems.” They noted how their original paper was the most widely cited paper in the history of Psychological Bulletin, but they argued that nothing came of their paper. The MTMM matrices published today show little-to-no improvement compared to the ones they published in 1959. In part, this may be because we need better measures (i.e., a higher ratio of construct to method variance). 5.3.1.4.2.3 Multitrait-Multimethod Correlation Matrix This example is courtesy of W. Joel Schneider. First, we simulate data for an MTMM model using a fixed model with population parameters using the simstandard package (Schneider, 2021): Code model_fixed &lt;- &#39; Verbal =~ .5*VO1 + .6*VO2 + .7*VO3 + .7*VW1 + .6*VW2 + .5*VW3 + .6*VM1 + .7*VM2 + .5*VM3 Spatial =~ .7*SO1 + .7*SO2 + .6*SO3 + .6*SW1 + .7*SW2 + .5*SW3 + .7*SM1 + .5*SM2 + .7*SM3 Quant =~ .5*QO1 + .7*QO2 + .5*QO3 + .5*QW1 + .6*QW2 + .7*QW3 + .5*QM1 + .6*QM2 + .7*QM3 Oral =~ .4*VO1 + .5*VO2 + .3*VO3 + .3*SO1 + .3*SO2 + .5*SO3 + .6*QO1 + .3*QO2 + .4*QO3 Written =~ .6*VW1 + .4*VW2 + .3*VW3 + .6*SW1 + .5*SW2 + .5*SW3 + .4*QW1 + .4*QW2 + .5*QW3 Manipulative =~ .5*VM1 + .5*VM2 + .3*VM3 + .5*SM1 + .5*SM2 + .6*SM3 + .4*QM1 + .3*QM2 + .3*QM3 Verbal ~~ .7*Spatial + .6*Quant Spatial ~~ .5*Quant &#39; MTMM_data &lt;- sim_standardized( model_fixed, n = 10000, observed = TRUE, latent = FALSE, errors = FALSE) A multitrait-multimethod matrix correlation matrix is below. Code round(cor(MTMM_data), 2) 5.3.1.4.2.4 Construct Validity Beyond Campbell and Fiske There are a number of other approaches that can be helpful for establishing construct validity in ways that go beyond the approaches proposed by Campbell and Fiske (1959). One way is known as triangulation. Triangulation is conceptually depicted in Figure 5.12. Triangulation involves testing a hypothesis with multiple measures and/or methods to see if the findings are consistent—that is, whether the findings triangulate. Figure 5.12: Using Triangulation to Arrive at a Closer Estimate of the Construct Using Multiple Measures and/or Methods. A contemporary approach to multitrait-multimethod modeling uses confirmatory factor analysis, as described below. 5.3.1.4.2.5 MTMM in Confirmatory Factor Analysis (CFA) Using modern modeling approaches, there are even more advanced ways of examining an MTMM. For instance, you can use structural equation modeling (SEM) or confirmatory factor analysis (CFA) to derive a latent variable of a construct from multiple methods to be free from method-related error variance to generate purer assessments of the construct to see how it relates to other constructs. For an example of an MTMM in confirmatory factor analysis, see Figure 5.13 and Section 14.4.2.13 in Chapter 14 on factor analysis. Figure 5.13: Multitrait-Multimethod Model in Confirmatory Factor Analysis With Three Constructs (Internalizing, Externalizing, and Thought-Disordered Problems) and Three Methods (Mother-, Father-, and Teacher-report). The confirmatory factor analysis (CFA) model was fit in the lavaan package (Rosseel et al., 2022). Code modelMTMM &lt;- &#39; g =~ Verbal + Spatial + Quant Verbal =~ VO1 + VO2 + VO3 + VW1 + VW2 + VW3 + VM1 + VM2 + VM3 Spatial =~ SO1 + SO2 + SO3 + SW1 + SW2 + SW3 + SM1 + SM2 + SM3 Quant =~ QO1 + QO2 + QO3 + QW1 + QW2 + QW3 + QM1 + QM2 + QM3 Oral =~ VO1 + VO2 + VO3 + SO1 + SO2 + SO3 + QO1 + QO2 + QO3 Written =~ VW1 + VW2 + VW3 + SW1 + SW2 + SW3 + QW1 + QW2 + QW3 Manipulative =~ VM1 + VM2 + VM3 + SM1 + SM2 + SM3 + QM1 + QM2 + QM3 &#39; MTMM.fit &lt;- cfa( modelMTMM, data = MTMM_data, orthogonal = TRUE, missing = &quot;ml&quot;) summary( MTMM.fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 72 iterations Estimator ML Optimization method NLMINB Number of model parameters 111 Number of observations 10000 Number of missing patterns 1 Model Test User Model: Test statistic 254.449 Degrees of freedom 294 P-value (Chi-square) 0.954 Model Test Baseline Model: Test statistic 146391.496 Degrees of freedom 351 P-value 0.000 User Model versus Baseline Model: Comparative Fit Index (CFI) 1.000 Tucker-Lewis Index (TLI) 1.000 Robust Comparative Fit Index (CFI) 1.000 Robust Tucker-Lewis Index (TLI) 1.000 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -310030.970 Loglikelihood unrestricted model (H1) -309903.745 Akaike (AIC) 620283.939 Bayesian (BIC) 621084.287 Sample-size adjusted Bayesian (SABIC) 620731.545 Root Mean Square Error of Approximation: RMSEA 0.000 90 Percent confidence interval - lower 0.000 90 Percent confidence interval - upper 0.000 P-value H_0: RMSEA &lt;= 0.050 1.000 P-value H_0: RMSEA &gt;= 0.080 0.000 Robust RMSEA 0.000 90 Percent confidence interval - lower 0.000 90 Percent confidence interval - upper 0.000 P-value H_0: Robust RMSEA &lt;= 0.050 1.000 P-value H_0: Robust RMSEA &gt;= 0.080 0.000 Standardized Root Mean Square Residual: SRMR 0.006 Parameter Estimates: Standard errors Standard Information Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all g =~ Verbal 1.000 0.919 0.919 Spatial 1.159 0.032 36.586 0.000 0.767 0.767 Quant 0.711 0.022 31.970 0.000 0.651 0.651 Verbal =~ VO1 1.000 0.499 0.498 VO2 1.217 0.026 47.525 0.000 0.607 0.605 VO3 1.424 0.029 48.471 0.000 0.710 0.710 VW1 1.434 0.030 48.380 0.000 0.715 0.709 VW2 1.230 0.029 42.960 0.000 0.613 0.607 VW3 1.073 0.028 38.834 0.000 0.535 0.529 VM1 1.231 0.028 43.741 0.000 0.614 0.610 VM2 1.408 0.030 47.141 0.000 0.702 0.703 VM3 0.991 0.027 37.122 0.000 0.494 0.496 Spatial =~ SO1 1.000 0.693 0.699 SO2 1.011 0.015 65.536 0.000 0.700 0.700 SO3 0.861 0.014 61.111 0.000 0.597 0.596 SW1 0.863 0.015 58.554 0.000 0.598 0.597 SW2 1.011 0.015 65.716 0.000 0.700 0.701 SW3 0.725 0.015 47.749 0.000 0.502 0.505 SM1 1.024 0.016 64.837 0.000 0.709 0.713 SM2 0.712 0.016 45.721 0.000 0.493 0.495 SM3 1.024 0.015 66.814 0.000 0.709 0.712 Quant =~ QO1 1.000 0.501 0.504 QO2 1.376 0.026 52.340 0.000 0.689 0.694 QO3 1.015 0.023 44.104 0.000 0.508 0.504 QW1 1.017 0.025 40.585 0.000 0.509 0.506 QW2 1.176 0.026 44.977 0.000 0.588 0.595 QW3 1.388 0.027 50.622 0.000 0.695 0.696 QM1 1.003 0.025 40.236 0.000 0.502 0.504 QM2 1.185 0.027 43.897 0.000 0.593 0.590 QM3 1.408 0.029 48.103 0.000 0.705 0.702 Oral =~ VO1 1.000 0.405 0.404 VO2 1.219 0.037 32.726 0.000 0.494 0.492 VO3 0.732 0.028 26.016 0.000 0.297 0.297 SO1 0.722 0.029 24.751 0.000 0.293 0.295 SO2 0.732 0.030 24.777 0.000 0.297 0.296 SO3 1.264 0.039 32.392 0.000 0.512 0.512 QO1 1.440 0.045 32.186 0.000 0.583 0.587 QO2 0.765 0.030 25.496 0.000 0.310 0.312 QO3 0.993 0.036 27.328 0.000 0.402 0.399 Written =~ VW1 1.000 0.596 0.591 VW2 0.674 0.015 43.950 0.000 0.402 0.397 VW3 0.484 0.017 28.450 0.000 0.289 0.286 SW1 1.005 0.016 63.516 0.000 0.599 0.598 SW2 0.843 0.014 59.466 0.000 0.503 0.503 SW3 0.820 0.017 48.050 0.000 0.489 0.491 QW1 0.670 0.018 38.290 0.000 0.400 0.397 QW2 0.661 0.016 40.755 0.000 0.394 0.398 QW3 0.844 0.015 57.187 0.000 0.503 0.504 Manipulative =~ VM1 1.000 0.495 0.493 VM2 0.998 0.021 47.750 0.000 0.495 0.495 VM3 0.588 0.023 26.003 0.000 0.291 0.293 SM1 0.959 0.022 43.879 0.000 0.475 0.478 SM2 1.004 0.026 39.327 0.000 0.498 0.499 SM3 1.176 0.024 49.284 0.000 0.583 0.585 QM1 0.811 0.024 33.472 0.000 0.402 0.403 QM2 0.627 0.022 27.982 0.000 0.311 0.309 QM3 0.606 0.021 29.284 0.000 0.300 0.299 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all g ~~ Oral 0.000 0.000 0.000 Written 0.000 0.000 0.000 Manipulative 0.000 0.000 0.000 Oral ~~ Written 0.000 0.000 0.000 Manipulative 0.000 0.000 0.000 Written ~~ Manipulative 0.000 0.000 0.000 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .VO1 -0.007 0.010 -0.717 0.474 -0.007 -0.007 .VO2 -0.013 0.010 -1.253 0.210 -0.013 -0.013 .VO3 -0.008 0.010 -0.814 0.415 -0.008 -0.008 .VW1 0.015 0.010 1.463 0.143 0.015 0.015 .VW2 0.011 0.010 1.043 0.297 0.011 0.010 .VW3 -0.006 0.010 -0.572 0.567 -0.006 -0.006 .VM1 -0.011 0.010 -1.089 0.276 -0.011 -0.011 .VM2 -0.010 0.010 -0.994 0.320 -0.010 -0.010 .VM3 -0.012 0.010 -1.196 0.232 -0.012 -0.012 .SO1 -0.006 0.010 -0.587 0.557 -0.006 -0.006 .SO2 -0.014 0.010 -1.402 0.161 -0.014 -0.014 .SO3 -0.008 0.010 -0.825 0.409 -0.008 -0.008 .SW1 0.004 0.010 0.406 0.685 0.004 0.004 .SW2 0.007 0.010 0.674 0.500 0.007 0.007 .SW3 0.005 0.010 0.473 0.637 0.005 0.005 .SM1 -0.009 0.010 -0.914 0.361 -0.009 -0.009 .SM2 -0.008 0.010 -0.755 0.450 -0.008 -0.008 .SM3 -0.012 0.010 -1.187 0.235 -0.012 -0.012 .QO1 0.005 0.010 0.477 0.633 0.005 0.005 .QO2 0.005 0.010 0.457 0.648 0.005 0.005 .QO3 -0.001 0.010 -0.143 0.886 -0.001 -0.001 .QW1 0.010 0.010 0.950 0.342 0.010 0.009 .QW2 0.013 0.010 1.268 0.205 0.013 0.013 .QW3 0.025 0.010 2.554 0.011 0.025 0.026 .QM1 -0.000 0.010 -0.046 0.963 -0.000 -0.000 .QM2 0.009 0.010 0.876 0.381 0.009 0.009 .QM3 0.004 0.010 0.380 0.704 0.004 0.004 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .VO1 0.591 0.009 63.508 0.000 0.591 0.589 .VO2 0.395 0.007 54.567 0.000 0.395 0.392 .VO3 0.407 0.007 57.442 0.000 0.407 0.408 .VW1 0.151 0.004 36.640 0.000 0.151 0.148 .VW2 0.485 0.008 64.170 0.000 0.485 0.474 .VW3 0.652 0.010 67.068 0.000 0.652 0.638 .VM1 0.389 0.007 58.677 0.000 0.389 0.385 .VM2 0.260 0.005 50.781 0.000 0.260 0.261 .VM3 0.662 0.010 67.306 0.000 0.662 0.668 .SO1 0.417 0.007 58.772 0.000 0.417 0.424 .SO2 0.423 0.007 58.685 0.000 0.423 0.423 .SO3 0.383 0.007 53.146 0.000 0.383 0.383 .SW1 0.287 0.005 52.740 0.000 0.287 0.286 .SW2 0.255 0.005 53.088 0.000 0.255 0.255 .SW3 0.499 0.008 63.923 0.000 0.499 0.504 .SM1 0.261 0.005 55.403 0.000 0.261 0.264 .SM2 0.503 0.008 63.437 0.000 0.503 0.506 .SM3 0.150 0.004 37.860 0.000 0.150 0.151 .QO1 0.396 0.008 46.709 0.000 0.396 0.402 .QO2 0.415 0.007 57.018 0.000 0.415 0.421 .QO3 0.595 0.009 62.959 0.000 0.595 0.586 .QW1 0.592 0.009 65.191 0.000 0.592 0.586 .QW2 0.475 0.008 62.430 0.000 0.475 0.487 .QW3 0.260 0.005 48.515 0.000 0.260 0.261 .QM1 0.581 0.009 63.593 0.000 0.581 0.584 .QM2 0.561 0.009 63.074 0.000 0.561 0.556 .QM3 0.420 0.007 56.238 0.000 0.420 0.417 g 0.210 0.009 22.548 0.000 1.000 1.000 .Verbal 0.039 0.004 10.917 0.000 0.155 0.155 .Spatial 0.198 0.007 28.581 0.000 0.412 0.412 .Quant 0.144 0.006 25.995 0.000 0.576 0.576 Oral 0.164 0.009 18.420 0.000 1.000 1.000 Written 0.355 0.009 38.114 0.000 1.000 1.000 Manipulative 0.245 0.009 26.149 0.000 1.000 1.000 R-Square: Estimate VO1 0.411 VO2 0.608 VO3 0.592 VW1 0.852 VW2 0.526 VW3 0.362 VM1 0.615 VM2 0.739 VM3 0.332 SO1 0.576 SO2 0.577 SO3 0.617 SW1 0.714 SW2 0.745 SW3 0.496 SM1 0.736 SM2 0.494 SM3 0.849 QO1 0.598 QO2 0.579 QO3 0.414 QW1 0.414 QW2 0.513 QW3 0.739 QM1 0.416 QM2 0.444 QM3 0.583 Verbal 0.845 Spatial 0.588 Quant 0.424 A path diagram of the model is depicted in Figure 5.14 using the semPlot package (Epskamp, 2022). Code semPaths( MTMM.fit, what = &quot;std&quot;, layout = &quot;tree3&quot;, bifactor = c(&quot;Verbal&quot;,&quot;Spatial&quot;,&quot;Quant&quot;,&quot;g&quot;)) Figure 5.14: Multitrait-Multimethod Model in Confirmatory Factor Analysis. 5.3.1.5 Incremental Validity Accuracy is not enough for a measure to be useful. Proposed by Sechrest (1963), measures should also be judged by the extent to which they provide an increment in predictive efficiency over the information otherwise easily and cheaply available. Incremental validity deals with the incremental value or utility over a measure beyond other sources of information. It must be demonstrated that the addition of a measure will produce better predictions than are made on the basis of information ordinarily available. It is not enough to show that the measure is better than chance, and the measure should not just be capitalizing on shared method variance with the criterion or on increased reliability of the measure. That is, the measure should explain truly unique variance—variance that was not explained before. Incremental validity demonstrates added value, unless the other measure is cheaper or less time-consuming. Incremental validity is a specific kind of criterion-related validity: significantly increased \\(R^2\\) in hierarchical regression. The incremental validity of a measure can be evaluated by examining whether the measure explains significant unique variance in the criterion when accounting for other information, such as easily accessible information, traditionally available measures, or the current gold-standard measure. The extent of incremental validity of a measure can be quantified with the change in the coefficient of determination (\\(\\Delta R^2\\)) that compares (a) the model that includes the old predictor(s) to (b) the model that includes old predictors and the new predictor (measure). Code model1 &lt;- lm( criterion ~ oldpredictor, data = na.omit(mydataValidity)) model2 &lt;- lm( criterion ~ oldpredictor + predictor, data = na.omit(mydataValidity)) summary(model1) Call: lm(formula = criterion ~ oldpredictor, data = na.omit(mydataValidity)) Residuals: Min 1Q Median 3Q Max -21.9998 -4.6383 0.0737 4.4824 24.3993 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 20.42618 1.41547 14.43 &lt;2e-16 *** oldpredictor 0.79479 0.01394 57.01 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 7.015 on 900 degrees of freedom Multiple R-squared: 0.7831, Adjusted R-squared: 0.7829 F-statistic: 3250 on 1 and 900 DF, p-value: &lt; 2.2e-16 Code summary(model2) Call: lm(formula = criterion ~ oldpredictor + predictor, data = na.omit(mydataValidity)) Residuals: Min 1Q Median 3Q Max -20.7095 -4.2435 -0.1041 4.0814 23.1936 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 16.04252 1.33293 12.04 &lt;2e-16 *** oldpredictor 0.65476 0.01645 39.80 &lt;2e-16 *** predictor 0.36889 0.02745 13.44 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 6.405 on 899 degrees of freedom Multiple R-squared: 0.8194, Adjusted R-squared: 0.819 F-statistic: 2040 on 2 and 899 DF, p-value: &lt; 2.2e-16 Code model1Rsquare &lt;- summary(model1)$r.squared model2Rsquare &lt;- summary(model2)$r.squared model1RsquareAdj &lt;- summary(model1)$adj.r.squared model2RsquareAdj &lt;- summary(model2)$adj.r.squared deltaRsquare &lt;- getDeltaRsquare(model2)[[&quot;predictor&quot;]] The deltaR-square values: the change in the R-square observed when a single term is removed. Same as the square of the &#39;semi-partial correlation coefficient&#39; deltaRsquare oldpredictor 0.31824800 predictor 0.03627593 Code deltaRsquareAdj &lt;- model2RsquareAdj - model1RsquareAdj deltaRsquareAdj [1] 0.03611514 Code anova(model1, model2) Code getDeltaRsquare(model2) The deltaR-square values: the change in the R-square observed when a single term is removed. Same as the square of the &#39;semi-partial correlation coefficient&#39; deltaRsquare oldpredictor 0.31824800 predictor 0.03627593 \\(\\Delta R^2\\) was calculated using the rockchalk package (P. E. Johnson, 2022). The predictor shows significant incremental validity above the old predictor in predicting the criterion. Model 1 explains \\(78.31\\)% of the variance \\((R^2 = 0.78)\\), and Model 2 explains \\(81.94\\)% of the variance \\((R^2 = 0.82)\\). Thus, the predictor explains only \\(3.63\\)% additional variance in the criterion above the variance explained by the old predictor \\((\\Delta R^2)\\). Based on adjusted \\(R^2\\) (\\(R^2_{adj}\\)), the predictor explains only \\(3.61\\)% additional variance in the criterion above the variance explained by the old predictor (\\(\\Delta R^2_{adj}\\)). 5.3.1.6 Treatment Utility of Assessment S. C. Hayes et al. (1987) argued that it is not enough for measures to be reliable and valid. The authors raised another important consideration for the validity of a measure: its treatment utility or usefulness. They asked the question: What goal do assessments accomplish in clinical psychology? In clinical psychology, the goal of assessment is to lead to better treatment outcomes. Therefore, the treatment utility of a measure is the extent to which a measure is shown to contribute to beneficial treatment outcomes. That is, if a clinician has the information/results from having administered this measure, do the clients have better outcomes? The treatment utility of assessment is a specific kind of criterion-related validity, with the idea that “all criteria are not created equal.” And the criterion that is most important to optimize, from this perspective, when developing and selecting measures is a client’s treatment outcomes. S. C. Hayes et al. (1987) described different approaches to evaluating the extent to which a measure shows treatment utility. These are a priori group comparison approaches that examine whether a specific difference in the assessment approach relates to treatment outcome. They distinguished between (a) manipulated assessment and (b) manipulated use. Manipulated assessment and manipulated use are depicted in Figure 5.15. Figure 5.15: Research Designs That Evaluate the Treatment Utility of Assessment. In manipulated assessment, a single group of subjects is randomly divided into two subgroups, and either the collection or availability of assessment data is varied systematically. Therapists then design or implement treatment in accord with the data available. As an example, the measure of interest may be administered in one condition, and the standard measure may be administered in the other condition. Then the treatment outcomes would be compared across the two conditions. An advantage of manipulated assessment is that this type of design is more realistic than manipulated use. Making the assessment data available but not assigning a certain treatment based on the assessment outcomes better simulates a realistic clinical environment. Also, because the control group has no access to the data, it might serve as a stronger control. A disadvantage of manipulated assessment is that it depends on whether and how clinicians use the measure, which depends on how positively the measure was received by the clinicians. In manipulated use, the same assessment information is available for all subjects, but the researcher manipulates the way in which the assessment information is used. For example, one group gets a treatment matched to assessment outcomes, and the other group gets a cross-matched treatment that does not target the problem identified by the assessment. So, in one group, the assessment information is used to match the treatment to the client based on their assessment results, whereas the other group receives a standard treatment regardless of their assessment results. An advantage of this design is that you can be certain that the treatment decisions are explicitly informed by the assessment results because the researcher ensures this, whereas the decision of how the assessment information is used is up to the clinician in manipulated assessment. Relatively few measures show evidence of treatment utility of assessment. A review on the treatment utility of assessment is provided by Nelson-Gray (2003). 5.3.1.7 Discriminative Validity Discriminative validity is the degree to which a measure accurately identifies persons placed into groups on the basis of another measure. Discriminative validity is not to be confused with discriminant (divergent) validity and discriminant analysis. A measure shows discriminant validity if it does not correlate with things that it would not be theoretically expected to correlate with. A measure shows discriminative validity, by contrast, if the measure is able to accurately differentiate things (e.g., two groups such as men and women). Discriminant analysis is a model that combines predictors to differentiate between multiple groups. 5.3.1.8 Elaborative Validity Elaborative validity involves the extent to which a measure increases our theoretical understanding of the target construct or of neighboring constructs. It deals with a measure’s meaningfulness. Elaborative validity is a type of incremental theoretical validity. It is a combination of criterion-related validity and construct validity that examines how much a given measure increases our understanding of a construct’s nomological network. However, I am unaware of strong examples of measures that show strong elaborative validity in psychology. 5.3.1.9 Consequential Validity Consequential validity is a form of validity that differs from evidential validity, or a test’s potential to provide accurate, useful information based on research. Consequential validity takes a more macro-view and deals with the community, sociological, and public policy perspective. Consequential validity evaluates the consequences of our measures beyond the circumstances of their development, based on their impact. Measures can have positive, negative, or mixed effects on society. An example of consequential validity would be asking what the impacts of aptitude testing are on society—how aptitude testing affects society in the long run. Some would argue that, even in cases where the aptitude tests have some degree of evidential validity (i.e., they accurately assess to some degree what they tend to assess), they are consequentially invalid—that is, they have had a net negative effect on society and, due to their low value to society, are invalid. The tests themselves may be fine in terms of their accuracy, but consequential validity says that their validity depends on what we do with the test, i.e., how the test is used. Another example of consequential validity is when the validity of a measure changes over time due to changes in people’s or society’s response, as depicted in Figure 5.16. Judgments and predictions can change the way society reacts and the way people behave so that the predictions become either more or less accurate. A prediction that becomes more accurate as a result of people’s response to a prediction is a self-fulfilling prediction or prophecy. For instance, if school staff make a prediction that a child will be held back a grade, the teacher may not provide adequate opportunities for the child to learn, which may lead to the child being more likely to be held back. Figure 5.16: Invalidation of a Measure Due to Society’s Response to the Use of the Measure. A prediction that becomes less accurate as a result of people’s response to a prediction is a self-canceling prediction. The most effective prediction about a flu outbreak might be one that leads people to safer behavior; therefore lowering flu rates, which does not correspond to the initial prediction (Silver, 2012). Society’s response to a measure can invalidate the measure. For example, consider that an organization rates cities based on quality-of-life measures. If quality-of-life indices include the percent of solved cases by the city’s police (i.e., the clearance rate), cities may try to improve their ratings of “quality-of-life” by increasing their clearance rate either by increasing the number of cases they mark as solved (such as by marking cases falsely as resolved) or by decreasing the number of cases (such as by investigating fewer cases). That is, cities may “game” the system based on the quality-of-life indices used by various organizations. In this case, the clearance rate becomes a less accurate measure of a city’s quality of life. Another example of a measure that becomes less valid due to society’s response is the use of alumni donations as an indicator of the strength of a university that is used for generating university rankings. Such an indicator could lead schools to admit wealthier students who give the most donations, and students whose parents were alumni and provide lavish donations to the university. Yet another example could be standardized testing, where instructors may “teach to the test” such that better performance might not necessarily reflect better underlying competence. 5.3.1.10 Representational Validity Representational validity examines the extent to which the items or content of a measure “flesh out” and mirror the true nature and mechanisms of the construct. There are many different types of validity, but many of them are overlapping and can be considered types of other forms of validity. For instance, representational validity is a type of elaborative validity and content validity. 5.3.1.11 Factorial (Structural) Validity Factorial validity is examined in Section 14.1.4.2.2 on factor analysis. Factorial validity considers whether the factor structure of the measure(s) is consistent with the construct. According to factorial validity, if you claim that the measure is unidimensional with four items, factor analysis should support the unidimensionality. Thus, it involves testing empirically whether the measure has the same structure as would be expected based on theory. It is a type of construct validity. 5.3.1.12 Ecological Validity Ecological validity examines the extent to which a measure provides scores that are indicative of the behavior of a person in the natural environment. 5.3.1.13 Process-Focused Validity Process-focused validity attempts to get closer to the underlying mechanisms. Process-focused validity examines the degree to which respondents engage in a predictable set of psychological processes (which are specified a priori) when completing the measure (Bornstein, 2011; Furr, 2017). These psychological processes include effects of the instrument (e.g., observer versus third-party report versus projective test) as well as effects of the context (e.g., assessment setting, assessment instructions, affect state of the participant, etc.). To determine whether a test is valid in the process-focused technique, one experimentally manipulates variables that moderate the test score–criterion association—to better understand the underlying processes. The ideal outcome is a measure that both (1) has an adequate outcome (correlations where expected) as well as (2) adequate process validity—the psychological processes that one engages in are well hypothesized. The idea of process-focused validity is that if a measure does not inform us about process or mechanisms, it is not worth doing. Process-focused validity is a type of elaborative validity and construct validity. For instance, consider the common finding that low socioeconomic status is associated with poorer outcomes. To make an impact, process-focused validity would argue that we need to know the mechanisms that underlie this association, and it involves how a measure helps us understand process. 5.3.1.14 Diagnostic Validity Diagnostic validity is the extent to which the diagnostic category accurately captures the abnormal phenomenon of interest. It is a form of construct validity for diagnoses. 5.3.1.15 Social Validity Social validity involves the extent to which the proposed procedures (assessment, intervention, etc.) will be well-liked and acceptable by those who receive and implement them. 5.3.1.16 Cultural Validity Cultural validity refers to “the effectiveness of a measure or the accuracy of a clinical diagnosis to address the existence and importance of essential cultural factors” (Leong &amp; Kalibatseva, 2016, p. 58). Essential cultural factors may include values, beliefs, experiences, communication patterns, and approaches to knowledge (epistemologies). 5.3.2 Research Design (Experimental) Validity Aspects of research design validity, also called experimental validity, involve the validity of a research design for making various interpretations. Research design validity includes internal validity, external validity, and conclusion validity. 5.3.2.1 Internal Validity Internal validity is the extent to which causal inference is justified from the research design. This encompasses multiple features including temporal precedence—does the (purported) cause occur before the (purported) effect? covariation of cause and effect—correlation is necessary (even if insufficient) for causality no plausible alternative explanations—such as third variables that could influence both variables and explain their covariation There are number of potential threats to internal validity, which are important to consider when designing studies and interpreting findings. Examples of potential threats to internal validity include history, maturation, testing, instrumentation, regression, selection, experimental mortality, and an interaction of threats (Slack &amp; Draugalis, 2001). Research designs differ in the extent to which they show internal validity versus external validity. An experiment is a research design in which one or more variables (independent variables) are manipulated to observe how the manipulation influences the dependent variable. In an experiment, the researcher has greater control over the variables and attempts to hold everything else constant (e.g., by standardization and random assignment). In correlational designs, however, the researcher has less control over the variables. They may be able to statistically account for potential confounds using covariates or for the reverse direction of effect using longitudinal designs. Nevertheless, we can have greater confidence about whether a variable influences another variable in an experiment. Thus, experiments tend to have higher internal validity than correlational designs. 5.3.2.2 External Validity External validity is the extent to which the findings can be generalized to the broader population and the real world. External validity is crucial to consider for studies that intend to make inferences to people outside of those who were assessed. For instance, norm-referenced assessments attempt to identify the distribution of scores for a given population from a sample within that population. The validity of the norms of a norm-referenced assessment are important to consider (Achenbach, 2001). The validity of norms and external validity, more broadly, can depend highly on how representative the sample is of the target population and how appropriate this population is to a given participant. Some measures are known to have poor norms, including the Exner Comprehensive System (Exner, 1974; Exner &amp; Erdberg, 2005) for administering and scoring the Rorschach Inkblot Test, which has been known to over-pathologize (Wood, Teresa, et al., 2001; Wood, Nezworski, et al., 2001). The Rorschach is discussed in greater detail in Chapter 19. 5.3.2.2.1 Tradeoffs of Internal Validity and External Validity It is important to note that there is a tradeoff between internal and external validity—a single research design cannot have both high internal and high external validity. Some research designs are better suited for making causal inferences, whereas other designs tend to be better suited for making inferences that generalize to the real world. The research design that is best suited to making causal inferences is an experiment, where the researcher manipulates one variable (the independent variable) and holds all other variables constant to see how a change in the independent variable influences the outcome (dependent variable). Thus, experiments tend to have higher internal validity than other research designs. However, by manipulating one variable and holding everything else constant, the research takes place in a very standardized fashion that can become like studying a process in a vacuum. So, even if a process is theoretically causal in a vacuum, it may act very differently in the real world when it interacts with other processes. Observational designs have greater capacity for external validity than experimental designs because people can be observed in their natural environments to see how variables are related in the real world. However, the greater external validity comes at a cost of lower internal validity. Observational designs are not well-positioned to make causal inferences because they have multiple threats to internal validity, including issues of temporal precedence in cross-sectional observational designs, and there are numerous potential third variables (i.e., confounds) that could act as a common cause of both the predictor and outcome variables. Thus, just because two variables are associated does not necessarily mean that they are causally related. As the internal validity of a study’s design increases, its external validity tends to decrease. The greater control we have over variables (and, therefore, have greater confidence about causal inferences), the lower the likelihood that the findings reflect what happens in the real world because it is studying things in a metaphorical vacuum. Because no single research design can have both high internal and external validity, scientific inquiry needs a combination of many different research designs so we can be more confident in our inferences—experimental designs for making causal inferences and observational designs for making inferences that are more likely to reflect the real world. Case studies, because they have smaller sample sizes, tend to have lower external validity than both experimental and observational studies. Case studies also tend to have lower internal validity because they have less potential to control for threats to internal validity, such as potential confounds or temporal precedence. Nevertheless, case studies can still be useful for generating hypotheses that can then be tested empirically with a larger sample in experimental or observational studies. 5.3.2.3 (Statistical) Conclusion Validity Conclusion validity, also called statistical conclusion validity, considers the extent to which conclusions are reasonable about the association among variables based on the data. That is, were the correct statistical analyses performed, and are the interpretations of the findings from those analyses correct? 5.3.3 Putting It All Together: An Organizational Framework There are many types of measurement validity, but the central psychometric aspect of measurement validity is construct validity. That is, whether the measure accurately assesses the target construct is the most important consideration of measurement validity. As discussed earlier, construct validity includes the nomological network of the construct. Construct validity also subsumes other key types of measurement validity, including Convergent validity Discriminant (divergent) validity Criterion-related validity Concurrent validity Predictive validity Content validity The organization of types of measurement validity that are subsumed by construct validity is depicted in Figure 5.17. Figure 5.17: Organization of Types of Measurement Validity That Are Subsumed by Construct Validity. Moreover, many different types of reliability and validity can be viewed through the lens of construct validity: Internal consistency, which can be estimated as the coefficient of internal consistency, where the criterion for criterion-related validity is other items on the same measure Test–retest reliability, which can be estimated as the coefficient of stability, where the criterion for criterion-related validity is the same measure at another time point Parallel-forms reliability or convergent validity, which can be estimated as the coefficient of equivalence, where the criterion for criterion-related validity is the parallel form 5.4 Validity Is a Process, Not an Outcome Validity (and validation) is a continual process, not an outcome. Validation is never complete. When evaluating the validity of a measure, we must ask: Validity for what and to what degree? We would not just say that a measure is or is not valid. We would express the strength of evidence on a measure’s validity across the different types of validity for a particular purpose, with a particular population, in a particular context (consistent with generalizability theory). 5.5 Reliability Versus Validity Reliability and validity are not the same thing. Reliability deals with consistency, whereas validity deals with accuracy. A measure can be consistent but not accurate (see Figure 5.18). That is, a measure can be reliable but not valid. However, a measure cannot be accurate if it is inconsistent; that is, a measure cannot be valid and unreliable. Figure 5.18: Traditional Depiction of Reliability (Consistency) Versus Validity (Accuracy). The typical way of depicting the distinction between reliability and validity is in Figure 5.18, in which a measure can either have (a) low reliability and low validity, (b) high reliability and low validity, or (c) high reliability and high validity. However, it can be worth thinking about validity in terms of accuracy at the person level versus group level. When we distinguish between person- versus group-level accuracy, we can distinguish four general combinations of reliability and validity, as depicted in Figure 5.19: (a) low reliability, low accuracy at the person level, and low accuracy at the group level, (b) low reliability, low accuracy at the person level, and high accuracy at the group level, (c) high reliability, low accuracy at the person level, and low accuracy at the group level, and (d) high reliability, high accuracy at the person level, and high accuracy at the group level. However, as discussed before, reliability and validity are not binary states of low versus high—they exist to varying degrees. Figure 5.19: Depiction of Reliability Versus Validity, While Distinguishing Between Validity (Accuracy) at the Person Versus Group Level. Even though reliability and validity are not the same thing, there is a relation between reliability and validity. Validity depends on reliability. Reliability is necessary but insufficient for validity. However, measurement error (unreliability) can be systematic or random. If measurement error is systematic, it reduces the validity of the measure. If measurement error is random, it reduces the precision of an individual’s score on a measure, but the measure could still be a valid measure of the construct at the group level. However, random error would make it more difficult to make an accurate prediction for an individual person. Reliability places the upper bound on validity because a measure can be no more valid than it is reliable. In other words, a measure should not correlate more highly with another variable than it correlates with itself. Therefore, the maximum validity coefficient is the square root of the product of the reliability of each measure, as in Equation (5.1): \\[ \\begin{aligned} r_{xy_{\\text{max}}} &amp;= \\sqrt{r_{xx}r_{yy}} \\\\ \\text{maximum association between } x \\text{ and } y &amp;= \\sqrt{\\text{reliability of } x \\text{ and } y} \\end{aligned} \\tag{5.1} \\] So, the maximum validity coefficient is based on the reliability of each measure. To the extent that one of the measures is unreliable, the validity coefficient will be attenuated relative to the true validity (i.e., the true strength of association of the constructs), as we describe below. 5.6 Effect of Measurement Error on Associations Figure 5.20 depicts the classical test theory approach to understanding the validity of a measure, i.e., its association with another measure, which is the validity coefficient (\\(r_{xy}\\)). Figure 5.20: The Criterion-Related Validity of a Measure, i.e., Its Association With Another Measure, as Depicted in a Path Diagram. As described above, (random) measurement error weakens (or attenuates) the association between variables (Goodwin &amp; Leech, 2006; Schmidt &amp; Hunter, 1996). The greater the random measurement error, the weaker the association. Thus, the correlation between \\(x\\) and \\(y\\) depends on both the true correlation of \\(x\\) and \\(y\\) (\\(r_{x_{t}y_{t}}\\)) and the reliabilities of \\(x\\) (\\(r_{xx}\\)) and \\(y\\) (\\(r_{yy}\\)). So, measurement error in \\(x\\) and \\(y\\) can reduce the observed correlation below the true correlation. This is known as the attenuation formula (Equation (5.2)): \\[ \\small \\begin{aligned} r_{xy} &amp;= r_{x_{t}y_{t}} \\sqrt{r_{xx}r_{yy}} \\\\ \\text{observed association between } x \\text{ and } y &amp;= (\\text{true association of constructs}) \\times \\sqrt{\\text{reliability of } x \\text{ and } y} \\end{aligned} \\tag{5.2} \\] The lower the reliability, the greater the attenuation of the validity coefficient relative to the true association between the constructs. All of these \\(r\\) values (excluding the true correlation) are just estimates unless the sample size is infinite, so the observed association is an imperfect estimate. Hence, we need a correction for this attenuation (Schmidt &amp; Hunter, 1996). This correction for the attenuation of an association due to measurement error (unreliability) is known as the disattenuation of a correlation, i.e., correction for the attenuation of an association due to measurement error to get a more accurate estimate of the true association between constructs. Rearranging the terms from the attenuation formula, the formula for disattenuation of a correlation (i.e., the disattenuation formula) is in Equation (5.3): \\[ \\begin{aligned} r_{x_{t}y_{t}} &amp;= \\frac{r_{xy}}{\\sqrt{r_{xx}r_{yy}}} \\\\ \\text{true association of constructs} &amp;= \\frac{\\text{observed association between } x \\text{ and } y}{\\sqrt{\\text{reliability of } x \\text{ and } y}} \\end{aligned} \\tag{5.3} \\] All of this is implied in the path diagram (see Figure 5.20). The attenuation and disattenuation formulas are based on classical test theory, and therefore assume that all measurement error is random, that errors are uncorrelated, etc. Nevertheless, the attenuation formula can be informative for understanding how imperiled your research is when your measures have low reliability (i.e., when there is instability in the measure). Researchers recommend accounting for measurement reliability, to better estimate the association between constructs, either with the disattenuation formula (Schmidt &amp; Hunter, 1996) or with structural equation modeling, as described in the next chapter. 5.6.1 Test with Simulated Data 5.6.1.1 Reliability of Predictor Code cor.test( x = mydataValidity$predictorWithMeasurementErrorT1, y = mydataValidity$predictorWithMeasurementErrorT2) Pearson&#39;s product-moment correlation data: mydataValidity$predictorWithMeasurementErrorT1 and mydataValidity$predictorWithMeasurementErrorT2 t = 63.6, df = 948, p-value &lt; 2.2e-16 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.8872651 0.9114970 sample estimates: cor 0.9000747 5.6.1.2 Reliability of Criterion Code cor.test( x = mydataValidity$criterionWithMeasurementErrorT1, y = mydataValidity$criterionWithMeasurementErrorT2) Pearson&#39;s product-moment correlation data: mydataValidity$criterionWithMeasurementErrorT1 and mydataValidity$criterionWithMeasurementErrorT2 t = 49.587, df = 948, p-value &lt; 2.2e-16 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.8308430 0.8663438 sample estimates: cor 0.8495525 5.6.1.3 True Association Code cor.test( x = mydataValidity$predictor, y = mydataValidity$criterion) Pearson&#39;s product-moment correlation data: mydataValidity$predictor and mydataValidity$criterion t = 30.07, df = 900, p-value &lt; 2.2e-16 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.6737883 0.7390515 sample estimates: cor 0.7079278 5.6.1.4 Observed Association (After Adding Measurement Error) Code cor.test( x = mydataValidity$predictorWithMeasurementErrorT1, y = mydataValidity$criterionWithMeasurementErrorT1) Pearson&#39;s product-moment correlation data: mydataValidity$predictorWithMeasurementErrorT1 and mydataValidity$criterionWithMeasurementErrorT1 t = 23.73, df = 900, p-value &lt; 2.2e-16 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.5785273 0.6589657 sample estimates: cor 0.6203752 Using simulated data, when the reliability of the predictor is .90, the reliability of the criterion is .85, and the true association between the predictor and criterion is \\(r = .71\\), the observed association is attenuated to \\(r = .62\\). 5.6.2 Attenuation of True Correlation Due to Measurement Error The attenuation formula is presented in Equation (5.2). We extend it to a specific example in Equation (5.4): \\[ \\small \\begin{aligned} r_{xy} &amp;= r_{x_ty_t} \\sqrt{r_{xx} r_{yy}} \\\\ \\text{observed correlation between }x \\text{ and } y &amp;= \\text{(true association between construct } A \\text{ and construct } B) \\times \\\\ &amp; \\;\\;\\; \\sqrt{\\text{reliability of } x \\times \\text{reliability of } y} \\end{aligned} \\tag{5.4} \\] where \\(x = \\text{measure of construct} \\ A\\); \\(y = \\text{measure of construct} \\ B\\). Below is an example of how to find the observed correlation between the predictor and criterion if the true association between the constructs (i.e., correlation between true scores of constructs) is .70, the reliability of the predictor is .90, and the reliability of the criterion is .85: Code trueAssociation &lt;- .7 reliabilityOfPredictor &lt;- 0.9 reliabilityOfCriterion &lt;- 0.85 trueAssociation * sqrt(reliabilityOfPredictor * reliabilityOfCriterion) [1] 0.6122499 The petersenlab package (Petersen, 2024b) contains the attenuationCorrelation() function that estimates the observed association given the true association and the reliability of the predictor and criterion: Code attenuationCorrelation( trueAssociation = 0.7, reliabilityOfPredictor = 0.9, reliabilityOfCriterion = 0.85) [1] 0.6122499 The observed association (\\(r = .61\\)) is attenuated relative to the true association (\\(r = .70\\)). 5.6.3 Disattenuation of Observed Correlation Due to Measurement Error The disattenuation formula is presented in Equation (5.3). We extend it to a specific example in Equation (5.5): \\[ \\small \\begin{aligned} r_{x_ty_t} &amp;= \\frac{r_{xy}}{\\sqrt{r_{xx} r_{yy}}} \\\\ \\text{true association between construct } A \\text{ and construct } B &amp;= \\frac{\\text{observed correlation between } x \\text{ and } y}{\\sqrt{\\text{reliability of } x \\times \\text{reliability of } y}} \\end{aligned} \\tag{5.5} \\] where \\(x = \\text{measure of construct} \\ A\\); \\(y = \\text{measure of construct} \\ B\\) Find the true association between the construct assessed by the predictor and the construct assessed by the criterion given an observed association if the reliability of the predictor is .9, and the reliability of the criterion is .85: Code reliabilityOfPredictor &lt;- 0.9 reliabilityOfCriterion &lt;- 0.85 The observed (attenuated) association is as follows: Code observedAssociation &lt;- cor.test( x = mydataValidity$predictor, y = mydataValidity$criterion)$estimate observedAssociation cor 0.7079278 The true (disattenuated) association is as follows: Code observedAssociation / sqrt(reliabilityOfPredictor * reliabilityOfCriterion) cor 0.8093908 The petersenlab package (Petersen, 2024b) contains the disattenuationCorrelation() function that estimates the observed association given the true association and the reliability of the predictor and criterion: Code disattenuationCorrelation( observedAssociation = observedAssociation, reliabilityOfPredictor = 0.9, reliabilityOfCriterion = 0.85) cor 0.8093908 The disattenuation of an observed correlation due to measurement error can be demonstrated using structural equation modeling. For instance, consider the following observed association: Code cor.test( x = mydataValidity$predictorObservedSEM, y = mydataValidity$criterionObservedSEM)$estimate cor 0.6118049 The observed association can be estimated in structural equation modeling in the lavaan package (Rosseel et al., 2022) using the following syntax: Code observedSEM_syntax &lt;- &#39; criterionObservedSEM ~ predictorObservedSEM # Specify residual errors (measurement error) predictorObservedSEM ~~ predictorObservedSEM criterionObservedSEM ~~ criterionObservedSEM &#39; observedSEM_fit &lt;- sem( observedSEM_syntax, data = mydataValidity, missing = &quot;ML&quot;) summary( observedSEM_fit, standardized = TRUE) lavaan 0.6.17 ended normally after 9 iterations Estimator ML Optimization method NLMINB Number of model parameters 5 Used Total Number of observations 998 1000 Number of missing patterns 3 Model Test User Model: Test statistic 0.000 Degrees of freedom 0 Parameter Estimates: Standard errors Standard Information Observed Observed information based on Hessian Regressions: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all criterionObservedSEM ~ prdctrObsrvSEM 0.633 0.027 23.366 0.000 0.633 0.611 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .crtrnObsrvdSEM -0.012 0.027 -0.459 0.646 -0.012 -0.012 prdctrObsrvSEM 0.012 0.032 0.373 0.709 0.012 0.012 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all prdctrObsrvSEM 0.971 0.044 21.932 0.000 0.971 1.000 .crtrnObsrvdSEM 0.654 0.030 21.518 0.000 0.654 0.627 Code lavInspect(observedSEM_fit, &quot;rsquare&quot;) criterionObservedSEM 0.373 A path diagram of the observed association is depicted in Figure 5.21. Code semPaths( observedSEM_fit, what = &quot;Std.all&quot;, layout = &quot;tree2&quot;, edge.label.cex = 2) Figure 5.21: Observed Association, as Depicted in a Structural Equation Model. Now consider when we account for the degree of unreliability of each measure. We can account for the (un)reliability of each measure by specifying the residual errors as \\(1 - \\text{reliability}\\), as below: Code # Syntax that specifies the reliability programmatically disattenuationSEM_syntax &lt;- paste( &#39; # Factor loadings predictorLatent =~ 1*predictorObservedSEM criterionLatent =~ 1*criterionObservedSEM # Factor correlation criterionLatent ~ predictorLatent # Specify residual errors (measurement error) predictorObservedSEM ~~ (1 - &#39;, reliabilityOfPredictor, &#39;)*predictorObservedSEM criterionObservedSEM ~~ (1 - &#39;, reliabilityOfCriterion, &#39;)*criterionObservedSEM &#39;, sep = &quot;&quot;) # Syntax that substitutes in the reliability values disattenuationTraditionalSEM_syntax &lt;- &#39; # Factor loadings predictorLatent =~ 1*predictorObservedSEM criterionLatent =~ 1*criterionObservedSEM # Factor correlation criterionLatent ~ predictorLatent # Specify residual errors (measurement error) predictorObservedSEM ~~ (1 - .9)*predictorObservedSEM # where .9 is the reliability of the predictor criterionObservedSEM ~~ (1 - .85)*criterionObservedSEM # where .85 is the reliability of the criterion &#39; disattenuationSEM_fit &lt;- sem( disattenuationSEM_syntax, data = mydataValidity, missing = &quot;ML&quot;) summary( disattenuationSEM_fit, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 15 iterations Estimator ML Optimization method NLMINB Number of model parameters 5 Used Total Number of observations 998 1000 Number of missing patterns 3 Model Test User Model: Test statistic 0.000 Degrees of freedom 0 Parameter Estimates: Standard errors Standard Information Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all predictorLatent =~ prdctrObsrvSEM 1.000 0.933 0.947 criterionLatent =~ crtrnObsrvdSEM 1.000 0.946 0.925 Regressions: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all criterionLatent ~ predictorLatnt 0.706 0.030 23.152 0.000 0.697 0.697 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .prdctrObsrvSEM 0.012 0.032 0.373 0.709 0.012 0.012 .crtrnObsrvdSEM -0.005 0.033 -0.143 0.887 -0.005 -0.005 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .prdctrObsrvSEM 0.100 0.100 0.103 .crtrnObsrvdSEM 0.150 0.150 0.144 predictorLatnt 0.871 0.044 19.674 0.000 1.000 1.000 .criterionLatnt 0.460 0.031 14.960 0.000 0.514 0.514 R-Square: Estimate prdctrObsrvSEM 0.897 crtrnObsrvdSEM 0.856 criterionLatnt 0.486 A path diagram of the true association is depicted in Figures 5.22 and 5.23. Code semPaths( disattenuationSEM_fit, what = &quot;Std.all&quot;, layout = &quot;tree2&quot;, edge.label.cex = 2) Figure 5.22: Disattenuation of an Observed Association Due to Measurement Error, as Depicted in a Structural Equation Model. Figure 5.23: Disattenuation of an Observed Association Due to Measurement Error, as Depicted in a Structural Equation Model. The observed association (\\(\\beta = 0.61\\)) becomes \\(\\beta = 0.70\\) when it is disattenuated for measurement error. 5.7 Generalizability Theory (G-Theory) Generalizability theory (G-theory) is discussed in greater detail in the chapter on reliability in Section 4.11 and in the chapter on generalizability theory. As a brief reminder, G-theory is a measurement theory that, unlike classical test theory, does not treat all measurement differences across time, rater, or situation as “error” but rather as a phenomenon of interest. G-theory can simultaneously consider multiple aspects of reliability and validity in the same model, something that classical test theory cannot achieve. 5.8 Ways to Increase Validity Here are potential ways to increase the validity of the interpretation of a measure’s scores for a particular purpose: Make sure the measure’s scores are reliable. For potential ways to increase the reliability of measurement, see Section 4.14. But do not switch to a less valid measure or to items that are less valid merely because they are more reliable. Use multiple measures and multiple methods to remedy the effects of method bias. Design the measure with a particular population and purpose in mind. When describing the measure in papers or in public spheres, make it clear to others what the population and intended purposes are and what they are not. Make sure each item’s scores are valid, based on theory and empiricism, for the particular population and purpose. For instance, the items should show content validity—the items should assess facets of the target construct for the target population as defined by experts, without item intrusions from other constructs. The items’ scores should show convergent validity—the items’ scores should be related to other measures of the construct, within the population of interest. The items’ scores should show discriminant validity—the items’ scores should be more strongly related to measures that are intended to assess the same construct than they are to measures that are intended to assess other constructs. Obtain samples that are as representative of the population as possible, paying attention to including people who are traditionally under-represented in research (if such groups are part of the target population). Make sure that people in the population can understand, interpret, and respond to each item in a meaningful and comparable way. Make sure the measure and its items are not biased against any subgroup within the population of interest. Test bias is discussed in Chapter 16. Be careful to administer the measure to the population of interest under the conditions in which it is designed. If the measure must be administered to people from a different population or under different conditions from which it was designed, be careful to (a) note that the measure was not designed to be administered for these other populations or conditions, and (b) note that individuals’ scores may not accurately reflect their level on the construct. If interpretations are made based on these scores, make them cautiously and say how the differences in population or condition may have influenced the scores. Continue to monitor the validity of the measure’s scores for the given population and purpose. The validity of measures’ scores can change over time for a number of reasons. Cohort effects can lead items to become obsolete over time. If people or organizations change their behavior in response to a measure, this can invalidate a measure’s scores for the intended purpose, as described in Section 5.3.1.9 when discussing consequential validity. 5.9 Conclusion Validity is how much accuracy, utility, and meaningfulness the interpretation of a measure’s scores have for a particular purpose. Like reliability, validity is not one thing. There are multiple aspects of validity. Validity is also not a characteristic that resides in a test. The validity of a measure’s scores reflect an interaction of the properties of the test with the population for whom it is designed and the sample and context in which it is administered. Thus, when reporting validity in papers, it is important to adequately describe the aspects of validity that have been considered and the population, sample, and context in which the measure is assessed. 5.10 Suggested Readings Cronbach &amp; Meehl (1955); L. A. Clark &amp; Watson (2019) 5.11 Exercises 5.11.1 Questions Note: Several of the following questions use data from the Children of the National Longitudinal Survey of Youth (CNLSY). The CNLSY is a publicly available longitudinal data set provided by the Bureau of Labor Statistics (https://www.bls.gov/nls/nlsy79-children.htm#topical-guide; archived at https://perma.cc/EH38-HDRN). The CNLSY data file for these exercises is located on this book’s page of the Open Science Framework (https://osf.io/3pwza). Children’s behavior problems were rated in 1988 (time 1: T1) and then again in 1990 (time 2: T2) on the Behavior Problems Index (BPI). What is the criterion-related validity of the Antisocial Behavior subscale of the BPI in relation to the Hyperactive subscale of the BPI? Assume the true correlation between two constructs, antisocial behavior and hyperactivity, is \\(r = .8\\). And assume the reliability of the measure of antisocial behavior is \\(r = .7\\), and the reliability of the measure of hyperactivity is \\(r = .7090234\\). According to the attenuation formula (that attenuates the association due to measurement error), what would be the correlation between measures of antisocial behavior and hyperactivity that we would actually observe? Assume the true correlation between two constructs, antisocial behavior and hyperactivity, is \\(r = .8\\), the reliability of the Antisocial Behavior subscale of the BPI is \\(r = .7\\), and the reliability of the measure of Hyperactive subscale of the BPI is \\(r = .8\\). According to the disattenuation formula (to correct for attenuation of the association due to measurement error), what would be the true association between the constructs of antisocial behavior and hyperactivity? You are interested in whether a child’s levels of anxiety/depression can explain unique variance in children’s hyperactivity above and beyond their level of antisocial behavior. Is the child’s level of anxiety/depression (as rated on the Anxiety/Depression subscale of the BPI) significantly associated with the child’s level of hyperactivity (as rated on the Hyperactive subscale of the BPI) above and beyond the variance accounted for by the child’s level of antisocial behavior (as rated on the Antisocial Behavior subscale of the BPI). How much unique variance in hyperactivity is accounted for by their anxiety/depression? In Section 5.3.1.4.2.3, we simulated data for a multitrait-multimethod matrix. The simulated data includes data on participants’ verbal, spatial, and quantitative abilities, each assessed in three subtests in each of three methods: oral, written, and manipulative. Provide a multitrait-multimethod matrix of the data from the first subtest of each trait-by-method combination (VO1, SO1, QO1, VW1, SW1, QW1, VM1, SM1, QM1). Assume the reliability of the variables assessed orally is .7, the reliability of the variables assessed using the written method is .8, and the reliability of variables assessed using the manipulative method is .9. Interpret the multitrait-multimethod matrix you just created. What is the Heterotrait-Monotrait (HTMT) ratio for the measures of the verbal (VO1, VW1, VM1) and spatial (SO1, SW1, SM1) constructs? What does this indicate? 5.11.2 Answers The criterion-related validity is \\(r = .56\\). The observed correlation would be \\(r = .56\\). The true association would be \\(r = .75\\). Yes, the child’s level of anxiety/depression is significantly associated with their hyperactivity above and beyond their antisocial behavior \\((F[df = 1] = 217.25, p &lt; .001)\\). The child’s level of anxiety/depression accounted for \\(6.02\\%\\) unique variance in hyperactivity above and beyond their antisocial behavior. The multitrait-multimethod matrix is below: Figure 5.24: Multitrait-Multimethod Matrix. The convergent correlations (green cells) are statistically significant (\\(p\\text{s} &lt; .05\\)) and moderate in magnitude (\\(.45 &lt; r\\text{s} &lt; .53\\)), supporting the convergent validity of the measures. [The reliabilities of these measures are not provided, so we are not able to compare the magnitude of the convergent validities to the magnitude of reliability to see the extent to which the convergent validities may be attenuated due to measurement unreliability]. Evidence of discriminant validity is supported by three findings: First, the convergent correlations (green cells: \\(r\\text{s} =\\) .46–.53) are stronger than the heteromethod-heterotrait correlations (pink cells: \\(r\\text{s} =\\) .24–.34). Second, the convergent correlations (green cells: \\(r\\text{s} =\\) .46–.53) are stronger than the discriminant correlations (orange cells: \\(r\\text{s} =\\) .30–.43). Third, the patterns of intercorrelations between traits are the same, regardless of which measurement method is used. Verbal, spatial, and quantitative skills are intercorrelated for every measurement method used. The HTMT ratio for the measures of the verbal and spatial constructs is \\(0.853\\). The HTMT ratio is the average of the heterotrait-heteromethod correlations, relative to the average of the monotrait-heteromethod correlations. Given that the HTMT ratio is considerably less than 1 (and less than the common cutoff of .85), it indicates that the monotrait-heteromethod correlations are considerably larger than the heterotrait-heteromethod correlations. Thus, the HTMT provides evidence that the measures of verbal and spatial constructs show discriminant validity. References Achenbach, T. M. (2001). What are norms and why do we need valid ones? Clinical Psychology: Science and Practice, 8(4), 446–450. https://doi.org/10.1093/clipsy.8.4.446 American Educational Research Association, American Psychological Association, &amp; National Council on Measurement in Education. (2014). Standards for educational and psychological testing. American Educational Research Association. Bornstein, R. F. (2011). Toward a process-focused model of test score validity: Improving psychological assessment in science and practice. Psychological Assessment, 23(2), 532–544. https://doi.org/10.1037/a0022402 Campbell, D. T., &amp; Fiske, D. W. (1959). Convergent and discriminant validation by the multitrait-multimethod matrix. Psychological Bulletin, 56(2), 81–105. https://doi.org/10.1037/h0046016 Clark, L. A., &amp; Watson, D. (2019). Constructing validity: New developments in creating objective measuring instruments. Psychological Assessment, 31(12), 1412–1427. https://doi.org/10.1037/pas0000626 Cronbach, L. J., &amp; Meehl, P. E. (1955). Construct validity in psychological tests. Psychological Bulletin, 52(4), 281–302. https://doi.org/10.1037/h0040957 Epskamp, S. (2022). semPlot: Path diagrams and visual analysis of various SEM packages’ output. https://github.com/SachaEpskamp/semPlot Exner, J. E. (1974). The Rorschach: A comprehensive system. John Wiley &amp; Sons. Exner, J. E., &amp; Erdberg, S. P. (2005). The Rorschach, a comprehensive system: Advanced interpretation (3rd ed., Vol. 2). John Wiley &amp; Sons, Inc. Fiske, D. W., &amp; Campbell, D. T. (1992). Citations do not solve problems. Psychological Bulletin, 112(3), 393–395. https://doi.org/10.1037/0033-2909.112.3.393 Fornell, C., &amp; Larcker, D. F. (1981). Evaluating structural equation models with unobservable variables and measurement error. Journal of Marketing Research, 18(1), 39–50. https://doi.org/10.2307/3151312 Furr, R. M. (2017). Psychometrics: An introduction. SAGE publications. Furr, R. M., &amp; Heuckeroth, S. (2019). The “quantifying construct validity” procedure: Its role, value, interpretations, and computation. Assessment, 26(4), 555–566. https://doi.org/10.1177/1073191118820638 Goodwin, L. D., &amp; Leech, N. L. (2006). Understanding correlation: Factors that affect the size of r. The Journal of Experimental Education, 74(3), 249–266. https://doi.org/10.3200/JEXE.74.3.249-266 Hayes, S. C., Nelson, R. O., &amp; Jarrett, R. B. (1987). The treatment utility of assessment: A functional approach to evaluating assessment quality. American Psychologist, 42, 963–974. https://doi.org/10.1037/0003-066X.42.11.963 Henseler, J., Ringle, C. M., &amp; Sarstedt, M. (2015). A new criterion for assessing discriminant validity in variance-based structural equation modeling. Journal of the Academy of Marketing Science, 43(1), 115–135. https://doi.org/10.1007/s11747-014-0403-8 Johnson, P. E. (2022). rockchalk: Regression estimation and presentation. https://CRAN.R-project.org/package=rockchalk Jorgensen, T. D., Pornprasertmanit, S., Schoemann, A. M., &amp; Rosseel, Y. (2021). semTools: Useful tools for structural equation modeling. https://github.com/simsem/semTools/wiki Leong, F. T. L., &amp; Kalibatseva, Z. (2016). Threats to cultural validity in clinical diagnosis and assessment: Illustrated with the case of Asian Americans. In N. Zane, G. Bernal, &amp; F. T. L. Leong (Eds.), Evidence-based psychological practice with ethnic minorities: Culturally informed research and clinical strategies (pp. 57–74). American Psychological Association. Meehl, P. E. (1978). Theoretical risks and tabular asterisks: Sir Karl, Sir Ronald, and the slow progress of soft psychology. Journal of Consulting and Clinical Psychology, 46(4), 806–834. https://doi.org/10.1037/0022-006x.46.4.806 Nelson-Gray, R. O. (2003). Treatment utility of psychological assessment. Psychological Assessment, 15(4), 521–531. https://doi.org/10.1037/1040-3590.15.4.521 Petersen, I. T. (2024b). petersenlab: Package of R functions for the Petersen Lab. https://doi.org/10.5281/zenodo.7602890 Roemer, E., Schuberth, F., &amp; Henseler, J. (2021). HTMT2–an improved criterion for assessing discriminant validity in structural equation modeling. Industrial Management &amp; Data Systems, 121(12), 2637–2650. https://doi.org/10.1108/IMDS-02-2021-0082 Rönkkö, M., &amp; Cho, E. (2020). An updated guideline for assessing discriminant validity. Organizational Research Methods, 1094428120968614. https://doi.org/10.1177/1094428120968614 Rosseel, Y., Jorgensen, T. D., &amp; Rockwood, N. (2022). lavaan: Latent variable analysis. https://lavaan.ugent.be Royal, K. (2016). “Face validity” is not a legitimate type of validity evidence! The American Journal of Surgery, 212(5), 1026–1027. https://doi.org/10.1016/j.amjsurg.2016.02.018 Schmidt, F. L., &amp; Hunter, J. E. (1996). Measurement error in psychological research: Lessons from 26 research scenarios. Psychological Methods, 1(2), 199–223. https://doi.org/10.1037/1082-989X.1.2.199 Schneider, W. J. (2021). simstandard: Generate standardized data. https://github.com/wjschne/simstandard Sechrest, L. (1963). Incremental validity: A recommendation. Educational and Psychological Measurement, 23, 153–158. https://doi.org/10.1177/001316446302300113 Silver, N. (2012). The signal and the noise: Why so many predictions fail–but some don’t. Penguin. Slack, M. K., &amp; Draugalis, J., Jolaine R. (2001). Establishing the internal and external validity of experimental studies. American Journal of Health-System Pharmacy, 58(22), 2173–2181. https://doi.org/10.1093/ajhp/58.22.2173 Voorhees, C. M., Brady, M. K., Calantone, R., &amp; Ramirez, E. (2016). Discriminant validity testing in marketing: An analysis, causes for concern, and proposed remedies. Journal of the Academy of Marketing Science, 44(1), 119–134. https://doi.org/10.1007/s11747-015-0455-4 Wood, J. M., Nezworski, M. T., Garb, H. N., &amp; Lilienfeld, S. O. (2001). Problems with the norms of the Comprehensive System for the Rorschach: Methodological and conceptual considerations. Clinical Psychology: Science and Practice, 8(3), 397–402. https://doi.org/10.1093/clipsy.8.3.397 Wood, J. M., Teresa, P. M., Garb, H. N., &amp; Lilienfeld, S. O. (2001). The misperception of psychopathology: Problems with the norms of the Comprehensive System for the Rorschach. Clinical Psychology: Science and Practice, 8(3), 350–373. https://doi.org/10.1093/clipsy.8.3.350 For shorthand and to avoid repetition, we sometimes refer to the validity of the measure, but in such instances, we are actually referring to the validity of the interpretation of a measure’s scores for a given use.↩︎ "],["gTheory.html", "Chapter 6 Generalizability Theory 6.1 Overview of Generalizability Theory (G-Theory) 6.2 Getting Started 6.3 Conclusion 6.4 Suggested Readings 6.5 Exercises", " Chapter 6 Generalizability Theory Up to this point, we have discussed reliability from the perspective of classical test theory (CTT). However, as we discussed, CTT makes several assumptions that are unrealistic. For example, CTT assumes that all error is random. In CTT, there is an assumption that there exists a true score that is an accurate measure of the trait under a specific set of conditions. There are other measurement theories that conceptualize reliability differently than the way that CTT conceptualizes reliability. One such measurement theory is generalizability theory (Brennan, 1992), also known as G-theory and domain sampling theory. G-theory is also discussed in the chapters on reliability (Chapter 4, Section 4.11), validity (Chapter 5, Section 5.7) and structural equation modeling (Chapter 7, Section 7.14). 6.1 Overview of Generalizability Theory (G-Theory) G-theory is an alternative measurement theory to CTT that does not treat all measurement differences across time, rater, or situation as “error” but rather as a phenomenon of interest (Wiggins, 1973). G-theory is a measurement theory that is used to examine the extent to which scores are consistent across a specific set of conditions. In G-theory, the true score is conceived of as a person’s universe score—the mean of all observations for a person over all conditions in the universe—this allows us to estimate and recognize the magnitude of multiple influences on test performance. These multiple influences on test performance are called facets. 6.1.1 The Universe of Generalizability Instead of conceiving of all variability in a person’s scores as error, G-theory argues that we should describe the details of the particular test situation (universe) that lead to a specific test score. The universe is described in terms of its facets: settings observers (e.g., amount of training they had) instruments (e.g., number of items in test) occasions (time points) attributes (i.e., what we are assessing; the purpose of test administration) Measures with strong reliability show a high ratio of variance as a function of the person relative to the variance as a function of other facets or factors. To the extent that variance in scores is attributable to different settings, observers, instruments, occasions, attributes, or other facets, the reliability of the measure is weakened. 6.1.2 Universe Score A person’s universe score is the average of a person’s scores across all conditions in the universe. According to G-theory, given the exact same conditions of all the facets in the universe, the exact same test score should be obtained. This is the universe score, which is analogous to the true score in CTT. 6.1.3 G-Theory Perspective on Reliability G-theory asserts that the reliability of a test does not reside within the test itself; a test’s reliability depends on the circumstances under which it is developed, administered, and interpreted. A person’s test scores vary from testing to testing because of (many) variables in the testing situation. By assessing a person in multiple facets of the universe, this allows us to estimate and recognize the magnitude of multiple sources of measurement error. Such measurement error includes: day-to-day variation in performance (stability of the construct, test–retest reliability) variance in the item sampling (coefficient of internal consistency) variance due to both day-to-day and item sampling (coefficient of equivalence from parallel-forms reliability, or convergent validity, as discussed in Section 5.7) In G-theory, all sources of measurement error (facets) are considered simultaneously—something CTT cannot achieve (Shavelson et al., 1989). This occurs through specifying many different variance facets in the estimation of the true score, rather than just one source of error variance as in CTT. This specification allows us to take into consideration variance due to occasion effects, item effects, and occasion \\(\\times\\) item effects (i.e., main effects of the facets in addition to their interaction), as in Table 6.1. Table 6.1: Percent of Variance from Different Sources in Generalizability Theory Model With Three Facets: Person, Item, and Occasion (and Their Interactions). (Adapted from Webb &amp; Shavelson (2005), Table 1, p. 2. Webb, N. M., &amp; Shavelson, R. J. (2005). Generalizability theory: Overview. In B. S. Everitt &amp; D. C. Howell (Eds.), Encyclopedia of statistics in behavioral science (Vol. 2, pp. 717–719). John Wiley &amp; Sons, Ltd. https://doi.org/10.1002/0470013192.bsa703) Source Variance Accounted For (%) Person (p) 30 Item (i) 5 Occasion (o) 3 p x i 25 p x o 5 i x o 2 p x i x o 10 residual 20 A score’s usefulness largely depends on the extent to which it allows us to generalize accurately to behavior in a wider set of situations—i.e., a universe of generalization. The G-Theory equivalent of the CTT reliability coefficient of a measure is the generalizability coefficient or dependability coefficient. 6.1.4 G-Theory Perspective on Validity G-theory can simultaneously consider multiple aspects of reliability and validity in the same model. For instance, internal consistency reliability, test–retest reliability, inter-rater reliability, parallel-forms reliability, and convergent validity (in the D. T. Campbell &amp; Fiske, 1959 sense of the same construct assessed by a different method) can all be incorporated into a G-theory model. For example, a G-theory model could assess each participant across the following facets: time: e.g., T1 and T2 (test–retest reliability) items: e.g., questions within the same instrument (internal consistency reliability) and questions across different instruments (parallel-forms reliability) rater: e.g., self-report and other-report (inter-rater reliability) method: e.g., questionnaire and observation (convergent validity) Using such a G-theory model, we can determine the extent to which scores on a measure generalize to other conditions, measures, etc. Measures with strong convergent validity show a high ratio of variance as a function of the person relative to the variance as a function of measurement method. To the extent that variance in scores is attributable to different measurement methods, convergent validity is weakened. An example data structure that could leverage G-theory to partition the variance in scores as a function of different facets (person, time, item, rater, method) and their interactions is in Table 6.2. Table 6.2: Example Data Structure for Generalizability Theory With the Following Facets: Person, Time, Item, Rater, Method. Person Time Item Rater Method Score 1 1 “hits others” 1 questionnaire 10 1 1 “hits others” 1 observation 15 1 1 “hits others” 2 questionnaire 8 1 1 “hits others” 2 observation 13 1 1 “argues” 1 questionnaire 4 1 1 “argues” 1 observation 2 1 1 “argues” 2 questionnaire 5 1 1 “argues” 2 observation 7 1 2 “hits others” 1 questionnaire 8 1 2 “hits others” 1 observation 10 1 2 “hits others” 2 questionnaire 6 1 2 “hits others” 2 observation 7 1 2 “argues” 1 questionnaire 2 1 2 “argues” 1 observation 2 1 2 “argues” 2 questionnaire 4 1 2 “argues” 2 observation 6 2 1 “hits others” 1 questionnaire 5 … … … … … … In sum, G-theory can be a useful way of estimating the degree of reliability and validity of a measure’s scores in the same model. 6.1.5 Generalizability Study In G-theory, the goal is to conduct a generalizability study (G study) and a decision study (D study). A generalizability study examines the extent of variance in the scores that is attributable to various facets. The researcher must specify and define the universe (set of conditions) to which they would like to generalize their observations and in which they would like to study the reliability of the measure. For instance, it might involve randomly sampling from within that universe in terms of people, items, observers, conditions, timepoints, measurement methods, etc. 6.1.6 Decision Study After conducting a generalizability study, one can then use the estimates of the extent of variance in scores that are attributable to various facets (estimated from the generalizability study) to conduct a decision study (D study). A decision study examines how generalizable scores from a particular test are if the test is administered in different situations. In G-theory, reliability is estimated with the generalizability coefficient and the dependability coefficient. 6.1.7 Analysis Approach Traditionally, a generalizability theory approach would test the generalizability study and decision study using a factorial analysis of variance [ANOVA; Brennan (1992)], as exemplified in Section 6.2.4.1, (as opposed to simple ANOVA in CTT). However, ANOVA is limiting—it works best with balanced designs, such as with the same sample size in each condition/facet; but in most real-world applications, data are not equally balanced in each condition. So, it is better to fit G-theory models in a mixed model framework, as exemplified in Section 6.2.4.2. 6.1.8 Practical Challenges G-theory is strong theoretically, but it has not been widely implemented. G-theory can be challenging because the researcher must specify, define, and assess the universe to which they would like to generalize their observations and to understand the reliability of the measure. 6.2 Getting Started 6.2.1 Load Libraries Code library(&quot;petersenlab&quot;) #to install: install.packages(&quot;remotes&quot;); remotes::install_github(&quot;DevPsyLab/petersenlab&quot;) library(&quot;gtheory&quot;) library(&quot;MOTE&quot;) library(&quot;tidyverse&quot;) library(&quot;tinytex&quot;) library(&quot;knitr&quot;) library(&quot;rmarkdown&quot;) library(&quot;bookdown&quot;) 6.2.2 Prepare Data 6.2.2.1 Generate Data For reproducibility, we set the seed below. Using the same seed will yield the same answer every time. There is nothing special about this particular seed. Code set.seed(52242) Person &lt;- as.factor(rep(1:6, each = 8)) Occasion &lt;- Rater &lt;- as.factor(rep(1:2, each = 4, times = 6)) Item &lt;- as.factor(rep(1:4, times = 12)) Score &lt;- c( 9,9,7,4,9,8,5,5,9,8,4,6, 6,5,3,3,8,8,6,2,8,7,3,2, 9,8,6,3,9,6,6,2,10,9,8,7, 8,8,9,7,6,4,5,1,3,2,3,2) 6.2.2.2 Add Missing Data Adding missing data to dataframes helps make examples more realistic to real-life data and helps you get in the habit of programming to account for missing data. Code Score[30] &lt;- NA 6.2.2.3 Combine Data into Dataframe Code pio_cross_dat &lt;- data.frame(Person, Item, Score, Occasion) Below are examples implementing G-theory. The pio_cross_dat data file for these examples comes from the gtheory package (Moore, 2016). The examples are adapted from Huebner &amp; Lucht (2019). 6.2.3 Universe Score for Each Person Universe scores for each person are generated using the following syntax and are presented in Table ??. 6.2.4 Generalizability (G) Study Generalizability studies can be conducted in an ANOVA or mixed model framework. Below, we fit a generalizability study model in each framework. In these models, the item, person, and their interaction appear to be the three facets that account for the most variance in scores. Thus, when designing future studies, it would be important to assess and evaluate these facets. 6.2.4.1 ANOVA Framework Code summary(aov( Score ~ Person*Item*Occasion, data = pio_cross_dat)) Df Sum Sq Mean Sq Person 5 112.95 22.59 Item 3 119.16 39.72 Occasion 1 14.17 14.17 Person:Item 15 35.61 2.37 Person:Occasion 5 6.58 1.32 Item:Occasion 3 2.26 0.75 Person:Item:Occasion 14 12.08 0.86 1 observation deleted due to missingness 6.2.4.2 Mixed Model Framework The mixed model framework for estimating generalizability is described by Jiang (2018). Code summary(lmer( Score ~ 1 + (1|Person) + (1|Item) + (1|Occasion) + (1|Person:Occasion) + (1|Person:Item) + (1|Occasion:Item), data = pio_cross_dat)) Linear mixed model fit by REML [&#39;lmerMod&#39;] Formula: Score ~ 1 + (1 | Person) + (1 | Item) + (1 | Occasion) + (1 | Person:Occasion) + (1 | Person:Item) + (1 | Occasion:Item) Data: pio_cross_dat REML criterion at convergence: 174.7 Scaled residuals: Min 1Q Median 3Q Max -1.30232 -0.65304 -0.05246 0.63558 1.37019 Random effects: Groups Name Variance Std.Dev. Person:Item (Intercept) 0.787305 0.88730 Person:Occasion (Intercept) 0.117284 0.34247 Occasion:Item (Intercept) 0.001574 0.03967 Person (Intercept) 2.478043 1.57418 Item (Intercept) 3.123803 1.76743 Occasion (Intercept) 0.536633 0.73255 Residual 0.837426 0.91511 Number of obs: 47, groups: Person:Item, 24; Person:Occasion, 12; Occasion:Item, 8; Person, 6; Item, 4; Occasion, 2 Fixed effects: Estimate Std. Error t value (Intercept) 5.957 1.234 4.827 Code PxIxO &lt;- gstudy( data = pio_cross_dat, Score ~ (1|Person) + (1|Item) + (1|Occasion) + (1|Person:Item) + (1|Person:Occasion) + (1|Occasion:Item)) PxIxO $components source var percent n 1 Person:Item 0.787305090 10.0 1 2 Person:Occasion 0.117284076 1.5 1 3 Occasion:Item 0.001573802 0.0 1 4 Person 2.478043107 31.4 1 5 Item 3.123803216 39.6 1 6 Occasion 0.536633200 6.8 1 7 Residual 0.837426163 10.6 1 attr(,&quot;class&quot;) [1] &quot;gstudy&quot; &quot;list&quot; 6.2.5 Decision (D) Study The decision (D) study and generalizability (G) study, from which the generalizability and dependability coefficients can be estimated, were analyzed using the gtheory package (Moore, 2016). Code decisionStudy &lt;- dstudy( PxIxO, colname.objects = &quot;Person&quot;, data = pio_cross_dat, colname.scores = &quot;Score&quot;) 6.2.6 Generalizability Coefficient The generalizability coefficient is analogous to the reliability coefficient in CTT. It divides the estimated person variance component (the universe score variance) by the estimated observed-score variance (with some adjustment for the number of observations). In other words, variance in a reliable measure should mostly be due to person variance rather than variance as a function of items, occasion, raters, methods, or other factors. The generalizability coefficient uses relative error variance, so it characterizes the similarity in the relative standing of individuals, similar to CTT-based estimates of relative reliability, such as Cronbach’s alpha. Thus, the generalizability coefficient is an index of relative reliability. The generalizability coefficient ranges from 0–1, and higher scores reflect better reliability. Code decisionStudy$generalizability [1] 0.8731069 6.2.7 Dependability Coefficient The dependability coefficient is similar to the generalizability coefficient; however, it uses absolute error variance rather than relative error variance in the estimation. The dependability coefficient characterizes the absolute magnitude of differences across scores, not (just) the relative standing of individuals. Thus, the dependability coefficient is an index of absolute reliability. The dependability coefficient ranges from 0–1, and higher scores reflect better reliability. Code decisionStudy$dependability [1] 0.6374135 6.3 Conclusion G-theory provides an important reminder that reliability is not one thing. You cannot just say that a test “is reliable”; it is important to specify the facets across which the reliability and validity of a measure have been established (e.g., times, raters, items, groups, instruments). Generalizability theory can be a useful way of estimating multiple aspects of reliability and validity of measures in the same model. 6.4 Suggested Readings Brennan (2001) 6.5 Exercises 6.5.1 Questions You want to see how generalizable the Antisocial Behavior subscale of the BPI is. You conduct a generalizability study (“\\(G\\) study”) to see how generalizable the scores are across participants (\\(N = 3\\)), two measurement occasions, and three raters. What is the universe score (estimate of true score) for each participant? What percent of variance is attributable to: (a) individual differences among participants, (b) different raters, (c) the measurement occasions, and (d) the interactive effect of raters and measurement occasions? Using a decision study (“\\(D\\) study”), what are the generalizability and dependability coefficients? Is the measure reliable across the universe (range of factors) we examined it in? Interpret the results from the G study and D study. The data from your study are in Table 6.3 below: Table 6.3: Exercise 12: Table of Example Data for Evaluating Generalizability of Scores Across Participants, Occasions, and Raters. Participant Occasion Rater Score 1 1 1 7 2 1 1 2 3 1 1 0 1 2 1 10 2 2 1 5 3 2 1 1 1 1 2 8 2 1 2 3 3 1 2 1 1 2 2 11 2 2 2 6 3 2 2 1 1 1 3 6 2 1 3 2 3 1 3 4 1 2 3 12 2 2 3 6 3 2 3 5 6.5.2 Answers The universe score is \\(9\\), \\(4\\), and \\(2\\) for participants 1, 2, and 3, respectively. The percent of variance attributable to each of those factors is: (a) participant: \\(64.6\\%\\), (b) rater: \\(0.2\\%\\), (c) the measurement occasion: \\(16.2\\%\\), and (d) the interactive effect of rater and measurement occasion: \\(1.5\\%\\). The generalizability coefficient is \\(.90\\), and the dependability coefficient is \\(.81\\). The measure is fairly reliable across the universe examined, both in terms of relative differences (generalizability coefficient) and absolute differences (dependability coefficient). However, when using the measure in future work, it would be important to assess many participants (due to the strong presence of individual differences) across multiple occasions (due to the strong effect of occasion) and multiple raters (due to the moderated effect of rater as a function of participant). It will be important to account for the considerable sources of variance in scores on the measure in future work including: participant, occasion, participant \\(\\times\\) rater, and participant \\(\\times\\) occasion. By accounting for these sources of variance, we can get a purer estimate of each participant’s true score and the population’s true score. References Brennan, R. L. (1992). Generalizability theory. Educational Measurement: Issues and Practice, 11(4), 27–34. https://doi.org/10.1111/j.1745-3992.1992.tb00260.x Brennan, R. L. (2001). Generalizability theory. Springer New York. https://books.google.com/books?id=nbHbBwAAQBAJ Campbell, D. T., &amp; Fiske, D. W. (1959). Convergent and discriminant validation by the multitrait-multimethod matrix. Psychological Bulletin, 56(2), 81–105. https://doi.org/10.1037/h0046016 Huebner, A., &amp; Lucht, M. (2019). Generalizability theory in R. Practical Assessment, Research &amp; Evaluation, 24(5), 2. https://doi.org/10.7275/5065-gc10 Jiang, Z. (2018). Using the linear mixed-effect model framework to estimate generalizability variance components in R. Methodology, 14(3), 133–142. https://doi.org/10.1027/1614-2241/a000149 Moore, C. T. (2016). gtheory: Apply generalizability theory with R. http://EvaluationDashboard.com Shavelson, R. J., Webb, N. M., &amp; Rawley, R. L. (1989). Generalizability theory. American Psychologist, 44, 922–932. https://doi.org/10.1037/0003-066X.44.6.922 Webb, N. M., &amp; Shavelson, R. J. (2005). Generalizability theory: overview. In B. S. Everitt &amp; D. C. Howell (Eds.), Encyclopedia of statistics in behavioral science (Vol. 2, pp. 717–719). John Wiley &amp; Sons, Ltd. Wiggins, J. S. (1973). Personality and prediction: Principles of personality assessment. Addison-Wesley. "],["sem.html", "Chapter 7 Structural Equation Modeling 7.1 Overview of SEM 7.2 Getting Started 7.3 Types of Models 7.4 Estimating Latent Factors 7.5 Additional Types of SEM 7.6 Causal Diagrams: Directed Acyclic Graphs 7.7 Model Fit Indices 7.8 Correlation Matrix 7.9 Measurement Model (of a Given Construct) 7.10 Confirmatory Factor Analysis (CFA) 7.11 Structural Equation Model (SEM) 7.12 Benefits of SEM 7.13 Power Analysis Using Monte Carlo Simulation 7.14 Generalizability Theory 7.15 Conclusion 7.16 Suggested Readings 7.17 Exercises", " Chapter 7 Structural Equation Modeling “All models are wrong, but some are useful.” — George Box (1979, p. 202) 7.1 Overview of SEM Structural equation modeling is an advanced modeling approach that allows estimating latent variables to account for measurement error and to get purer estimates of constructs. 7.2 Getting Started 7.2.1 Load Libraries Code library(&quot;petersenlab&quot;) #to install: install.packages(&quot;remotes&quot;); remotes::install_github(&quot;DevPsyLab/petersenlab&quot;) library(&quot;lavaan&quot;) library(&quot;semTools&quot;) library(&quot;semPlot&quot;) library(&quot;simsem&quot;) library(&quot;snow&quot;) library(&quot;mice&quot;) library(&quot;quantreg&quot;) library(&quot;nonnest2&quot;) library(&quot;MOTE&quot;) library(&quot;tidyverse&quot;) library(&quot;here&quot;) library(&quot;tinytex&quot;) 7.2.2 Prepare Data 7.2.2.1 Simulate Data For reproducibility, I set the seed below. Using the same seed will yield the same answer every time. There is nothing special about this particular seed. The petersenlab package (Petersen, 2024b) includes a complement() function (archived at https://perma.cc/S26F-QSW3) that simulates data with a specified correlation in relation to an existing variable. PoliticalDemocracy refers to the Industrialization and Political Democracy data set from the lavaan package (Rosseel et al., 2022), and it contains measures of political democracy and industrialization in developing countries. Code sampleSize &lt;- 300 set.seed(52242) v1 &lt;- complement(PoliticalDemocracy$y1, .4) v2 &lt;- complement(PoliticalDemocracy$y1, .4) v3 &lt;- complement(PoliticalDemocracy$y1, .4) v4 &lt;- complement(PoliticalDemocracy$y1, .4) PoliticalDemocracy$v1 &lt;- v1 PoliticalDemocracy$v2 &lt;- v2 PoliticalDemocracy$v3 &lt;- v3 PoliticalDemocracy$v4 &lt;- v4 measure1 &lt;- rnorm(n = sampleSize, mean = 50, sd = 10) measure2 &lt;- measure1 + rnorm(n = sampleSize, mean = 0, sd = 15) measure3 &lt;- measure1 + measure2 + rnorm(n = sampleSize, mean = 0, sd = 15) 7.2.2.2 Add Missing Data Adding missing data to dataframes helps make examples more realistic to real-life data and helps you get in the habit of programming to account for missing data. Code measure1[c(5,10)] &lt;- NA measure2[c(10,15)] &lt;- NA measure3[c(10)] &lt;- NA PoliticalDemocracy &lt;- as.data.frame(lapply( PoliticalDemocracy, function(cc) cc[ sample( c(TRUE, NA), prob = c(0.9, 0.1), size = length(cc), replace = TRUE)])) 7.2.2.3 Combine Data into Dataframe Code mydataSEM &lt;- data.frame(measure1, measure2, measure3) 7.3 Types of Models 7.3.1 Path Analysis Model To understand structural equation modeling (SEM), it is helpful to first understand path analysis. Path analysis is similar to multiple regression. Path analysis allows examining the association between multiple predictor variables (or independent variables) in relation to an outcome variable (or dependent variable). Unlike multiple regression, however, path analysis also allows inclusion of multiple dependent variables in the same model. Unlike SEM, path analysis uses only manifest (observed) variables, not latent variables (described next). SEM is path analysis, but with latent (unobserved) variables. That is, a SEM model is a model that includes latent variables in addition to observed variables, where one attempts to model (i.e., explain) the structure of associations between variance using a series of equations (hence structural equation modeling). 7.3.2 Components of a Structural Equation Model 7.3.2.1 Measurement Model The measurement model is a crucial sub-component of any SEM model. A SEM model consists of two components: a measurement model and a structural model. The measurement model is a confirmatory factor analysis (CFA) model that identifies how many latent factors are estimated, and which items load onto which latent factor. The measurement model can also specify correlated residuals. Basically, the measurement model specifies your best understanding of the structure of the latent construct(s) given how they were assessed. Before fitting the structural component of a SEM, it is important to have a well-fitting measurement model for each construct in the model. In Section 7.3.2.1, I present an example of a measurement model. 7.3.2.2 Structural Model The structural component of a SEM model includes the regression paths that specify the hypothesized causal relations among the latent variables. 7.3.3 Confirmatory Factor Analysis Model Confirmatory factor analysis (CFA) is a subset of SEM. CFA includes the measurement model but not the structural component of the model. In Section 7.10, I present an example of a CFA model. I discuss CFA models in greater depth in Chapter 14. 7.3.4 Structural Equation Model SEM is CFA, but it adds regression paths that specify hypothesized causal relations between the latent variables, which is called the structural component of the model. The structural model includes the hypothesized causal relations between latent variables. A SEM model includes both the measurement model and the structural model (see Figure 7.1, Civelek, 2018). SEM fits a model to observed data, or the variance-covariance matrix, and evaluates the degree of model misfit. That is, fit indices evaluate how likely it is that a given model gave rise to the observed data. In Section 7.11, I present an example of a SEM model. Figure 7.1: Demarcation Between Measurement Model and Structural Model. (Figure adapted from Civelek (2018), Figure 1, p. 7. Civelek, M. E. (2018). Essentials of structural equation modeling. Zea E-Books. https://doi.org/10.13014/K2SJ1HR5) SEM is flexible in allowing you to specify measurement error and correlated errors. Thus, you do not need the same assumptions as in classical test theory, which assumes that errors are random and uncorrelated. But the flexibility of SEM also poses challenges because you must explicitly decide what to include—and not include—in your model. This flexibility can be both a blessing and a curse. If the model fit is unacceptable, you can try fitting a different model to see which fits better. Nevertheless, it is important to use theory as a guide when specifying and comparing competing models, and not just rely solely on model fit comparison. For example, the model you fit should depend on how you conceptualize each construct: as reflective or formative. 7.4 Estimating Latent Factors 7.4.1 Model Identification 7.4.1.1 Types of Model Identification There are important practical issues to consider with both reflective and formative models. An important practical issue is model identification—adding enough constraints so that there is only one, best answer. The model is identified when each of the estimated parameters has a unique solution. Degrees of freedom in a SEM model is the number of known values minus the number of estimated parameters. The number of known values in a SEM model is the number of variances and covariances in the variance-covariance matrix of the manifest (observed) variables in addition to the number of means (i.e., the number of manifest variables), which can be calculated as: \\(\\frac{m(m + 1)}{2} + m\\), where \\(m = \\text{the number of manifest variables}\\). You can never estimate more parameters than the number of known values. A model with zero degrees of freedom is considered “saturated”—it will have perfect fit because the model estimates as many parameters as there are known values. All things equal (i.e., in terms of model fit with the same number of manifest variables), a model with more degrees of freedom is preferred for its parsimony, because fewer parameters are estimated. Based on the number of known values compared to the number of estimated parameters, a model can be considered either just identified, under-identified, or over-identified. A just identified model is a model in which the number of known values is equal to the number of parameters to be estimated (degrees of freedom = 0). An under-identified model is a model in which the number of known values is less than the number of parameters to be estimated (degrees of freedom &lt; 0). An over-identified model is a model in which the number of known values is greater than the number of parameters to be estimated (degrees of freedom &gt; 0). As an example, there are 14 known values for a model with 4 manifest variables (\\(\\frac{4(4 + 1)}{2} + 4 = 14\\)): 4 variances, 6 covariances, and 4 means. Here is the variance-covariance matrix: Code vcovMatrix4measures &lt;- cov( PoliticalDemocracy[,c(&quot;y1&quot;,&quot;y2&quot;,&quot;y3&quot;,&quot;y4&quot;)], use = &quot;pairwise.complete.obs&quot;) vcovMatrix4measures[upper.tri(vcovMatrix4measures)] &lt;- NA vcovMatrix4measures y1 y2 y3 y4 y1 6.387379 NA NA NA y2 6.060140 16.424848 NA NA y3 5.613202 6.329419 11.066049 NA y4 5.393572 9.910746 7.151898 11.22069 Here are the variances: Code variances4measures &lt;- diag(vcovMatrix4measures) variances4measures y1 y2 y3 y4 6.387379 16.424848 11.066049 11.220691 Here are the covariances: Code covariances4measures &lt;- vcovMatrix4measures[lower.tri(vcovMatrix4measures)] covariances4measures [1] 6.060140 5.613202 5.393572 6.329419 9.910746 7.151898 Here are the means: Code means4Measures &lt;- apply( PoliticalDemocracy[,c(&quot;y1&quot;,&quot;y2&quot;,&quot;y3&quot;,&quot;y4&quot;)], 2, mean, na.rm = TRUE) means4Measures y1 y2 y3 y4 5.347059 4.284975 6.367156 4.356618 7.4.1.2 Approaches to Model Identification The three most widely used approaches to identifying latent factors are: Marker variable Effects coding Standardized latent factor 7.4.1.2.1 Marker Variable Method In the marker variable method, one of the indicators (i.e., manifest variables) is set to have a loading of 1. Here are examples of using the marker variable method for identification of a latent variable: Code markerVariable_syntax &lt;- &#39; #Factor loadings latentFactor =~ y1 + y2 + y3 + y4 &#39; markerVariable_fullSyntax &lt;- &#39; #Factor loadings latentFactor =~ 1*y1 + y2 + y3 + y4 #Latent variance latentFactor ~~ latentFactor #Estimate residual variances of manifest variables y1 ~~ y1 y2 ~~ y2 y3 ~~ y3 y4 ~~ y4 #Estimate intercepts of manifest variables y1 ~ 1 y2 ~ 1 y3 ~ 1 y4 ~ 1 &#39; markerVariableModelFit &lt;- sem( markerVariable_syntax, data = PoliticalDemocracy, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) markerVariableModelFit_full &lt;- lavaan( markerVariable_fullSyntax, data = PoliticalDemocracy, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) Code semPaths( markerVariableModelFit, what = &quot;est&quot;, layout = &quot;tree2&quot;, edge.label.cex = 0.8) Figure 7.2: Identifying a Latent Variable Using the Marker Variable Approach. 7.4.1.2.2 Effects Coding Method In the effects coding method, the average of the factor loadings is set to be 1. The effects coding method is useful if you are interested in the means or variances of the latent factor, because the metric of the latent factor is on the metric of the indicators. Here are examples of using the effects coding method for identification of a latent variable: Code effectsCoding_abbreviatedSyntax &lt;- &#39; #Factor loadings latentFactor =~ y1 + y2 + y3 + y4 &#39; effectsCoding_syntax &lt;- &#39; #Factor loadings latentFactor =~ NA*y1 + label1*y1 + label2*y2 + label3*y3 + label4*y4 #Constrain factor loadings label1 == 4 - label2 - label3 - label4 # 4 = number of indicators &#39; effectsCoding_fullSyntax &lt;- &#39; #Factor loadings latentFactor =~ label1*y1 + label2*y2 + label3*y3 + label4*y4 #Constrain factor loadings label1 == 4 - label2 - label3 - label4 # 4 = number of indicators #Latent variance latentFactor ~~ latentFactor #Estimate residual variances of manifest variables y1 ~~ y1 y2 ~~ y2 y3 ~~ y3 y4 ~~ y4 #Estimate intercepts of manifest variables y1 ~ 1 y2 ~ 1 y3 ~ 1 y4 ~ 1 &#39; effectsCodingModelFit_abbreviated &lt;- sem( effectsCoding_abbreviatedSyntax, data = PoliticalDemocracy, effect.coding = &quot;loadings&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) effectsCodingModelFit &lt;- sem( effectsCoding_syntax, data = PoliticalDemocracy, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) effectsCodingModelFit_full &lt;- lavaan( effectsCoding_fullSyntax, data = PoliticalDemocracy, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) Code semPaths( effectsCodingModelFit, what = &quot;est&quot;, layout = &quot;tree2&quot;, edge.label.cex = 0.8) Figure 7.3: Identifying a Latent Variable Using the Effects Coding Approach. 7.4.1.2.3 Standardized Latent Factor Method In the standardized latent factor method, the latent factor is set to have a mean of 0 and a standard deviation of 1. The standardized latent factor method is a useful approach if you are not interested in the means or variances of the latent factors and want to freely estimate the factor loadings. Here are examples of using the standardized latent factor method for identification of a latent variable: Code standardizedLatent_abbreviatedsyntax &lt;- &#39; #Factor loadings latentFactor =~ y1 + y2 + y3 + y4 &#39; standardizedLatent_syntax &lt;- &#39; #Factor loadings latentFactor =~ NA*y1 + y2 + y3 + y4 #Latent mean latentFactor ~ 0 #Latent variance latentFactor ~~ 1*latentFactor &#39; standardizedLatent_fullSyntax &lt;- &#39; #Factor loadings latentFactor =~ NA*y1 + y2 + y3 + y4 #Latent mean latentFactor ~ 0 #Latent variance latentFactor ~~ 1*latentFactor #Estimate residual variances of manifest variables y1 ~~ y1 y2 ~~ y2 y3 ~~ y3 y4 ~~ y4 #Estimate intercepts of manifest variables y1 ~ 1 y2 ~ 1 y3 ~ 1 y4 ~ 1 &#39; standardizedLatentFit_abbreviated &lt;- sem( standardizedLatent_abbreviatedsyntax, data = PoliticalDemocracy, std.lv = TRUE, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) standardizedLatentFit &lt;- sem( standardizedLatent_syntax, data = PoliticalDemocracy, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) standardizedLatentFit_full &lt;- lavaan( standardizedLatent_fullSyntax, data = PoliticalDemocracy, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) Code semPaths( standardizedLatentFit, what = &quot;est&quot;, layout = &quot;tree2&quot;, edge.label.cex = 0.8) Figure 7.4: Identifying a Latent Variable Using the Standardized Latent Factor Approach. 7.4.2 Types of Latent Factors 7.4.2.1 Reflective Latent Factors For a reflective model with 4 indicators, we would need to estimate 12 parameters: a factor loading, error term, and intercept for each of the 4 indicators. Here are the parameters estimated: Code reflectiveModel_syntax &lt;- &#39; #Reflective model factor loadings reflective =~ y1 + y2 + y3 + y4 &#39; reflectiveModelFit &lt;- sem( reflectiveModel_syntax, data = PoliticalDemocracy, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, std.lv = TRUE) reflectiveModelParameters &lt;- parameterEstimates( reflectiveModelFit)[!is.na(parameterEstimates(reflectiveModelFit)$z),] row.names(reflectiveModelParameters) &lt;- NULL reflectiveModelParameters Here are the degrees of freedom: Code fitMeasures(reflectiveModelFit, &quot;df&quot;) df 2 Here is a model diagram: Code semPaths( reflectiveModelFit, what = &quot;Std.all&quot;, layout = &quot;tree2&quot;, edge.label.cex = 0.8) Figure 7.5: Example of a Reflective Model. Thus, for a reflective model, we only have to estimate a small number of parameters to specify what is happening in our model, so the model is parsimonious. With 4 indicators, the number of known values (14) is greater than the number of parameters (12). We have 2 degrees of freedom (\\(14 - 12 = 2\\)). Because the degrees of freedom is greater than 0, it is easy to identify the model—the model is over-identified. A reflective model with 3 indicators would have 9 known values (\\(\\frac{3(3 + 1)}{2} + 3 = 9\\)), 9 parameters (3 factor loadings, 3 error terms, 3 intercepts), and 0 degrees of freedom, and it would be identifiable because it would be just-identified. 7.4.2.2 Formative Latent Factors However, for a formative model, we must specify more parameters: a factor loading, intercept, and variance for each of the 4 indicators, all 6 permissive correlations, and 1 error term for the latent variable, for a total of 19 parameters. Here are the parameters estimated: Code formativeModel_syntax &lt;- &#39; #Formative model factor loadings formative &lt;~ v1 + v2 + v3 + v4 formative ~~ formative &#39; formativeModelFit &lt;- sem( formativeModel_syntax, data = PoliticalDemocracy, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) formativeModelParameters &lt;- parameterEstimates(formativeModelFit) formativeModelParameters Here are the degrees of freedom: Code PT &lt;- lavaanify( formativeModel_syntax, fixed.x = TRUE, # sem() sets fixed.x = TRUE by default meanstructure = TRUE # estimator = &quot;MLR&quot; and missing = &quot;ML&quot; both set meanstructure = TRUE ) lav_partable_df(PT) [1] -5 Code formativeModelFit lavaan 0.6.17 ended normally after 1 iteration Estimator ML Optimization method NLMINB Number of model parameters 5 Used Total Number of observations 44 75 Number of missing patterns 1 Model Test User Model: Test statistic NA Degrees of freedom -5 P-value (Unknown) NA Here is a model diagram: Code semPaths( formativeModelFit, what = &quot;Std.all&quot;, layout = &quot;tree2&quot;, edge.label.cex = 0.8) Figure 7.6: Example of an Under-Identified Formative Model. For a formative model with 4 measures, the number of known values (14) is less than the number of parameters (19). The number of degrees of freedom is negative (\\(14 - 19 = -5\\)), thus the model is not able to be identified—the model is under-identified. Thus, for a formative model, we need more parameters than we have data—the model is under-identified. Therefore, to estimate a formative model with 4 indicators, we must add assumptions and other variables that are consequences of the formative construct. Options for identifying a formative construct are described by Treiblmaier et al. (2011). See below for an example formative model that is identified because of additional assumptions. Code formativeModel2_syntax &lt;- &#39; #Formative model factor loadings formative &lt;~ 1*v1 + v2 + v3 + v4 reflective =~ y1 + y2 + y3 + y4 formative ~~ 1*formative reflective ~ formative &#39; formativeModel2Fit &lt;- sem( formativeModel2_syntax, data = PoliticalDemocracy, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) formativeModel2Parameters &lt;- parameterEstimates(formativeModel2Fit) formativeModel2Parameters Code fitMeasures(formativeModel2Fit, &quot;df&quot;) df 14 Code semPaths( formativeModel2Fit, what = &quot;Std.all&quot;, layout = &quot;tree2&quot;, edge.label.cex = 0.8) Figure 7.7: Example of an Identified Formative Model. Thus, formative constructs are challenging to use in a SEM framework. To estimate a formative construct in a SEM framework, the formative construct must be used in the context of a model that allows some constraints. A formative latent factor includes a disturbance term, and is thus not entirely determined by the causal indicators (Bollen &amp; Bauldry, 2011). A composite (such as in principal component analysis), by contrast, has no disturbance term and is therefore completely determined by the composite indicators (Bollen &amp; Bauldry, 2011). Emerging techniques such as confirmatory composite analysis allow estimation of formative composites (Schuberth, 2023; Yu et al., 2023). Below is an example of confirmatory composite analysis using the Henseler-Ogasawara specification (adapted from: https://confirmatorycompositeanalysis.com/tutorials-lavaan; archived at: https://perma.cc/7LSU-PTZR) (Schuberth, 2023): Code formativeModel3_syntax &lt;- &#39; # Specification of the reflective latent factor reflective =~ y1 + y2 + y3 + y4 # Specification of the associations between the observed variables v1 - v4 # and the emergent variable &quot;formative&quot; in terms of composite loadings. formative =~ NA*v1 + l11*v1+ l21*v2 + 1*v3 + l41*v4 # Label the variance of the formative composite formative ~~ varformative*formative # Specification of the associations between the observed variables v1 - v4 # and their excrescent variables in terms of composite loadings. nu11 =~ 1*v1 + l22*v2 + l32*v3 + l42*v4 nu12 =~ 0*v1 + 1*v2 + l33*v3 + l43*v4 nu13 =~ 0*v1 + 0*v2 + l34*v3 + 1*v4 # Label the variances of the excrescent variables nu11 ~~ varnu11*nu11 nu12 ~~ varnu12*nu12 nu13 ~~ varnu13*nu13 # Specify the effect of formative on reflective reflective ~ formative # The H-O specification assumes that the excrescent variables are uncorrelated. # Therefore, the covariance between the excrescent variables is fixed to 0: nu11 ~~ 0*nu12 + 0*nu13 nu12 ~~ 0*nu13 # Moreover, the H-O specification assumes that the excrescent variables are uncorrelated # with the emergent and latent variables. Therefore, the covariances between # the emergent and the excrescent varibales are fixed to 0: formative ~~ 0*nu11 + 0*nu12 + 0*nu13 reflective =~ 0*nu11 + 0*nu12 + 0*nu13 # In lavaan, the =~ command is originally used to specify a common factor model, # which assumes that each observed variable is affected by a random measurement error. # It is assumed that the observed variables forming composites are free from # random measurement error. Therefore, the variances of the random measurement errors # originally attached to the observed variables by the common factor model are fixed to 0: v1 ~~ 0*v1 v2 ~~ 0*v2 v3 ~~ 0*v3 v4 ~~ 0*v4 # Calculate the unstandardized weights to form the formative latent variable w1 := (-l32 + l22*l33 + l34*l42 - l22*l34*l43)/( 1 - l11*l32 - l21*l33 + l11*l22*l33 - l34*l41 + l11*l34*l42 + l21* l34* l43 - l11* l22* l34* l43) w2 := (-l33 + l34*l43)/(1 - l11*l32 - l21*l33 + l11*l22*l33 - l34*l41 + l11*l34*l42 + l21*l34*l43 - l11*l22*l34*l43) w3 := 1/(1 - l11*l32 - l21*l33 + l11*l22*l33 - l34*l41 + l11*l34*l42 + l21*l34*l43 - l11*l22*l34*l43) w4 := -l34/(1 - l11*l32 - l21*l33 + l11*l22*l33 - l34*l41 + l11*l34*l42 + l21*l34*l43 - l11*l22*l34*l43) # Calculate the variances varv1 := l11^2*varformative + varnu11 varv2 := l21^2*varformative + l22^2*varnu11 + varnu12 varv3 := varformative + l32^2*varnu11 + l33^2*varnu12 + l34^2*varnu13 varv4 := l41^2*varformative + l42^2*varnu11 + l43^2*varnu12 + varnu13 # Calculate the standardized weights to form the formative latent variable w1std := w1*(varv1/varformative)^(1/2) w2std := w2*(varv2/varformative)^(1/2) w3std := w3*(varv3/varformative)^(1/2) w4std := w4*(varv4/varformative)^(1/2) &#39; formativeModel3Fit &lt;- sem( formativeModel3_syntax, data = PoliticalDemocracy, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) formativeModel3Parameters &lt;- parameterEstimates(formativeModel3Fit) formativeModel3Parameters Code fitMeasures(formativeModel3Fit, &quot;df&quot;) df 14 Code semPaths( formativeModel3Fit, what = &quot;Std.all&quot;, layout = &quot;tree2&quot;, edge.label.cex = 0.8) Below is an example of confirmatory composite analysis using the refined Henseler-Ogasawara specification (Yu et al., 2023): Code formativeModel4_syntax &lt;- &#39; # Specification of the reflective latent factor reflective =~ y1 + y2 + y3 + y4 # Specification of the associations between the observed variables v1 - v4 # and the emergent variable &quot;formative&quot; in terms of composite loadings. formative =~ NA*v1 + l11*v1 + l21*v2 + 1*v3 + l41*v4 # Label the variance of the formative composite formative ~~ varformative*formative # Specification of the associations between the observed variables v1 - v4 # and their excrescent variables in terms of composite loadings. nu1 =~ 1*v2 + l12*v1 nu2 =~ 1*v3 + l23*v2 nu3 =~ 1*v4 + l34*v3 # Label the variances of the excrescent variables nu1 ~~ varnu1*nu1 nu2 ~~ varnu2*nu2 nu3 ~~ varnu3*nu3 # Specify the effect of formative on reflective reflective ~ formative # Constrain the covariances between excrescent variables and # other variables in the structural model to zero. Moreover, # label the covariances among excrescent variables. nu1 ~~ 0*formative + 0*reflective + cov12*nu2 + cov13*nu3 nu2 ~~ 0*formative + 0*reflective + cov23*nu3 nu3 ~~ 0*formative + 0*reflective # Fix the variances of the disturbance terms to zero. v1 ~~ 0*v1 v2 ~~ 0*v2 v3 ~~ 0*v3 v4 ~~ 0*v4 # Calculate the unstandardized weights to form the formative latent variable w1 := ((1)*((1)*((1)))) / ((l11)*((1)*((1)*((1)))) + -(l21)*((l12)*((1)*((1)))) + (1)*((l12)*((l23)*((1)))) + -(l41)*((l12)*((l23)*((l34))))) w2 := -((l12)*((1)*((1)))) / ((l11)*((1)*((1)*((1)))) + -(l21)*((l12)*((1)*((1)))) + (1)*((l12)*((l23)*((1)))) + -(l41)*((l12)*((l23)*((l34))))) w3 := ((l12)*((l23)*((1)))) / ((l11)*((1)*((1)*((1)))) + -(l21)*((l12)*((1)*((1)))) + (1)*((l12)*((l23)*((1)))) + -(l41)*((l12)*((l23)*((l34))))) w4 := -((l12)*((l23)*((l34)))) / ((l11)*((1)*((1)*((1)))) + -(l21)*((l12)*((1)*((1)))) + (1)*((l12)*((l23)*((1)))) + -(l41)*((l12)*((l23)*((l34))))) # Calculate the variances varv1 := ((l11) * (varformative)) * (l11) + ((l12) * (varnu1)) * (l12) varv2 := ((l21) * (varformative)) * (l21) + ((1) * (varnu1) + (l23) * (cov12)) * (1) + ((1) * (cov12) + (l23) * (varnu2)) * (l23) varv3 := ((1) * (varformative)) * (1) + ((1) * (varnu2) + (l34) * (cov23)) * (1) + ((1) * (cov23) + (l34) * (varnu3)) * (l34) varv4 := ((l41) * (varformative)) * (l41) + ((1) * (varnu3)) * (1) # Calculate the standardized weights to form the formative latent variable wstdv1 := ((w1) * (sqrt(varv1))) * (1/sqrt(varformative)) wstdv2 := ((w2) * (sqrt(varv2))) * (1/sqrt(varformative)) wstdv3 := ((w3) * (sqrt(varv3))) * (1/sqrt(varformative)) wstdv4 := ((w4) * (sqrt(varv4))) * (1/sqrt(varformative)) &#39; formativeModel4Fit &lt;- sem( formativeModel4_syntax, data = PoliticalDemocracy, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) formativeModel4Parameters &lt;- parameterEstimates(formativeModel4Fit) formativeModel4Parameters Code fitMeasures(formativeModel4Fit, &quot;df&quot;) df 14 Code semPaths( formativeModel4Fit, what = &quot;Std.all&quot;, layout = &quot;tree2&quot;, edge.label.cex = 0.8) You can generate the weights for the indicators (to be used in the model syntax) for the refined Henseler-Ogasawara specification using the following code: Code library(calculus) # First, construct the loading matrix loadingMatrix &lt;- matrix(c(&#39;l11&#39;,&#39;l21&#39;,1,&#39;l41&#39;,&#39;l12&#39;,1,0,0,0,&#39;l23&#39;,1,0,0,0,&#39;l34&#39;,1),4,4) # Check the structure loadingMatrix # Invert matrix, the first row contains the (unstandardized) weights # these can be copy and pasted to the lavaan model to specify the weights as new parameters mxinv(loadingMatrix) Florian Schubert provides an R function to create the full lavaan syntax for confirmatory composite analysis at the following link: https://github.com/FloSchuberth/HOspecification 7.5 Additional Types of SEM Up to this point, we have discussed SEM with dimensional constructs. It also worth knowing about additional types of SEM models, including latent class models and mixture models, that handle categorical constructs. However, most disorders are more accurately conceptualized as dimensional than as categorical (Markon et al., 2011), so just because you can estimate categorical latent factors does not necessarily mean that one should. 7.5.1 Latent Class Models In latent class models, the construct is not dimensional, but rather categorical. The categorical constructs are latent classifications and are called latent classes. For instance, the construct could be a diagnosis that influences scores on the measures. Latent class models examine qualitative differences in kind, rather than quantitative differences in degree. 7.5.2 Mixture Models Mixture models allow for a combination of latent categorical constructs (classes) and latent dimensional constructs. That is, it allows for both qualitative and quantitative differences. However, this additional model complexity also necessitates a larger sample size for estimation. SEM generally requires a 3-digit sample size (\\(N = 100+\\)), whereas mixture models typically require a 4- or 5-digit sample size (\\(N = 1,000+\\)). 7.5.3 Exploratory Structural Equation Models We describe exploratory structural equation models in Section 14.1.4.3.3. 7.6 Causal Diagrams: Directed Acyclic Graphs A key tool when designing a structural equation model is a conceptual depiction of the hypothesized causal processes. A causal diagram depicts the hypothesized causal processes that link two or more variables. A common form of causal diagrams is the directed acyclic graph (DAG). DAGs provide a helpful tool to communicate about causal questions and help identify how to avoid bias (i.e., over-estimation) in associations between variables due to confounding (i.e., common causes) (Digitale et al., 2022). Free tools to create DAGs include the R package dagitty (Textor et al., 2017) and the associated browser-based extension, DAGitty: https://dagitty.net (archived at https://perma.cc/U9BY-VZE2). Path analytic diagrams (i.e., causal diagrams with boxes, circles, and lines) are described in Section 4.1.1 of Chapter 4. 7.7 Model Fit Indices Various model fit indices can be used for evaluating how well a model fits the data and for comparing the fit of two competing models. Fit indices known as absolute fit indices compare whether the model fits better than the best-possible fitting model (i.e., a saturated model). Examples of absolute fit indices include the chi-square test, root mean square error of approximation (RMSEA), and the standardized root mean square residual (SRMR). The chi-square test evaluates whether the model has a significant degree of misfit relative to the best-possible fitting model (a saturated model that fits as many parameters as possible; i.e., as many parameters as there are degrees of freedom); the null hypothesis of a chi-square test is that there is no difference between the predicted data (i.e., the data that would be observed if the model were true) and the observed data. Thus, a non-significant chi-square test indicates good model fit. However, because the null hypothesis of the chi-square test is that the model-implied covariance matrix is exactly equal to the observed covariance matrix (i.e., a model of perfect fit), this may be an unrealistic comparison. Models are simplifications of reality, and our models are virtually never expected to be a perfect description of reality. Thus, we would say a model is “useful” and partially validated if “it helps us to understand the relation between variables and does a ‘reasonable’ job of matching the data…A perfect fit may be an inappropriate standard, and a high chi-square estimate may indicate what we already know—that the hypothesized model holds approximately, not perfectly.” (Bollen, 1989, p. 268). The power of the chi-square test depends on sample size, and a large sample will likely detect small differences as significantly worse than the best-possible fitting model (Bollen, 1989). RMSEA is an index of absolute fit. Lower values indicate better fit. SRMR is an index of absolute fit with no penalty for model complexity. Lower values indicate better fit. There are also various fit indices known as incremental, comparative, or relative fit indices that compare whether the model fits better than the worst-possible fitting model (i.e., a “baseline” or “null” model). Incremental fit indices include a chi-square difference test, the comparative fit index (CFI), and the Tucker-Lewis index (TLI). Unlike the chi-square test comparing the model to the best-possible fitting model, a significant chi-square test of the relative fit index indicates better fit—i.e., that the model fits better than the worst-possible fitting model. CFI is another relative fit index that compares the model to the worst-possible fitting model. Higher values indicate better fit. TLI is another relative fit index. Higher values indicate better fit. Parsimony fit include fit indices that use information criteria fit indices, including the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). BIC penalizes model complexity more so than AIC. Lower AIC and BIC values indicate better fit. Chi-square difference tests and CFI can be used to compare two nested models. AIC and BIC can be used to compare two non-nested models. Criteria for acceptable fit and good fit of SEM models are in Table 7.1. In addition, dynamic fit indexes have been proposed based on simulation to identify fit index cutoffs that are tailored to the characteristics of the specific model and data (McNeish &amp; Wolf, 2023). Table 7.1: Criteria for Acceptable and Good Fit of Structural Equation Models Based on Fit Indices. SEM Fit Index Acceptable Fit Good Fit RMSEA \\(\\leq\\) .08 \\(\\leq\\) .05 CFI \\(\\geq\\) .90 \\(\\geq\\) .95 TLI \\(\\geq\\) .90 \\(\\geq\\) .95 SRMR \\(\\leq\\) .10 \\(\\leq\\) .08 However, good model fit does not necessarily indicate a true model. In addition to global fit indices, it can also be helpful to examine evidence of local fit, such as the residual covariance matrix. The residual covariance matrix represents the difference between the observed covariance matrix and the model-implied covariance matrix (the observed covariance matrix minus the model-implied covariance matrix). Standardizing the covariance matrix by converting each to a correlation matrix can be helpful for interpreting the magnitude of any local misfit. This is known as a residual correlation matrix. Correlation residuals greater than |.10| are possible evidence for poor local fit (Kline, 2023). If a correlation residual is positive, it suggests that the model underpredicts the observed association between the two variables (i.e., the observed covariance is greater than the model-implied covariance). If a correlation residual is negative, it suggests that the model overpredicts their observed association between the two variables (i.e., the observed covariance is smaller than the model-implied covariance). If the two variables are connected by only indirect pathways, it may be helpful to respecify the model with direct pathways between the two variables, such as a direct effect (i.e., regression path) or a covariance path. 7.8 Correlation Matrix Code cor(mydataSEM, use = &quot;pairwise.complete.obs&quot;) measure1 measure2 measure3 measure1 1.0000000 0.5444728 0.6782616 measure2 0.5444728 1.0000000 0.7766733 measure3 0.6782616 0.7766733 1.0000000 Correlation matrices of various types using the cor.table() function from the petersenlab package (Petersen, 2024b) are in Tables 7.2, 7.3, and 7.4. Code cor.table(mydataSEM, dig = 2) cor.table(mydataSEM, type = &quot;manuscript&quot;, dig = 2) cor.table(mydataSEM, type = &quot;manuscriptBig&quot;, dig = 2) Table 7.2: Correlation Matrix with r, n, and p-values. measure1 measure2 measure3 1. measure1.r 1.00 .54*** .68*** 2. sig NA .00 .00 3. n 298 297 298 4. measure2.r .54*** 1.00 .78*** 5. sig .00 NA .00 6. n 297 298 298 7. measure3.r .68*** .78*** 1.00 8. sig .00 .00 NA 9. n 298 298 299 Table 7.3: Correlation Matrix with Asterisks for Significant Associations. measure1 measure2 measure3 1. measure1 1.00 2. measure2 .54*** 1.00 3. measure3 .68*** .78*** 1.00 Table 7.4: Correlation Matrix. measure1 measure2 measure3 1. measure1 1.00 2. measure2 .54 1.00 3. measure3 .68 .78 1.00 7.9 Measurement Model (of a Given Construct) Even though CFA models are measurement models, I provide separate examples of a measurement model and CFA models in my examples because CFA is often used to test competing factor structures. For instance, you could use CFA to test whether the variance in several measures’ scores is best explained with one factor or two factors. In the measurement model below, I present a simple one-factor model with three measures. The measurement model is what we settle on as the estimation of each construct before we add the structural component to estimate the relations among latent variables. Basically, we add the structural component onto the measurement model. In Section 7.10, I present a CFA model with multiple latent factors. The measurement models were fit in the lavaan package (Rosseel et al., 2022). 7.9.1 Specify the Model Code measurementModel_syntax &lt;- &#39; #Factor loadings latentFactor =~ measure1 + measure2 + measure3 &#39; measurementModel_fullSyntax &lt;- &#39; #Factor loadings (free the factor loading of the first indicator) latentFactor =~ NA*measure1 + measure2 + measure3 #Fix latent mean to zero latentFactor ~ 0 #Fix latent variance to one latentFactor ~~ 1*latentFactor #Estimate covariances among latent variables (not applicable because there is only one latent variable) #Estimate residual variances of manifest variables measure1 ~~ measure1 measure2 ~~ measure2 measure3 ~~ measure3 #Free intercepts of manifest variables measure1 ~ int1*1 measure2 ~ int2*1 measure3 ~ int3*1 &#39; 7.9.1.1 Summary of Model Features Code summary(measurementModel_syntax) Length Class Mode 1 character character Code summary(measurementModel_fullSyntax) Length Class Mode 1 character character 7.9.1.2 Model Syntax in Table Form: Code lavaanify(measurementModel_syntax) Code lavaanify(measurementModel_fullSyntax) 7.9.2 Fit the Model Code measurementModelFit &lt;- cfa( measurementModel_syntax, data = mydataSEM, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, std.lv = TRUE) measurementModelFit_full &lt;- lavaan( measurementModel_fullSyntax, data = mydataSEM, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) 7.9.3 Display Summary Output This measurement model with three indicators is just-identified—the number of parameters estimated is equal to the number of known values, thus leaving 0 degrees of freedom. In the model, all three indicators load strongly on the latent factor (measure 1: \\(\\beta = 0.69\\); measure 2: \\(\\beta = 0.79\\); measure 3: \\(\\beta = 0.98\\)). Thus, the loadings of this measurement model would be consistent with a reflective latent construct. In terms of interpretation, all three indicators loaded positively on the latent factor, so higher levels of the latent factor are indicated by higher levels on the indicators. However, one of the estimated observed variances is negative, so the model is not able to be estimated accurately. Thus, we would need to make additional adjustments in order to estimate the model. Code summary( measurementModelFit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 23 iterations Estimator ML Optimization method NLMINB Number of model parameters 9 Used Total Number of observations 299 300 Number of missing patterns 3 Model Test User Model: Standard Scaled Test Statistic 0.000 0.000 Degrees of freedom 0 0 Model Test Baseline Model: Test statistic 459.500 389.896 Degrees of freedom 3 3 P-value 0.000 0.000 Scaling correction factor 1.179 User Model versus Baseline Model: Comparative Fit Index (CFI) 1.000 1.000 Tucker-Lewis Index (TLI) 1.000 1.000 Robust Comparative Fit Index (CFI) 1.000 Robust Tucker-Lewis Index (TLI) 1.000 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -3588.224 -3588.224 Loglikelihood unrestricted model (H1) -3588.224 -3588.224 Akaike (AIC) 7194.449 7194.449 Bayesian (BIC) 7227.753 7227.753 Sample-size adjusted Bayesian (SABIC) 7199.210 7199.210 Root Mean Square Error of Approximation: RMSEA 0.000 NA 90 Percent confidence interval - lower 0.000 NA 90 Percent confidence interval - upper 0.000 NA P-value H_0: RMSEA &lt;= 0.050 NA NA P-value H_0: RMSEA &gt;= 0.080 NA NA Robust RMSEA 0.000 90 Percent confidence interval - lower 0.000 90 Percent confidence interval - upper 0.000 P-value H_0: Robust RMSEA &lt;= 0.050 NA P-value H_0: Robust RMSEA &gt;= 0.080 NA Standardized Root Mean Square Residual: SRMR 0.000 0.000 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all latentFactor =~ measure1 7.198 0.575 12.525 0.000 7.198 0.689 measure2 13.487 0.855 15.780 0.000 13.487 0.789 measure3 28.122 1.343 20.943 0.000 28.122 0.984 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .measure1 50.136 0.605 82.859 0.000 50.136 4.797 .measure2 50.489 0.990 51.024 0.000 50.489 2.952 .measure3 99.720 1.652 60.361 0.000 99.720 3.491 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .measure1 57.444 5.116 11.228 0.000 57.444 0.526 .measure2 110.536 12.756 8.665 0.000 110.536 0.378 .measure3 25.203 42.395 0.594 0.552 25.203 0.031 latentFactor 1.000 1.000 1.000 R-Square: Estimate measure1 0.474 measure2 0.622 measure3 0.969 Code summary( measurementModelFit_full, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 23 iterations Estimator ML Optimization method NLMINB Number of model parameters 9 Used Total Number of observations 299 300 Number of missing patterns 3 Model Test User Model: Standard Scaled Test Statistic 0.000 0.000 Degrees of freedom 0 0 Model Test Baseline Model: Test statistic 459.500 389.896 Degrees of freedom 3 3 P-value 0.000 0.000 Scaling correction factor 1.179 User Model versus Baseline Model: Comparative Fit Index (CFI) 1.000 1.000 Tucker-Lewis Index (TLI) 1.000 1.000 Robust Comparative Fit Index (CFI) 1.000 Robust Tucker-Lewis Index (TLI) 1.000 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -3588.224 -3588.224 Loglikelihood unrestricted model (H1) -3588.224 -3588.224 Akaike (AIC) 7194.449 7194.449 Bayesian (BIC) 7227.753 7227.753 Sample-size adjusted Bayesian (SABIC) 7199.210 7199.210 Root Mean Square Error of Approximation: RMSEA 0.000 NA 90 Percent confidence interval - lower 0.000 NA 90 Percent confidence interval - upper 0.000 NA P-value H_0: RMSEA &lt;= 0.050 NA NA P-value H_0: RMSEA &gt;= 0.080 NA NA Robust RMSEA 0.000 90 Percent confidence interval - lower 0.000 90 Percent confidence interval - upper 0.000 P-value H_0: Robust RMSEA &lt;= 0.050 NA P-value H_0: Robust RMSEA &gt;= 0.080 NA Standardized Root Mean Square Residual: SRMR 0.000 0.000 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all latentFactor =~ measure1 7.198 0.575 12.525 0.000 7.198 0.689 measure2 13.487 0.855 15.780 0.000 13.487 0.789 measure3 28.122 1.343 20.943 0.000 28.122 0.984 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ltntFct 0.000 0.000 0.000 .measur1 (int1) 50.136 0.605 82.859 0.000 50.136 4.797 .measur2 (int2) 50.489 0.990 51.024 0.000 50.489 2.952 .measur3 (int3) 99.720 1.652 60.361 0.000 99.720 3.491 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all latentFactor 1.000 1.000 1.000 .measure1 57.444 5.116 11.228 0.000 57.444 0.526 .measure2 110.536 12.756 8.665 0.000 110.536 0.378 .measure3 25.203 42.395 0.594 0.552 25.203 0.031 R-Square: Estimate measure1 0.474 measure2 0.622 measure3 0.969 7.9.4 Estimates of Model Fit You can extract specific fit indices using the following syntax: Code fitMeasures( measurementModelFit, fit.measures = c( &quot;chisq&quot;, &quot;df&quot;, &quot;pvalue&quot;, &quot;chisq.scaled&quot;, &quot;df.scaled&quot;, &quot;pvalue.scaled&quot;, &quot;chisq.scaling.factor&quot;, &quot;baseline.chisq&quot;,&quot;baseline.df&quot;,&quot;baseline.pvalue&quot;, &quot;rmsea&quot;, &quot;cfi&quot;, &quot;tli&quot;, &quot;srmr&quot;, &quot;rmsea.robust&quot;, &quot;cfi.robust&quot;, &quot;tli.robust&quot;)) chisq df pvalue 0.0 0.0 NA chisq.scaled df.scaled pvalue.scaled 0.0 0.0 NA chisq.scaling.factor baseline.chisq baseline.df NA 459.5 3.0 baseline.pvalue rmsea cfi 0.0 0.0 1.0 tli srmr rmsea.robust 1.0 0.0 0.0 cfi.robust tli.robust 1.0 1.0 Because the model is just-identified, many fit statistics are not able to be estimated. 7.9.5 Residuals Code residuals(measurementModelFit, type = &quot;cor&quot;) $type [1] &quot;cor.bollen&quot; $cov measr1 measr2 measr3 measure1 0 measure2 0 0 measure3 0 0 0 $mean measure1 measure2 measure3 0 0 0 7.9.6 Modification Indices Code modificationindices(measurementModelFit, sort. = TRUE) 7.9.7 Factor Scores Code measurementModelFit_factorScores &lt;- lavPredict(measurementModelFit) 7.9.8 Internal Consistency Reliability Internal consistency reliability of items composing the latent factors, as quantified by omega (\\(\\omega\\)) and average variance extracted (AVE), was estimated using the semTools package (Jorgensen et al., 2021). Code compRelSEM(measurementModelFit) latentFactor 0.925 Code AVE(measurementModelFit) latentFactor 0.841 7.9.9 Path Diagram A path diagram of the model is in Figure 7.8. Code semPaths( measurementModelFit, what = &quot;Std.all&quot;, layout = &quot;tree2&quot;, edge.label.cex = 2) Figure 7.8: Measurement Model. 7.10 Confirmatory Factor Analysis (CFA) The confirmatory factor analysis (CFA) models were fit in the lavaan package (Rosseel et al., 2022). The examples were adapted from the lavaan documentation: https://lavaan.ugent.be/tutorial/cfa.html (archived at https://perma.cc/GKY3-9YE4). In this CFA model, we estimate three latent factors with three indicators loading on each latent factor. 7.10.1 Specify the Model Code cfaModel_syntax &lt;- &#39; #Factor loadings visual =~ x1 + x2 + x3 textual =~ x4 + x5 + x6 speed =~ x7 + x8 + x9 &#39; cfaModel_fullSyntax &lt;- &#39; #Factor loadings (free the factor loading of the first indicator) visual =~ NA*x1 + x2 + x3 textual =~ NA*x4 + x5 + x6 speed =~ NA*x7 + x8 + x9 #Fix latent means to zero visual ~ 0 textual ~ 0 speed ~ 0 #Fix latent variances to one visual ~~ 1*visual textual ~~ 1*textual speed ~~ 1*speed #Estimate covariances among latent variables visual ~~ textual visual ~~ speed textual ~~ speed #Estimate residual variances of manifest variables x1 ~~ x1 x2 ~~ x2 x3 ~~ x3 x4 ~~ x4 x5 ~~ x5 x6 ~~ x6 x7 ~~ x7 x8 ~~ x8 x9 ~~ x9 #Free intercepts of manifest variables x1 ~ int1*1 x2 ~ int2*1 x3 ~ int3*1 x4 ~ int4*1 x5 ~ int5*1 x6 ~ int6*1 x7 ~ int7*1 x8 ~ int8*1 x9 ~ int9*1 &#39; 7.10.1.1 Model Syntax in Table Form: Code lavaanify(cfaModel_syntax) Code lavaanify(cfaModel_fullSyntax) 7.10.2 Fit the Model Code cfaModelFit &lt;- cfa( cfaModel_syntax, data = HolzingerSwineford1939, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, std.lv = TRUE) cfaModelFit_full &lt;- lavaan( cfaModel_fullSyntax, data = HolzingerSwineford1939, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) 7.10.3 Display Summary Output In this model, all nine indicators load strongly on their respective latent factor. Thus, this measurement model would be defensible. In terms of interpretation, all indicators load positively on their respective latent factor, so higher levels of the latent factor are indicated by higher levels on the indicators. Code summary( cfaModelFit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 42 iterations Estimator ML Optimization method NLMINB Number of model parameters 30 Number of observations 301 Number of missing patterns 1 Model Test User Model: Standard Scaled Test Statistic 85.306 87.132 Degrees of freedom 24 24 P-value (Chi-square) 0.000 0.000 Scaling correction factor 0.979 Yuan-Bentler correction (Mplus variant) Model Test Baseline Model: Test statistic 918.852 880.082 Degrees of freedom 36 36 P-value 0.000 0.000 Scaling correction factor 1.044 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.931 0.925 Tucker-Lewis Index (TLI) 0.896 0.888 Robust Comparative Fit Index (CFI) 0.932 Robust Tucker-Lewis Index (TLI) 0.897 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -3737.745 -3737.745 Scaling correction factor 1.093 for the MLR correction Loglikelihood unrestricted model (H1) -3695.092 -3695.092 Scaling correction factor 1.043 for the MLR correction Akaike (AIC) 7535.490 7535.490 Bayesian (BIC) 7646.703 7646.703 Sample-size adjusted Bayesian (SABIC) 7551.560 7551.560 Root Mean Square Error of Approximation: RMSEA 0.092 0.093 90 Percent confidence interval - lower 0.071 0.073 90 Percent confidence interval - upper 0.114 0.115 P-value H_0: RMSEA &lt;= 0.050 0.001 0.001 P-value H_0: RMSEA &gt;= 0.080 0.840 0.862 Robust RMSEA 0.091 90 Percent confidence interval - lower 0.070 90 Percent confidence interval - upper 0.113 P-value H_0: Robust RMSEA &lt;= 0.050 0.001 P-value H_0: Robust RMSEA &gt;= 0.080 0.820 Standardized Root Mean Square Residual: SRMR 0.060 0.060 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual =~ x1 0.900 0.100 8.973 0.000 0.900 0.772 x2 0.498 0.088 5.681 0.000 0.498 0.424 x3 0.656 0.080 8.151 0.000 0.656 0.581 textual =~ x4 0.990 0.061 16.150 0.000 0.990 0.852 x5 1.102 0.055 20.146 0.000 1.102 0.855 x6 0.917 0.058 15.767 0.000 0.917 0.838 speed =~ x7 0.619 0.086 7.193 0.000 0.619 0.570 x8 0.731 0.093 7.875 0.000 0.731 0.723 x9 0.670 0.099 6.761 0.000 0.670 0.665 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual ~~ textual 0.459 0.073 6.258 0.000 0.459 0.459 speed 0.471 0.119 3.954 0.000 0.471 0.471 textual ~~ speed 0.283 0.085 3.311 0.001 0.283 0.283 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 4.936 0.067 73.473 0.000 4.936 4.235 .x2 6.088 0.068 89.855 0.000 6.088 5.179 .x3 2.250 0.065 34.579 0.000 2.250 1.993 .x4 3.061 0.067 45.694 0.000 3.061 2.634 .x5 4.341 0.074 58.452 0.000 4.341 3.369 .x6 2.186 0.063 34.667 0.000 2.186 1.998 .x7 4.186 0.063 66.766 0.000 4.186 3.848 .x8 5.527 0.058 94.854 0.000 5.527 5.467 .x9 5.374 0.058 92.546 0.000 5.374 5.334 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 0.549 0.156 3.509 0.000 0.549 0.404 .x2 1.134 0.112 10.135 0.000 1.134 0.821 .x3 0.844 0.100 8.419 0.000 0.844 0.662 .x4 0.371 0.050 7.382 0.000 0.371 0.275 .x5 0.446 0.057 7.870 0.000 0.446 0.269 .x6 0.356 0.047 7.658 0.000 0.356 0.298 .x7 0.799 0.097 8.222 0.000 0.799 0.676 .x8 0.488 0.120 4.080 0.000 0.488 0.477 .x9 0.566 0.119 4.768 0.000 0.566 0.558 visual 1.000 1.000 1.000 textual 1.000 1.000 1.000 speed 1.000 1.000 1.000 R-Square: Estimate x1 0.596 x2 0.179 x3 0.338 x4 0.725 x5 0.731 x6 0.702 x7 0.324 x8 0.523 x9 0.442 Code summary( cfaModelFit_full, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 42 iterations Estimator ML Optimization method NLMINB Number of model parameters 30 Number of observations 301 Number of missing patterns 1 Model Test User Model: Standard Scaled Test Statistic 85.306 87.132 Degrees of freedom 24 24 P-value (Chi-square) 0.000 0.000 Scaling correction factor 0.979 Yuan-Bentler correction (Mplus variant) Model Test Baseline Model: Test statistic 918.852 880.082 Degrees of freedom 36 36 P-value 0.000 0.000 Scaling correction factor 1.044 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.931 0.925 Tucker-Lewis Index (TLI) 0.896 0.888 Robust Comparative Fit Index (CFI) 0.932 Robust Tucker-Lewis Index (TLI) 0.897 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -3737.745 -3737.745 Scaling correction factor 1.093 for the MLR correction Loglikelihood unrestricted model (H1) -3695.092 -3695.092 Scaling correction factor 1.043 for the MLR correction Akaike (AIC) 7535.490 7535.490 Bayesian (BIC) 7646.703 7646.703 Sample-size adjusted Bayesian (SABIC) 7551.560 7551.560 Root Mean Square Error of Approximation: RMSEA 0.092 0.093 90 Percent confidence interval - lower 0.071 0.073 90 Percent confidence interval - upper 0.114 0.115 P-value H_0: RMSEA &lt;= 0.050 0.001 0.001 P-value H_0: RMSEA &gt;= 0.080 0.840 0.862 Robust RMSEA 0.091 90 Percent confidence interval - lower 0.070 90 Percent confidence interval - upper 0.113 P-value H_0: Robust RMSEA &lt;= 0.050 0.001 P-value H_0: Robust RMSEA &gt;= 0.080 0.820 Standardized Root Mean Square Residual: SRMR 0.060 0.060 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual =~ x1 0.900 0.100 8.973 0.000 0.900 0.772 x2 0.498 0.088 5.681 0.000 0.498 0.424 x3 0.656 0.080 8.151 0.000 0.656 0.581 textual =~ x4 0.990 0.061 16.150 0.000 0.990 0.852 x5 1.102 0.055 20.146 0.000 1.102 0.855 x6 0.917 0.058 15.767 0.000 0.917 0.838 speed =~ x7 0.619 0.086 7.193 0.000 0.619 0.570 x8 0.731 0.093 7.875 0.000 0.731 0.723 x9 0.670 0.099 6.761 0.000 0.670 0.665 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual ~~ textual 0.459 0.073 6.258 0.000 0.459 0.459 speed 0.471 0.119 3.954 0.000 0.471 0.471 textual ~~ speed 0.283 0.085 3.311 0.001 0.283 0.283 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual 0.000 0.000 0.000 textual 0.000 0.000 0.000 speed 0.000 0.000 0.000 .x1 (int1) 4.936 0.067 73.473 0.000 4.936 4.235 .x2 (int2) 6.088 0.068 89.855 0.000 6.088 5.179 .x3 (int3) 2.250 0.065 34.579 0.000 2.250 1.993 .x4 (int4) 3.061 0.067 45.694 0.000 3.061 2.634 .x5 (int5) 4.341 0.074 58.452 0.000 4.341 3.369 .x6 (int6) 2.186 0.063 34.667 0.000 2.186 1.998 .x7 (int7) 4.186 0.063 66.766 0.000 4.186 3.848 .x8 (int8) 5.527 0.058 94.854 0.000 5.527 5.467 .x9 (int9) 5.374 0.058 92.546 0.000 5.374 5.334 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual 1.000 1.000 1.000 textual 1.000 1.000 1.000 speed 1.000 1.000 1.000 .x1 0.549 0.156 3.509 0.000 0.549 0.404 .x2 1.134 0.112 10.135 0.000 1.134 0.821 .x3 0.844 0.100 8.419 0.000 0.844 0.662 .x4 0.371 0.050 7.382 0.000 0.371 0.275 .x5 0.446 0.057 7.870 0.000 0.446 0.269 .x6 0.356 0.047 7.658 0.000 0.356 0.298 .x7 0.799 0.097 8.222 0.000 0.799 0.676 .x8 0.488 0.120 4.080 0.000 0.488 0.477 .x9 0.566 0.119 4.768 0.000 0.566 0.558 R-Square: Estimate x1 0.596 x2 0.179 x3 0.338 x4 0.725 x5 0.731 x6 0.702 x7 0.324 x8 0.523 x9 0.442 7.10.4 Estimates of Model Fit According to model fit estimates, the model fit is good according to SRMR and acceptable according to CFI, but the model fit is weaker according to RMSEA and TLI. Thus, we may want to consider adjustments to improve the model fit. In general, we want to make decisions regarding what parameters to estimate based on theory in conjunction with empiricism. Code fitMeasures( cfaModelFit, fit.measures = c( &quot;chisq&quot;, &quot;df&quot;, &quot;pvalue&quot;, &quot;chisq.scaled&quot;, &quot;df.scaled&quot;, &quot;pvalue.scaled&quot;, &quot;chisq.scaling.factor&quot;, &quot;baseline.chisq&quot;,&quot;baseline.df&quot;,&quot;baseline.pvalue&quot;, &quot;rmsea&quot;, &quot;cfi&quot;, &quot;tli&quot;, &quot;srmr&quot;, &quot;rmsea.robust&quot;, &quot;cfi.robust&quot;, &quot;tli.robust&quot;)) chisq df pvalue 85.306 24.000 0.000 chisq.scaled df.scaled pvalue.scaled 87.132 24.000 0.000 chisq.scaling.factor baseline.chisq baseline.df 0.979 918.852 36.000 baseline.pvalue rmsea cfi 0.000 0.092 0.931 tli srmr rmsea.robust 0.896 0.060 0.091 cfi.robust tli.robust 0.932 0.897 7.10.5 Residuals Code residuals(cfaModelFit, type = &quot;cor&quot;) $type [1] &quot;cor.bollen&quot; $cov x1 x2 x3 x4 x5 x6 x7 x8 x9 x1 0.000 x2 -0.030 0.000 x3 -0.008 0.094 0.000 x4 0.071 -0.012 -0.068 0.000 x5 -0.009 -0.027 -0.151 0.005 0.000 x6 0.060 0.030 -0.026 -0.009 0.003 0.000 x7 -0.140 -0.189 -0.084 0.037 -0.036 -0.014 0.000 x8 -0.039 -0.052 -0.012 -0.067 -0.036 -0.022 0.075 0.000 x9 0.149 0.073 0.147 0.048 0.067 0.056 -0.038 -0.032 0.000 $mean x1 x2 x3 x4 x5 x6 x7 x8 x9 0 0 0 0 0 0 0 0 0 7.10.6 Modification Indices Modification indices indicate potential additional parameters that could be estimated and that would improve model fit. For instance, the modification indices in Table ?? (generated from the syntax below) indicate a few additional factor loadings (i.e., cross-loadings) or correlated residuals that could substantially improve model fit. However, it is generally not recommended to blindly estimate additional parameters solely based on modification indices. Rather, it is generally advised to consider modification indices in light of theory. Code modificationindices(cfaModelFit, sort. = TRUE) 7.10.7 Factor Scores Code cfaModelFit_factorScores &lt;- lavPredict(cfaModelFit) 7.10.8 Internal Consistency Reliability Internal consistency reliability of items composing the latent factors, as quantified by omega (\\(\\omega\\)) and average variance extracted (AVE), was estimated using the semTools package (Jorgensen et al., 2021). Code compRelSEM(cfaModelFit) visual textual speed 0.612 0.885 0.686 Code AVE(cfaModelFit) visual textual speed 0.371 0.721 0.424 7.10.9 Path Diagram Below is a path diagram of the model generated using the semPlot package (Epskamp, 2022). Code semPaths( cfaModelFit, what = &quot;Std.all&quot;, layout = &quot;tree2&quot;, edge.label.cex = 0.8) Figure 7.9: Confirmatory Factor Analysis Model. 7.10.10 Modify Model Based on Modification Indices In the model below, I modified the model based on estimating an additional factor loading (assuming the additional factor loading is theoretically supported). This could be supported, for instance, if a given test involves considerable skills in both the visual domain and in speed of processing. When the same indicator loads simultaneously on two factors, this is called a cross-loading. Cross-loadings can complicate the interpretation of latent factors, as discussed in Section 14.1.4.5 of the chapter on factor analysis. 7.10.10.1 Specify the Model Code cfaModel2_syntax &lt;- &#39; #Factor loadings textual =~ x4 + x5 + x6 visual =~ x1 + x2 + x3 + x9 speed =~ x7 + x8 + x9 &#39; cfaModel2_fullSyntax &lt;- &#39; #Factor loadings (free the factor loading of the first indicator) textual =~ NA*x4 + x5 + x6 visual =~ NA*x1 + x2 + x3 + x9 speed =~ NA*x7 + x8 + x9 #Fix latent means to zero visual ~ 0 textual ~ 0 speed ~ 0 #Fix latent variances to one visual ~~ 1*visual textual ~~ 1*textual speed ~~ 1*speed #Estimate covariances among latent variables visual ~~ textual visual ~~ speed textual ~~ speed #Estimate residual variances of manifest variables x1 ~~ x1 x2 ~~ x2 x3 ~~ x3 x4 ~~ x4 x5 ~~ x5 x6 ~~ x6 x7 ~~ x7 x8 ~~ x8 x9 ~~ x9 #Free intercepts of manifest variables x1 ~ int1*1 x2 ~ int2*1 x3 ~ int3*1 x4 ~ int4*1 x5 ~ int5*1 x6 ~ int6*1 x7 ~ int7*1 x8 ~ int8*1 x9 ~ int9*1 &#39; 7.10.10.2 Fit the Model Code cfaModel2Fit &lt;- cfa( cfaModel2_syntax, data = HolzingerSwineford1939, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, std.lv = TRUE) cfaModel2Fit_full &lt;- lavaan( cfaModel2_fullSyntax, data = HolzingerSwineford1939, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) 7.10.10.3 Display Summary Output Code summary( cfaModel2Fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 39 iterations Estimator ML Optimization method NLMINB Number of model parameters 31 Number of observations 301 Number of missing patterns 1 Model Test User Model: Standard Scaled Test Statistic 52.382 51.725 Degrees of freedom 23 23 P-value (Chi-square) 0.000 0.001 Scaling correction factor 1.013 Yuan-Bentler correction (Mplus variant) Model Test Baseline Model: Test statistic 918.852 880.082 Degrees of freedom 36 36 P-value 0.000 0.000 Scaling correction factor 1.044 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.967 0.966 Tucker-Lewis Index (TLI) 0.948 0.947 Robust Comparative Fit Index (CFI) 0.968 Robust Tucker-Lewis Index (TLI) 0.950 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -3721.283 -3721.283 Scaling correction factor 1.065 for the MLR correction Loglikelihood unrestricted model (H1) -3695.092 -3695.092 Scaling correction factor 1.043 for the MLR correction Akaike (AIC) 7504.566 7504.566 Bayesian (BIC) 7619.487 7619.487 Sample-size adjusted Bayesian (SABIC) 7521.172 7521.172 Root Mean Square Error of Approximation: RMSEA 0.065 0.064 90 Percent confidence interval - lower 0.042 0.041 90 Percent confidence interval - upper 0.089 0.088 P-value H_0: RMSEA &lt;= 0.050 0.133 0.143 P-value H_0: RMSEA &gt;= 0.080 0.158 0.144 Robust RMSEA 0.064 90 Percent confidence interval - lower 0.040 90 Percent confidence interval - upper 0.088 P-value H_0: Robust RMSEA &lt;= 0.050 0.156 P-value H_0: Robust RMSEA &gt;= 0.080 0.147 Standardized Root Mean Square Residual: SRMR 0.041 0.041 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all textual =~ x4 0.989 0.061 16.150 0.000 0.989 0.851 x5 1.103 0.055 20.196 0.000 1.103 0.856 x6 0.916 0.058 15.728 0.000 0.916 0.838 visual =~ x1 0.885 0.091 9.673 0.000 0.885 0.759 x2 0.511 0.082 6.233 0.000 0.511 0.435 x3 0.667 0.072 9.214 0.000 0.667 0.590 x9 0.387 0.066 5.895 0.000 0.387 0.384 speed =~ x7 0.666 0.075 8.874 0.000 0.666 0.612 x8 0.804 0.080 9.997 0.000 0.804 0.795 x9 0.450 0.065 6.901 0.000 0.450 0.447 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all textual ~~ visual 0.453 0.074 6.091 0.000 0.453 0.453 speed 0.206 0.076 2.731 0.006 0.206 0.206 visual ~~ speed 0.301 0.084 3.567 0.000 0.301 0.301 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x4 3.061 0.067 45.694 0.000 3.061 2.634 .x5 4.341 0.074 58.452 0.000 4.341 3.369 .x6 2.186 0.063 34.667 0.000 2.186 1.998 .x1 4.936 0.067 73.473 0.000 4.936 4.235 .x2 6.088 0.068 89.855 0.000 6.088 5.179 .x3 2.250 0.065 34.579 0.000 2.250 1.993 .x9 5.374 0.058 92.546 0.000 5.374 5.334 .x7 4.186 0.063 66.766 0.000 4.186 3.848 .x8 5.527 0.058 94.854 0.000 5.527 5.467 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x4 0.373 0.050 7.403 0.000 0.373 0.276 .x5 0.444 0.057 7.827 0.000 0.444 0.267 .x6 0.357 0.046 7.674 0.000 0.357 0.298 .x1 0.576 0.135 4.256 0.000 0.576 0.424 .x2 1.120 0.109 10.296 0.000 1.120 0.811 .x3 0.830 0.087 9.561 0.000 0.830 0.651 .x9 0.558 0.066 8.482 0.000 0.558 0.550 .x7 0.740 0.089 8.266 0.000 0.740 0.625 .x8 0.375 0.105 3.583 0.000 0.375 0.367 textual 1.000 1.000 1.000 visual 1.000 1.000 1.000 speed 1.000 1.000 1.000 R-Square: Estimate x4 0.724 x5 0.733 x6 0.702 x1 0.576 x2 0.189 x3 0.349 x9 0.450 x7 0.375 x8 0.633 Code summary( cfaModel2Fit_full, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 39 iterations Estimator ML Optimization method NLMINB Number of model parameters 31 Number of observations 301 Number of missing patterns 1 Model Test User Model: Standard Scaled Test Statistic 52.382 51.725 Degrees of freedom 23 23 P-value (Chi-square) 0.000 0.001 Scaling correction factor 1.013 Yuan-Bentler correction (Mplus variant) Model Test Baseline Model: Test statistic 918.852 880.082 Degrees of freedom 36 36 P-value 0.000 0.000 Scaling correction factor 1.044 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.967 0.966 Tucker-Lewis Index (TLI) 0.948 0.947 Robust Comparative Fit Index (CFI) 0.968 Robust Tucker-Lewis Index (TLI) 0.950 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -3721.283 -3721.283 Scaling correction factor 1.065 for the MLR correction Loglikelihood unrestricted model (H1) -3695.092 -3695.092 Scaling correction factor 1.043 for the MLR correction Akaike (AIC) 7504.566 7504.566 Bayesian (BIC) 7619.487 7619.487 Sample-size adjusted Bayesian (SABIC) 7521.172 7521.172 Root Mean Square Error of Approximation: RMSEA 0.065 0.064 90 Percent confidence interval - lower 0.042 0.041 90 Percent confidence interval - upper 0.089 0.088 P-value H_0: RMSEA &lt;= 0.050 0.133 0.143 P-value H_0: RMSEA &gt;= 0.080 0.158 0.144 Robust RMSEA 0.064 90 Percent confidence interval - lower 0.040 90 Percent confidence interval - upper 0.088 P-value H_0: Robust RMSEA &lt;= 0.050 0.156 P-value H_0: Robust RMSEA &gt;= 0.080 0.147 Standardized Root Mean Square Residual: SRMR 0.041 0.041 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all textual =~ x4 0.989 0.061 16.150 0.000 0.989 0.851 x5 1.103 0.055 20.196 0.000 1.103 0.856 x6 0.916 0.058 15.728 0.000 0.916 0.838 visual =~ x1 0.885 0.091 9.673 0.000 0.885 0.759 x2 0.511 0.082 6.233 0.000 0.511 0.435 x3 0.667 0.072 9.214 0.000 0.667 0.590 x9 0.387 0.066 5.895 0.000 0.387 0.384 speed =~ x7 0.666 0.075 8.874 0.000 0.666 0.612 x8 0.804 0.080 9.997 0.000 0.804 0.795 x9 0.450 0.065 6.901 0.000 0.450 0.447 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all textual ~~ visual 0.453 0.074 6.091 0.000 0.453 0.453 visual ~~ speed 0.301 0.084 3.567 0.000 0.301 0.301 textual ~~ speed 0.206 0.076 2.731 0.006 0.206 0.206 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual 0.000 0.000 0.000 textual 0.000 0.000 0.000 speed 0.000 0.000 0.000 .x1 (int1) 4.936 0.067 73.473 0.000 4.936 4.235 .x2 (int2) 6.088 0.068 89.855 0.000 6.088 5.179 .x3 (int3) 2.250 0.065 34.579 0.000 2.250 1.993 .x4 (int4) 3.061 0.067 45.694 0.000 3.061 2.634 .x5 (int5) 4.341 0.074 58.452 0.000 4.341 3.369 .x6 (int6) 2.186 0.063 34.667 0.000 2.186 1.998 .x7 (int7) 4.186 0.063 66.766 0.000 4.186 3.848 .x8 (int8) 5.527 0.058 94.854 0.000 5.527 5.467 .x9 (int9) 5.374 0.058 92.546 0.000 5.374 5.334 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual 1.000 1.000 1.000 textual 1.000 1.000 1.000 speed 1.000 1.000 1.000 .x1 0.576 0.135 4.256 0.000 0.576 0.424 .x2 1.120 0.109 10.296 0.000 1.120 0.811 .x3 0.830 0.087 9.561 0.000 0.830 0.651 .x4 0.373 0.050 7.403 0.000 0.373 0.276 .x5 0.444 0.057 7.827 0.000 0.444 0.267 .x6 0.357 0.046 7.674 0.000 0.357 0.298 .x7 0.740 0.089 8.266 0.000 0.740 0.625 .x8 0.375 0.105 3.583 0.000 0.375 0.367 .x9 0.558 0.066 8.482 0.000 0.558 0.550 R-Square: Estimate x1 0.576 x2 0.189 x3 0.349 x4 0.724 x5 0.733 x6 0.702 x7 0.375 x8 0.633 x9 0.450 7.10.10.4 Estimates of Model Fit After fitting the additional factor loading, the model fits well according to RMSEA, CFI, and SRMR, and the model fit is acceptable according to TLI. Thus, this model could be defensible. Code fitMeasures( cfaModel2Fit, fit.measures = c( &quot;chisq&quot;, &quot;df&quot;, &quot;pvalue&quot;, &quot;chisq.scaled&quot;, &quot;df.scaled&quot;, &quot;pvalue.scaled&quot;, &quot;chisq.scaling.factor&quot;, &quot;baseline.chisq&quot;,&quot;baseline.df&quot;,&quot;baseline.pvalue&quot;, &quot;rmsea&quot;, &quot;cfi&quot;, &quot;tli&quot;, &quot;srmr&quot;, &quot;rmsea.robust&quot;, &quot;cfi.robust&quot;, &quot;tli.robust&quot;)) chisq df pvalue 52.382 23.000 0.000 chisq.scaled df.scaled pvalue.scaled 51.725 23.000 0.001 chisq.scaling.factor baseline.chisq baseline.df 1.013 918.852 36.000 baseline.pvalue rmsea cfi 0.000 0.065 0.967 tli srmr rmsea.robust 0.948 0.041 0.064 cfi.robust tli.robust 0.968 0.950 7.10.10.5 Residuals Code residuals(cfaModel2Fit, type = &quot;cor&quot;) $type [1] &quot;cor.bollen&quot; $cov x4 x5 x6 x1 x2 x3 x9 x7 x8 x4 0.000 x5 0.005 0.000 x6 -0.008 0.003 0.000 x1 0.080 -0.001 0.069 0.000 x2 -0.015 -0.029 0.028 -0.033 0.000 x3 -0.069 -0.152 -0.026 -0.008 0.083 0.000 x9 -0.018 0.000 -0.009 -0.003 -0.019 0.023 0.000 x7 0.066 -0.006 0.015 -0.073 -0.156 -0.037 -0.003 0.000 x8 -0.033 -0.002 0.012 0.042 -0.012 0.045 0.002 0.000 0.000 $mean x4 x5 x6 x1 x2 x3 x9 x7 x8 0 0 0 0 0 0 0 0 0 7.10.10.6 Path Diagram Below is a path diagram of the model generated using the semPlot package (Epskamp, 2022). Code semPaths( cfaModel2Fit, what = &quot;Std.all&quot;, layout = &quot;tree2&quot;, edge.label.cex = 0.8) Figure 7.10: Modified Confirmatory Factor Analysis Model. 7.10.10.7 Compare Model Fit The modified model with the cross-loading and the original model are considered “nested” models. The original model is nested within the modified model because the modified model includes all of the terms of the original model along with additional terms. To confirm that the models are nested, I use the net() function from the semTools package (Jorgensen et al., 2021). Code net(cfaModelFit, cfaModel2Fit) If cell [R, C] is TRUE, the model in row R is nested within column C. If the models also have the same degrees of freedom, they are equivalent. NA indicates the model in column C did not converge when fit to the implied means and covariance matrix from the model in row R. The hidden diagonal is TRUE because any model is equivalent to itself. The upper triangle is hidden because for models with the same degrees of freedom, cell [C, R] == cell [R, C]. For all models with different degrees of freedom, the upper diagonal is all FALSE because models with fewer degrees of freedom (i.e., more parameters) cannot be nested within models with more degrees of freedom (i.e., fewer parameters). cfaModel2Fit cfaModelFit cfaModel2Fit (df = 23) cfaModelFit (df = 24) TRUE Model fit of nested models can be compared with a chi-square difference test. Code anova(cfaModelFit, cfaModel2Fit) In this case, the model with the cross-loading fits significantly better (i.e., has a significantly smaller chi-square value) than the model without the cross-loading. One can also compare nested models using a robust likelihood ratio test: Code cfaModelFitML &lt;- cfa( cfaModel_syntax, data = HolzingerSwineford1939, missing = &quot;ML&quot;, estimator = &quot;ML&quot;, std.lv = TRUE) cfaModel2FitML &lt;- cfa( cfaModel2_syntax, data = HolzingerSwineford1939, missing = &quot;ML&quot;, estimator = &quot;ML&quot;, std.lv = TRUE) vuongtest( cfaModelFitML, cfaModel2FitML, nested = TRUE) Model 1 Class: lavaan Call: lavaan::lavaan(model = cfaModel2_syntax, data = HolzingerSwineford1939, ... Model 2 Class: lavaan Call: lavaan::lavaan(model = cfaModel_syntax, data = HolzingerSwineford1939, ... Variance test H0: Model 1 and Model 2 are indistinguishable H1: Model 1 and Model 2 are distinguishable w2 = 0.120, p = 0.00049 Robust likelihood ratio test of distinguishable models H0: Model 2 fits as well as Model 1 H1: Model 1 fits better than Model 2 LR = 32.923, p = 8.11e-08 For non-nested models, one can compare model fit with AIC, BIC, or the Vuong test. Code fitMeasures( cfaModelFitML, fit.measures = c( &quot;aic&quot;,&quot;bic&quot;,&quot;bic2&quot;)) aic bic bic2 7535.490 7646.703 7551.560 Code fitMeasures( cfaModel2FitML, fit.measures = c( &quot;aic&quot;,&quot;bic&quot;,&quot;bic2&quot;)) aic bic bic2 7504.566 7619.487 7521.172 Code vuongtest( cfaModelFitML, cfaModel2FitML, nested = FALSE) Model 1 Class: lavaan Call: lavaan::lavaan(model = cfaModel_syntax, data = HolzingerSwineford1939, ... Model 2 Class: lavaan Call: lavaan::lavaan(model = cfaModel2_syntax, data = HolzingerSwineford1939, ... Variance test H0: Model 1 and Model 2 are indistinguishable H1: Model 1 and Model 2 are distinguishable w2 = 0.120, p = 0.00049 Non-nested likelihood ratio test H0: Model fits are equal for the focal population H1A: Model 1 fits better than Model 2 z = -2.734, p = 0.997 H1B: Model 2 fits better than Model 1 z = -2.734, p = 0.003128 7.11 Structural Equation Model (SEM) The structural equation models were fit in the lavaan package (Rosseel et al., 2022). The examples were adapted from the lavaan documentation: https://lavaan.ugent.be/tutorial/sem.html (archived at https://perma.cc/8NG9-7JAG). In this model, we fit a measurement model with three latent factors in addition to a structural model with regressions estimated among the latent factors. 7.11.1 Specify the Model Code semModel_syntax &lt;- &#39; #Measurement model factor loadings ind60 =~ x1 + x2 + x3 dem60 =~ y1 + y2 + y3 + y4 dem65 =~ y5 + y6 + y7 + y8 #Regression paths dem60 ~ ind60 dem65 ~ ind60 + dem60 #Covariances among residual variances (correlated errors) y1 ~~ y5 y2 ~~ y4 + y6 y3 ~~ y7 y4 ~~ y8 y6 ~~ y8 &#39; semModel_fullSyntax &lt;- &#39; #Measurement model factor loadings (free the factor loading of the first indicator) ind60 =~ NA*x1 + x2 + x3 dem60 =~ NA*y1 + y2 + y3 + y4 dem65 =~ NA*y5 + y6 + y7 + y8 #Regression paths dem60 ~ ind60 dem65 ~ ind60 + dem60 #Covariances among residual variances (correlated errors) y1 ~~ y5 y2 ~~ y4 + y6 y3 ~~ y7 y4 ~~ y8 y6 ~~ y8 #Fix latent means to zero ind60 ~ 0 dem60 ~ 0 dem65 ~ 0 #Fix latent variances to one ind60 ~~ 1*ind60 dem60 ~~ 1*dem60 dem65 ~~ 1*dem65 #Estimate covariances among latent variables (not necessary because the latent variables are already linked via regression paths) #Estimate residual variances of manifest variables x1 ~~ x1 x2 ~~ x2 x3 ~~ x3 y1 ~~ y1 y2 ~~ y2 y3 ~~ y3 y4 ~~ y4 y5 ~~ y5 y6 ~~ y6 y7 ~~ y7 y8 ~~ y8 #Free intercepts of manifest variables x1 ~ intx1*1 x2 ~ intx2*1 x3 ~ intx3*1 y1 ~ inty1*1 y2 ~ inty2*1 y3 ~ inty3*1 y4 ~ inty4*1 y5 ~ inty5*1 y6 ~ inty6*1 y7 ~ inty7*1 y8 ~ inty8*1 &#39; 7.11.1.1 Model Syntax in Table Form: Code lavaanify(semModel_syntax) Code lavaanify(semModel_fullSyntax) 7.11.2 Fit the Model Code semModelFit &lt;- sem( semModel_syntax, data = PoliticalDemocracy, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, std.lv = TRUE) semModelFit_full &lt;- lavaan( semModel_fullSyntax, data = PoliticalDemocracy, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) 7.11.3 Display Summary Output 7.11.3.1 Interpreting lavaan Output Output from a SEM model includes information such as regression coefficients, intercepts, variances, and model fit indices. As noted above, there are two chi-square tests. In lavaan syntax, one is labeled “Model Test User Model” and the other is labeled “Model Test Baseline Model.” The chi-square test labeled “Model Test User Model” refers to the chi-square test of whether the model fits worse than the best-possible fitting model (the saturated model). In this case, the p-value of the robust chi-square test is \\(0.17\\). Thus, the model does not show significant misfit—i.e., the model does not fit significantly worse than the best-possible fitting model. The chi-square test labeled “Model Test Baseline Model” refers to the chi-square test of whether the model fits better than the worst-possible fitting model (the null model). In this case, the p-value of the robust chi-square test in comparison to the worse-possible fitting model is &lt; .05. Thus, the model fits significantly better than the worst-possible-fitting model. In terms of the model findings, ind60 was significantly positively associated with dem60, dem60 was significantly positively associated with dem65, and ind60 was marginally significantly positively associated with dem65. Code summary( semModelFit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 82 iterations Estimator ML Optimization method NLMINB Number of model parameters 42 Number of observations 75 Number of missing patterns 36 Model Test User Model: Standard Scaled Test Statistic 38.748 42.709 Degrees of freedom 35 35 P-value (Chi-square) 0.304 0.174 Scaling correction factor 0.907 Yuan-Bentler correction (Mplus variant) Model Test Baseline Model: Test statistic 601.314 594.441 Degrees of freedom 55 55 P-value 0.000 0.000 Scaling correction factor 1.012 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.993 0.986 Tucker-Lewis Index (TLI) 0.989 0.978 Robust Comparative Fit Index (CFI) 0.988 Robust Tucker-Lewis Index (TLI) 0.981 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -1418.424 -1418.424 Scaling correction factor 0.989 for the MLR correction Loglikelihood unrestricted model (H1) -1399.050 -1399.050 Scaling correction factor 0.952 for the MLR correction Akaike (AIC) 2920.848 2920.848 Bayesian (BIC) 3018.183 3018.183 Sample-size adjusted Bayesian (SABIC) 2885.810 2885.810 Root Mean Square Error of Approximation: RMSEA 0.038 0.054 90 Percent confidence interval - lower 0.000 0.000 90 Percent confidence interval - upper 0.094 0.106 P-value H_0: RMSEA &lt;= 0.050 0.585 0.424 P-value H_0: RMSEA &gt;= 0.080 0.126 0.240 Robust RMSEA 0.055 90 Percent confidence interval - lower 0.000 90 Percent confidence interval - upper 0.113 P-value H_0: Robust RMSEA &lt;= 0.050 0.422 P-value H_0: Robust RMSEA &gt;= 0.080 0.279 Standardized Root Mean Square Residual: SRMR 0.046 0.046 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ind60 =~ x1 0.645 0.056 11.566 0.000 0.645 0.908 x2 1.495 0.124 12.084 0.000 1.495 0.991 x3 1.189 0.112 10.581 0.000 1.189 0.863 dem60 =~ y1 1.879 0.239 7.858 0.000 2.143 0.834 y2 2.477 0.308 8.043 0.000 2.825 0.714 y3 2.134 0.330 6.469 0.000 2.434 0.733 y4 2.523 0.254 9.918 0.000 2.879 0.864 dem65 =~ y5 0.547 0.228 2.398 0.016 2.090 0.791 y6 0.656 0.275 2.390 0.017 2.509 0.753 y7 0.715 0.315 2.265 0.024 2.732 0.830 y8 0.696 0.311 2.240 0.025 2.660 0.805 Regressions: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all dem60 ~ ind60 0.549 0.162 3.381 0.001 0.481 0.481 dem65 ~ ind60 0.690 0.391 1.766 0.077 0.180 0.180 dem60 2.900 1.362 2.129 0.033 0.865 0.865 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .y1 ~~ .y5 0.728 0.511 1.427 0.154 0.728 0.318 .y2 ~~ .y4 1.087 0.847 1.284 0.199 1.087 0.234 .y6 1.672 0.957 1.747 0.081 1.672 0.275 .y3 ~~ .y7 0.343 0.750 0.457 0.647 0.343 0.083 .y4 ~~ .y8 0.501 0.457 1.097 0.273 0.501 0.153 .y6 ~~ .y8 1.720 0.843 2.040 0.041 1.720 0.400 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 5.064 0.084 60.554 0.000 5.064 7.131 .x2 4.829 0.176 27.401 0.000 4.829 3.202 .x3 3.524 0.163 21.557 0.000 3.524 2.557 .y1 5.512 0.300 18.389 0.000 5.512 2.145 .y2 4.273 0.468 9.127 0.000 4.273 1.080 .y3 6.558 0.385 17.026 0.000 6.558 1.974 .y4 4.391 0.392 11.214 0.000 4.391 1.319 .y5 5.132 0.312 16.434 0.000 5.132 1.942 .y6 2.979 0.390 7.642 0.000 2.979 0.894 .y7 6.310 0.387 16.315 0.000 6.310 1.918 .y8 4.019 0.392 10.247 0.000 4.019 1.217 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 0.088 0.023 3.792 0.000 0.088 0.175 .x2 0.040 0.097 0.410 0.682 0.040 0.017 .x3 0.486 0.090 5.418 0.000 0.486 0.256 .y1 2.009 0.506 3.973 0.000 2.009 0.304 .y2 7.675 1.433 5.354 0.000 7.675 0.490 .y3 5.116 1.157 4.420 0.000 5.116 0.463 .y4 2.803 0.883 3.175 0.001 2.803 0.253 .y5 2.618 0.686 3.815 0.000 2.618 0.375 .y6 4.819 0.987 4.884 0.000 4.819 0.434 .y7 3.362 0.714 4.709 0.000 3.362 0.311 .y8 3.835 1.001 3.831 0.000 3.835 0.352 ind60 1.000 1.000 1.000 .dem60 1.000 0.768 0.768 .dem65 1.000 0.068 0.068 R-Square: Estimate x1 0.825 x2 0.983 x3 0.744 y1 0.696 y2 0.510 y3 0.537 y4 0.747 y5 0.625 y6 0.566 y7 0.689 y8 0.648 dem60 0.232 dem65 0.932 Code summary( semModelFit_full, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 82 iterations Estimator ML Optimization method NLMINB Number of model parameters 42 Number of observations 75 Number of missing patterns 36 Model Test User Model: Standard Scaled Test Statistic 38.748 42.709 Degrees of freedom 35 35 P-value (Chi-square) 0.304 0.174 Scaling correction factor 0.907 Yuan-Bentler correction (Mplus variant) Model Test Baseline Model: Test statistic 601.314 594.441 Degrees of freedom 55 55 P-value 0.000 0.000 Scaling correction factor 1.012 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.993 0.986 Tucker-Lewis Index (TLI) 0.989 0.978 Robust Comparative Fit Index (CFI) 0.988 Robust Tucker-Lewis Index (TLI) 0.981 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -1418.424 -1418.424 Scaling correction factor 0.989 for the MLR correction Loglikelihood unrestricted model (H1) -1399.050 -1399.050 Scaling correction factor 0.952 for the MLR correction Akaike (AIC) 2920.848 2920.848 Bayesian (BIC) 3018.183 3018.183 Sample-size adjusted Bayesian (SABIC) 2885.810 2885.810 Root Mean Square Error of Approximation: RMSEA 0.038 0.054 90 Percent confidence interval - lower 0.000 0.000 90 Percent confidence interval - upper 0.094 0.106 P-value H_0: RMSEA &lt;= 0.050 0.585 0.424 P-value H_0: RMSEA &gt;= 0.080 0.126 0.240 Robust RMSEA 0.055 90 Percent confidence interval - lower 0.000 90 Percent confidence interval - upper 0.113 P-value H_0: Robust RMSEA &lt;= 0.050 0.422 P-value H_0: Robust RMSEA &gt;= 0.080 0.279 Standardized Root Mean Square Residual: SRMR 0.046 0.046 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ind60 =~ x1 0.645 0.056 11.566 0.000 0.645 0.908 x2 1.495 0.124 12.084 0.000 1.495 0.991 x3 1.189 0.112 10.581 0.000 1.189 0.863 dem60 =~ y1 1.879 0.239 7.858 0.000 2.143 0.834 y2 2.477 0.308 8.043 0.000 2.825 0.714 y3 2.134 0.330 6.469 0.000 2.434 0.733 y4 2.523 0.254 9.918 0.000 2.879 0.864 dem65 =~ y5 0.547 0.228 2.398 0.016 2.090 0.791 y6 0.656 0.275 2.390 0.017 2.509 0.753 y7 0.715 0.315 2.265 0.024 2.732 0.830 y8 0.696 0.311 2.240 0.025 2.660 0.805 Regressions: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all dem60 ~ ind60 0.549 0.162 3.381 0.001 0.481 0.481 dem65 ~ ind60 0.690 0.391 1.766 0.077 0.180 0.180 dem60 2.900 1.362 2.129 0.033 0.865 0.865 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .y1 ~~ .y5 0.728 0.511 1.427 0.154 0.728 0.318 .y2 ~~ .y4 1.087 0.847 1.284 0.199 1.087 0.234 .y6 1.672 0.957 1.747 0.081 1.672 0.275 .y3 ~~ .y7 0.343 0.750 0.457 0.647 0.343 0.083 .y4 ~~ .y8 0.501 0.457 1.097 0.273 0.501 0.153 .y6 ~~ .y8 1.720 0.843 2.040 0.041 1.720 0.400 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ind60 0.000 0.000 0.000 .dem60 0.000 0.000 0.000 .dem65 0.000 0.000 0.000 .x1 (intx1) 5.064 0.084 60.554 0.000 5.064 7.131 .x2 (intx2) 4.829 0.176 27.401 0.000 4.829 3.202 .x3 (intx3) 3.524 0.163 21.557 0.000 3.524 2.557 .y1 (inty1) 5.512 0.300 18.389 0.000 5.512 2.145 .y2 (inty2) 4.273 0.468 9.127 0.000 4.273 1.080 .y3 (inty3) 6.558 0.385 17.026 0.000 6.558 1.974 .y4 (int4) 4.391 0.392 11.214 0.000 4.391 1.319 .y5 (int5) 5.132 0.312 16.434 0.000 5.132 1.942 .y6 (int6) 2.979 0.390 7.642 0.000 2.979 0.894 .y7 (int7) 6.310 0.387 16.315 0.000 6.310 1.918 .y8 (int8) 4.019 0.392 10.247 0.000 4.019 1.217 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ind60 1.000 1.000 1.000 .dem60 1.000 0.768 0.768 .dem65 1.000 0.068 0.068 .x1 0.088 0.023 3.792 0.000 0.088 0.175 .x2 0.040 0.097 0.410 0.682 0.040 0.017 .x3 0.486 0.090 5.418 0.000 0.486 0.256 .y1 2.009 0.506 3.973 0.000 2.009 0.304 .y2 7.675 1.433 5.354 0.000 7.675 0.490 .y3 5.116 1.157 4.420 0.000 5.116 0.463 .y4 2.803 0.883 3.175 0.001 2.803 0.253 .y5 2.618 0.686 3.815 0.000 2.618 0.375 .y6 4.819 0.987 4.884 0.000 4.819 0.434 .y7 3.362 0.714 4.709 0.000 3.362 0.311 .y8 3.835 1.001 3.831 0.000 3.835 0.352 R-Square: Estimate dem60 0.232 dem65 0.932 x1 0.825 x2 0.983 x3 0.744 y1 0.696 y2 0.510 y3 0.537 y4 0.747 y5 0.625 y6 0.566 y7 0.689 y8 0.648 7.11.4 Estimates of Model Fit Code fitMeasures( semModelFit, fit.measures = c( &quot;chisq&quot;, &quot;df&quot;, &quot;pvalue&quot;, &quot;chisq.scaled&quot;, &quot;df.scaled&quot;, &quot;pvalue.scaled&quot;, &quot;chisq.scaling.factor&quot;, &quot;baseline.chisq&quot;,&quot;baseline.df&quot;,&quot;baseline.pvalue&quot;, &quot;rmsea&quot;, &quot;cfi&quot;, &quot;tli&quot;, &quot;srmr&quot;, &quot;rmsea.robust&quot;, &quot;cfi.robust&quot;, &quot;tli.robust&quot;)) chisq df pvalue 38.748 35.000 0.304 chisq.scaled df.scaled pvalue.scaled 42.709 35.000 0.174 chisq.scaling.factor baseline.chisq baseline.df 0.907 601.314 55.000 baseline.pvalue rmsea cfi 0.000 0.038 0.993 tli srmr rmsea.robust 0.989 0.046 0.055 cfi.robust tli.robust 0.988 0.981 7.11.5 Residuals Code residuals(semModelFit, type = &quot;cor&quot;) $type [1] &quot;cor.bollen&quot; $cov x1 x2 x3 y1 y2 y3 y4 y5 y6 y7 y8 x1 0.000 x2 -0.005 0.000 x3 0.000 0.007 0.000 y1 0.031 -0.025 -0.099 0.000 y2 -0.091 -0.076 -0.082 0.000 0.000 y3 0.017 -0.012 -0.086 0.072 -0.076 0.000 y4 0.103 0.067 0.033 -0.050 0.019 -0.015 0.000 y5 0.150 0.101 0.013 -0.026 -0.001 -0.026 -0.012 0.000 y6 -0.027 -0.056 -0.071 0.062 0.011 -0.147 0.049 -0.010 0.000 y7 -0.038 -0.058 -0.059 -0.001 0.012 -0.017 -0.001 0.016 -0.003 0.000 y8 0.032 -0.029 -0.081 -0.013 0.027 -0.058 0.031 -0.047 0.013 0.035 0.000 $mean x1 x2 x3 y1 y2 y3 y4 y5 y6 y7 y8 0.009 0.005 -0.014 -0.001 0.004 -0.012 -0.010 0.005 -0.001 -0.004 0.000 7.11.6 Modification Indices Modification indices are generated using the syntax below and are in Table ??. Code modificationindices(semModelFit, sort. = TRUE) 7.11.7 Factor Scores Code semModelFit_factorScores &lt;- lavPredict(semModelFit) 7.11.8 Internal Consistency Reliability Internal consistency reliability of items composing the latent factors, as quantified by omega (\\(\\omega\\)) and average variance extracted (AVE), was estimated using the semTools package (Jorgensen et al., 2021). Code compRelSEM(semModelFit) ind60 dem60 dem65 0.938 0.855 0.843 Code AVE(semModelFit) ind60 dem60 dem65 0.861 0.605 0.632 7.11.9 Path Diagram Below is a path diagram of the model generated using the semPlot package (Epskamp, 2022). Code semPaths( semModelFit, what = &quot;Std.all&quot;, layout = &quot;tree2&quot;, edge.label.cex = 0.7) Figure 7.11: Example Structural Equation Model. 7.12 Benefits of SEM There are many benefits of fitting a model in SEM (or in other latent variable approaches). First, unlike classical test theory, SEM can allow correlated errors. With SEM, you do not need to make as restrictive assumptions as in classical test theory. Second, unlike multiple regression, SEM can handle multiple dependent variables simultaneously. Third, SEM uses all available information (data) using a technique called full information maximum likelihood (FIML), even if participants have missing scores on some variables. By contrast, multiple regression and many other statistical analyses use listwise deletion, in which they discard participants if they have a missing score on any of the model variables. Fourth, as described in the next section (7.12.1), SEM can be used to account for different forms of measurement error (e.g., method bias). Accounting for measurement error allows disattenuating associations with other constructs. I provide an example showing that SEM disattenuates associations for measurement error in Section 5.6.3. All of these benefits allow SEM to generate purer estimation of constructs, more accurate estimates of people’s levels on constructs, and more accurate estimates of associations between constructs. 7.12.1 Accounting for Method Bias SEM/CFA can be used to account for method biases and other forms of measurement error. You can use indicators that reflect different method biases, so that the method biases are discarded as unique errors, and are not combined in the “common variance” of the latent construct. SEM/CFA can be used to fit a multitrait-multimethod matrix to account for method variance. I provide an example of fitting a multitrait-multimethod matrix in CFA in Sections 5.3.1.4.2.5 and 14.4.2.13. But estimation of a multitrait-multimethod matrix in CFA can be challenging without making additional constraints/assumptions. A more practical utility of SEM is that it allows one to obtain “purer” estimates of latent constructs (and people’s standing on them) by discarding measurement error, and you do not have to assume all errors are uncorrelated! 7.13 Power Analysis Using Monte Carlo Simulation Power analysis for latent variable modeling approaches like SEM is more complicated than it is for other statistical analyses, such as correlation, multiple regression, t tests, analysis of variance, etc. Statistical power for these more straightforward analytical approaches can be estimated in G*Power (Faul et al., 2009): https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower.html (archived at https://perma.cc/F3RW-AXMQ) I provide an example of how to conduct power analysis of an SEM model to determine the sample size needed to detect a hypothesized effect of a given effect size. To perform the power analysis, I use Monte Carlo simulation (Hancock &amp; French, 2013; Muthén &amp; Muthén, 2002). It is named after the Casino de Monte-Carlo in Monaco because Monte Carlo simulations involve random samples (random chance), as might be found in casino gambling. Monte Carlo simulations can be used to determine the sample size needed to detect a target parameter of a given effect size, using the following four steps (Y. A. Wang &amp; Rhemtulla, 2021): Specify the sample size, a hypothesized true population SEM model, and all of its parameter values (e.g., factor loadings, intercepts, residuals, means, variances, covariances, regression paths, sample size); Generate a large number (e.g., 1,000) of random samples based on the hypothesized model and population values specified; Fit a SEM model to each of the generated samples, and for each one, record whether the target parameter is significantly different from zero; Calculate power as the proportion of simulated samples that produce a statistically significant estimate of the target parameter. These four steps can be repeated with different sample sizes to identify the sample size that is needed to have a particular level of power (e.g., .80). Power analysis of structural equation models using Monte Carlo simulation was estimated using the simsem package (Pornprasertmanit et al., 2021). These examples were adapted from the simsem documentation: https://github.com/simsem/simsem/wiki/Vignette (archived at https://perma.cc/BQF7-PQNK) https://github.com/simsem/simsem/wiki/Example-18:-Simulation-with-Varying-Sample-Size (archived at https://perma.cc/8YCN-HENK) https://github.com/simsem/simsem/wiki/Example-19:-Simulation-with-Varying-Sample-Size-and-Percent-Missing (archived at https://perma.cc/ZWL4-FHJ3) https://github.com/simsem/simsem/blob/master/SupportingDocs/Examples/Version05/ex18/ex18.R (archived at https://perma.cc/5W93-BLFD) https://github.com/simsem/simsem/blob/master/SupportingDocs/Examples/Version05/ex19/ex19.R (archived at https://perma.cc/7DJY-QYSW) 7.13.1 Specify Population Model The population model was used to generate the simulated data. It is important to specify each parameter value in the population model based on theory and/or prior empirical research, especially meta-analysis of the target population (when possible). Code populationModel &lt;- &#39; #Specify measurement model factor loadings ind60 =~ .7*x1 + .7*x2 + .7*x3 dem60 =~ .7*y1 + .7*y2 + .7*y3 + .7*y4 dem65 =~ .7*y5 + .7*y6 + .7*y7 + .7*y8 #Specify regression coefficients dem60 ~ .4*ind60 dem65 ~ .25*ind60 + .85*dem60 #Fix latent means to zero ind60 ~ 0 dem60 ~ 0 dem65 ~ 0 #Fix latent variances to one ind60 ~~ 1*ind60 dem60 ~~ 1*dem60 dem65 ~~ 1*dem65 #Specify covariances among latent variables (not necessary because the latent variables are already linked via regression paths) #Specify residual variances of manifest variables x1 ~~ (1-.7^2)*x1 x2 ~~ (1-.7^2)*x2 x3 ~~ (1-.7^2)*x3 y1 ~~ (1-.7^2)*y1 y2 ~~ (1-.7^2)*y2 y3 ~~ (1-.7^2)*y3 y4 ~~ (1-.7^2)*y4 y5 ~~ (1-.7^2)*y5 y6 ~~ (1-.7^2)*y6 y7 ~~ (1-.7^2)*y7 y8 ~~ (1-.7^2)*y8 #Specify intercepts of manifest variables x1 ~ 0*1 x2 ~ 0*1 x3 ~ 0*1 y1 ~ 0*1 y2 ~ 0*1 y3 ~ 0*1 y4 ~ 0*1 y5 ~ 0*1 y6 ~ 0*1 y7 ~ 0*1 y8 ~ 0*1 &#39; 7.13.1.1 Show the Model’s Fixed and Default Values Code populationModel_fit &lt;- lavaan(populationModel, do.fit = FALSE) Code summary(populationModel_fit, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 did not run (perhaps do.fit = FALSE)? ** WARNING ** Estimates below are simply the starting values Estimator ML Optimization method NLMINB Number of model parameters 0 Number of observations 0 Parameter Estimates: Latent Variables: Estimate Std.lv Std.all ind60 =~ x1 0.700 0.700 0.700 x2 0.700 0.700 0.700 x3 0.700 0.700 0.700 dem60 =~ y1 0.700 0.754 0.726 y2 0.700 0.754 0.726 y3 0.700 0.754 0.726 y4 0.700 0.754 0.726 dem65 =~ y5 0.700 1.007 0.816 y6 0.700 1.007 0.816 y7 0.700 1.007 0.816 y8 0.700 1.007 0.816 Regressions: Estimate Std.lv Std.all dem60 ~ ind60 0.400 0.371 0.371 dem65 ~ ind60 0.250 0.174 0.174 dem60 0.850 0.636 0.636 Intercepts: Estimate Std.lv Std.all ind60 0.000 0.000 0.000 .dem60 0.000 0.000 0.000 .dem65 0.000 0.000 0.000 .x1 0.000 0.000 0.000 .x2 0.000 0.000 0.000 .x3 0.000 0.000 0.000 .y1 0.000 0.000 0.000 .y2 0.000 0.000 0.000 .y3 0.000 0.000 0.000 .y4 0.000 0.000 0.000 .y5 0.000 0.000 0.000 .y6 0.000 0.000 0.000 .y7 0.000 0.000 0.000 .y8 0.000 0.000 0.000 Variances: Estimate Std.lv Std.all ind60 1.000 1.000 1.000 .dem60 1.000 0.862 0.862 .dem65 1.000 0.483 0.483 .x1 0.510 0.510 0.510 .x2 0.510 0.510 0.510 .x3 0.510 0.510 0.510 .y1 0.510 0.510 0.473 .y2 0.510 0.510 0.473 .y3 0.510 0.510 0.473 .y4 0.510 0.510 0.473 .y5 0.510 0.510 0.335 .y6 0.510 0.510 0.335 .y7 0.510 0.510 0.335 .y8 0.510 0.510 0.335 R-Square: Estimate dem60 0.138 dem65 0.517 x1 0.490 x2 0.490 x3 0.490 y1 0.527 y2 0.527 y3 0.527 y4 0.527 y5 0.665 y6 0.665 y7 0.665 y8 0.665 7.13.1.2 Model-Implied Covariance and Correlation Matrix 7.13.1.2.1 Model-implied covariance matrix: Code fitted(populationModel_fit) $cov x1 x2 x3 y1 y2 y3 y4 y5 y6 y7 y8 x1 1.000 x2 0.490 1.000 x3 0.490 0.490 1.000 y1 0.196 0.196 0.196 1.078 y2 0.196 0.196 0.196 0.568 1.078 y3 0.196 0.196 0.196 0.568 0.568 1.078 y4 0.196 0.196 0.196 0.568 0.568 0.568 1.078 y5 0.289 0.289 0.289 0.532 0.532 0.532 0.532 1.525 y6 0.289 0.289 0.289 0.532 0.532 0.532 0.532 1.015 1.525 y7 0.289 0.289 0.289 0.532 0.532 0.532 0.532 1.015 1.015 1.525 y8 0.289 0.289 0.289 0.532 0.532 0.532 0.532 1.015 1.015 1.015 1.525 $mean x1 x2 x3 y1 y2 y3 y4 y5 y6 y7 y8 0 0 0 0 0 0 0 0 0 0 0 7.13.1.2.2 Model-implied correlation matrix Code cov2cor(fitted(populationModel_fit)$cov) x1 x2 x3 y1 y2 y3 y4 y5 y6 y7 y8 x1 1.000 x2 0.490 1.000 x3 0.490 0.490 1.000 y1 0.189 0.189 0.189 1.000 y2 0.189 0.189 0.189 0.527 1.000 y3 0.189 0.189 0.189 0.527 0.527 1.000 y4 0.189 0.189 0.189 0.527 0.527 0.527 1.000 y5 0.234 0.234 0.234 0.415 0.415 0.415 0.415 1.000 y6 0.234 0.234 0.234 0.415 0.415 0.415 0.415 0.665 1.000 y7 0.234 0.234 0.234 0.415 0.415 0.415 0.415 0.665 0.665 1.000 y8 0.234 0.234 0.234 0.415 0.415 0.415 0.415 0.665 0.665 0.665 1.000 7.13.2 Specify Analysis Model Code analysisModel_syntax &lt;- &#39; #Measurement model factor loadings (free the factor loading of the first indicator) ind60 =~ NA*x1 + x2 + x3 dem60 =~ NA*y1 + y2 + y3 + y4 dem65 =~ NA*y5 + y6 + y7 + y8 #Regression paths dem60 ~ ind60 dem65 ~ ind60 + dem60 #Fix latent means to zero ind60 ~ 0 dem60 ~ 0 dem65 ~ 0 #Fix latent variances to one ind60 ~~ 1*ind60 dem60 ~~ 1*dem60 dem65 ~~ 1*dem65 #Estimate covariances among latent variables (not necessary because the latent variables are already linked via regression paths) #Estimate residual variances of manifest variables x1 ~~ x1 x2 ~~ x2 x3 ~~ x3 y1 ~~ y1 y2 ~~ y2 y3 ~~ y3 y4 ~~ y4 y5 ~~ y5 y6 ~~ y6 y7 ~~ y7 y8 ~~ y8 #Free intercepts of manifest variables x1 ~ intx1*1 x2 ~ intx2*1 x3 ~ intx3*1 y1 ~ inty1*1 y2 ~ inty2*1 y3 ~ inty3*1 y4 ~ inty4*1 y5 ~ inty5*1 y6 ~ inty6*1 y7 ~ inty7*1 y8 ~ inty8*1 &#39; 7.13.3 Specify Distribution of Data Specifying the expected distributions of the data variables is an optional step, but it can help give you more realistic estimates of your likely power, especially when the data are non-normally distributed. First, identify the order in which the indicator variables appear in the model, so you can know the order to specify skewness and kurtosis (which I specify in the next section): Code names(fitted(populationModel_fit)$mean) [1] &quot;x1&quot; &quot;x2&quot; &quot;x3&quot; &quot;y1&quot; &quot;y2&quot; &quot;y3&quot; &quot;y4&quot; &quot;y5&quot; &quot;y6&quot; &quot;y7&quot; &quot;y8&quot; Specify the skewness and kurtosis of the data variables. In this example, I set the variables \\(x1\\)–\\(x3\\) (the first three variables) to have skewness of 1.3 and kurtosis of 1.8, and I set the variables \\(y1\\)–\\(y8\\) (the next eight variables) to have a skewness of 2 and a kurtosis of 4. Code numberOfIndicators &lt;- length(fitted(populationModel_fit)$mean) Code indicatorDistributions &lt;- bindDist( p = numberOfIndicators, skewness = c(rep(1.3, 3), rep(2, 8)), kurtosis = c(rep(1.8, 3), rep(4, 8))) 7.13.4 Specify Extent and Type of Missing Data 7.13.4.1 Specify Missingness Specifying the extent and pattern of missingness is an optional step, but it can help give you more realistic estimates of your likely power, especially when there is extensive missing data and/or the data are not missing completely at random (MCAR). For an example of specifying the extent and pattern of missingness in the context of a Monte Carlo power analysis, see Beaujean (2014). In this example, I set 10% of values to be missing for variables \\(x1\\), \\(x2\\), and \\(x3\\). I set 15% of values to be missing for variables \\(y1\\)–\\(y8\\). I assumed the missingness mechanism to be MCAR. To set missingness to be missing at random (MAR), add a covariate in the missingness formula (see Beaujean, 2014). I set the model to use full information maximum likelihood (FIML) estimation to handle missingness. If you set \\(m\\) to a value greater than zero, it will use multiple imputation instead of FIML. Code percentMissingByVariable &lt;- &#39; x1 ~ p(0.10) x2 ~ p(0.10) x3 ~ p(0.10) y1 ~ p(0.15) y2 ~ p(0.15) y3 ~ p(0.15) y4 ~ p(0.15) y5 ~ p(0.15) y6 ~ p(0.15) y7 ~ p(0.15) y8 ~ p(0.15) &#39; Code missingnessModel &lt;- miss( logit = percentMissingByVariable, m = 0) 7.13.4.2 Plot of Extent of Missing Data Specified Code plotLogitMiss(percentMissingByVariable) Figure 7.12: Percent Missingness Specified for Each Variable. 7.13.5 Specify Sample Sizes and Repetitions Specify the sample sizes to evaluate in the Monte Carlo simulation and the number of repetitions per sample size. Code sampleSizes &lt;- 150:700 repetitionsPerSampleSize &lt;- 2 7.13.6 Monte Carlo Simulation to Generate Data from the Population Parameter Values Conduct Monte Carlo simulation with \\(2\\) repetitions per sample size \\((N\\text{s} = 150–700)\\). The multicore backend was used for parallel processing. Parallel processing distributes a larger computation task across multiple computing processes or cores, and runs them simultaneously (in parallel) to speed up execution time (if multiple processes or cores are available). If you choose to do processing in serial rather than parallel (by setting multicore = FALSE), you will need to run a special set.seed() command, prior to running the sim() command, to get reproducible results with those obtained from parallel processing: set.seed(seedNumber, \"L'Ecuyer-CMRG\"), where seedNumber is the value used for the seed. Warning: this code takes a while to run based on \\(551\\) different sample sizes \\((700 - 150 + 1)\\) and \\(2\\) repetitions per sample size, for a total of \\(1102\\) iterations \\(([700 - 150 + 1]\\) sample sizes \\(\\times\\) \\(2\\) repetitions per sample size \\(= 1102\\) iterations). You can reduce the number of sample sizes and/or repetitions per sample size to be faster. Code output &lt;- simsem::sim( n = rep( sampleSizes, each = repetitionsPerSampleSize), model = analysisModel_syntax, generate = populationModel, miss = missingnessModel, indDist = indicatorDistributions, lavaanfun = &quot;lavaan&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, std.lv = TRUE, seed = 52242, multicore = TRUE) Progress tracker is not available when &#39;multicore&#39; is TRUE. Return the seed to the normal seed behavior so that future calls to set.seed() use the default settings: Code RNGkind(&quot;default&quot;, &quot;default&quot;, &quot;default&quot;) 7.13.7 Summary of Population Model Code summaryPopulation(output) ind60=~x1 ind60=~x2 ind60=~x3 dem60=~y1 dem60=~y2 dem60=~y3 Population Value 0.7 0.7 0.7 0.7 0.7 0.7 dem60=~y4 dem65=~y5 dem65=~y6 dem65=~y7 dem65=~y8 dem60~ind60 Population Value 0.7 0.7 0.7 0.7 0.7 0.4 dem65~ind60 dem65~dem60 ind60~1 dem60~1 dem65~1 ind60~~ind60 Population Value 0.25 0.85 0 0 0 1 dem60~~dem60 dem65~~dem65 x1~~x1 x2~~x2 x3~~x3 y1~~y1 y2~~y2 Population Value 1 1 0.51 0.51 0.51 0.51 0.51 y3~~y3 y4~~y4 y5~~y5 y6~~y6 y7~~y7 y8~~y8 x1~1 x2~1 x3~1 y1~1 Population Value 0.51 0.51 0.51 0.51 0.51 0.51 0 0 0 0 y2~1 y3~1 y4~1 y5~1 y6~1 y7~1 y8~1 Population Value 0 0 0 0 0 0 0 7.13.8 Summary of Simulated Data Code summary(output) RESULT OBJECT Model Type [1] &quot;lavaan&quot; ========= Fit Indices Cutoffs ============ N chisq.scaled aic bic rmsea.scaled cfi.scaled tli.scaled srmr 1 150 67.537 4054.772 4170.183 0.056 1 1.019 0.057 2 288 65.373 7636.635 7764.941 0.048 1 1.016 0.049 3 425 63.224 11192.543 11333.650 0.039 1 1.014 0.041 4 562 61.076 14748.451 14902.359 0.031 1 1.012 0.034 5 700 58.912 18330.314 18497.117 0.022 1 1.009 0.026 ========= Parameter Estimates and Standard Errors ============ Estimate Average Estimate SD Average SE Power (Not equal 0) Std Est ind60=~x1 0.699 0.073 0.070 1.000 0.701 ind60=~x2 0.698 0.073 0.071 1.000 0.699 ind60=~x3 0.697 0.073 0.070 1.000 0.699 dem60=~y1 0.696 0.080 0.077 1.000 0.706 dem60=~y2 0.695 0.079 0.077 1.000 0.705 dem60=~y3 0.698 0.078 0.078 1.000 0.708 dem60=~y4 0.697 0.078 0.077 1.000 0.707 dem65=~y5 0.692 0.082 0.072 1.000 0.795 dem65=~y6 0.691 0.078 0.071 1.000 0.794 dem65=~y7 0.689 0.079 0.072 1.000 0.793 dem65=~y8 0.692 0.078 0.072 1.000 0.795 dem60~ind60 0.403 0.094 0.092 0.990 0.370 dem65~ind60 0.259 0.106 0.105 0.726 0.178 dem65~dem60 0.863 0.158 0.141 1.000 0.634 x1~~x1 0.500 0.080 0.076 0.997 0.505 x2~~x2 0.504 0.078 0.076 0.998 0.509 x3~~x3 0.504 0.084 0.076 0.996 0.508 y1~~y1 0.565 0.091 0.086 1.000 0.500 y2~~y2 0.567 0.092 0.086 1.000 0.501 y3~~y3 0.561 0.089 0.085 1.000 0.496 y4~~y4 0.561 0.095 0.086 1.000 0.497 y5~~y5 0.585 0.099 0.092 0.999 0.367 y6~~y6 0.586 0.101 0.093 0.999 0.368 y7~~y7 0.588 0.102 0.092 0.999 0.370 y8~~y8 0.583 0.098 0.092 1.000 0.366 intx1 0.000 0.057 0.053 0.049 -0.002 intx2 0.001 0.055 0.053 0.053 -0.001 intx3 0.001 0.057 0.053 0.050 -0.001 inty1 0.002 0.060 0.058 0.044 -0.001 inty2 0.004 0.057 0.058 0.043 0.001 inty3 0.001 0.059 0.058 0.045 -0.002 inty4 0.001 0.060 0.058 0.046 -0.002 inty5 0.000 0.068 0.068 0.054 -0.003 inty6 0.000 0.070 0.068 0.054 -0.003 inty7 0.002 0.072 0.068 0.056 -0.001 inty8 0.001 0.071 0.068 0.054 -0.002 Std Est SD Std Ave SE r_coef.n r_se.n ind60=~x1 0.053 0.051 0.050 -0.801 ind60=~x2 0.052 0.052 0.028 -0.784 ind60=~x3 0.055 0.051 0.008 -0.808 dem60=~y1 0.050 0.048 0.037 -0.751 dem60=~y2 0.051 0.048 -0.003 -0.755 dem60=~y3 0.050 0.048 0.025 -0.755 dem60=~y4 0.050 0.048 -0.006 -0.721 dem65=~y5 0.040 0.035 0.137 -0.734 dem65=~y6 0.039 0.035 0.125 -0.734 dem65=~y7 0.039 0.036 0.097 -0.733 dem65=~y8 0.036 0.035 0.116 -0.728 dem60~ind60 0.073 0.072 -0.047 -0.806 dem65~ind60 0.070 0.070 -0.019 -0.805 dem65~dem60 0.068 0.063 -0.099 -0.690 x1~~x1 0.074 0.072 0.055 -0.732 x2~~x2 0.073 0.072 0.070 -0.740 x3~~x3 0.076 0.071 0.054 -0.737 y1~~y1 0.070 0.068 0.029 -0.620 y2~~y2 0.072 0.067 -0.015 -0.618 y3~~y3 0.070 0.068 0.024 -0.653 y4~~y4 0.071 0.067 0.027 -0.570 y5~~y5 0.062 0.055 0.023 -0.552 y6~~y6 0.062 0.056 0.018 -0.520 y7~~y7 0.061 0.056 -0.001 -0.550 y8~~y8 0.057 0.056 0.050 -0.557 intx1 0.057 0.054 0.051 -0.920 intx2 0.055 0.054 0.058 -0.919 intx3 0.057 0.054 0.008 -0.918 inty1 0.058 0.055 0.002 -0.906 inty2 0.054 0.055 -0.047 -0.910 inty3 0.056 0.055 -0.004 -0.909 inty4 0.058 0.055 -0.039 -0.903 inty5 0.055 0.054 0.034 -0.907 inty6 0.056 0.054 0.013 -0.906 inty7 0.058 0.054 -0.024 -0.906 inty8 0.057 0.054 0.031 -0.911 ========= Correlation between Fit Indices ============ chisq.scaled aic bic rmsea.scaled cfi.scaled tli.scaled chisq.scaled 1.000 -0.188 -0.188 0.916 -0.828 -0.903 aic -0.188 1.000 1.000 -0.382 0.407 0.332 bic -0.188 1.000 1.000 -0.383 0.407 0.332 rmsea.scaled 0.916 -0.382 -0.383 1.000 -0.922 -0.945 cfi.scaled -0.828 0.407 0.407 -0.922 1.000 0.960 tli.scaled -0.903 0.332 0.332 -0.945 0.960 1.000 srmr 0.533 -0.820 -0.820 0.676 -0.692 -0.659 n -0.190 0.999 0.999 -0.384 0.408 0.333 srmr n chisq.scaled 0.533 -0.190 aic -0.820 0.999 bic -0.820 0.999 rmsea.scaled 0.676 -0.384 cfi.scaled -0.692 0.408 tli.scaled -0.659 0.333 srmr 1.000 -0.820 n -0.820 1.000 ================== Replications ===================== Number of replications = 1102 Number of converged replications = 1102 Number of nonconverged replications: 1. Nonconvergent Results = 0 2. Nonconvergent results from multiple imputation = 0 3. At least one SE were negative or NA = 0 4. Nonpositive-definite latent or observed (residual) covariance matrix (e.g., Heywood case or linear dependency) = 0 NOTE: The sample size is varying. NOTE: The data generation model is not the same as the analysis model. See the summary of the population underlying data generation by the summaryPopulation function. 7.13.9 Summary of Parameters Code summaryParam(output, alpha = .05, detail = TRUE) 7.13.10 Time to Completion Code summaryTime(output) ============ Wall Time ============ 1. Error Checking and setting up data-generation and analysis template: 0.001 2. Set combinations of n, pmMCAR, and pmMAR: 0.001 3. Setting up simulation conditions for each replication: 0.017 4. Total time elapsed running all replications: 4234.622 5. Combining outputs from different replications: 0.087 ============ Average Time in Each Replication ============ 1. Data Generation: 0.192 2. Impose Missing Values: 0.043 3. User-defined Data-Transformation Function: 0.000 4. Main Data Analysis: 9.066 5. Extracting Outputs: 6.018 ============ Summary ============ Start Time: 2024-04-29 02:56:01 End Time: 2024-04-29 04:06:36 Wall (Actual) Time: 4234.728 System (Processors) Time: 16881.213 Units: seconds 7.13.11 Cutoffs of Fit Indices 7.13.11.1 Plot of Cutoffs of Fit Indices At \\(\\alpha = .05\\) Code plotCutoff(output, alpha = .05) Figure 7.13: Plot of Cutoffs of Fit Indices From Monte Carlo Power Analysis. 7.13.11.2 Cutoffs of Fit Indices at Particular Sample Size At \\(N = 200\\), \\(\\alpha = .05\\) Code getCutoff(output, alpha = .05, nVal = 200) 7.13.12 Statistical Power 7.13.12.1 Plot of Power to Detect Various Parameters as a Function of Sample Size At \\(\\alpha = .05\\); dashed horizontal line represents power (\\(1 - \\beta\\)) of .8 Code par(mfrow = c(1,2)) plotPower( output, powerParam = &quot;dem65~dem60&quot;, alpha = .05) abline(h = 0.8, lwd = 2, lty = 2) plotPower( output, powerParam = &quot;dem65~ind60&quot;, alpha = .05) abline(h = 0.8, lwd = 2, lty = 2) Figure 7.14: Plot of Power to Detect Various Parameters as a Function of Sample Size, From Monte Carlo Power Analysis. 7.13.12.2 Sample Size Needed to Detect a Given Parameter At power (\\(1 - \\beta\\)) = .80, \\(\\alpha = .05\\) Code powerEstimates &lt;- getPower(output, alpha = .05) findPower(powerEstimates, iv = &quot;N&quot;, power = .80) ind60=~x1 ind60=~x2 ind60=~x3 dem60=~y1 dem60=~y2 dem60=~y3 Inf Inf Inf Inf Inf Inf dem60=~y4 dem65=~y5 dem65=~y6 dem65=~y7 dem65=~y8 dem60~ind60 Inf Inf Inf Inf Inf 150 dem65~ind60 dem65~dem60 x1~~x1 x2~~x2 x3~~x3 y1~~y1 463 Inf 150 150 150 Inf y2~~y2 y3~~y3 y4~~y4 y5~~y5 y6~~y6 y7~~y7 Inf Inf Inf 150 150 150 y8~~y8 intx1 intx2 intx3 inty1 inty2 Inf NA NA NA NA NA inty3 inty4 inty5 inty6 inty7 inty8 NA NA NA NA NA NA 7.13.12.3 Power to Detect Each Parameter at a Given Sample Size At \\(N = 200\\), \\(\\alpha = .05\\) Code getPower(output, alpha = .05, nVal = 200) iv.N ind60=~x1 ind60=~x2 ind60=~x3 dem60=~y1 dem60=~y2 dem60=~y3 dem60=~y4 [1,] 200 1 1 1 1 1 1 1 dem65=~y5 dem65=~y6 dem65=~y7 dem65=~y8 dem60~ind60 dem65~ind60 [1,] 1 1 1 1 0.9601611 0.4583257 dem65~dem60 x1~~x1 x2~~x2 x3~~x3 y1~~y1 y2~~y2 y3~~y3 y4~~y4 [1,] 1 0.9890479 0.9936008 0.9854814 1 1 1 1 y5~~y5 y6~~y6 y7~~y7 y8~~y8 intx1 intx2 intx3 [1,] 0.9966877 0.998075 0.9982373 1 0.04946284 0.06170937 0.06016819 inty1 inty2 inty3 inty4 inty5 inty6 [1,] 0.04500498 0.0398541 0.03380893 0.05307253 0.05531198 0.05669535 inty7 inty8 [1,] 0.06331062 0.05130184 7.14 Generalizability Theory There are also SEM approaches for performing generalizability theory analyses. The reader is referred to examples by Vispoel et al. (2018), Vispoel et al. (2019), Vispoel et al. (2022), and Vispoel et al. (2023). 7.15 Conclusion Structural equation modeling (SEM) is an advanced modeling approach that allows estimating latent variables as the common variance from multiple measures. SEM holds promise to account for measurement error and method biases, which allows one to get more accurate estimates of constructs, people’s standing on constructs (i.e., individual differences), and associations between constructs. 7.16 Suggested Readings MacCallum &amp; Austin (2000) 7.17 Exercises 7.17.1 Questions Note: Several of the following questions use data from the Children of the National Longitudinal Survey of Youth Survey (CNLSY). The CNLSY is a publicly available longitudinal data set provided by the Bureau of Labor Statistics (https://www.bls.gov/nls/nlsy79-children.htm#topical-guide; archived at https://perma.cc/EH38-HDRN). The CNLSY data file for these exercises is located on the book’s page of the Open Science Framework (https://osf.io/3pwza). Children’s behavior problems were rated in 1988 (time 1: T1) and then again in 1990 (time 2: T2) on the Behavior Problems Index (BPI). Below are the items corresponding to the Antisocial subscale of the BPI: cheats or tells lies bullies or is cruel/mean to others does not seem to feel sorry after misbehaving breaks things deliberately is disobedient at school has trouble getting along with teachers has sudden changes in mood or feeling Fit a confirmatory factor analysis model to the seven items of the Antisocial subscale of the Behavior Problems Index at T1. Set the first indicator to be the referent indicator (to set the scale of the latent factor) by setting its loading to one. Allow the factor loadings of the other indicators to be freely estimated. Set the mean (intercept) of the latent factor to be zero. Do not allow the residuals to be correlated. Use full information maximum likelihood (FIML) to account for missing data. Use robust standard errors to account for non-normally distributed data. This is an over-simplification, but for now let us assume a model fits “well” if CFI \\(\\geq .95\\), RMSEA \\(&lt; .08\\), and SRMR \\(&lt; .08\\) (Schreiber et al., 2006). Did the model fit well? What does this indicate? Examine the modification indices. Which modification would result in the greatest improvement in model fit? Why do you think this modification would improve model fit? Fit the modified confirmatory factor analysis model to make the suggested revision you identified in 1b. Provide a figure of the model with standardized coefficients. The modified model and the original model are considered “nested” models. The original model is nested within the modified model because the modified model includes all of the terms of the original model along with additional terms. Model fit of nested models can be directly compared with a chi-square difference test. Did the modified model fit better than the original model? Did the modified model fit well? What does this indicate? Which item is most strongly with the latent factor? Which item is most weakly associated with the latent factor? What is the estimate of internal consistency reliability of the items, based on coefficient omega? Fit a confirmatory factor analysis model to the seven items of the Antisocial subscale of the Behavior Problems Index at both T1 and T2 simultaneously in the same model. Allow the items at T1 to load onto a different factor than the items at T2 (i.e., a two-factor model—one antisocial at each time point). Set the scale of the latent factors by standardizing the latent factors—set their means to one and their variances to zero. This allows you to freely estimate the factor loadings of all items (instead of setting a reference indicator). Estimate the covariance between the two latent factors. Treat exogenous covariates as random variables (whose means, variances, and covariances are estimated) by specifying fixed.x = FALSE. Use full information maximum likelihood (FIML) to account for missing data. Use robust standard errors to account for non-normally distributed data. Apply the same modification you noted in 1b above to each factor. Because the two latent factors are standardized, the “covariance” path between the two latent factors represents a correlation. What is the correlation between the latent factors? What is the correlation between the sum scores (bpi_antisocialT1Sum, bpi_antisocialT2Sum)? Which is greater and why? Change the covariance path to a regression path from the latent factor at T1 predicting the latent factor at T2. Also include the sum score of anxious/depressed symptoms at T1 (bpi_anxiousDepressedSum) as a predictor of antisocial behavior at T2. Do anxious/depressed symptoms at T1 predict antisocial behavior at T2 controlling for prior levels of antisocial behavior at T1? Interpret the findings. You plan to conduct a study that would examine whether stress and harsh parenting predict children’s antisocial behavior. Your hypothesis is that stress and harsh parenting both lead to children’s antisocial behavior. You would like to apply for a grant to test these hypotheses, but you first want to know what sample size you would need to have adequate power to detect the hypothesized effects. Because you read this book, you remember that measurement error attenuates the associations you would observe, which would make it less likely that you would be able to detect the true effect (if there truly is an effect). As a result, you plan to assess each construct with multiple measurement methods/measures. You plan to model each construct with a latent variable in a structural equation modeling framework to account for measurement error and disattenuate the associations, which will make the associations more closely approximate the true effect and will make it more likely that you will detect the effect if it exists. You plan to assess stress with three methods (self-report, friend report, cortisol), harsh parenting with four methods (parents’ self-report, spousal report, child report, observation), and children’s antisocial behavior with four methods (parent report, teacher report, child report, observation). You conduct a power analysis with the following assumptions that you made based on theory and prior empirical research: The factor loading for each measure on its latent variable is .75 Stress influences children’s antisocial behavior with a regression coefficient of .20 Harsh parenting influences children’s antisocial behavior with a regression coefficient of .45 Stress influences harsh parenting with a regression coefficient of .4 Set the intercepts of the indicators to zero. Set the scale of the latent factors by standardizing the latent factors—set their means to one and their variances to zero. Do not estimate correlated errors. You expect each measure to show 10% missingness, and for missingness to be completely at random (MCAR). Use full information maximum likelihood (FIML) to handle missing values. As is common with measures in clinical psychology, you expect each measure to be positively skewed with a skewness of 2.5 and leptokurtic with a kurtosis of 5. Use robust standard errors to account for non-normally distributed data. Using a seed of 52242, an alpha level of .05, and one repetition per sample size, in the simsem package: What sample size would you need to have adequate power to detect the effect of stress on antisocial behavior and the effect of harsh parenting on antisocial behavior? Due to financial and time constraints of the grant, you are only able to collect a sample size of 150. What power would you have to detect the effect of stress on antisocial behavior and the effect of harsh parenting on antisocial behavior? Your study finds that neither stress nor harsh parenting predicts antisocial behavior. How would you interpret each of these findings? Re-run the power analysis with normally distributed values (skewness \\(= 0\\), kurtosis \\(= 0\\)), using 50 repetitions with a sample size of 150. Did power to detect the hypothesized effects increase or decrease? What does this indicate? Re-run the power analysis using multiple imputation instead of FIML (and normally distributed values); use 50 repetitions with a sample size of 150 and use five imputations. Did power to detect the hypothesized effects increase or decrease? What does this indicate? 7.17.2 Answers The model did not fit well according to CFI \\((0.88)\\) and RMSEA \\((0.09)\\). SRMR \\((0.05)\\) was acceptable. The poor model fit indicates that it is unlikely that the causal process described by the hypothesized model gave rise to the observed data. The modification that would result in the greatest model fit according to the modification indices is to allow indicators 5 and 6 to be correlated. These indicators reflect “disobedience at school” and “trouble getting along with teachers,” respectively. It is likely that allowing these two residuals to correlate would improve model fit because they both assess children’s behavior in the school context, and so they would continue to be associated with each other even after accounting for variance from the latent factor. Figure 7.15: Figure of the Confirmatory Factor Analysis Model With Standardized Coefficients. The modified model \\((\\chi^2[df = 13] = 33.68)\\) fit significantly better than the original model \\((\\chi^2[df = 14] = 391.38)\\) according to a chi-square difference test \\((\\Delta\\chi^2[df = 1] = 833.51, p &lt; .001)\\). The model fit well according to CFI \\((0.9923114)\\), RMSEA \\((0.02416)\\), and SRMR \\((0.0133989)\\). This indicates that there is evidence that one factor may do a good job of explaining the covariance among the indicators, especially when allowing the residuals of items 5 and 6 to correlate. The item that shows the strongest association with the latent factor is item 2 (“bullies or is cruel/mean to others”: standardized factor loading = \\(.63\\)). The item that shows the weakest association with the latent factor is item 7 (“sudden changes in mood or feeling”: standardized factor loading = \\(.40\\)). Thus, meanness seems more core to the construct of antisocial behavior compared to sudden mood changes. The estimate of internal consistency reliability of items, based on coefficient omega (\\(\\omega\\)), is \\(.68\\). The correlation between the latent factor at T1 and T2 is \\(\\phi = .76\\). The correlation between the sum score at T1 and T2 is \\(r = .50\\). This indicates that the correlation of individual differences across time (rank-order stability) is stronger for the latent factor than for the sum scores. This is likely because the latent factors account for measurement error whereas the sum scores do not, and associations are attenuated due to measurement error. Thus, the association of the latent factor at T1 and T2 likely more accurately reflects the “true” cross-time association of the construct (compared to the association of the sum scores at T1 and T2). Yes, anxious/depressed symptoms significantly predicted antisocial behavior at T2 while controlling for prior levels of antisocial behavior \\((B = 0.11, β = .11, SE = 0.02, p &lt; .001)\\). That is, anxious/depressed symptoms predicted relative (rank-order) changes in antisocial behavior from T1 to T2. This suggests that anxiety/depression may be a pathway to antisocial behavior for some children. Because the data come from an observational design, however, we cannot infer causality. For instance, the association could owe to the opposite direction of effect (antisocial behavior could lead to anxiety/depression) or to a third variable (e.g., victimization could lead to both antisocial behavior and anxiety/depression; i.e., antisocial behavior and anxiety/depression could share a common cause). You would need a sample size of \\(615\\) to detect the effect of stress on children’s antisocial behavior. You would need a sample size of \\(114\\) to detect the effect of harsh parenting on children’s antisocial behavior. You would have a power of \\(.23\\) to detect the effect of stress on children’s antisocial behavior. You would have a power of \\(.91\\) to detect the effect of harsh parenting on children’s antisocial behavior. Because your study was well-powered to detect the effect of harsh parenting (\\(\\text{power} = .91\\)) and you found no statistically significant association between harsh parenting and children’s antisocial behavior, it suggests that harsh parenting did not influence antisocial behavior in this sample (at least not with a large enough effect size to be practically significant). Because your study was under-powered to detect the effect of stress (\\(\\text{power} = .23\\)) and you found no statistically significant association between stress and children’s antisocial behavior, we do not know whether you did not detect an association because (a) stress did not influence antisocial behavior in this sample (i.e., your hypotheses were incorrect), or (b) there was an effect of stress (i.e., your hypotheses were correct), but your sample size was too small and/or your measurements were too unreliable to detect the effect given the effect size. Power to detect the hypothesized effects increased when the data were normally distributed (compared to when the data were non-normally distributed). You would have a power of \\(.44\\) to detect the effect of stress on children’s antisocial behavior. You would have a power of \\(1.00\\) to detect the effect of harsh parenting on children’s antisocial behavior. This indicates that statistical power tends to be lower when data are non-normally distributed (compared to when data are normally distributed). Power to detect the hypothesized effects decreased when using multiple imputation compared to FIML. You would have a power of \\(.36\\) to detect the effect of stress on children’s antisocial behavior. You would have a power of \\(.98\\) to detect the effect of harsh parenting on children’s antisocial behavior. This is consistent with prior findings that statistical power with multiple imputation is lower than with FIML unless the number of imputations is large (J. Graham et al., 2007). References Beaujean, A. A. (2014). Latent variable modeling using R: A step-by-step guide. Routledge. Bollen, K. A. (1989). Structural equations with latent variables. John Wiley &amp; Sons. Bollen, K. A., &amp; Bauldry, S. (2011). Three Cs in measurement models: Causal indicators, composite indicators, and covariates. Psychological Methods, 16(3), 265–284. https://doi.org/10.1037/a0024448 Box, G. E. P. (1979). Robustness in the strategy of scientific model building. In R. L. Launer &amp; G. N. Wilkinson (Eds.), Robustness in statistics. Academic Press. Civelek, M. E. (2018). Essentials of structural equation modeling. Zea E-Books. Digitale, J. C., Martin, J. N., &amp; Glymour, M. M. (2022). Tutorial on directed acyclic graphs. Journal of Clinical Epidemiology, 142, 264–267. https://doi.org/10.1016/j.jclinepi.2021.08.001 Epskamp, S. (2022). semPlot: Path diagrams and visual analysis of various SEM packages’ output. https://github.com/SachaEpskamp/semPlot Faul, F., Erdfelder, E., Buchner, A., &amp; Lang, A.-G. (2009). Statistical power analyses using g*power 3.1: Tests for correlation and regression analyses. Behavior Research Methods, 41(4), 1149–1160. https://doi.org/10.3758/brm.41.4.1149 Graham, J., Olchowski, A., &amp; Gilreath, T. (2007). How many imputations are really needed? Some practical clarifications of multiple imputation theory. Prevention Science, 8(3), 206–213. https://doi.org/10.1007/s11121-007-0070-9 Hancock, G. R., &amp; French, B. F. (2013). Power analysis in structural equation modeling. In Structural equation modeling: A second course, 2nd ed. (pp. 117–159). IAP Information Age Publishing. Jorgensen, T. D., Pornprasertmanit, S., Schoemann, A. M., &amp; Rosseel, Y. (2021). semTools: Useful tools for structural equation modeling. https://github.com/simsem/semTools/wiki Kline, R. B. (2023). Principles and practice of structural equation modeling (5th ed.). Guilford Publications. MacCallum, R. C., &amp; Austin, J. T. (2000). Applications of structural equation modeling in psychological research. Annual Review of Psychology, 51(1), 201–226. https://doi.org/10.1146/annurev.psych.51.1.201 Markon, K. E., Chmielewski, M., &amp; Miller, C. J. (2011). The reliability and validity of discrete and continuous measures of psychopathology: A quantitative review. Psychological Bulletin, 137(5), 856–879. https://doi.org/10.1037/a0023678 McNeish, D., &amp; Wolf, M. G. (2023). Dynamic fit index cutoffs for confirmatory factor analysis models. Psychological Methods, 28(1), 61–88. https://doi.org/10.1037/met0000425 Muthén, L. K., &amp; Muthén, B. O. (2002). How to use a Monte Carlo study to decide on sample size and determine power. Structural Equation Modeling: A Multidisciplinary Journal, 9(4), 599–620. https://doi.org/10.1207/s15328007sem0904_8 Petersen, I. T. (2024b). petersenlab: Package of R functions for the Petersen Lab. https://doi.org/10.5281/zenodo.7602890 Pornprasertmanit, S., Miller, P., Schoemann, A., &amp; Jorgensen, T. D. (2021). simsem: SIMulated structural equation modeling. http://www.simsem.org Rosseel, Y., Jorgensen, T. D., &amp; Rockwood, N. (2022). lavaan: Latent variable analysis. https://lavaan.ugent.be Schreiber, J. B., Nora, A., Stage, F. K., Barlow, E. A., &amp; King, J. (2006). Reporting structural equation modeling and confirmatory factor analysis results: A review. Journal of Educational Research, 99(6), 323–337. https://doi.org/10.3200/JOER.99.6.323-338 Schuberth, F. (2023). The Henseler-Ogasawara specification of composites in structural equation modeling: A tutorial. Psychological Methods, 28(4), 843–859. https://doi.org/10.1037/met0000432 Textor, J., Zander, B. van der, Gilthorpe, M. S., Liśkiewicz, M., &amp; Ellison, G. T. (2017). Robust causal inference using directed acyclic graphs: The R package “dagitty”. International Journal of Epidemiology, 45(6), 1887–1894. https://doi.org/10.1093/ije/dyw341 Treiblmaier, H., Bentler, P. M., &amp; Mair, P. (2011). Formative constructs implemented via common factors. Structural Equation Modeling: A Multidisciplinary Journal, 18(1), 1–17. https://doi.org/10.1080/10705511.2011.532693 Vispoel, W. P., Hong, H., &amp; Lee, H. (2023). Benefits of doing generalizability theory analyses within structural equation modeling frameworks: Illustrations using the Rosenberg self-esteem scale. Structural Equation Modeling: A Multidisciplinary Journal, 1–17. https://doi.org/10.1080/10705511.2023.2187734 Vispoel, W. P., Lee, H., Xu, G., &amp; Hong, H. (2022). Integrating bifactor models into a generalizability theory based structural equation modeling framework. The Journal of Experimental Education, 1–21. https://doi.org/10.1080/00220973.2022.2092833 Vispoel, W. P., Morris, C. A., &amp; Kilinc, M. (2018). Applications of generalizability theory and their relations to classical test theory and structural equation modeling. Psychological Methods, 23(1), 1–26. https://doi.org/10.1037/met0000107 Vispoel, W. P., Morris, C. A., &amp; Kilinc, M. (2019). Using generalizability theory with continuous latent response variables. Psychological Methods, 24(2), 153–178. https://doi.org/10.1037/met0000177 Wang, Y. A., &amp; Rhemtulla, M. (2021). Power analysis for parameter estimation in structural equation modeling: A discussion and tutorial. Advances in Methods and Practices in Psychological Science, 4(1), 1–17. Yu, X., Schuberth, F., &amp; Henseler, J. (2023). Specifying composites in structural equation modeling: A refinement of the Henseler-Ogasawara specification. Statistical Analysis and Data Mining: The ASA Data Science Journal, 16(4), 348–357. https://doi.org/https://doi.org/10.1002/sam.11608 "],["irt.html", "Chapter 8 Item Response Theory 8.1 Overview of IRT 8.2 Getting Started 8.3 Comparison of Scoring Approaches 8.4 Rasch Model (1-Parameter Logistic) 8.5 Two-Parameter Logistic Model 8.6 Two-Parameter Multidimensional Logistic Model 8.7 Three-Parameter Logistic Model 8.8 Four-Parameter Logistic Model 8.9 Graded Response Model 8.10 Conclusion 8.11 Suggested Readings 8.12 Exercises", " Chapter 8 Item Response Theory In the chapter on reliability, we introduced classical test theory. Classical test theory is a measurement theory of how test scores relate to a construct. Classical test theory provides a way to estimate the relation between the measure (or item) and the construct. For instance, with a classical test theory approach, to estimate the relation between an item and the construct, you would compute an item–total correlation. An item–total correlation is the correlation of an item with the total score on the measure (e.g., sum score). The item–total correlation approximates the relation between an item and the construct. However, the item–total correlation is a crude estimate of the relation between an item and the construct. And there are many other ways to characterize the relation between an item and a construct. One such way is with item response theory (IRT). 8.1 Overview of IRT Unlike classical test theory, which is a measurement theory of how test scores relate to a construct, IRT is a measurement theory that describes how an item is related to a construct. For instance, given a particular person’s level on the construct, what is their chance of answering “TRUE” on a particular item? IRT is an approach to latent variable modeling. In IRT, we estimate a person’s construct score (i.e., level on the construct) based on their item responses. The construct is estimated as a latent factor that represents the common variance among all items as in structural equation modeling or confirmatory factor analysis. The person’s level on the construct is called theta (\\(\\theta\\)). When dealing with performance-based tests, theta is sometimes called “ability.” 8.1.1 Item Characteristic Curve In IRT, we can plot an item characteristic curve (ICC). The ICC is a plot of the model-derived probability of a symptom being present (or a correct response) as a function of a person’s standing on a latent continuum. For instance, we can create empirical ICCs that can take any shape (see Figure 8.1). Figure 8.1: Empirical Item Characteristic Curves of the Probability of Endorsement of a Given Item as a Function of the Person’s Sum Score. In a model-implied ICC, we fit a logistic (sigmoid) curve to each item’s probability of a symptom being present as a function of a person’s level on the latent construct. The model-implied ICCs for the same 10 items from Figure 8.1 are depicted in Figure 8.2. Figure 8.2: Item Characteristic Curves of the Probability of Endorsement of a Given Item as a Function of the Person’s Level on the Latent Construct. ICCs can be summed across items to get the test characteristic curve (TCC): Figure 8.3: Test Characteristic Curve of the Expected Total Score on the Test as a Function of the Person’s Level on the Latent Construct. An ICC provides more information than an item–total correlation. Visually, we can see the utility of various items by looking at the items’ ICC plots. For instance, consider what might be a useless item for diagnostic purposes. For a particular item, among those with a low total score (level on the construct), 90% respond with “TRUE” to the item, whereas among everyone else, 100% respond with “TRUE” (see Figure 8.4). This item has a ceiling effect and provides only a little information about who would be considered above clinical threshold for a disorder. So, the item is not very clinically useful. Figure 8.4: Item Characteristic Curve of an Item with a Ceiling Effect That is not Diagnostically Useful. Now, consider a different item. For those with a low level on the construct, 0% respond with “TRUE”, so it has a floor effect and tells us nothing about the lower end of the construct. But for those with a higher level on the construct, 70% respond with true (see Figure 8.5). So, the item tells us something about the higher end of the distribution, and could be diagnostically useful. Thus, an ICC allows us to immediately tell the utility of items. Figure 8.5: Item Characteristic Curve of an Item With a Floor Effect That is Diagnostically Useful. 8.1.2 Parameters We can estimate up to four parameters in an IRT model and can glean up to four key pieces of information from an item’s ICC: Difficulty (severity) Discrimination Guessing Inattention/careless errors 8.1.2.1 Difficulty (Severity) The item’s difficulty parameter is the item’s location on the latent construct. It is quantified by the intercept, i.e., the location on the x-axis of the inflection point of the ICC. In a 1- or 2-parameter model, the inflection point is where 50% of the sample endorses the item (or gets the item correct), that is, the point on the x-axis where the ICC crosses .5 probability on the y-axis (i.e., the level on the construct at which the probability of endorsing the item is equal to the probability of not endorsing the item). Item difficulty is similar to item means or intercepts in structural equation modeling or factor analysis. Some items are more useful at the higher levels of the construct, whereas other items are more useful at the lower levels of the construct. See Figure 8.6 for an example of an item with a low difficulty and an item with a high difficulty. Figure 8.6: Item Characteristic Curves of an Item With Low Difficulty Versus High Difficulty. The dashed horizontal line indicates a probability of item endorsement of .50. The dashed vertical line is the item difficulty, i.e., the person’s level on the construct (the location on the x-axis) at the inflection point of the item characteristic curve. In a two-parameter logistic model, the inflection point corresponds to the probability of item endorsement is 50%. Thus, in a two-parameter logistic model, the difficulty of an item is the person’s level on the construct where the probability of endorsing the item is 50%. When dealing with a measure of clinical symptoms (e.g., depression), the difficulty parameter is sometimes called severity, because symptoms that are endorsed less frequently tend to be more severe [e.g., suicidal behavior; Krueger et al. (2004)]. One way of thinking about the severity parameter of an item is: “How severe does your psychopathology have to be for half of people to endorse the symptom?” When dealing with a measure of performance, aptitude, or intelligence, the parameter would be more likely to be called difficulty: “How high does your ability have to be for half of people to pass the item?” An item with a low difficulty would be considered easy, because even people with a low ability tend to pass the item. An item with a high difficulty would be considered difficult, because only people with a high ability tend to pass the item. 8.1.2.2 Discrimination The item’s discrimination parameter is how well the item can distinguish between those who were higher versus lower on the construct, that is, how strongly the item is correlated with the construct (i.e., the latent factor). It is similar to the factor loading in structural equation modeling or factor analysis. It is quantified by the slope of the ICC, i.e., the steepness of the line at its steepest point. The slope reflects the inverse of how much range of construct levels it would take to flip 50/50 whether a person is likely to pass or fail an item. Some items have ICCs that go up fast (have a steep slope). These items provide a fine distinction between people with lower versus higher levels on the construct and therefore have high discrimination. Some items go up gradually (less steep slope), so it provides less precision and information, and has a low discrimination. See Figure 8.7 for an example of an item with a low discrimination and an item with a high discrimination. Figure 8.7: Item Characteristic Curves of an Item With Low Discrimination Versus High Discrimination. The discrimination of an item is the slope of the line at its inflection point. 8.1.2.3 Guessing The item’s guessing parameter is reflected by the lower asymptote of the ICC. If the item has a lower asymptote above zero, it suggests that the probability of getting the item correct (or endorsing the item) never reaches zero, for any level of the construct. On an educational test, this could correspond to the person’s likelihood of being able to answer the item correctly by chance just by guessing. For example, for a 4-option multiple choice test, a respondent would be expected to get a given item correct 25% of the time just by guessing. See Figure 8.8 for an example of an item from a true/false exam and Figure 8.9 for an example of an item from a 4-option multiple choice exam. Figure 8.8: Item Characteristic Curve of an Item from a True/False Exam, There Test Takers Get the Item Correct at Least 50% of the Time. Figure 8.9: Item Characteristic Curve of an Item From a 4-Option Multiple Choice Exam, Where Test Takers Get the Item Correct at Least 25% of the Time. 8.1.2.4 Inattention/Careless Errors The item’s inattention (or careless error) parameter is the reflected by the upper asymptote of the ICC. If the item has an upper asymptote below one, it suggests that the probability of getting the item correct (or endorsing the item) never reaches one, for any level on the construct. See Figure 8.10 for an example of an item whose probability of endorsement (or getting it correct) exceeds .85. Figure 8.10: Item Characteristic Curve of an Item Where the Probability of Getting an Item Correct Never Exceeds .85. 8.1.3 Models IRT models can be fit that estimate one or more of these four item parameters. 8.1.3.1 1-Parameter and Rasch models A Rasch model estimates the item difficulty parameter and holds everything else fixed across items. It fixes the item discrimination to be one for each item. In the Rasch model, the probability that a person \\(j\\) with a level on the construct of \\(\\theta\\) gets a score of one (instead of zero) on item \\(i\\), based on the difficulty (\\(b\\)) of the item, is estimated using Equation (8.1): \\[\\begin{equation} P(X = 1|\\theta_j, b_i) = \\frac{e^{\\theta_j - b_i}}{1 + e^{\\theta_j - b_i}} \\tag{8.1} \\end{equation}\\] The petersenlab package (Petersen, 2024b) contains the fourPL() function that estimates the probability of item endorsement as function of the item characteristics from the Rasch model and the person’s level on the construct (theta). To estimate the probability of endorsement from the Rasch model, specify \\(b\\) and \\(\\theta\\), while keeping the defaults for the other parameters. Code library(&quot;petersenlab&quot;) #to install: install.packages(&quot;remotes&quot;); remotes::install_github(&quot;DevPsyLab/petersenlab&quot;) Code fourPL &lt;- function(a = 1, b, c = 0, d = 1, theta){ c + (d - c) * (exp(a * (theta - b))) / (1 + exp(a * (theta - b))) } Code fourPL(b, theta) Code fourPL(b = 1, theta = 0) [1] 0.2689414 A one-parameter logistic (1-PL) IRT model, similar to a Rasch model, estimates the item difficulty parameter, and holds everything else fixed across items (see Figure 8.11). The one-parameter logistic model holds the item discrimination fixed across items, but does not fix it to one, unlike the Rasch model. In the one-parameter logistic model, the probability that a person \\(j\\) with a level on the construct of \\(\\theta\\) gets a score of one (instead of zero) on item \\(i\\), based on the difficulty (\\(b\\)) of the item and the items’ (fixed) discrimination (\\(a\\)), is estimated using Equation (8.2): \\[\\begin{equation} P(X = 1|\\theta_j, b_i, a) = \\frac{e^{a(\\theta_j - b_i)}}{1 + e^{a(\\theta_j - b_i)}} \\tag{8.2} \\end{equation}\\] The petersenlab package (Petersen, 2024b) contains the fourPL() function that estimates the probability of item endorsement as function of the item characteristics from the one-parameter logistic model and the person’s level on the construct (theta). To estimate the probability of endorsement from the one-parameter logistic model, specify \\(a\\), \\(b\\), and \\(\\theta\\), while keeping the defaults for the other parameters. Code fourPL(a, b, theta) Rasch and one-parameter logistic models are common and are the easiest to fit. However, they make fairly strict assumptions. They assume that items have the same discrimination. Figure 8.11: One-Parameter Logistic Model in Item Response Theory. A one-parameter logistic model is only valid if there is not crossing of lines in empirical ICCs (see Figure 8.12). Figure 8.12: Empirical Item Characteristic Curves of the Probability of Endorsement of a Given Item as a Function of the Person’s Sum Score. The empirical item characteristic curves of these items do not cross each other. 8.1.3.2 2-Parameter A two-parameter logistic (2-PL) IRT model estimates item difficulty and discrimination, and it holds the asymptotes fixed across items (see Figure 8.13). Two-parameter logistic models are also common. In the two-parameter logistic model, the probability that a person \\(j\\) with a level on the construct of \\(\\theta\\) gets a score of one (instead of zero) on item \\(i\\), based on the difficulty (\\(b\\)) and discrimination (\\(a\\)) of the item, is estimated using Equation (8.3): \\[\\begin{equation} P(X = 1|\\theta_j, b_i, a_i) = \\frac{e^{a_i(\\theta_j - b_i)}}{1 + e^{a_i(\\theta_j - b_i)}} \\tag{8.3} \\end{equation}\\] The petersenlab package (Petersen, 2024b) contains the fourPL() function that estimates the probability of item endorsement as function of the item characteristics from the two-parameter logistic model and the person’s level on the construct (theta). To estimate the probability of endorsement from the two-parameter logistic model, specify \\(a\\), \\(b\\), and \\(\\theta\\), while keeping the defaults for the other parameters. Code fourPL(a, b, theta) Code fourPL(a = 0.6, b = 0, theta = -1) [1] 0.3543437 Figure 8.13: Two-Parameter Logistic Model in Item Response Theory. 8.1.3.3 3-Parameter A three-parameter logistic (3-PL) IRT model estimates item difficulty, discrimination, and guessing (lower asymptote), and it holds the upper asymptote fixed across items (see Figure 8.14). This model would provide information about where an item drops out. Three-parameter logistic models are less common to estimate because it adds considerable computational complexity and requires a large sample size, and the guessing parameter is often not as important as difficulty and discrimination. Nevertheless, 3-parameter logistic models are sometimes estimated in the education literature to account for getting items correct by random guessing. In the three-parameter logistic model, the probability that a person \\(j\\) with a level on the construct of \\(\\theta\\) gets a score of one (instead of zero) on item \\(i\\), based on the difficulty (\\(b\\)), discrimination (\\(a\\)), and guessing parameter (\\(c\\)) of the item, is estimated using Equation (8.4): \\[\\begin{equation} P(X = 1|\\theta_j, b_i, a_i, c_i) = c_i + (1 - c_i) \\frac{e^{a_i(\\theta_j - b_i)}}{1 + e^{a_i(\\theta_j - b_i)}} \\tag{8.4} \\end{equation}\\] The petersenlab package (Petersen, 2024b) contains the fourPL() function that estimates the probability of item endorsement as function of the item characteristics from the three-parameter logistic model and the person’s level on the construct (theta). To estimate the probability of endorsement from the three-parameter logistic model, specify \\(a\\), \\(b\\), \\(c\\), and \\(\\theta\\), while keeping the defaults for the other parameters. Code fourPL(a, b, c, theta) Code fourPL(a = 0.8, b = -1, c = .25, theta = -1) [1] 0.625 Figure 8.14: Three-Parameter Logistic Model in Item Response Theory. 8.1.3.4 4-Parameter A four-parameter logistic (4-PL) IRT model estimates item difficulty, discrimination, guessing, and careless errors (see Figure 8.15). The fourth parameter adds considerable computational complexity and is rare to estimate. In the four-parameter logistic model, the probability that a person \\(j\\) with a level on the construct of \\(\\theta\\) gets a score of one (instead of zero) on item \\(i\\), based on the difficulty (\\(b\\)), discrimination (\\(a\\)), guessing parameter (\\(c\\)), and careless error parameter (\\(d\\)) of the item, is estimated using Equation (8.5) (Magis, 2013): \\[\\begin{equation} P(X = 1|\\theta_j, b_i, a_i, c_i, d_i) = c_i + (d_i - c_i) \\frac{e^{a_i(\\theta_j - b_i)}}{1 + e^{a_i(\\theta_j - b_i)}} \\tag{8.5} \\end{equation}\\] The petersenlab package (Petersen, 2024b) contains the fourPL() function that estimates the probability of item endorsement as function of the item characteristics from the four-parameter logistic model and the person’s level on the construct (theta). To estimate the probability of endorsement from the four-parameter logistic model, specify \\(a\\), \\(b\\), \\(c\\), \\(d\\), and \\(\\theta\\). Code fourPL(a, b, c, d, theta) Code fourPL(a = 1.5, b = 1, c = .15, d = 0.85, theta = 3) [1] 0.8168019 Figure 8.15: Four-Parameter Logistic Model in Item Response Theory. 8.1.3.5 Graded Response Model Graded response models and generalized partial credit models can be estimated with one, two, three, or four parameters. However, they use polytomous data (not dichotomous data), as described in the section below. The two-parameter graded response model takes the general form of Equation (8.6): \\[\\begin{equation} P(X_{ji} = x_{ji}|\\theta_j) = P^*_{x_{ji}}(\\theta_j) - P^*_{x_{ji} + 1}(\\theta_j) \\tag{8.6} \\end{equation}\\] where: \\[\\begin{equation} P^*_{x_{ji}}(\\theta_j) = P(X_{ji} \\geq x_{ji}|\\theta_j, b_{ic}, a_i) = \\frac{1}{1 + e^{a_i(\\theta_j - b_{ic})}} \\tag{8.7} \\end{equation}\\] In the model, \\(a_i\\) an item-specific discrimination parameter, \\(b_{ic}\\) is an item- and category-specific difficulty parameter, and \\(θ_n\\) is an estimate of a person’s standing on the latent variable. In the model, \\(i\\) represents unique items, \\(c\\) represents different categories that are rated, and \\(j\\) represents participants. 8.1.4 Type of Data IRT models are most commonly estimated with binary or dichotomous data. For example, the measures have questions or items that can be considered collapsed into two groups (e.g., true/false, correct/incorrect, endorsed/not endorsed). IRT models can also be estimated with polytomous data (e.g., likert scale), which adds computational complexity. IRT models with polytomous data can be fit with a graded response model or generalized partial credit model. For example, see Figure 8.16 for an example of an item boundary characteristic curve for an item from a 5-level likert scale (based on a cumulative distribution). If an item has \\(k\\) response categories, it has \\(k - 1\\) thresholds. For example, an item with 5-level likert scale (1 = strongly disagree; 2 = disagree; 3 = neither agree nor disagree; 4 = agree; 5 = strongly agree) has 4 thresholds: one from 1–2, one from 2–3, one from 3–4, and one from 4–5. The item boundary characteristic curve is the probability that a person selects a response category higher than \\(k\\) of a polytomous item. As depicted, one likert scale item does equivalent work as 4 binary items. See Figure 8.17 for the same 5-level likert scale item plotted with an item response category characteristic curve (based on a static, non-cumulative distribution). Figure 8.16: Item Boundary Characteristic Curves From Two-Parameter Graded Response Model in Item Response Theory. Figure 8.17: Item Response Category Characteristic Curves From Two-Parameter Graded Response Model in Item Response Theory. IRT does not handle continuous data well, with some exceptions (Y. Chen et al., 2019) such as in a Bayesian framework (Bürkner, 2021). If you want to use continuous data, you might consider moving to a factor analysis framework. 8.1.5 Sample Size Sample size requirements depend on the complexity of the model. A 1-parameter model often requires ~100 participants. A 2-parameter model often requires ~1,000 participants. A 3-parameter model often requires ~10,000 participants. 8.1.6 Reliability (Information) IRT conceptualizes reliability in a different way than classical test theory does. Both IRT and classical test theory conceptualize reliability as involving the precision of a measure’s scores. In classical test theory, (im)precision—as operationalized by the standard error of measurement—is estimated with a single index across the whole range of the construct. That is, in classical test theory, the same standard error of measurement applies to all scores in the population (Embretson, 1996). However, IRT estimates how much measurement precision (information) or imprecision (standard error of measurement) each item, and the test as a whole, has at different construct levels. This allows IRT to conceptualize reliability in such a way that precision/reliability can differ at different construct levels, unlike in classical test theory (Embretson, 1996). Thus, IRT does not have one index of reliability; rather, its estimate of reliability differs at different levels on the construct. Based on an item’s difficulty and discrimination, we can calculate how much information each item provides. In IRT, information is how much measurement precision or consistency an item (or the measure) provides. In other words, information is the degree to which an item (or measure) reduces the standard error of measurement, that is, how much it reduces uncertainty of a person’s level on the construct. As a reminder (from Equation (4.11)), the standard error of measurement is calculated as: \\[ \\text{standard error of measurement (SEM)} = \\sigma_x \\sqrt{1 - r_{xx}} \\] where \\(\\sigma_x = \\text{standard deviation of observed scores on the item } x\\), and \\(r_{xx} = \\text{reliability of the item } x\\). The standard error of measurement is used to generate confidence intervals for people’s scores. In IRT, the standard error of measurement (at a given construct level) can be calculated as the inverse of the square root of the amount of test information at that construct level, as in Equation (8.8): \\[\\begin{equation} \\text{SEM}(\\theta) = \\frac{1}{\\sqrt{\\text{information}(\\theta)}} \\tag{8.8} \\end{equation}\\] The petersenlab package (Petersen, 2024b) contains the standardErrorIRT() function that estimates the standard error of measurement at a person’s level on the construct (theta) from the amount of information that the item (or test) provides. Code standardErrorIRT &lt;- function(information){ 1/sqrt(information) } Code standardErrorIRT(information) Code standardErrorIRT(0.6) [1] 1.290994 The standard error of measurement tends to be higher (i.e., reliability/information tends to be lower) at the extreme levels of the construct where there are fewer items. The formula for information for item \\(i\\) at construct level \\(\\theta\\) in a Rasch model is in Equation (8.9) (Baker &amp; Kim, 2017): \\[\\begin{equation} \\text{information}_i(\\theta) = P_i(\\theta)Q_i(\\theta) \\tag{8.9} \\end{equation}\\] where \\(P_i(\\theta)\\) is the probability of getting a one instead of a zero on item \\(i\\) at a given level on the latent construct, and \\(Q_i(\\theta) = 1 - P_i(\\theta)\\). The petersenlab package (Petersen, 2024b) contains the itemInformation() function that estimates the amount of information an item provides as function of the item characteristics from the Rasch model and the person’s level on the construct (theta). To estimate the amount of information an item provides in a Rasch model, specify \\(b\\) and \\(\\theta\\), while keeping the defaults for the other parameters. Code itemInformation &lt;- function(a = 1, b, c = 0, d = 1, theta){ P &lt;- NULL information &lt;- NULL for(i in 1:length(theta)){ P[i] &lt;- fourPL(b = b, a = a, c = c, d = d, theta = theta[i]) information[i] &lt;- ((a^2) * (P[i] - c)^2 * (d - P[i])^2) / ((d - c)^2 * P[i] * (1 - P[i])) } return(information) } Code itemInformation(b, theta) Code itemInformation(b = 1, theta = 0) [1] 0.1966119 The formula for information for item \\(i\\) at construct level \\(\\theta\\) in a two-parameter logistic model is in Equation (8.10) (Baker &amp; Kim, 2017): \\[\\begin{equation} \\text{information}_i(\\theta) = a^2_iP_i(\\theta)Q_i(\\theta) \\tag{8.10} \\end{equation}\\] The petersenlab package (Petersen, 2024b) contains the itemInformation() function that estimates the amount of information an item provides as function of the item characteristics from the two-parameter logistic model and the person’s level on the construct (theta). To estimate the amount of information an item provides in a two-parameter logistic model, specify \\(a\\), \\(b\\), and \\(\\theta\\), while keeping the defaults for the other parameters. Code itemInformation(a, b, theta) Code itemInformation(a = 0.6, b = 0, theta = -1) [1] 0.08236233 The formula for information for item \\(i\\) at construct level \\(\\theta\\) in a three-parameter logistic model is in Equation (8.11) (Baker &amp; Kim, 2017): \\[\\begin{equation} \\text{information}_i(\\theta) = a^2_i\\bigg[\\frac{Q_i(\\theta)}{P_i(\\theta)}\\bigg]\\bigg[\\frac{(P_i(\\theta) - c_i)^2}{(1 - c_i)^2}\\bigg] \\tag{8.11} \\end{equation}\\] The petersenlab package (Petersen, 2024b) contains the itemInformation() function that estimates the amount of information an item provides as function of the item characteristics from the three-parameter logistic model and the person’s level on the construct (theta). To estimate the amount of information an item provides in a three-parameter logistic model, specify \\(a\\), \\(b\\), \\(c\\), and \\(\\theta\\), while keeping the defaults for the other parameters. Code itemInformation(a, b, c, theta) Code itemInformation(a = 0.8, b = -1, c = .25, theta = -1) [1] 0.096 The formula for information for item \\(i\\) at construct level \\(\\theta\\) in a four-parameter logistic model is in Equation (8.12) (Magis, 2013): \\[\\begin{equation} \\text{information}_i(\\theta) = \\frac{a^2_i[P_i(\\theta) - c_i]^2[d_i - P_i(\\theta)^2]}{(d_i - c_i)^2 P_i(\\theta)[1 - P_i(\\theta)]} \\tag{8.12} \\end{equation}\\] The petersenlab package (Petersen, 2024b) contains the itemInformation() function that estimates the amount of information an item provides as function of the item characteristics from the four-parameter logistic model and the person’s level on the construct (theta). To estimate the amount of information an item provides in a four-parameter logistic model, specify \\(a\\), \\(b\\), \\(c\\), \\(d\\), and \\(\\theta\\). Code itemInformation(a, b, c, d, theta) Code itemInformation(a = 1.5, b = 1, c = .15, d = 0.85, theta = 3) [1] 0.01503727 Reliability at a given level of the construct (\\(\\theta\\)) can be estimated as in Equation (8.13): \\[ \\begin{aligned} \\text{reliability}(\\theta) &amp;= \\frac{\\text{information}(\\theta)}{\\text{information}(\\theta) + \\sigma^2(\\theta)} \\\\ &amp;= \\frac{\\text{information}(\\theta)}{\\text{information}(\\theta) + 1} \\end{aligned} \\tag{8.13} \\] where \\(\\sigma^2(\\theta)\\) is the variance of theta, which is fixed to one in most IRT models. The petersenlab package (Petersen, 2024b) contains the reliabilityiRT() function that estimates the amount of reliability an item or a measure provides as function of its information and the variance of people’s construct levels (\\(\\theta\\)). Code reliabilityIRT &lt;- function(information, varTheta = 1){ information / (information + varTheta) } Code reliabilityIRT(information, varTheta = 1) Code reliabilityIRT(10) [1] 0.9090909 Consider some hypothetical items depicted with ICCs in Figure 8.18. Figure 8.18: Item Characteristic Curves From Two-Parameter Logistic Model in Item Response Theory. The dashed horizontal line indicates a probability of item endorsement of .50. The dashed vertical line is the item difficulty, i.e., the person’s level on the construct (the location on the x-axis) at the inflection point of the item characteristic curve. In a two-parameter logistic model, the inflection point corresponds to the probability of item endorsement is 50%. Thus, in a two-parameter logistic model, the difficulty of an item is the person’s level on the construct where the probability of endorsing the item is 50%. We can present the ICC in terms of an item information curve (see Figure 8.19). On the x-axis, the information peak is located at the difficulty/severity of the item. The higher the discrimination, the higher the information peak on the y-axis. Figure 8.19: Item Information From Two-Parameter Logistic Model in Item Response Theory. The dashed vertical line is the item difficulty, which is located at the peak of the item information curve. We can aggregate (sum) information across items to determine how much information the measure as a whole provides. This is called the test information curve (see Figure 8.20). Note that we get more information from likert/multiple response items compared to binary/dichotomous items. Having 10 items with a 5-level response scale yields as much information as 40 dichotomous items. Figure 8.20: Test Information Curve From Two-Parameter Logistic Model in Item Response Theory. Based on test information, we can calculate the standard error of measurement (see Figure 8.21). Notice how the degree of (un)reliability differs at different construct levels. Figure 8.21: Test Standard Error of Measurement From Two-Parameter Logistic Model in Item Response Theory. Based on test information, we can estimate the reliability (see Figure 8.22). Notice how the degree of (un)reliability differs at different construct levels. Figure 8.22: Test Reliability From Two-Parameter Logistic Model in Item Response Theory. 8.1.7 Efficient Assessment One of the benefits of IRT is for item selection to develop brief assessments. For instance, you could use two items to estimate where the person is on the construct: low, middle, or high (see Figure 8.23). If the responses to the two items do not meet expectations, for instance, the person passes the difficult item but fails the easy item, we would keep assessing additional items to determine their level on the construct. If two items perform similarly, that is, they have the same difficulty and discrimination, they are redundant, and we can sacrifice one of them. This leads to greater efficiency and better measurement in terms of reliability and validity. For more information on designing and evaluating short forms compared to their full-scale counterparts, see Smith et al. (2000). Figure 8.23: Visual Representation of an Efficient Assessment Based on Item Characteristic Curves from Two-Parameter Logistic Model in Item Response Theory. IRT forms the basis of computerized adaptive testing, which is discussed in Chapter 21. As discussed earlier, briefer measures can increase reliability and validity of measurement if the items are tailored to the ability level of the participant. The idea of adaptive testing is that, instead of having a standard scale for all participants, the items adapt to each person. An example of a measure that has used computerized adaptive testing is the Graduate Record Examination (GRE). With adaptive testing, it is important to develop a comprehensive item bank that spans the difficulty range of interest. The starting construct level is the 50th percentile. If the respondent gets the first item correct, it moves to the next item that would provide the most information for the person, based on a split of the remaining sample (e.g., 75th percentile). And so on… The goal of adaptive testing is to find the construct level where the respondent keeps getting items right and wrong 50% of the time. Adaptive testing is a promising approach that saves time because it tailors which items are administered to which person (based on their construct level) to get the most reliable estimate in the shortest time possible. However, it assumes that if you get a more difficult item correct, that you would have gotten easier items correct, which might not be true in all contexts (especially for constructs that are not unidimensional). Although most uses of IRT have been in cognitive and educational testing, IRT may also benefit other domains of assessment including clinical assessment (Gibbons et al., 2016; Reise &amp; Waller, 2009; Thomas, 2019). 8.1.7.1 A Good Measure According to IRT a good measure should: fit your goals of the assessment, in terms of the range of interest regarding levels on the construct, have good items that yield lots of information, and have a good set of items that densely cover the construct within the range of interest, without redundancy. First, a good measure should fit your goals of the assessment, in terms of the “range of interest” or the “target range” of levels on the construct. For instance, if your goal is to perform diagnosis, you would only care about the high end of the construct (e.g., 1–3 standard deviations above the mean)—there is no use discriminating between “nothing”, “almost nothing”, and “a little bit.” For secondary prevention, i.e., early identification of risk to prevent something from getting worse, you would be interested in finding people with elevated risk—e.g., you would need to know who is 1 or more standard deviations above the mean, but you would not need to discriminate beyond that. For assessing individual differences, you would want items that discriminate across the full range, including at the lower end. The items’ difficulty should span the range of interest. Second, a good measure should have good items that yield lots of information. For example, the items should have strong discrimination, that is, the items are strongly related to the construct. The items should have sufficient variability in responses. This can be achieved by having items with more response options (e.g., likert/multiple choice items, as opposed to binary items), items that differ in difficulty, and (at least some) items that are not too difficult or too easy (to avoid ceiling/floor effects). Third, a good measure should have a good set of items that densely cover the construct within the range of interest, without redundancy. The items should not have the same difficulty or they would be considered redundant, and one of the redundant items could be dropped. The items’ difficulty should densely cover the construct within the range of interest. For instance, if the construct range of interest is 1–2 standard deviations above the mean, the items should have difficulty that densely cover this range (e.g., 1.0, 1.05, 1.10, 1.15, 1.20, 1.25, 1.30, …, 2.0). With items that (1) span the range of interest, (2) have high discrimination and information, and (3) densely cover the range of interest without redundancy, the measure should have a high information in the range of interest. This would allow it to efficiently and accurately assess the construct for the intended purpose. An example of a bad measure for assessing the full range of individual differences is depicted in terms of ICCs in Figure 8.24 and in terms of test information in Figure 8.25. The measure performs poorly for the intended purpose, because its items do not (a) span the range of interest (−3 to 3 standard deviations from the mean of the latent construct), (b) have high discrimination and information, and (c) densely cover the range of interest without redundancy. Figure 8.24: Visual Representation of a Bad Measure Based on Item Characteristic Curves of Items From a Bad Measure Estimated from Two-Parameter Logistic Model in Item Response Theory. Figure 8.25: Visual Representation of a Bad Measure Based on the Test Information Curve. An example of a good measure for distinguishing clinical-range versus sub-clinical range is depicted in terms of ICCs in Figure 8.26 and in terms of test information in Figure 8.27. The measure is good for the intended purpose, in terms of having items that (a) span the range of interest (1–3 standard deviations above the mean of the latent construct), (b) have high discrimination and information, and (c) densely cover the range of interest without redundancy. Figure 8.26: Visual Representation of a Good Measure (For Distinguishing Clinical-Range Versus Sub-clinical Range) Based on Item Characteristic Curves of Items From a Good Measure Estimated From Two-Parameter Logistic Model in Item Response Theory. Figure 8.27: Visual Representation of a Good Measure (For Distinguishing Clinical-Range Versus Sub-clinical Range) Based on the Test Information Curve. 8.1.8 Assumptions of IRT IRT has several assumptions: monotonicity unidimensionality item invariance local independence 8.1.8.1 Monotonicity The monotonicity assumption holds that a person’s probability of endorsing a higher level on the item increases as a person’s level on the latent construct increases. For instance, for each item assessing externalizing problems, as a child increases in their level of externalizing problems, they are expected to be rated with a higher level on that item. Monotonicity can be evaluated in multiple ways. For instance, monotonicity can be evaluated using visual inspection of empirical item characteristic curves. Another way to evaluate monotonicity is with Mokken scale analysis, such as using the mokken package in R. 8.1.8.2 Unidimensionality The unidimensionality assumption holds that the items have one predominant dimension, which reflects the underlying (latent) construct. The dimensionality of a set of items can be evaluated using factor analysis. Although items that are intended to assess a given latent latent construct are expected to be unidimensional, models have been developed that allow multiple latent dimensions, as shown in Section 8.6. These multidimensional IRT models allow borrowing information from a given latent factor in the estimation of other latent factor(s) to account for the covariation. 8.1.8.3 Item Invariance The item invariance assumption holds that the items function similarly (i.e., have the same parameters) for all people and subgroups in the population. The extent to which items may violate the item invariance assumption can be evaluated empirically using tests of differential item functioning (DIF). Tests of measurement invariance are the equivalent of tests of differential item functioning for factor analysis/structural equation models. Test of differential item functioning and measurement invariance are described in the chapter on test bias. 8.1.8.4 Local Independence The local independence assumptions holds that the items are uncorrelated when controlling for the latent dimension. That is, IRT models assume that the items’ errors (residuals) are uncorrelated with each other. Factor analysis and structural equation models can relax this assumption and allow items’ error terms to correlate with each other. 8.2 Getting Started 8.2.1 Load Libraries Code library(&quot;petersenlab&quot;) #to install: install.packages(&quot;remotes&quot;); remotes::install_github(&quot;DevPsyLab/petersenlab&quot;) library(&quot;mirt&quot;) library(&quot;lavaan&quot;) library(&quot;semTools&quot;) library(&quot;semPlot&quot;) library(&quot;lme4&quot;) library(&quot;MOTE&quot;) library(&quot;tidyverse&quot;) library(&quot;here&quot;) library(&quot;tinytex&quot;) 8.2.2 Load Data LSAT7 is a data set from the mirt package (Chalmers, 2020) that contains five items from the Law School Admissions Test. Code mydataIRT &lt;- expand.table(LSAT7) 8.2.3 Descriptive Statistics Code itemstats(mydataIRT, ts.tables = TRUE) $overall N mean_total.score sd_total.score ave.r sd.r alpha 1000 3.707 1.199 0.143 0.052 0.453 $itemstats N mean sd total.r total.r_if_rm alpha_if_rm Item.1 1000 0.828 0.378 0.530 0.246 0.396 Item.2 1000 0.658 0.475 0.600 0.247 0.394 Item.3 1000 0.772 0.420 0.611 0.313 0.345 Item.4 1000 0.606 0.489 0.592 0.223 0.415 Item.5 1000 0.843 0.364 0.461 0.175 0.438 $proportions 0 1 Item.1 0.172 0.828 Item.2 0.342 0.658 Item.3 0.228 0.772 Item.4 0.394 0.606 Item.5 0.157 0.843 $total.score_frequency 0 1 2 3 4 5 Freq 12 40 114 205 321 308 $total.score_means 0 1 Item.1 2.313953 3.996377 Item.2 2.710526 4.224924 Item.3 2.359649 4.104922 Item.4 2.827411 4.278878 Item.5 2.426752 3.945433 $total.score_sds 0 1 Item.1 1.162389 0.9841483 Item.2 1.058885 0.9038319 Item.3 1.087593 0.9043068 Item.4 1.103158 0.8661396 Item.5 1.177807 1.0415877 8.3 Comparison of Scoring Approaches A measure that is a raw symptom count (i.e., a count of how many symptoms a person endorses) is low in precision and has a high standard error of measurement. Some diagnostic measures provide an ordinal response scale for each symptom. For example, the Structured Clinical Interview of Mental Disorders (SCID) provides a response scale from 0 to 2, where 0 = the symptom is absent, 1 = the symptom is sub-threshold, and 2 = the symptom is present. If your measure was a raw symptom sum, as opposed to a count of how many symptoms were present, the measure would be slightly more precise and have a somewhat smaller standard error of measurement. A weighted symptom sum is the classical test theory analog of IRT. In classical test theory, proportion correct (or endorsed) would correspond to item difficulty and the item–total correlation (i.e., a point-biserial correlation) would correspond to item discrimination. If we were to compute a weighted sum of each item according to its strength of association with the construct (i.e., the item–total correlation), this measure would be somewhat more precise than the raw symptom sum, but it is not a latent variable method. In IRT analysis, the weight for each item influences the estimate of a person’s level on the construct. IRT down-weights the poorly discriminating items and up-weights the strongly discriminating items. This leads to greater precision and a lower standard error of measurement than non-latent scoring approaches. According to Embretson (1996), many perspectives have changed because of IRT. First, according to classical test theory, longer tests are more reliable than shorter tests, as described in Section 4.5.5.4 in the chapter on reliability. However, according to IRT, shorter tests (i.e., tests with fewer items) can be more reliable than longer tests. Item selection using IRT can lead to briefer assessments that have greater reliability than longer scales. For example, adaptive tests that tailor the difficulty of the items to the ability level of the participant. Second, in classical test theory, a score’s meaning is tied to its location in a distribution (i.e., the norm-referenced standard). In IRT, however, the people and items are calibrated on a common scale. Based on a child’s IRT-estimated ability level (i.e., level on the construct), we can have a better sense of what the child knows and does not know, because it indicates the difficulty level at which they would tend to get items correct 50% of the time; the person would likely fail items with a higher difficulty compared to this level, whereas the person would likely pass items with a lower difficulty compared to this level. Consider Binet’s distribution of ability that arranges the items from easiest to most difficult. Based on the item difficulty and content of the items and the child’s performance, we can have a better indication that a child can perform items successfully in a particular range (e.g., count to 10) but might not be able to perform more difficult items (e.g., tie their shoes). From an intervention perspective, this would allow working in the “window of opportunity” or the zone of proximal development. Thus, IRT can provide more meaningful understanding of a person’s ability compared to traditional classical test theory interpretations such as the child being at the “63rd percentile” for a child of their age, which lacks conceptual meaning. According to Cooper &amp; Balsis (2009), our current diagnostic system relies heavily on how many symptoms a person endorses as an index of severity, but this assumes that all symptom endorsements have the same overall weight (severity). Using IRT, we can determine the relative severity of each item (symptom)—and it is clear that some symptoms indicate more severity than others. From this analysis, a respondent can endorse fewer, more severe items, and have overall more severe psychopathology than an individual who endorses more, less severe items. Basically, not all items are equally severe—know your items! 8.4 Rasch Model (1-Parameter Logistic) A one-parameter logistic (1PL) item response theory (IRT) model is a model fit to dichotomous data, which estimates a different difficulty (\\(b\\)) parameter for each item. Discrimination (\\(a\\)) is not estimated (i.e., it is fixed at the same value—one—across items). Rasch models were fit using the mirt package (Chalmers, 2020). 8.4.1 Fit Model Code raschModel &lt;- mirt( data = mydataIRT, model = 1, itemtype = &quot;Rasch&quot;, SE = TRUE) 8.4.2 Model Summary Code summary(raschModel) F1 h2 Item.1 0.511 0.261 Item.2 0.511 0.261 Item.3 0.511 0.261 Item.4 0.511 0.261 Item.5 0.511 0.261 SS loadings: 1.304 Proportion Var: 0.261 Factor correlations: F1 F1 1 Code coef(raschModel, simplify = TRUE, IRTpars = TRUE) $items a b g u Item.1 1 -1.868 0 1 Item.2 1 -0.791 0 1 Item.3 1 -1.461 0 1 Item.4 1 -0.521 0 1 Item.5 1 -1.993 0 1 $means F1 0 $cov F1 F1 1.022 8.4.3 Factor Scores One can obtain factor scores (i.e., theta; and their associated standard errors) for each participant using the fscores() function. Code raschModel_factorScores &lt;- fscores(raschModel, full.scores.SE = TRUE) 8.4.4 Plots 8.4.4.1 Test Curves The test curves suggest that the measure is most reliable (i.e., provides the most information has the smallest standard error of measurement) at lower levels of the construct. 8.4.4.1.1 Test Characteristic Curve A test characteristic curve (TCC) plot of the expected total score as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) is in Figure 8.28. Code plot(raschModel, type = &quot;score&quot;) Figure 8.28: Test Characteristic Curve From Rasch Item Response Theory Model. 8.4.4.1.2 Test Information Curve A plot of test information as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) is in Figure 8.29. Code plot(raschModel, type = &quot;info&quot;) Figure 8.29: Test Information Curve From Rasch Item Response Theory Model. 8.4.4.1.3 Test Reliability The estimate of marginal reliability is below: Code marginal_rxx(raschModel) [1] 0.4205639 A plot of test reliability as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) is in Figure 8.30. Code plot(raschModel, type = &quot;rxx&quot;) Figure 8.30: Test Reliability From Rasch Item Response Theory Model. 8.4.4.1.4 Test Standard Error of Measurement A plot of test standard error of measurement (SEM) as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) is in Figure 8.31. Code plot(raschModel, type = &quot;SE&quot;) Figure 8.31: Test Standard Error of Measurement From Rasch Item Response Theory Model. 8.4.4.1.5 Test Information Curve and Test Standard Error of Measurement A plot of test information and standard error of measurement (SEM) as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) is in Figure 8.32. Code plot(raschModel, type = &quot;infoSE&quot;) Figure 8.32: Test Information Curve and Standard Error of Measurement From Rasch Item Response Theory Model. 8.4.4.2 Item Curves 8.4.4.2.1 Item Characteristic Curves Item characteristic curve (ICC) plots of the probability of item endorsement (or getting the item correct) as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) are in Figures 8.33 and 8.34. Code plot(raschModel, type = &quot;itemscore&quot;, facet_items = FALSE) Figure 8.33: Item Characteristic Curves From Rasch Item Response Theory Model. Code plot(raschModel, type = &quot;itemscore&quot;, facet_items = TRUE) Figure 8.34: Item Characteristic Curves From Rasch Item Response Theory Model. 8.4.4.2.2 Item Information Curves Plots of item information as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) are in Figures 8.35 and 8.36. Code plot(raschModel, type = &quot;infotrace&quot;, facet_items = FALSE) Figure 8.35: Item Information Curves from Rasch Item Response Theory Model. Code plot(raschModel, type = &quot;infotrace&quot;, facet_items = TRUE) Figure 8.36: Item Information Curves from Rasch Item Response Theory Model. 8.4.5 CFA A one-parameter logistic model can also be fit in a CFA framework, sometimes called item factor analysis. The item factor analysis models were fit in the lavaan package (Rosseel et al., 2022). Code onePLModel_cfa &lt;- &#39; # Factor Loadings (i.e., discrimination parameters) latent =~ loading*Item.1 + loading*Item.2 + loading*Item.3 + loading*Item.4 + loading*Item.5 # Item Thresholds (i.e., difficulty parameters) Item.1 | threshold1*t1 Item.2 | threshold2*t1 Item.3 | threshold3*t1 Item.4 | threshold4*t1 Item.5 | threshold5*t1 &#39; onePLModel_cfa_fit = sem( model = onePLModel_cfa, data = mydataIRT, ordered = c(&quot;Item.1&quot;, &quot;Item.2&quot;, &quot;Item.3&quot;, &quot;Item.4&quot;,&quot;Item.5&quot;), mimic = &quot;Mplus&quot;, estimator = &quot;WLSMV&quot;, std.lv = TRUE, parameterization = &quot;theta&quot;) summary( onePLModel_cfa_fit, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE) lavaan 0.6.17 ended normally after 13 iterations Estimator DWLS Optimization method NLMINB Number of model parameters 10 Number of equality constraints 4 Number of observations 1000 Model Test User Model: Standard Scaled Test Statistic 22.305 24.361 Degrees of freedom 9 9 P-value (Chi-square) 0.008 0.004 Scaling correction factor 0.926 Shift parameter 0.283 simple second-order correction (WLSMV) Model Test Baseline Model: Test statistic 244.385 228.667 Degrees of freedom 10 10 P-value 0.000 0.000 Scaling correction factor 1.072 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.943 0.930 Tucker-Lewis Index (TLI) 0.937 0.922 Robust Comparative Fit Index (CFI) 0.895 Robust Tucker-Lewis Index (TLI) 0.884 Root Mean Square Error of Approximation: RMSEA 0.038 0.041 90 Percent confidence interval - lower 0.019 0.022 90 Percent confidence interval - upper 0.059 0.061 P-value H_0: RMSEA &lt;= 0.050 0.808 0.738 P-value H_0: RMSEA &gt;= 0.080 0.000 0.000 Robust RMSEA 0.080 90 Percent confidence interval - lower 0.038 90 Percent confidence interval - upper 0.122 P-value H_0: Robust RMSEA &lt;= 0.050 0.106 P-value H_0: Robust RMSEA &gt;= 0.080 0.539 Standardized Root Mean Square Residual: SRMR 0.065 0.065 Parameter Estimates: Parameterization Theta Standard errors Robust.sem Information Expected Information saturated (h1) model Unstructured Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all latent =~ Item.1 (ldng) 0.599 0.038 15.831 0.000 0.599 0.514 Item.2 (ldng) 0.599 0.038 15.831 0.000 0.599 0.514 Item.3 (ldng) 0.599 0.038 15.831 0.000 0.599 0.514 Item.4 (ldng) 0.599 0.038 15.831 0.000 0.599 0.514 Item.5 (ldng) 0.599 0.038 15.831 0.000 0.599 0.514 Thresholds: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all Itm.1|1 (thr1) -1.103 0.056 -19.666 0.000 -1.103 -0.946 Itm.2|1 (thr2) -0.474 0.048 -9.826 0.000 -0.474 -0.407 Itm.3|1 (thr3) -0.869 0.052 -16.706 0.000 -0.869 -0.745 Itm.4|1 (thr4) -0.313 0.047 -6.678 0.000 -0.313 -0.269 Itm.5|1 (thr5) -1.174 0.058 -20.092 0.000 -1.174 -1.007 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .Item.1 1.000 1.000 0.736 .Item.2 1.000 1.000 0.736 .Item.3 1.000 1.000 0.736 .Item.4 1.000 1.000 0.736 .Item.5 1.000 1.000 0.736 latent 1.000 1.000 1.000 Scales y*: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all Item.1 0.858 0.858 1.000 Item.2 0.858 0.858 1.000 Item.3 0.858 0.858 1.000 Item.4 0.858 0.858 1.000 Item.5 0.858 0.858 1.000 R-Square: Estimate Item.1 0.264 Item.2 0.264 Item.3 0.264 Item.4 0.264 Item.5 0.264 Code fitMeasures( onePLModel_cfa_fit, fit.measures = c( &quot;chisq&quot;, &quot;df&quot;, &quot;pvalue&quot;, &quot;baseline.chisq&quot;,&quot;baseline.df&quot;,&quot;baseline.pvalue&quot;, &quot;rmsea&quot;, &quot;cfi&quot;, &quot;tli&quot;, &quot;srmr&quot;)) chisq df pvalue baseline.chisq baseline.df 22.305 9.000 0.008 244.385 10.000 baseline.pvalue rmsea cfi tli srmr 0.000 0.038 0.943 0.937 0.065 Code residuals(onePLModel_cfa_fit, type = &quot;cor&quot;) $type [1] &quot;cor.bollen&quot; $cov Item.1 Item.2 Item.3 Item.4 Item.5 Item.1 0.000 Item.2 -0.038 0.000 Item.3 0.026 0.168 0.000 Item.4 0.032 -0.061 0.012 0.000 Item.5 0.022 -0.129 0.001 -0.104 0.000 $mean Item.1 Item.2 Item.3 Item.4 Item.5 0 0 0 0 0 $th Item.1|t1 Item.2|t1 Item.3|t1 Item.4|t1 Item.5|t1 0 0 0 0 0 Code modificationindices(onePLModel_cfa_fit, sort. = TRUE) Code compRelSEM(onePLModel_cfa_fit) latent 0.467 Code AVE(onePLModel_cfa_fit) latent 0.264 Code onePLModel_cfa_factorScores &lt;- lavPredict(onePLModel_cfa_fit) A path diagram of the one-parameter item factor analysis is in Figure 8.37. Code semPaths( onePLModel_cfa_fit, what = &quot;std&quot;, layout = &quot;tree2&quot;, edge.label.cex = 1.5) Figure 8.37: Item Factor Analysis Diagram of One-Parameter Logistic Model. 8.4.6 Mixed Model A Rasch model can also be fit in a mixed model framework. The Rasch model below was fit using the lme4 package (Bates et al., 2022). First, we convert the data from wide form to long form for the mixed model: Code mydataIRT_long &lt;- mydataIRT %&gt;% mutate(ID = 1:nrow(mydataIRT)) %&gt;% pivot_longer(cols = Item.1:Item.5) %&gt;% rename( item = name, response = value) Then, we can estimate the Rasch model using a logit or probit link: Code raschModel_mixed_logit &lt;- glmer( response ~ -1 + item + (1|ID), mydataIRT_long, family = binomial(link = &quot;logit&quot;)) summary(raschModel_mixed_logit) Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [glmerMod] Family: binomial ( logit ) Formula: response ~ -1 + item + (1 | ID) Data: mydataIRT_long AIC BIC logLik deviance df.resid 5354.5 5393.6 -2671.3 5342.5 4994 Scaled residuals: Min 1Q Median 3Q Max -2.7785 -0.6631 0.3599 0.5639 1.5080 Random effects: Groups Name Variance Std.Dev. ID (Intercept) 0.9057 0.9517 Number of obs: 5000, groups: ID, 1000 Fixed effects: Estimate Std. Error z value Pr(&gt;|z|) itemItem.1 1.85118 0.09903 18.693 &lt; 2e-16 *** itemItem.2 0.78581 0.08025 9.792 &lt; 2e-16 *** itemItem.3 1.45004 0.09015 16.085 &lt; 2e-16 *** itemItem.4 0.51687 0.07787 6.637 3.2e-11 *** itemItem.5 1.97373 0.10221 19.310 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Correlation of Fixed Effects: itmI.1 itmI.2 itmI.3 itmI.4 itemItem.2 0.173 itemItem.3 0.192 0.178 itemItem.4 0.158 0.169 0.166 itemItem.5 0.192 0.170 0.191 0.155 Code raschModel_mixed_probit &lt;- glmer( response ~ -1 + item + (1|ID), mydataIRT_long, family = binomial(link = &quot;probit&quot;)) summary(raschModel_mixed_probit) Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [glmerMod] Family: binomial ( probit ) Formula: response ~ -1 + item + (1 | ID) Data: mydataIRT_long AIC BIC logLik deviance df.resid 5362.0 5401.1 -2675.0 5350.0 4994 Scaled residuals: Min 1Q Median 3Q Max -2.7185 -0.6990 0.3660 0.5725 1.4276 Random effects: Groups Name Variance Std.Dev. ID (Intercept) 0.2992 0.547 Number of obs: 5000, groups: ID, 1000 Fixed effects: Estimate Std. Error z value Pr(&gt;|z|) itemItem.1 1.10493 0.05634 19.612 &lt; 2e-16 *** itemItem.2 0.47513 0.04801 9.896 &lt; 2e-16 *** itemItem.3 0.87437 0.05262 16.615 &lt; 2e-16 *** itemItem.4 0.31178 0.04689 6.649 2.95e-11 *** itemItem.5 1.16956 0.05739 20.379 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Correlation of Fixed Effects: itmI.1 itmI.2 itmI.3 itmI.4 itemItem.2 0.155 itemItem.3 0.176 0.158 itemItem.4 0.142 0.149 0.147 itemItem.5 0.180 0.154 0.175 0.141 One can extract item difficulty and people’s factor scores (i.e., theta), as adapted from James Uanhoro (https://www.jamesuanhoro.com/post/2018/01/02/using-glmer-to-perform-rasch-analysis/; archived at: https://perma.cc/84WP-TQBG): Code # Item difficulty item.diff &lt;- -1 * coef(summary(raschModel_mixed_logit))[,&quot;Estimate&quot;] # Regression coefficients * -1 item.diff &lt;- data.frame( item = paste(&quot;Item&quot;, 1:5, sep = &quot;.&quot;), item.diff = as.numeric(item.diff)) item.diff Code # Factor Scores (Theta) raschModel_mixed_logit_theta &lt;- ranef(raschModel_mixed_logit)$ID[,&quot;(Intercept)&quot;] 8.5 Two-Parameter Logistic Model A two-parameter logistic (2PL) IRT model is a model fit to dichotomous data, which estimates a different difficulty (\\(b\\)) and discrimination (\\(a\\)) parameter for each item. 2PL models were fit using the mirt package (Chalmers, 2020). 8.5.1 Fit Model Code twoPLModel &lt;- mirt( data = mydataIRT, model = 1, itemtype = &quot;2PL&quot;, SE = TRUE) 8.5.2 Model Summary Code summary(twoPLModel) F1 h2 Item.1 0.502 0.252 Item.2 0.536 0.287 Item.3 0.708 0.501 Item.4 0.410 0.168 Item.5 0.397 0.157 SS loadings: 1.366 Proportion Var: 0.273 Factor correlations: F1 F1 1 Code coef(twoPLModel, simplify = TRUE, IRTpars = TRUE) $items a b g u Item.1 0.988 -1.879 0 1 Item.2 1.081 -0.748 0 1 Item.3 1.706 -1.058 0 1 Item.4 0.765 -0.635 0 1 Item.5 0.736 -2.520 0 1 $means F1 0 $cov F1 F1 1 8.5.3 Factor Scores Code twoPLModel_factorScores &lt;- fscores(twoPLModel, full.scores.SE = TRUE) 8.5.4 Plots 8.5.4.1 Test Curves The test curves suggest that the measure is most reliable (i.e., provides the most information has the smallest standard error of measurement) at lower levels of the construct. 8.5.4.1.1 Test Characteristic Curve A test characteristic curve (TCC) plot of the expected total score as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) is in Figure 8.38. Code plot(twoPLModel, type = &quot;score&quot;) Figure 8.38: Test Characteristic Curve From Two-Parameter Logistic Item Response Theory Model. 8.5.4.1.2 Test Information Curve A plot of test information as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) is in Figure 8.39. Code plot(twoPLModel, type = &quot;info&quot;) Figure 8.39: Test Information Curve From Two-Parameter Logistic Item Response Theory Model. 8.5.4.1.3 Test Reliability The estimate of marginal reliability is below: Code marginal_rxx(twoPLModel) [1] 0.4417618 A plot of test reliability as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) is in Figure 8.40. Code plot(twoPLModel, type = &quot;rxx&quot;) Figure 8.40: Test Reliability From Two-Parameter Logistic Item Response Theory Model. 8.5.4.1.4 Test Standard Error of Measurement A plot of test standard error of measurement (SEM) as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) is in Figure 8.41. Code plot(twoPLModel, type = &quot;SE&quot;) Figure 8.41: Test Standard Error of Measurement From Two-Parameter Logistic Item Response Theory Model. 8.5.4.1.5 Test Information Curve and Standard Errors A plot of test information and standard error of measurement (SEM) as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) is in Figure 8.42. Code plot(twoPLModel, type = &quot;infoSE&quot;) Figure 8.42: Test Information Curve and Standard Error of Measurement From Two-Parameter Logistic Item Response Theory Model. 8.5.4.2 Item Curves 8.5.4.2.1 Item Characteristic Curves Item characteristic curve (ICC) plots of the probability of item endorsement (or getting the item correct) as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) are in Figures 8.43 and 8.44. Code plot(twoPLModel, type = &quot;itemscore&quot;, facet_items = FALSE) Figure 8.43: Item Characteristic Curves From Two-Parameter Logistic Item Response Theory Model. Code plot(twoPLModel, type = &quot;itemscore&quot;, facet_items = TRUE) Figure 8.44: Item Characteristic Curves From Two-Parameter Logistic Item Response Theory Model. 8.5.4.2.2 Item Information Curves Plots of item information as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) are in Figures 8.45 and 8.46. Code plot(twoPLModel, type = &quot;infotrace&quot;, facet_items = FALSE) Figure 8.45: Item Information Curves From Two-Parameter Logistic Item Response Theory Model. Code plot(twoPLModel, type = &quot;infotrace&quot;, facet_items = TRUE) Figure 8.46: Item Information Curves From Two-Parameter Logistic Item Response Theory Model. 8.5.4.3 Convert Discrimination To Factor Loading As described by Aiden Loe (archived at https://perma.cc/H3QN-JAWW), one can convert a discrimination parameter to a standardized factor loading using Equation (8.14): \\[\\begin{equation} f = \\frac{a}{\\sqrt{1 + a^2}} \\tag{8.14} \\end{equation}\\] where \\(a\\) is equal to: \\(\\text{discrimination}/1.702\\). The petersenlab package (Petersen, 2024b) contains the discriminationToFactorLoading() function that converts discrimination parameters to standardized factor loadings. Code discriminationParameters &lt;- coef( twoPLModel, simplify = TRUE)$items[,1] discriminationParameters Item.1 Item.2 Item.3 Item.4 Item.5 0.9879254 1.0808847 1.7058006 0.7651853 0.7357980 Code discriminationToFactorLoading(discriminationParameters) Item.1 Item.2 Item.3 Item.4 Item.5 0.5020091 0.5360964 0.7078950 0.4100462 0.3968194 8.5.5 CFA A two-parameter logistic model can also be fit in a CFA framework, sometimes called item factor analysis. The item factor analysis models were fit in the lavaan package (Rosseel et al., 2022). Code twoPLModel_cfa &lt;- &#39; # Factor Loadings (i.e., discrimination parameters) latent =~ loading1*Item.1 + loading2*Item.2 + loading3*Item.3 + loading4*Item.4 + loading5*Item.5 # Item Thresholds (i.e., difficulty parameters) Item.1 | threshold1*t1 Item.2 | threshold2*t1 Item.3 | threshold3*t1 Item.4 | threshold4*t1 Item.5 | threshold5*t1 &#39; twoPLModel_cfa_fit = sem( model = twoPLModel_cfa, data = mydataIRT, ordered = c(&quot;Item.1&quot;, &quot;Item.2&quot;, &quot;Item.3&quot;, &quot;Item.4&quot;,&quot;Item.5&quot;), mimic = &quot;Mplus&quot;, estimator = &quot;WLSMV&quot;, std.lv = TRUE, parameterization = &quot;theta&quot;) summary( twoPLModel_cfa_fit, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE) lavaan 0.6.17 ended normally after 28 iterations Estimator DWLS Optimization method NLMINB Number of model parameters 10 Number of observations 1000 Model Test User Model: Standard Scaled Test Statistic 9.131 11.688 Degrees of freedom 5 5 P-value (Chi-square) 0.104 0.039 Scaling correction factor 0.784 Shift parameter 0.041 simple second-order correction (WLSMV) Model Test Baseline Model: Test statistic 244.385 228.667 Degrees of freedom 10 10 P-value 0.000 0.000 Scaling correction factor 1.072 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.982 0.969 Tucker-Lewis Index (TLI) 0.965 0.939 Robust Comparative Fit Index (CFI) 0.943 Robust Tucker-Lewis Index (TLI) 0.886 Root Mean Square Error of Approximation: RMSEA 0.029 0.037 90 Percent confidence interval - lower 0.000 0.008 90 Percent confidence interval - upper 0.058 0.064 P-value H_0: RMSEA &lt;= 0.050 0.871 0.757 P-value H_0: RMSEA &gt;= 0.080 0.001 0.004 Robust RMSEA 0.079 90 Percent confidence interval - lower 0.015 90 Percent confidence interval - upper 0.139 P-value H_0: Robust RMSEA &lt;= 0.050 0.176 P-value H_0: Robust RMSEA &gt;= 0.080 0.547 Standardized Root Mean Square Residual: SRMR 0.045 0.045 Parameter Estimates: Parameterization Theta Standard errors Robust.sem Information Expected Information saturated (h1) model Unstructured Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all latent =~ Item.1 (ldn1) 0.587 0.099 5.916 0.000 0.587 0.506 Item.2 (ldn2) 0.627 0.099 6.338 0.000 0.627 0.531 Item.3 (ldn3) 0.979 0.175 5.594 0.000 0.979 0.699 Item.4 (ldn4) 0.479 0.076 6.325 0.000 0.479 0.432 Item.5 (ldn5) 0.417 0.084 4.961 0.000 0.417 0.384 Thresholds: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all Itm.1|1 (thr1) -1.097 0.070 -15.602 0.000 -1.097 -0.946 Itm.2|1 (thr2) -0.480 0.052 -9.200 0.000 -0.480 -0.407 Itm.3|1 (thr3) -1.043 0.108 -9.623 0.000 -1.043 -0.745 Itm.4|1 (thr4) -0.298 0.045 -6.556 0.000 -0.298 -0.269 Itm.5|1 (thr5) -1.091 0.060 -18.265 0.000 -1.091 -1.007 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .Item.1 1.000 1.000 0.743 .Item.2 1.000 1.000 0.718 .Item.3 1.000 1.000 0.511 .Item.4 1.000 1.000 0.813 .Item.5 1.000 1.000 0.852 latent 1.000 1.000 1.000 Scales y*: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all Item.1 0.862 0.862 1.000 Item.2 0.847 0.847 1.000 Item.3 0.715 0.715 1.000 Item.4 0.902 0.902 1.000 Item.5 0.923 0.923 1.000 R-Square: Estimate Item.1 0.257 Item.2 0.282 Item.3 0.489 Item.4 0.187 Item.5 0.148 Code fitMeasures( twoPLModel_cfa_fit, fit.measures = c( &quot;chisq&quot;, &quot;df&quot;, &quot;pvalue&quot;, &quot;baseline.chisq&quot;,&quot;baseline.df&quot;,&quot;baseline.pvalue&quot;, &quot;rmsea&quot;, &quot;cfi&quot;, &quot;tli&quot;, &quot;srmr&quot;)) chisq df pvalue baseline.chisq baseline.df 9.131 5.000 0.104 244.385 10.000 baseline.pvalue rmsea cfi tli srmr 0.000 0.029 0.982 0.965 0.045 Code residuals(twoPLModel_cfa_fit, type = &quot;cor&quot;) $type [1] &quot;cor.bollen&quot; $cov Item.1 Item.2 Item.3 Item.4 Item.5 Item.1 0.000 Item.2 -0.043 0.000 Item.3 -0.064 0.060 0.000 Item.4 0.077 -0.026 -0.026 0.000 Item.5 0.091 -0.069 -0.004 -0.006 0.000 $mean Item.1 Item.2 Item.3 Item.4 Item.5 0 0 0 0 0 $th Item.1|t1 Item.2|t1 Item.3|t1 Item.4|t1 Item.5|t1 0 0 0 0 0 Code modificationindices(twoPLModel_cfa_fit, sort. = TRUE) Code compRelSEM(twoPLModel_cfa_fit) latent 0.468 Code AVE(twoPLModel_cfa_fit) latent 0.296 Code twoPLModel_cfa_factorScores &lt;- lavPredict(twoPLModel_cfa_fit) Code semPaths( twoPLModel_cfa_fit, what = &quot;std&quot;, layout = &quot;tree2&quot;, edge.label.cex = 1.5) Figure 8.47: Item Factor Analysis Diagram of Two-Parameter Logistic Model. 8.6 Two-Parameter Multidimensional Logistic Model A 2PL multidimensional IRT model is a model that allows multiple dimensions (latent factors) and is fit to dichotomous data, which estimates a different difficulty (\\(b\\)) and discrimination (\\(a\\)) parameter for each item. Multidimensional IRT models were fit using the mirt package (Chalmers, 2020). In this example, I estimate a 2PL multidimensional IRT model by estimating two factors. 8.6.1 Fit Model Code twoPL2FactorModel &lt;- mirt( data = mydataIRT, model = 2, itemtype = &quot;2PL&quot;, SE = TRUE) 8.6.2 Model Summary Code summary(twoPL2FactorModel) Rotation: oblimin Rotated factor loadings: F1 F2 h2 Item.1 0.7943 -0.0111 0.623 Item.2 0.0804 0.4630 0.255 Item.3 -0.0129 0.8628 0.734 Item.4 0.2794 0.1925 0.165 Item.5 0.2930 0.1772 0.165 Rotated SS loadings: 0.801 1.027 Factor correlations: F1 F2 F1 1.000 F2 0.463 1 Code coef(twoPL2FactorModel, simplify = TRUE) $items a1 a2 d g u Item.1 -2.007 0.870 2.648 0 1 Item.2 -0.849 -0.522 0.788 0 1 Item.3 -2.153 -1.836 2.483 0 1 Item.4 -0.756 -0.028 0.485 0 1 Item.5 -0.757 0.000 1.864 0 1 $means F1 F2 0 0 $cov F1 F2 F1 1 0 F2 0 1 8.6.3 Factor Scores Code twoPL2FactorModel_factorScores &lt;- fscores(twoPL2FactorModel, full.scores.SE = TRUE) 8.6.4 Compare model fit The modified model with two factors and the original one-factor model are considered “nested” models. The original model is nested within the modified model because the modified model includes all of the terms of the original model along with additional terms. Model fit of nested models can be compared with a chi-square difference test. Code anova(twoPLModel, twoPL2FactorModel) Using a chi-square difference test to compare two nested models, the two-factor model fits significantly better than the one-factor model. 8.6.5 Plots 8.6.5.1 Test Curves 8.6.5.1.1 Test Characteristic Curve A test characteristic curve (TCC) plot of the expected total score as a function of a person’s level on each latent construct (theta; \\(\\theta\\)) is in Figure 8.48. Code plot(twoPL2FactorModel, type = &quot;score&quot;) Figure 8.48: Test Characteristic Curve From Two-Parameter Multidimensional Item Response Theory Model. 8.6.5.1.2 Test Information Curve A plot of test information as a function of a person’s level on each latent construct (theta; \\(\\theta\\)) is in Figure 8.49. Code plot(twoPL2FactorModel, type = &quot;info&quot;) Figure 8.49: Test Information Curve From Two-Parameter Multidimensional Item Response Theory Model. 8.6.5.1.3 Test Standard Error of Measurement A plot of test standard error of measurement (SEM) as a function of a person’s level on each latent construct (theta; \\(\\theta\\)) is in Figure 8.50. Code plot(twoPL2FactorModel, type = &quot;SE&quot;) Figure 8.50: Test Standard Error of Measurement From Two-Parameter Multidimensional Item Response Theory Model. 8.6.5.2 Item Curves 8.6.5.2.1 Item Characteristic Curves Item characteristic curve (ICC) plots of the probability of item endorsement (or getting the item correct) as a function of a person’s level on each latent construct (theta; \\(\\theta\\)) are in Figures 8.51 and 8.52. Code plot(twoPL2FactorModel, type = &quot;itemscore&quot;, facet_items = FALSE) Figure 8.51: Item Characteristic Curves From Two-Parameter Multidimensional Item Response Theory Model. Code plot(twoPL2FactorModel, type = &quot;itemscore&quot;, facet_items = TRUE) Figure 8.52: Item Characteristic Curves From Two-Parameter Multidimensional Item Response Theory Model. 8.6.5.2.2 Item Information Curves Plots of item information as a function of a person’s level on each latent construct (theta; \\(\\theta\\)) are in Figures 8.53 and 8.54. Code plot(twoPL2FactorModel, type = &quot;infotrace&quot;, facet_items = FALSE) Figure 8.53: Item Information Curves From Two-Parameter Multidimensional Item Response Theory Model. Code plot(twoPL2FactorModel, type = &quot;infotrace&quot;, facet_items = TRUE) Figure 8.54: Item Information Curves From Two-Parameter Multidimensional Item Response Theory Model. 8.6.6 CFA A two-parameter multidimensional model can also be fit in a CFA framework, sometimes called item factor analysis. The item factor analysis models were fit in the lavaan package (Rosseel et al., 2022). Code twoPLModelMultidimensional_cfa &lt;- &#39; # Factor Loadings (i.e., discrimination parameters) latent1 =~ loading1*Item.1 + loading4*Item.4 + loading5*Item.5 latent2 =~ loading2*Item.2 + loading3*Item.3 # Item Thresholds (i.e., difficulty parameters) Item.1 | threshold1*t1 Item.2 | threshold2*t1 Item.3 | threshold3*t1 Item.4 | threshold4*t1 Item.5 | threshold5*t1 &#39; twoPLModelMultidimensional_cfa_fit = sem( model = twoPLModelMultidimensional_cfa, data = mydataIRT, ordered = c(&quot;Item.1&quot;, &quot;Item.2&quot;, &quot;Item.3&quot;, &quot;Item.4&quot;,&quot;Item.5&quot;), mimic = &quot;Mplus&quot;, estimator = &quot;WLSMV&quot;, std.lv = TRUE, parameterization = &quot;theta&quot;) summary( twoPLModelMultidimensional_cfa_fit, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE) lavaan 0.6.17 ended normally after 41 iterations Estimator DWLS Optimization method NLMINB Number of model parameters 11 Number of observations 1000 Model Test User Model: Standard Scaled Test Statistic 1.882 2.469 Degrees of freedom 4 4 P-value (Chi-square) 0.757 0.650 Scaling correction factor 0.775 Shift parameter 0.039 simple second-order correction (WLSMV) Model Test Baseline Model: Test statistic 244.385 228.667 Degrees of freedom 10 10 P-value 0.000 0.000 Scaling correction factor 1.072 User Model versus Baseline Model: Comparative Fit Index (CFI) 1.000 1.000 Tucker-Lewis Index (TLI) 1.023 1.018 Robust Comparative Fit Index (CFI) 1.000 Robust Tucker-Lewis Index (TLI) 1.032 Root Mean Square Error of Approximation: RMSEA 0.000 0.000 90 Percent confidence interval - lower 0.000 0.000 90 Percent confidence interval - upper 0.033 0.038 P-value H_0: RMSEA &lt;= 0.050 0.994 0.989 P-value H_0: RMSEA &gt;= 0.080 0.000 0.000 Robust RMSEA 0.000 90 Percent confidence interval - lower 0.000 90 Percent confidence interval - upper 0.088 P-value H_0: Robust RMSEA &lt;= 0.050 0.798 P-value H_0: Robust RMSEA &gt;= 0.080 0.072 Standardized Root Mean Square Residual: SRMR 0.021 0.021 Parameter Estimates: Parameterization Theta Standard errors Robust.sem Information Expected Information saturated (h1) model Unstructured Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all latent1 =~ Item.1 (ldn1) 0.731 0.139 5.268 0.000 0.731 0.590 Item.4 (ldn4) 0.560 0.094 5.962 0.000 0.560 0.488 Item.5 (ldn5) 0.472 0.098 4.832 0.000 0.472 0.427 latent2 =~ Item.2 (ldn2) 0.660 0.114 5.774 0.000 0.660 0.551 Item.3 (ldn3) 1.265 0.358 3.529 0.000 1.265 0.784 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all latent1 ~~ latent2 0.696 0.090 7.718 0.000 0.696 0.696 Thresholds: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all Itm.1|1 (thr1) -1.172 0.095 -12.280 0.000 -1.172 -0.946 Itm.2|1 (thr2) -0.488 0.055 -8.923 0.000 -0.488 -0.407 Itm.3|1 (thr3) -1.202 0.218 -5.502 0.000 -1.202 -0.745 Itm.4|1 (thr4) -0.308 0.048 -6.442 0.000 -0.308 -0.269 Itm.5|1 (thr5) -1.114 0.066 -16.862 0.000 -1.114 -1.007 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .Item.1 1.000 1.000 0.652 .Item.4 1.000 1.000 0.761 .Item.5 1.000 1.000 0.818 .Item.2 1.000 1.000 0.697 .Item.3 1.000 1.000 0.385 latent1 1.000 1.000 1.000 latent2 1.000 1.000 1.000 Scales y*: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all Item.1 0.807 0.807 1.000 Item.4 0.873 0.873 1.000 Item.5 0.904 0.904 1.000 Item.2 0.835 0.835 1.000 Item.3 0.620 0.620 1.000 R-Square: Estimate Item.1 0.348 Item.4 0.239 Item.5 0.182 Item.2 0.303 Item.3 0.615 Code fitMeasures( twoPLModelMultidimensional_cfa_fit, fit.measures = c( &quot;chisq&quot;, &quot;df&quot;, &quot;pvalue&quot;, &quot;baseline.chisq&quot;,&quot;baseline.df&quot;,&quot;baseline.pvalue&quot;, &quot;rmsea&quot;, &quot;cfi&quot;, &quot;tli&quot;, &quot;srmr&quot;)) chisq df pvalue baseline.chisq baseline.df 1.882 4.000 0.757 244.385 10.000 baseline.pvalue rmsea cfi tli srmr 0.000 0.000 1.000 1.023 0.021 Code residuals(twoPLModelMultidimensional_cfa_fit, type = &quot;cor&quot;) $type [1] &quot;cor.bollen&quot; $cov Item.1 Item.4 Item.5 Item.2 Item.3 Item.1 0.000 Item.4 0.008 0.000 Item.5 0.034 -0.048 0.000 Item.2 0.000 0.016 -0.028 0.000 Item.3 -0.031 0.009 0.032 0.000 0.000 $mean Item.1 Item.4 Item.5 Item.2 Item.3 0 0 0 0 0 $th Item.1|t1 Item.4|t1 Item.5|t1 Item.2|t1 Item.3|t1 0 0 0 0 0 Code modificationindices(twoPLModelMultidimensional_cfa_fit, sort. = TRUE) Code compRelSEM(twoPLModelMultidimensional_cfa_fit) latent1 latent2 0.321 0.426 Code AVE(twoPLModelMultidimensional_cfa_fit) latent1 latent2 0.263 0.504 Code twoPLModelMultidimensional_cfa_factorScores &lt;- lavPredict(twoPLModelMultidimensional_cfa_fit) Code semPaths( twoPLModelMultidimensional_cfa_fit, what = &quot;std&quot;, layout = &quot;tree2&quot;, edge.label.cex = 1.5) Figure 8.55: Item Factor Analysis Diagram of Two-Parameter Multidimensional Logistic Model. 8.7 Three-Parameter Logistic Model A three-parameter logistic (3PL) IRT model is a model fit to dichotomous data, which estimates a different difficulty (\\(b\\)), discrimination (\\(a\\)), and guessing parameter for each item. 3PL models were fit using the mirt package (Chalmers, 2020). 8.7.1 Fit Model Code threePLModel &lt;- mirt( data = mydataIRT, model = 1, itemtype = &quot;3PL&quot;, SE = TRUE) 8.7.2 Model Summary Code summary(threePLModel) F1 h2 Item.1 0.509 0.259 Item.2 0.750 0.562 Item.3 0.700 0.489 Item.4 0.397 0.158 Item.5 0.411 0.169 SS loadings: 1.637 Proportion Var: 0.327 Factor correlations: F1 F1 1 Code coef(threePLModel, simplify = TRUE, IRTpars = TRUE) $items a b g u Item.1 1.007 -1.853 0.000 1 Item.2 1.928 -0.049 0.295 1 Item.3 1.667 -1.068 0.000 1 Item.4 0.736 -0.655 0.000 1 Item.5 0.767 -2.436 0.000 1 $means F1 0 $cov F1 F1 1 8.7.3 Factor Scores Code threePLModel_factorScores &lt;- fscores(threePLModel, full.scores.SE = TRUE) 8.7.4 Plots 8.7.4.1 Test Curves The test curves suggest that the measure is most reliable (i.e., provides the most information has the smallest standard error of measurement) at lower levels of the construct. 8.7.4.1.1 Test Characteristic Curve A test characteristic curve (TCC) plot of the expected total score as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) is in Figure 8.56. Code plot(threePLModel, type = &quot;score&quot;) Figure 8.56: Test Characteristic Curve From Three-Parameter Logistic Item Response Theory Model. 8.7.4.1.2 Test Information Curve A plot of test information as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) is in Figure 8.57. Code plot(threePLModel, type = &quot;info&quot;) Figure 8.57: Test Information Curve From Three-Parameter Logistic Item Response Theory Model. 8.7.4.1.3 Test Reliability The estimate of marginal reliability is below: Code marginal_rxx(threePLModel) [1] 0.4681812 A plot of test reliability as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) is in Figure 8.58. Code plot(threePLModel, type = &quot;rxx&quot;) Figure 8.58: Test Reliability From Three-Parameter Logistic Item Response Theory Model. 8.7.4.1.4 Test Standard Error of Measurement A plot of test standard error of measurement (SEM) as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) is in Figure 8.59. Code plot(threePLModel, type = &quot;SE&quot;) Figure 8.59: Test Standard Error of Measurement From Three-Parameter Logistic Item Response Theory Model. 8.7.4.1.5 Test Information Curve and Standard Errors A plot of test information and standard error of measurement (SEM) as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) is in Figure 8.60. Code plot(threePLModel, type = &quot;infoSE&quot;) Figure 8.60: Test Information Curve and Standard Error of Measurement From Three-Parameter Logistic Item Response Theory Model. 8.7.4.2 Item Curves 8.7.4.2.1 Item Characteristic Curves Item characteristic curve (ICC) plots of the probability of item endorsement (or getting the item correct) as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) are in Figures 8.61 and 8.62. Code plot(threePLModel, type = &quot;itemscore&quot;, facet_items = FALSE) Figure 8.61: Item Characteristic Curves From Three-Parameter Logistic Item Response Theory Model. Code plot(threePLModel, type = &quot;itemscore&quot;, facet_items = TRUE) Figure 8.62: Item Characteristic Curves From Three-Parameter Logistic Item Response Theory Model. 8.7.4.2.2 Item Information Curves Plots of item information as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) are in Figures 8.63 and 8.64. Code plot(threePLModel, type = &quot;infotrace&quot;, facet_items = FALSE) Figure 8.63: Item Information Curves From Three-Parameter Logistic Item Response Theory Model. Code plot(threePLModel, type = &quot;infotrace&quot;, facet_items = TRUE) Figure 8.64: Item Information Curves From Three-Parameter Logistic Item Response Theory Model. 8.8 Four-Parameter Logistic Model A four-parameter logistic (4PL) IRT model is a model fit to dichotomous data, which estimates a different difficulty (\\(b\\)), discrimination (\\(a\\)), guessing, and careless errors parameter for each item. 4PL models were fit using the mirt package (Chalmers, 2020). 8.8.1 Fit Model Code fourPLModel &lt;- mirt( data = mydataIRT, model = 1, itemtype = &quot;4PL&quot;, SE = TRUE, technical = list(NCYCLES = 2000)) 8.8.2 Model Summary Code summary(fourPLModel) F1 h2 Item.1 0.834 0.695 Item.2 0.980 0.961 Item.3 0.762 0.580 Item.4 0.876 0.768 Item.5 0.648 0.420 SS loadings: 3.425 Proportion Var: 0.685 Factor correlations: F1 F1 1 Code coef(fourPLModel, simplify = TRUE, IRTpars = TRUE) $items a b g u Item.1 2.570 -1.619 0.000 0.911 Item.2 8.490 0.093 0.370 0.992 Item.3 2.002 -0.646 0.271 0.999 Item.4 3.094 -1.224 0.000 0.708 Item.5 1.450 -2.166 0.002 0.920 $means F1 0 $cov F1 F1 1 8.8.3 Factor Scores Code fourPLModel_factorScores &lt;- fscores(fourPLModel, full.scores.SE = TRUE) 8.8.4 Plots 8.8.4.1 Test Curves The test curves suggest that the measure is most reliable (i.e., provides the most information has the smallest standard error of measurement) at middle to lower levels of the construct. 8.8.4.1.1 Test Characteristic Curve A test characteristic curve (TCC) plot of the expected total score as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) is in Figure 8.65. Code plot(fourPLModel, type = &quot;score&quot;) Figure 8.65: Test Characteristic Curve From Four-Parameter Logistic Item Response Theory Model. 8.8.4.1.2 Test Information Curve A plot of test information as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) is in Figure 8.66. Code plot(fourPLModel, type = &quot;info&quot;) Figure 8.66: Test Information Curve From Four-Parameter Logistic Item Response Theory Model. 8.8.4.1.3 Test Reliability The estimate of marginal reliability is below: Code marginal_rxx(fourPLModel) [1] 0.5060376 A plot of test reliability as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) is in Figure 8.67. Code plot(fourPLModel, type = &quot;rxx&quot;) Figure 8.67: Test Reliability From Four-Parameter Logistic Item Response Theory Model. 8.8.4.1.4 Test Standard Error of Measurement A plot of test standard error of measurement (SEM) as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) is in Figure 8.68. Code plot(fourPLModel, type = &quot;SE&quot;) Figure 8.68: Test Standard Error of Measurement From Four-Parameter Logistic Item Response Theory Model. 8.8.4.1.5 Test Information Curve and Standard Errors A plot of test information and standard error of measurement (SEM) as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) is in Figure 8.69. Code plot(fourPLModel, type = &quot;infoSE&quot;) Figure 8.69: Test Information Curve and Standard Error of Measurement From Four-Parameter Logistic Item Response Theory Model. 8.8.4.2 Item Curves 8.8.4.2.1 Item Characteristic Curves Item characteristic curve (ICC) plots of the probability of item endorsement (or getting the item correct) as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) are in Figures 8.70 and 8.71. Code plot(fourPLModel, type = &quot;itemscore&quot;, facet_items = FALSE) Figure 8.70: Item Characteristic Curves From Four-Parameter Logistic Item Response Theory Model. Code plot(fourPLModel, type = &quot;itemscore&quot;, facet_items = TRUE) Figure 8.71: Item Characteristic Curves From Four-Parameter Logistic Item Response Theory Model. 8.8.4.2.2 Item Information Curves Plots of item information as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) are in Figures 8.72 and 8.73. Code plot(fourPLModel, type = &quot;infotrace&quot;, facet_items = FALSE) Figure 8.72: Item Information Curves From Four-Parameter Logistic Item Response Theory Model. Code plot(fourPLModel, type = &quot;infotrace&quot;, facet_items = TRUE) Figure 8.73: Item Information Curves From Four-Parameter Logistic Item Response Theory Model. 8.9 Graded Response Model A two-parameter graded response model (GRM) is an IRT model fit to polytomous data (in this case, a 1–4 likert scale), which estimates a different difficulty (\\(b\\)) and discrimination (\\(a\\)) parameter for each item. It estimates four parameters for each item: difficulty [for each of three threshold transitions: 1–2 (\\(b_1\\)), 2–3 (\\(b_2\\)), and 3–4 (\\(b_3\\))] and discrimination (\\(a\\)). GRM models were fit using the mirt package (Chalmers, 2020). 8.9.1 Fit Model Science is a data set from the mirt package (Chalmers, 2020) that contains four items evaluating people’s attitudes to science and technology on a 1–4 Likert scale. The data are from the Consumer Protection and Perceptions of Science and Technology section of the 1992 Euro-Barometer Survey of people in Great Britain. Code gradedResponseModel &lt;- mirt( data = Science, model = 1, itemtype = &quot;graded&quot;, SE = TRUE) 8.9.2 Model Summary Code summary(gradedResponseModel) F1 h2 Comfort 0.522 0.273 Work 0.584 0.342 Future 0.803 0.645 Benefit 0.541 0.293 SS loadings: 1.552 Proportion Var: 0.388 Factor correlations: F1 F1 1 Code coef(gradedResponseModel, simplify = TRUE, IRTpars = TRUE) $items a b1 b2 b3 Comfort 1.042 -4.669 -2.534 1.407 Work 1.226 -2.385 -0.735 1.849 Future 2.293 -2.282 -0.965 0.856 Benefit 1.095 -3.058 -0.906 1.542 $means F1 0 $cov F1 F1 1 8.9.3 Factor Scores Code gradedResponseModel_factorScores &lt;- fscores(gradedResponseModel, full.scores.SE = TRUE) 8.9.4 Plots 8.9.4.1 Test Curves The test curves suggest that the measure is most reliable (i.e., provides the most information and has the smallest standard error of measurement) across a wide range of construct. In general, this measure with polytomous (Likert-scale) items provides more information than the measure with binary items that were examined above. This is consistent with the idea that polytomous items tend to provide more information than binary/dichotomous items. 8.9.4.1.1 Test Characteristic Curve A test characteristic curve (TCC) plot of the expected total score as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) is in Figure 8.74. Code plot(gradedResponseModel, type = &quot;score&quot;) Figure 8.74: Test Characteristic Curve From Graded Response Model. 8.9.4.1.2 Test Information Curve A plot of test information as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) is in Figure 8.75. Code plot(gradedResponseModel, type = &quot;info&quot;) Figure 8.75: Test Information Curve From Graded Response Model. 8.9.4.1.3 Test Reliability The estimate of marginal reliability is below: Code marginal_rxx(gradedResponseModel) [1] 0.6687901 A plot of test reliability as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) is in Figure 8.76. Code plot(gradedResponseModel, type = &quot;rxx&quot;) Figure 8.76: Test Reliability From Graded Response Model. 8.9.4.1.4 Test Standard Error of Measurement A plot of test standard error of measurement (SEM) as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) is in Figure 8.77. Code plot(gradedResponseModel, type = &quot;SE&quot;) Figure 8.77: Test Standard Error of Measurement From Graded Response Model. 8.9.4.1.5 Test Information Curve and Standard Errors A plot of test information and standard error of measurement (SEM) as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) is in Figure 8.78. Code plot(gradedResponseModel, type = &quot;infoSE&quot;) Figure 8.78: Test Information Curve and Standard Error of Measurement From Graded Response Model. 8.9.4.2 Item Curves 8.9.4.2.1 Item Characteristic Curves Item characteristic curve (ICC) plots of the expected score on the item as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) are in Figures 8.79 and 8.80. Code plot(gradedResponseModel, type = &quot;itemscore&quot;, facet_items = FALSE) Figure 8.79: Item Characteristic Curves From Graded Response Model. Code plot(gradedResponseModel, type = &quot;itemscore&quot;, facet_items = TRUE) Figure 8.80: Item Characteristic Curves From Graded Response Model. 8.9.4.2.2 Item Information Curves Plots of item information as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) are in Figures 8.81 and 8.82. Code plot(gradedResponseModel, type = &quot;infotrace&quot;, facet_items = FALSE) Figure 8.81: Item Information Curves From Graded Response Model. Code plot(gradedResponseModel, type = &quot;infotrace&quot;, facet_items = TRUE) Figure 8.82: Item Information Curves From Graded Response Model. 8.9.4.2.3 Item Response Category Characteristic Curves A plot of the probability of item threshold endorsement as a function of a person’s level on the latent construct (theta; \\(\\theta\\)) is in Figure 8.83. Code plot(gradedResponseModel, type = &quot;trace&quot;) Figure 8.83: Item Response Category Characteristic Curves From Graded Response Model. 8.9.4.2.4 Item Boundary Characteristic Curves (aka Item Operation Characteristic Curves) A plot of item boundary characteristic curves is in Figure 8.84. The plot of item boundary characteristic curves was adapted from an example by Aiden Loe: https://aidenloe.github.io/irtplots.html (archived at https://perma.cc/D4YH-RV6N) Code modelCoefficients &lt;- coef( gradedResponseModel, IRTpars = TRUE, simplify = TRUE)$items theta &lt;- seq(from = -6, to = 6, by = .1) difficultyThresholds &lt;- grep( &quot;b&quot;, dimnames(modelCoefficients)[[2]], value = TRUE) numberDifficultyThresholds &lt;- length(difficultyThresholds) items &lt;- dimnames(modelCoefficients)[[1]] numberOfItems &lt;- length(items) lst &lt;- lapply( 1:numberOfItems, function(x) data.frame( matrix(ncol = numberDifficultyThresholds + 1, nrow = length(theta), dimnames = list(NULL, c(&quot;theta&quot;, difficultyThresholds))))) for(i in 1:numberOfItems){ for(j in 1:numberDifficultyThresholds){ lst[[i]][,1] &lt;- theta lst[[i]][,j + 1] &lt;- fourPL( a = modelCoefficients[i,1], b = modelCoefficients[i,j + 1], theta = theta) } } names(lst) &lt;- items dat &lt;- bind_rows(lst, .id = &quot;item&quot;) longer_data &lt;- pivot_longer( dat, cols = all_of(difficultyThresholds)) ggplot( longer_data, aes(theta, value, group = interaction(item, name), color = item)) + geom_line() + ylab(&quot;Probability of Endorsing an Item Response Category that is Higher than the Boundary&quot;) + theme_bw() + theme(axis.title.y = element_text(size = 10)) Figure 8.84: Item Boundary Category Characteristic Curves From Graded Response Model. 8.10 Conclusion Item response theory is a measurement theory and advanced modeling approach that allows estimating latent variables as the common variance from multiple items, and it allows estimating how the items relate to the construct (latent variable). IRT holds promise to enable the development of briefer assessments, including short forms and adaptive assessments, that have strong reliability and validity. However, there are situations where IRT models may not be preferable, such as when assessing a formative construct, when using small sample sizes, or when assumptions of IRT are violated. 8.11 Suggested Readings If you are interested in learning more about IRT, I highly recommend the book by Embretson &amp; Reise (2000). 8.12 Exercises 8.12.1 Questions Note: Several of the following questions use data from the Children of the National Longitudinal Survey of Youth Survey (CNLSY). The CNLSY is a publicly available longitudinal data set provided by the Bureau of Labor Statistics (https://www.bls.gov/nls/nlsy79-children.htm#topical-guide; archived at https://perma.cc/EH38-HDRN). The CNLSY data file for these exercises is located on the book’s page of the Open Science Framework (https://osf.io/3pwza). Children’s behavior problems were rated in 1988 (time 1: T1) and then again in 1990 (time 2: T2) on the Behavior Problems Index (BPI). Below are the items corresponding to the Antisocial subscale of the BPI: cheats or tells lies bullies or is cruel/mean to others does not seem to feel sorry after misbehaving breaks things deliberately is disobedient at school has trouble getting along with teachers has sudden changes in mood or feeling Fit a one-parameter (Rasch) model to the seven items of the Antisocial subscale of the BPI at T1. This will estimate the difficulty for each item threshold (one threshold from 0 to 1, and one threshold from 1 to 2), while constraining the discrimination for each item to be the same. Which item has the lowest difficulty (i.e., severity) in terms of endorsing a score of one (i.e., “sometimes true”) as opposed to zero (i.e., “not true”)? Which item has the highest difficulty in terms of endorsing a score of 2 (i.e., “often true”)? What do these estimates of item difficulty indicate? Fit a graded response model to the seven items of the Antisocial subscale of the BPI at T1. This will estimate the difficulty for each item threshold (one threshold from 0 to 1, and one threshold from 1 to 2), while allowing each item to have a different discrimination. Provide a figure of the item characteristic curves. Provide a figure of the item boundary characteristic curves. Which item has the lowest discrimination? Which item has the highest discrimination? What do these estimates of item discrimination indicate? Provide a figure of the item information curves. Examining the item information curves, which item provides the most information at upper construct levels (2–4 standard deviations above the mean)? Which item provides the most information at lower construct levels (2–4 standard deviations below the mean)? Provide a figure of the test information curve. Examining the test information curve, where (at what construct levels) does the measure do the best job of assessing? Based on its information curve, describe what purposes the test would be better- or worse-suited for. Fit a multidimensional graded response model to the seven items of the Antisocial subscale of the BPI at T1, by estimating two latent factors. Which items loaded onto Factor 1? Which items loaded onto Factor 2? Provide a possible explanation as two why some of the items “broke off” (from Factor 1) and loaded onto a separate factor (Factor 2). The one-factor graded response model (in #2) and the two-factor graded response model are considered “nested” models. The one-factor model is nested within the two-factor model because the two-factor model includes all of the terms of the one-factor model along with additional terms. Model fit of nested models can be directly compared with a chi-square difference test. Did the two-factor model fit better than the one-factor model? 8.12.2 Answers Item 7 (“sudden changes in mood or feeling”) has the lowest difficulty in terms of endorsing a score of one \\((b_1 = -0.95)\\). Item 5 (“disobedient at school”) has the highest difficulty in terms of endorsing a score of two \\((b_2 = 3.55)\\). The difficulty parameter indicates the construct-level at the inflection point of the item characteristic curve. In a one- or two-parameter model, the inflection point occurs where 50% of respondents endorse the item. Thus, in this model, the difficulty parameter indicates the construct-level at which 50% of respondents endorse the item. It takes a very high level of antisocial behavior for a child to be endorsed as being often disobedient at school, whereas it does not take a high construct-level for a child to be endorsed as sometimes showing sudden changes in mood. Below is a figure of item characteristic curves: Figure 8.85: Exercise 1a: Item Characteristic Curves. Below is a figure of item boundary characteristic curves: Figure 8.86: Exercise 2b: Item Boundary Characteristic Curves. Item 7 (“sudden changes in mood or feeling”) has the lowest discrimination \\((a = 0.89)\\). Item 6 (“has trouble getting along with teachers”) has the highest discrimination \\((a = 2.06)\\). The discrimination parameter represents the steepness of the slope of the item characteristic curve. It indicates how strongly endorsing an item discriminates (differentiates) between lower versus higher construct levels. In other words, it indicates how strongly the item is associated with the construct. Item 7 shows the weakest association with the construct, whereas item 6 shows the strongest association with the construct. That suggests that “trouble getting along with teachers” is more core to the construct of antisocial behavior than “sudden changes in mood.” Below is a figure of item information curves: Figure 8.87: Exercise 2c: Item Information Curves. Item 6 (“has trouble getting along with teachers”) provides the most information at upper construct levels (2–4 standard deviations above the mean). Item 7 (“has trouble getting along with teachers”) provides the most information at lower construct levels (2–4 standard deviations below the mean). Item 1 (“cheats or tells lies”) provides the most information at somewhat low construct levels (0–2 standard deviations below the mean). Figure 8.88: Exercise 2e: Test Information Curve. The measure does the best job of assessing (i.e., provides the most information) at construct levels from 1–3 standard deviations above the mean. Because the measure provides the most information at upper construct levels and provides little information at lower construct levels, the measure would be best used for assessing clinical versus sub-clinical levels of antisocial behavior rather than assessing individual differences in antisocial behavior across a community sample. Items 1, 2, 3, 4, and 7 loaded onto Factor 1. Items 5 and 6 loaded onto Factor 2. Items 5 (“disobedient at school”) and 6 (“trouble getting along with teachers”) both deal with school-related antisocial behavior. Thus, the items assessing school-related antisocial behavior may share variance owing to the shared context of the behavior (school). Yes, the two-factor model fit significantly better than the one-factor model according to a chi-square difference test \\((\\Delta\\chi^2[df = 6.00] = 273.84, p &lt; .001)\\). Thus, antisocial behavior may not be a monolithic construct, but may depend on the context in which the behavior occurs. References Baker, F. B., &amp; Kim, S.-H. (2017). The basics of item response theory using R. Springer. Bates, D., Maechler, M., Bolker, B., &amp; Walker, S. (2022). lme4: Linear mixed-effects models using Eigen and S4. https://github.com/lme4/lme4/ Bürkner, P.-C. (2021). Bayesian item response modeling in R with brms and Stan. Journal of Statistical Software, 100(5), 1–54. https://doi.org/10.18637/jss.v100.i05 Chalmers, P. (2020). mirt: Multidimensional item response theory. https://CRAN.R-project.org/package=mirt Chen, Y., Prudêncio, R. B. C., Diethe, T., &amp; Flach, P. (2019). \\(\\beta\\)3-IRT: A new item response model and its applications. arXiv:1903.04016. https://arxiv.org/abs/1903.04016 Cooper, L. D., &amp; Balsis, S. (2009). When less is more: How fewer diagnostic criteria can indicate greater severity. Psychological Assessment, 21(3), 285–293. https://doi.org/10.1037/a0016698 Embretson, S. E. (1996). The new rules of measurement. Psychological Assessment, 8, 341–349. https://doi.org/10.1037/1040-3590.8.4.341 Embretson, S. E., &amp; Reise, S. P. (2000). Item response theory for psychologists (Vol. 4). Lawrence Erlbaum Associates. Gibbons, R. D., Weiss, D. J., Frank, E., &amp; Kupfer, D. (2016). Computerized adaptive diagnosis and testing of mental health disorders. Annual Review of Clinical Psychology, 12(1), 83–104. https://doi.org/10.1146/annurev-clinpsy-021815-093634 Krueger, R. F., Nichol, P. E., Hicks, B. M., Markon, K. E., Patrick, C. J., lacono, W. G., &amp; McGue, M. (2004). Using latent trait modeling to conceptualize an alcohol problems continuum. Psychological Assessment, 16(2), 107–119. https://doi.org/10.1037/1040-3590.16.2.107 Magis, D. (2013). A note on the item information function of the four-parameter logistic model. Applied Psychological Measurement, 37(4), 304–315. https://doi.org/10.1177/0146621613475471 Petersen, I. T. (2024b). petersenlab: Package of R functions for the Petersen Lab. https://doi.org/10.5281/zenodo.7602890 Reise, S. P., &amp; Waller, N. G. (2009). Item response theory and clinical measurement. Annual Review of Clinical Psychology, 5(1), 27–48. https://doi.org/10.1146/annurev.clinpsy.032408.153553 Rosseel, Y., Jorgensen, T. D., &amp; Rockwood, N. (2022). lavaan: Latent variable analysis. https://lavaan.ugent.be Smith, G. T., McCarthy, D. M., &amp; Anderson, K. G. (2000). On the sins of short-form development. Psychological Assessment, 12(1), 102–111. https://doi.org/10.1037/1040-3590.12.1.102 Thomas, M. L. (2019). Advances in applications of item response theory to clinical assessment. Psychological Assessment, 31(12), 1442–1455. https://doi.org/10.1037/pas0000597 "],["prediction.html", "Chapter 9 Prediction 9.1 Overview of Prediction 9.2 Getting Started 9.3 Receiver Operating Characteristic (ROC) Curve 9.4 Prediction Accuracy Across Cutoffs 9.5 Prediction Accuracy at a Given Cutoff 9.6 Optimal Cutoff Specification 9.7 Accuracy at Every Possible Cutoff 9.8 Regression for Prediction of Continuous Outcomes 9.9 Pseudo-Prediction 9.10 Ways to Improve Prediction Accuracy 9.11 Conclusion 9.12 Suggested Readings 9.13 Exercises", " Chapter 9 Prediction “It is very difficult to predict—especially the future.” — Neils Bohr 9.1 Overview of Prediction In psychology, we are often interested in predicting behavior. Behavior is complex. The same behavior can occur for different reasons. Behavior is probabilistically influenced by many processes, including processes internal to the person in addition to external processes. Moreover, people’s behavior occurs in the context of a dynamic system with nonlinear, probabilistic, and cascading influences that change across time. The ever-changing system makes behavior challenging to predict. And, similar to chaos theory, one small change in the system can lead to large differences later on. Predictions can come in different types. Some predictions involve categorical data, whereas other predictions involve continuous data. When dealing with categorical data, we can evaluate predictions using a 2x2 table known as a confusion matrix (see Figure 9.4), or with logistic regression models. When dealing with continuous data, we can evaluate predictions using multiple regression or similar variants such as structural equation modeling and mixed models. Let’s consider a prediction example, assuming the following probabilities: The probability of contracting HIV is .3% The probability of a positive test for HIV is 1% The probability of a positive test if you have HIV is 95% What is the probability of HIV if you have a positive test? As we will see, the probability is: \\(\\frac{95\\% \\times .3\\%}{1\\%} = 28.5\\%\\). So based on the above probabilities, if you have a positive test, the probability that you have HIV is 28.5%. Most people tend to vastly over-estimate the likelihood that the person has HIV in this example. Why? Because they do not pay enough attention to the base rate (in this example, the base rate of HIV is .3%). 9.1.1 Issues Around Probability 9.1.1.1 Types of Probabilities It is important to distinguish between different types of probabilities: marginal probabilities, joint probabilities, and conditional probabilities. 9.1.1.1.1 Base Rate (Marginal Probability) A base rate is the probability of an event. Base rates are marginal probabilities. A marginal probability is the probability of an event irrespective of the outcome of another variable. For instance, we can consider the following marginal probabilities: \\(P(C_i)\\) is the probability (i.e., base rate) of a classification, \\(C\\), independent of other things. A base rate is often used as the “prior probability” in a Bayesian model. In our example above, \\(P(C_i)\\) is the base rate (i.e., prevalence) of HIV in the population: \\(P(\\text{HIV}) = .3\\%\\). \\(P(R_i)\\) is the probability (base rate) of a response, \\(R\\), independent of other things. In the example above, \\(P(R_i)\\) is the base rate of a positive test for HIV: \\(P(\\text{positive test}) = 1\\%\\). The base rate of a positive test is known as the positivity rate or selection ratio. 9.1.1.1.2 Joint Probability A joint probability is the probability of two (or more) events occurring simultaneously. For instance, the probability of events \\(A\\) and \\(B\\) both occurring together is \\(P(A, B)\\). A joint probability can be calculated using the marginal probability of each event, as in Equation (9.1): \\[\\begin{equation} P(A, B) = P(A) \\cdot P(B) \\tag{9.1} \\end{equation}\\] Conversely (and rearranging the terms for the calculation of conditional probability), a joint probability can also be calculated using the conditional probability and marginal probability, as in Equation (9.2): \\[\\begin{equation} P(A, B) = P(A | B) \\cdot P(B) \\tag{9.2} \\end{equation}\\] 9.1.1.1.3 Conditional Probability A conditional probability is the probability of one event occurring given the occurrence of another event. Conditional probabilities are written as: \\(P(A | B)\\). This is read as the probability that event \\(A\\) occurs given that event \\(B\\) occurred. For instance, we can consider the following conditional probabilities: \\(P(C | R)\\) is the probability of a classification, \\(C\\), given a response, \\(R\\). In other words, \\(P(C | R)\\) is the probability of having HIV given a positive test: \\(P(\\text{HIV} | \\text{positive test})\\). \\(P(R | C)\\) is the probability of a response, \\(R\\), given a classification, \\(C\\). In the example above, \\(P(R | C)\\) is the probability of having a positive test given that a person has HIV: \\(P(\\text{positive test} | \\text{HIV}) = 95\\%\\). A conditional probability can be calculated using the joint probability and marginal probability (base rate), as in Equation (9.3): \\[\\begin{equation} P(A | B) = \\frac{P(A, B)}{P(B)} \\tag{9.3} \\end{equation}\\] 9.1.1.2 Confusion of the Inverse A conditional probability is not the same thing as its reverse (or inverse) conditional probability. Unless the base rate of the two events (\\(C\\) and \\(R\\)) are the same, \\(P(C | R) \\neq P(R | C)\\). However, people frequently make the mistake of thinking that two inverse conditional probabilities are the same. This mistake is known as the “confusion of the inverse”, or the “inverse fallacy”, or the “conditional probability fallacy”. The confusion of inverse probabilities is the logical error of representative thinking that leads people to assume that the probability of \\(C\\) given \\(R\\) is the same as the probability of \\(R\\) given C, even though this is not true. As a few examples to demonstrate the logical fallacy, if 93% of breast cancers occur in high-risk women, this does not mean that 93% of high-risk women will eventually get breast cancer. As another example, if 77% of car accidents take place within 15 miles of a driver’s home, this does not mean that you will get in an accident 77% of times you drive within 15 miles of your home. Which car is the most frequently stolen? It is often the Honda Accord or Honda Civic—probably because they are among the most popular/commonly available cars. The probability that the car is a Honda Accord given that a car was stolen (\\(p(\\text{Honda Accord } | \\text{ Stolen})\\)) is what the media reports and what the police care about. However, that is not what buyers and car insurance companies should care about. Instead, they care about the probability that the car will be stolen given that it is a Honda Accord (\\(p(\\text{Stolen } | \\text{ Honda Accord})\\)). 9.1.1.3 Bayes’ Theorem An alternative way of calculating a conditional probability is using the inverse conditional probability (instead of the joint probability). This is known as Bayes’ theorem. Bayes’ theorem can help us calculate a conditional probability of some classification, \\(C\\), given some response, \\(R\\), if we know the inverse conditional probability and the base rate (marginal probability) of each. Bayes’ theorem is in Equation (9.4): \\[ \\begin{aligned} P(C | R) &amp;= \\frac{P(R | C) \\cdot P(C_i)}{P(R_i)} \\end{aligned} \\tag{9.4} \\] Or, equivalently (rearranging the terms): \\[\\begin{equation} \\frac{P(C | R)}{P(R | C)} = \\frac{P(C_i)}{P(R_i)} \\tag{9.5} \\end{equation}\\] Or, equivalently (rearranging the terms): \\[\\begin{equation} \\frac{P(C | R)}{P(C_i)} = \\frac{P(R | C)}{P(R_i)} \\tag{9.6} \\end{equation}\\] More generally, Bayes’ theorem has been described as: \\[ \\begin{aligned} P(H | E) &amp;= \\frac{P(E | H) \\cdot P(H)}{P(E)} \\\\ \\text{posterior probability} &amp;= \\frac{\\text{likelihood} \\times \\text{prior probability}}{\\text{model evidence}} \\\\ \\end{aligned} \\tag{9.7} \\] where \\(H\\) is the hypothesis, and \\(E\\) is the evidence—the new information that was not used in computing the prior probability. In Bayesian terms, the posterior probability is the conditional probability of one event occurring given another event—it is the updated probability after the evidence is considered. In this case, the posterior probability is the probability of the classification occurring (\\(C\\)) given the response (\\(R\\)). The likelihood is the inverse conditional probability—the probability of the response (\\(R\\)) occurring given the classification (\\(C\\)). The prior probability is the marginal probability of the event (i.e., the classification) occurring, before we take into account any new information. The model evidence is the marginal probability of the other event occurring—i.e., the marginal probability of seeing the evidence. In the HIV example above, we can calculate the conditional probability of HIV given a positive test using three terms: the conditional probability of a positive test given HIV (i.e., the sensitivity of the test), the base rate of HIV, and the base rate of a positive test for HIV. The conditional probability of HIV given a positive test is in Equation (9.8): \\[ \\begin{aligned} P(C | R) &amp;= \\frac{P(R | C) \\cdot P(C_i)}{P(R_i)} \\\\ P(\\text{HIV} | \\text{positive test}) &amp;= \\frac{P(\\text{positive test} | \\text{HIV}) \\cdot P(\\text{HIV})}{P(\\text{positive test})} \\\\ &amp;= \\frac{\\text{sensitivity of test} \\times \\text{base rate of HIV}}{\\text{base rate of positive test}} \\\\ &amp;= \\frac{95\\% \\times .3\\%}{1\\%} = \\frac{.95 \\times .003}{.01}\\\\ &amp;= 28.5\\% \\end{aligned} \\tag{9.8} \\] The petersenlab package (Petersen, 2024b) contains the pAgivenB() function that estimates the probability of one event, \\(A\\), given another event, \\(B\\). Code pAgivenB &lt;- function(pBgivenA, pA, pB){ value &lt;- pBgivenA * pA / pB value } Code pAgivenB(pBgivenA = .95, pA = .003, pB = .01) [1] 0.285 Thus, assuming the probabilities in the example above, the conditional probability of having HIV if a person has a positive test is 28.5%. Given a positive test, chances are higher than not that the person does not have HIV. Bayes’ theorem can be depicted visually (Ballesteros-Pérez et al., 2018). If we have 100,000 people in our population, we would be able to fill out a 2-by-2 confusion matrix, as depicted in Figure 9.1. Figure 9.1: Confusion Matrix: 2x2 Prediction Matrix. TP = true positives; TN = true negatives; FP = false positives; FN = false negatives; BR = base rate; SR = selection ratio. We know that .3% of the population contracts HIV, so 300 people in the population of 100,000 would contract HIV. Therefore, we put 300 in the marginal sum of those with HIV (\\(.003 \\times 100,000 = 300\\)), i.e., the base rate of HIV. That means 99,700 people do not contract HIV (\\(100,000 - 300 = 99,700\\)). We know that 1% of the population tests positive for HIV, so we put 1,000 in the marginal sum of those who test positive \\(.01 \\times 100,000 = 1,000\\), i.e., the marginal probability of a positive test (the selection ratio). That means 99,000 people test negative for HIV (\\(100,000 - 1,000 = 99,000\\)). We also know that 95% of those who have HIV test positive for HIV. Three hundred people have HIV, so 95% of them (i.e., 285 people; \\(.95 \\times 300 = 285\\)) tested positive for HIV (true positives). Because we know that 300 people have HIV and that 285 of those with HIV tested positive, that means that 15 people with HIV tested negative (\\(300 - 15 = 285\\); false negatives). We know that 1,000 people tested positive for HIV, and 285 with HIV tested positive, so that means that 715 people without HIV tested positive (\\(1,000 - 285 = 715\\); false positives). We know that 99,000 people tested negative for HIV, and 15 with HIV tested negative, so that means that 98,985 people without HIV tested negative (\\(99,000 - 15 = 98,985\\); true negatives). So, to answer the question of what is the probability of having HIV if you have a positive test, we divide the number of people with HIV who had a positive test (285) by the total number of people who had a positive test (1000), which leads to a probability of 28.5%. This can be depicted visually in Figures 9.2 and 9.3.2 Figure 9.2: Bayes’ Theorem (and Confusion Matrix) Depicted Visually, Where the Marginal Probability is the Base Rate (BR). The four boxes represent the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). Note: Boxes are not drawn to scale; otherwise, some regions would be too small to include text. Figure 9.3: Bayes’ Theorem (and Confusion Matrix) Depicted Visually, where the Marginal Probability is the Selection Ratio (SR). The four boxes represent the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). Note: Boxes are not drawn to scale; otherwise, some regions would be too small to include text. Now let’s see what happens if the person tests positive a second time. We would revise our “prior probability” for HIV from the general prevalence in the population (0.3%) to be the “posterior probability” of HIV given a first positive test (28.5%). This is known as Bayesian updating. We would also update the “evidence” to be the marginal probability of getting a second positive test. If we do not know a marginal probability (i.e., base rate) of an event (e.g., getting a second positive test), we can calculate a marginal probability with the law of total probability using conditional probabilities and the marginal probability of another event (e.g., having HIV). According to the law of total probability, the probability of getting a positive test is the probability that a person with HIV gets a positive test (i.e., sensitivity) times the base rate of HIV plus the probability that a person without HIV gets a positive test (i.e., false positive rate) times the base rate of not having HIV, as in Equation (9.9): \\[ \\begin{aligned} P(\\text{not } C_i) &amp;= 1 - P(C_i) \\\\ P(R_i) &amp;= P(R | C) \\cdot P(C_i) + P(R | \\text{not } C) \\cdot P(\\text{not } C_i) \\\\ 1\\% &amp;= 95\\% \\times .3\\% + P(R | \\text{not } C) \\times 99.7\\% \\\\ \\end{aligned} \\tag{9.9} \\] In this case, we know the marginal probability (\\(P(R_i)\\)), and we can use that to solve for the unknown conditional probability that reflects the false positive rate (\\(P(R | \\text{not } C)\\)), as in Equation (9.10): \\[ \\scriptsize \\begin{aligned} P(R_i) &amp;= P(R | C) \\cdot P(C_i) + P(R | \\text{not } C) \\cdot P(\\text{not } C_i) &amp;&amp; \\\\ P(R_i) - [P(R | \\text{not } C) \\cdot P(\\text{not } C_i)] &amp;= P(R | C) \\cdot P(C_i) &amp;&amp; \\text{Move } P(R | \\text{not } C) \\text{ to the left side} \\\\ - [P(R | \\text{not } C) \\cdot P(\\text{not } C_i)] &amp;= P(R | C) \\cdot P(C_i) - P(R_i) &amp;&amp; \\text{Move } P(R_i) \\text{ to the right side} \\\\ P(R | \\text{not } C) \\cdot P(\\text{not } C_i) &amp;= P(R_i) - [P(R | C) \\cdot P(C_i)] &amp;&amp; \\text{Multiply by } -1 \\\\ P(R | \\text{not } C) &amp;= \\frac{P(R_i) - [P(R | C) \\cdot P(C_i)]}{P(\\text{not } C_i)} &amp;&amp; \\text{Divide by } P(R | \\text{not } C) \\\\ &amp;= \\frac{1\\% - [95\\% \\times .3\\%]}{99.7\\%} = \\frac{.01 - [.95 \\times .003]}{.997}\\\\ &amp;= .7171515\\% \\\\ \\end{aligned} \\tag{9.10} \\] The petersenlab package (Petersen, 2024b) contains the pBgivenNotA() function that estimates the probability of one event, \\(B\\), given that another event, \\(A\\), did not occur. Code pBgivenNotA &lt;- function(pBgivenA, pA, pB){ value &lt;- (pB - (pBgivenA * pA)) / (1 - pA) value } Code pBgivenNotA(pBgivenA = .95, pA = .003, pB = .01) [1] 0.007171515 With this conditional probability (\\(P(R | \\text{not } C)\\)), the updated marginal probability of having HIV (\\(P(C_i)\\)), and the updated marginal probability of not having HIV (\\(P(\\text{not } C_i)\\)), we can now calculate an updated estimate of the marginal probability of getting a second positive test. The probability of getting a second positive test is the probability that a person with HIV gets a second positive test (i.e., sensitivity) times the updated probability of HIV plus the probability that a person without HIV gets a second positive test (i.e., false positive rate) times the updated probability of not having HIV, as in Equation (9.11): \\[ \\begin{aligned} P(R_{i}) &amp;= P(R | C) \\cdot P(C_i) + P(R | \\text{not } C) \\cdot P(\\text{not } C_i) \\\\ &amp;= 95\\% \\times 28.5\\% + .7171515\\% \\times 71.5\\% = .95 \\times .285 + .007171515 \\times .715 \\\\ &amp;= 27.58776\\% \\end{aligned} \\tag{9.11} \\] The petersenlab package (Petersen, 2024b) contains the pB() function that estimates the marginal probability of one event, \\(B\\). Code pB &lt;- function(pBgivenA, pA, pBgivenNotA){ value &lt;- (pBgivenA * pA) + pBgivenNotA * (1 - pA) value } Code pB(pBgivenA = .95, pA = .285, pBgivenNotA = .007171515) [1] 0.2758776 Code pB( pBgivenA = .95, pA = pAgivenB( pBgivenA = .95, pA = .003, pB = .01), pBgivenNotA = pBgivenNotA( pBgivenA = .95, pA = .003, pB = .01)) [1] 0.2758776 We then substitute the updated marginal probability of HIV (\\(P(C_i)\\)) and the updated marginal probability of getting a second positive test (\\(P(R_i)\\)) into Bayes’ theorem to get the probability that the person has HIV if they have a second positive test (assuming the errors of each test are independent, i.e., uncorrelated), as in Equation (9.12): \\[ \\begin{aligned} P(C | R) &amp;= \\frac{P(R | C) \\cdot P(C_i)}{P(R_i)} \\\\ P(\\text{HIV} | \\text{a second positive test}) &amp;= \\frac{P(\\text{a second positive test} | \\text{HIV}) \\cdot P(\\text{HIV})}{P(\\text{a second positive test})} \\\\ &amp;= \\frac{\\text{sensitivity of test} \\times \\text{updated base rate of HIV}}{\\text{updated base rate of positive test}} \\\\ &amp;= \\frac{95\\% \\times 28.5\\%}{27.58776\\%} \\\\ &amp;= 98.14\\% \\end{aligned} \\tag{9.12} \\] The petersenlab package (Petersen, 2024b) contains the pAgivenB() function that estimates the probability of one event, \\(A\\), given another event, \\(B\\). Code pAgivenB(pBgivenA = .95, pA = .285, pB = .2758776) [1] 0.9814135 Code pAgivenB( pBgivenA = .95, pA = pAgivenB( pBgivenA = .95, pA = .003, pB = .01), pB = pB( pBgivenA = .95, pA = pAgivenB( pBgivenA = .95, pA = .003, pB = .01), pBgivenNotA = pBgivenNotA( pBgivenA = .95, pA = .003, pB = .01))) [1] 0.9814134 Thus, a second positive test greatly increases the posterior probability that the person has HIV from 28.5% to over 98%. As seen in the rearranged formula in Equation (9.5), the ratio of the conditional probabilities is equal to the ratio of the base rates. Thus, it is important to consider base rates. People have a strong tendency to ignore (or give insufficient weight to) base rates when making predictions. The failure to consider the base rate when making predictions when given specific information about a case is a cognitive bias known as the base-rate fallacy or as base rate neglect. For example, people tend to say that the probability of a rare event is more likely than it actually is given specific information. As seen in the rearranged formula in Equation (9.6), the inverse conditional probabilities (\\(P(C | R)\\) and \\(P(R | C)\\)) are not equal unless the base rates of \\(C\\) and \\(R\\) are the same. If the base rates are not equal, we are making at least some prediction errors. If \\(P(C_i) &gt; P(R_i)\\), our predictions must include some false negatives. If \\(P(R_i) &gt; P(C_i)\\), our predictions must include some false positives. Using the law of total probability, we can substitute the calculation of the marginal probability (\\(P(R_i)\\)) into Bayes’ theorem to get an alternative formulation of Bayes’ theorem, as in Equation (9.13): \\[ \\begin{aligned} P(C | R) &amp;= \\frac{P(R | C) \\cdot P(C_i)}{P(R_i)} \\\\ &amp;= \\frac{P(R | C) \\cdot P(C_i)}{P(R | C) \\cdot P(C_i) + P(R | \\text{not } C) \\cdot P(\\text{not } C_i)} \\\\ &amp;= \\frac{P(R | C) \\cdot P(C_i)}{P(R | C) \\cdot P(C_i) + P(R | \\text{not } C) \\cdot [1 - P(C_i)]} \\end{aligned} \\tag{9.13} \\] Instead of using marginal probability (base rate) of \\(R\\), as in the original formulation of Bayes’ theorem, it uses the conditional probability, \\(P(R|\\text{not } C)\\). Thus, it uses three terms: two conditional probabilities—\\(P(R|C)\\) and \\(P(R|\\text{not } C)\\)—and one marginal probability, \\(P(C_i)\\). This alternative formulation of Bayes’ theorem can be used to calculate positive predictive value, based on sensitivity, specificity, and the base rate, as presented in Equation (9.49). Let us see how the alternative formulation of Bayes’ theorem applies to the HIV example above. We can calculate the probability of HIV given a positive test using three terms: the conditional probability that a person with HIV gets a positive test (i.e., sensitivity), the conditional probability that a person without HIV gets a positive test (i.e., false positive rate), and the base rate of HIV. Using the \\(P(R|\\text{not } C)\\) calculated in Equation (9.10), the conditional probability of HIV given a single positive test is in Equation (9.14): \\[ \\small \\begin{aligned} P(C | R) &amp;= \\frac{P(R | C) \\cdot P(C_i)}{P(R | C) \\cdot P(C_i) + P(R | \\text{not } C) \\cdot [1 - P(C_i)]} \\\\ &amp;= \\frac{\\text{sensitivity of test} \\times \\text{base rate of HIV}}{\\text{sensitivity of test} \\times \\text{base rate of HIV} + \\text{false positive rate of test} \\times (1 - \\text{base rate of HIV})} \\\\ &amp;= \\frac{95\\% \\times .3\\%}{95\\% \\times .3\\% + .7171515\\% \\times (1 - .3\\%)} = \\frac{.95 \\times .003}{.95 \\times .003 + .007171515 \\times (1 - .003)}\\\\ &amp;= 28.5\\% \\end{aligned} \\tag{9.14} \\] Code pAgivenBalternative &lt;- function(pBgivenA, pA, pBgivenNotA){ value &lt;- (pBgivenA * pA) / ((pBgivenA * pA) + (pBgivenNotA * (1 - pA))) value } Code pAgivenBalternative( pBgivenA = .95, pA = .003, pBgivenNotA = .007171515) [1] 0.285 Code pAgivenBalternative( pBgivenA = .95, pA = .003, pBgivenNotA = pBgivenNotA( pBgivenA = .95, pA = .003, pB = .01)) [1] 0.285 The petersenlab package (Petersen, 2024b) contains the pAgivenB() function that estimates the probability of one event, \\(A\\), given another event, \\(B\\). Code pAgivenB(pBgivenA = .95, pA = .003, pBgivenNotA = .007171515) [1] 0.285 Code pAgivenB( pBgivenA = .95, pA = .003, pBgivenNotA = pBgivenNotA( pBgivenA = .95, pA = .003, pB = .01)) [1] 0.285 To calculate the conditional probability of HIV given a second positive test, we update our priors because the person has now tested positive for HIV. We update the prior probability of HIV (\\(P(C_i)\\)) based on the posterior probability of HIV after a positive test (\\(P(C | R)\\)) that we calculated above. We can calculate the conditional probability of HIV given a second positive test using three terms: the conditional probability that a person with HIV gets a positive test (i.e., sensitivity; which stays the same), the conditional probability that a person without HIV gets a positive test (i.e., false positive rate; which stays the same), and the updated marginal probability of HIV. The conditional probability of HIV given a second positive test is in Equation (9.15): \\[ \\scriptsize \\begin{aligned} P(C | R) &amp;= \\frac{P(R | C) \\cdot P(C_i)}{P(R | C) \\cdot P(C_i) + P(R | \\text{not } C) \\cdot [1 - P(C_i)]} \\\\ &amp;= \\frac{\\text{sensitivity of test} \\times \\text{updated base rate of HIV}}{\\text{sensitivity of test} \\times \\text{updated base rate of HIV} + \\text{false positive rate of test} \\times (1 - \\text{updated base rate of HIV})} \\\\ &amp;= \\frac{95\\% \\times 28.5\\%}{95\\% \\times 28.5\\% + .7171515\\% \\times (1 - 28.5\\%)} = \\frac{.95 \\times .285}{.95 \\times .285 + .007171515 \\times (1 - .285)}\\\\ &amp;= 98.14\\% \\end{aligned} \\tag{9.15} \\] The petersenlab package (Petersen, 2024b) contains the pAgivenB() function that estimates the probability of one event, \\(A\\), given another event, \\(B\\). Code pAgivenBalternative( pBgivenA = .95, pA = .285, pBgivenNotA = .007171515) [1] 0.9814134 Code pAgivenBalternative( pBgivenA = .95, pA = .285, pBgivenNotA = pBgivenNotA( pBgivenA = .95, pA = .003, pB = .01)) [1] 0.9814134 Code pAgivenB( pBgivenA = .95, pA = .285, pBgivenNotA = .007171515) [1] 0.9814134 Code pAgivenB( pBgivenA = .95, pA = .285, pBgivenNotA = pBgivenNotA( pBgivenA = .95, pA = .003, pB = .01)) [1] 0.9814134 If we want to compare the relative probability of two outcomes, we can use the odds form of Bayes’ theorem, as in Equation (9.16): \\[ \\begin{aligned} P(C | R) &amp;= \\frac{P(R | C) \\cdot P(C_i)}{P(R_i)} \\\\ P(\\text{not } C | R) &amp;= \\frac{P(R | \\text{not } C) \\cdot P(\\text{not } C_i)}{P(R_i)} \\\\ \\frac{P(C | R)}{P(\\text{not } C | R)} &amp;= \\frac{\\frac{P(R | C) \\cdot P(C_i)}{P(R_i)}}{\\frac{P(R | \\text{not } C) \\cdot P(\\text{not } C_i)}{P(R_i)}} \\\\ &amp;= \\frac{P(R | C) \\cdot P(C_i)}{P(R | \\text{not } C) \\cdot P(\\text{not } C_i)} \\\\ &amp;= \\frac{P(C_i)}{P(\\text{not } C_i)} \\times \\frac{P(R | C)}{P(R | \\text{not } C)} \\\\ \\text{posterior odds} &amp;= \\text{prior odds} \\times \\text{likelihood ratio} \\end{aligned} \\tag{9.16} \\] In sum, the marginal probability, including the prior probability or base rate, should be weighed heavily in predictions unless there are sufficient data to indicate otherwise, i.e., to update the posterior probability based on new evidence. Bayes’ theorem provides a powerful tool to anchor predictions to the base rate unless sufficient evidence changes the posterior probability (by updating the evidence and prior probability). 9.1.2 Prediction Accuracy 9.1.2.1 Decision Outcomes To consider how we can evaluate the accuracy of predictions, consider an example adapted from Meehl &amp; Rosen (1955). The military conducts a test of its prospective members to screen out applicants who would likely fail basic training. To evaluate the accuracy of our predictions using the test, we can examine a confusion matrix. A confusion matrix is a matrix that presents the predicted outcome on one dimension and the actual outcome (truth) on the other dimension. If the predictions and outcomes are dichotomous, the confusion matrix is a 2x2 matrix with two rows and two columns that represent four possible predicted-actual combinations (decision outcomes): true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). When discussing the four decision outcomes, “true” means an accurate judgment, whereas “false” means an inaccurate judgment. “Positive” means that the judgment was that the person has the characteristic of interest, whereas “negative” means that the judgment was that the person does not have the characteristic of interest. A true positive is a correct judgment (or prediction) where the judgment was that the person has (or will have) the characteristic of interest, and, in truth, they actually have (or will have) the characteristic. A true negative is a correct judgment (or prediction) where the judgment was that the person does not have (or will not have) the characteristic of interest, and, in truth, they actually do not have (or will not have) the characteristic. A false positive is an incorrect judgment (or prediction) where the judgment was that the person has (or will have) the characteristic of interest, and, in truth, they actually do not have (or will not have) the characteristic. A false negative is an incorrect judgment (or prediction) where the judgment was that the person does not have (or will not have) the characteristic of interest, and, in truth, they actually do have (or will have) the characteristic. An example of a confusion matrix is in Figure 9.4. Figure 9.4: Confusion Matrix: 2x2 Prediction Matrix. TP = true positives; TN = true negatives; FP = false positives; FN = false negatives; BR = base rate; SR = selection ratio. With the information in the confusion matrix, we can calculate the marginal sums and the proportion of people in each cell (in parentheses), as depicted in Figure 9.5. Figure 9.5: Confusion Matrix: 2x2 Prediction Matrix With Marginal Sums. TP = true positives; TN = true negatives; FP = false positives; FN = false negatives. That is, we can sum across the rows and columns to identify how many people actually showed poor adjustment (\\(n = 100\\)) versus good adjustment (\\(n = 1,900\\)), and how many people were selected to reject (\\(n = 508\\)) versus retain (\\(n = 1,492\\)). If we sum the column of predicted marginal sums (\\(508 + 1,492\\)) or the row of actual marginal sums (\\(100 + 1,900\\)), we get the total number of people (\\(N = 2,000\\)). Based on the marginal sums, we can compute the marginal probabilities, as depicted in Figure 9.6. Figure 9.6: Confusion Matrix: 2x2 Prediction Matrix With Marginal Sums And Marginal Probabilities. TP = true positives; TN = true negatives; FP = false positives; FN = false negatives; BR = base rate; SR = selection ratio. The marginal probability of the person having the characteristic of interest (i.e., showing poor adjustment) is called the base rate (BR). That is, the base rate is the proportion of people who have the characteristic. It is calculated by dividing the number of people with poor adjustment (\\(n = 100\\)) by the total number of people (\\(N = 2,000\\)): \\(BR = \\frac{FN + TP}{N}\\). Here, the base rate reflects the prevalence of poor adjustment. In this case, the base rate is .05, so there is a 5% chance that an applicant will be poorly adjusted. The marginal probability of good adjustment is equal to 1 minus the base rate of poor adjustment. The marginal probability of predicting that a person has the characteristic (i.e., rejecting a person) is called the selection ratio (SR). The selection ratio is the proportion of people who will be selected (in this case, rejected rather than retained); i.e., the proportion of people who are identified as having the characteristic. The selection ratio is calculated by dividing the number of people selected to reject (\\(n = 508\\)) by the total number of people (\\(N = 2,000\\)): \\(SR = \\frac{TP + FP}{N}\\). In this case, the selection ratio is .25, so 25% of people are rejected. The marginal probability of not selecting someone to reject (i.e., the marginal probability of retaining) is equal to 1 minus the selection ratio. The selection ratio might be something that the test dictates according to its cutoff score. Or, the selection ratio might be imposed by external factors that place limits on how many people you can assign a positive test value. For instance, when deciding whether to treat a client, the selection ratio may depend on how many therapists are available and how many cases can be treated. 9.1.2.2 Percent Accuracy Based on the confusion matrix, we can calculate the prediction accuracy based on the percent accuracy of the predictions. The percent accuracy is the number of correct predictions divided by the total number of predictions, and multiplied by 100. In the context of a confusion matrix, this is calculated as: \\(100\\% \\times \\frac{\\text{TP} + \\text{TN}}{N}\\). In this case, our percent accuracy was 78%—that is, 78% of our predictions were accurate, and 22% of our predictions were inaccurate. 9.1.2.3 Percent Accuracy by Chance 78% sounds pretty accurate. And it is much higher than 50%, so we are doing a pretty good job, right? Well, it is important to compare our accuracy to what accuracy we would expect to get by chance alone, if predictions were made by a random process rather than using a test’s scores. Our selection ratio was 25.4%. How accurate would we be if we randomly selected 25.4% of people to reject? To determine what accuracy we could get by chance alone given the selection ratio and the base rate, we can calculate the chance probability of true positives and the chance probability of true negatives. The probability of a given cell in the confusion matrix is a joint probability—the probability of two events occurring simultaneously. To calculate a joint probability, we multiply the probability of each event. So, to get the chance expectancies of true positives, we would multiply the respective marginal probabilities, as in Equation (9.17): \\[ \\begin{aligned} P(TP) &amp;= P(\\text{Poor adjustment}) \\times P(\\text{Reject})\\\\ &amp;= BR \\times SR \\\\ &amp;= .05 \\times .254 \\\\ &amp;= .0127 \\end{aligned} \\tag{9.17} \\] To get the chance expectancies of true negatives, we would multiply the respective marginal probabilities, as in Equation (9.18): \\[ \\begin{aligned} P(TN) &amp;= P(\\text{Good adjustment}) \\times P(\\text{Retain})\\\\ &amp;= (1 - BR) \\times (1 - SR) \\\\ &amp;= .95 \\times .746 \\\\ &amp;= .7087 \\end{aligned} \\tag{9.18} \\] To get the percent accuracy by chance, we sum the chance expectancies for the correct predictions (TP and TN): \\(.0127 + .7087 = .7214\\). Thus, the percent accuracy you can get by chance alone is 72%. This is because most of our predictions are to retain people, and the base rate of poor adjustment is quite low (.05). Our measure with 78% accuracy provides only a 6% increment in correct predictions. Thus, you cannot judge how good your judgment or prediction is until you know how you would do by random chance. The chance expectancies for each cell of the confusion matrix are in Figure 9.7. Figure 9.7: Chance Expectancies in 2x2 Prediction Matrix. TP = true positives; TN = true negatives; FP = false positives; FN = false negatives; BR = base rate; SR = selection ratio. 9.1.2.4 Predicting from the Base Rate Now, let us consider how well you would do if you were to predict from the base rate. Predicting from the base rate is also called “betting from the base rate”, and it involves setting the selection ratio by taking advantage of the base rate so that you go with the most likely outcome in every prediction. Because the base rate is quite low (.05), we could predict from the base rate by selecting no one to reject (i.e., setting the selection ratio at zero). Our percent accuracy by chance if we predict from the base rate would be calculated by multiplying the marginal probabilities, as we did above, but with a new selection ratio, as in Equation (9.19): \\[ \\begin{aligned} P(TP) &amp;= P(\\text{Poor adjustment}) \\times P(\\text{Reject})\\\\ &amp;= BR \\times SR \\\\ &amp;= .05 \\times 0 \\\\ &amp;= 0 \\\\ \\\\ P(TN) &amp;= P(\\text{Good adjustment}) \\times P(\\text{Retain})\\\\ &amp;= (1 - BR) \\times (1 - SR) \\\\ &amp;= .95 \\times 1 \\\\ &amp;= .95 \\end{aligned} \\tag{9.19} \\] We sum the chance expectancies for the correct predictions (TP and TN): \\(0 + .95 = .95\\). Thus, our percent accuracy by predicting from the base rate is 95%. This is damning to our measure because it is a much higher accuracy than the accuracy of our measure. That is, we can be much more accurate than our measure simply by predicting from the base rate and selecting no one to reject. Going with the most likely outcome in every prediction (predicting from the base rate) can be highly accurate (in terms of percent accuracy) as noted by Meehl &amp; Rosen (1955), especially when the base rate is very low or very high. This should serve as an important reminder that we need to compare the accuracy of our measures to the accuracy by (1) random chance and (2) predicting from the base rate. There are several important implications of the impact of base rates on prediction accuracy. One implication is that using the same test in different settings with different base rates will markedly change the accuracy of the test. Oftentimes, using a test will actually decrease the predictive accuracy when the base rate deviates greatly from .50. But percent accuracy is not everything. Percent accuracy treats different kinds of errors as if they are equally important. However, the value we place on different kinds of errors may be different, as described next. 9.1.2.5 Different Kinds of Errors Have Different Costs Some errors have a high cost, and some errors have a low cost. Among the four decision outcomes, there are two types of errors: false positives and false negatives. The extent to which false positives and false negatives are costly depends on the prediction problem. So, even though you can often be most accurate by going with the base rate, it may be advantageous to use a screening instrument despite lower overall accuracy because of the huge difference in costs of false positives versus false negatives in some cases. Consider the example of a screening instrument for HIV. False positives would be cases where we said that someone is at high risk of HIV when they are not, whereas false negatives are cases where we said that someone is not at high risk when they actually are. The costs of false positives include a shortage of blood, some follow-up testing, and potentially some anxiety, but that is about it. The costs of false negatives may be people getting HIV. In this case, the costs of false negatives greatly outweigh the costs of false positives, so we use a screening instrument to try to identify the cases at high risk for HIV because of the important consequences of failing to do so, even though using the screening instrument will lower our overall accuracy level. Another example is when the Central Intelligence Agency (CIA) used a screen for protective typists during wartime to try to detect spies. False positives would be cases where the CIA believes that a person is a spy when they are not, and the CIA does not hire them. False negatives would be cases where the CIA believes that a person is not a spy when they actually are, and the CIA hires them. In this case, a false positive would be fine, but a false negative would be really bad. How you weigh the costs of different errors depends considerably on the domain and context. Possible costs of false positives to society include: unnecessary and costly treatment with side effects and sending an innocent person to jail (despite our presumption of innocence in the United States criminal justice system that a person is innocent until proven guilty). Possible costs of false negatives to society include: setting a guilty person free, failing to detect a bomb or tumor, and preventing someone from getting treatment who needs it. The differential costs of different errors also depend on how much flexibility you have in the selection ratio in being able to set a stringent versus loose selection ratio. Consider if there is a high cost of getting rid of people during the selection process. For example, if you must hire 100 people and only 100 people apply for the position, you cannot lose people, so you need to hire even high-risk people. However, if you do not need to hire many people, then you can hire more conservatively. Any time the selection ratio differs from the base rate, you will make errors. For example, if you reject 25% of applicants, and the base rate of poor adjustment is 5%, then you are making errors of over-rejecting (false positives). By contrast, if you reject 1% of applicants and the base rate of poor adjustment is 5%, then you are making errors of under-rejecting or over-accepting (false negatives). A low base rate makes it harder to make predictions, and tends to lead to less accurate predictions. For instance, it is very challenging to predict low base rate behaviors, including suicide (Kessler et al., 2020). The difficulty in predicting events with a low base rate is apparent with the true score formula from classical test theory: \\(X = T + e\\). As described in Equation (4.10), reliability is the ratio of true score variance to observed score variance. As true score variance increases, reliability increases. If the base rate is .05, the maximum variance of the true scores is .05. The lower true score variance makes the measure less reliable and hard to make accurate predictions. 9.1.2.6 Sensitivity, Specificity, PPV, and NPV As described earlier, percent accuracy is not the only important aspect of accuracy. Percent accuracy can be misleading because it is highly influenced by base rates. You can have a high percent accuracy by predicting from the base rate and saying that no one has the condition (if the base rate is low) or that everyone has the condition (if the base rate is high). Thus, it is also important to consider other aspects of accuracy, including sensitivity (SN), specificity (SP), positive predictive value (PPV), and negative predictive value (NPV). We want our predictions to be sensitive to be able to detect the characteristic but also to be specific so that we classify only people actually with the characteristic as having the characteristic. Let us return to the confusion matrix in Figure 9.8. If we know the frequency of each of the four predicted-actual combinations of the confusion matrix (TP, TN, FP, FN), we can calculate sensitivity, specificity, PPV, and NPV. Figure 9.8: Confusion Matrix: 2x2 Prediction Matrix. TP = true positives; TN = true negatives; FP = false positives; FN = false negatives. Sensitivity is the proportion of those with the characteristic (\\(\\text{TP} + \\text{FN}\\)) that we identified with our measure (\\(\\text{TP}\\)): \\(\\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{86}{86 + 14} = .86\\). Specificity is the proportion of those who do not have the characteristic (\\(\\text{TN} + \\text{FP}\\)) that we correctly classify as not having the characteristic (\\(\\text{TN}\\)): \\(\\frac{\\text{TN}}{\\text{TN} + \\text{FP}} = \\frac{1,478}{1,478 + 422} = .78\\). PPV is the proportion of those who we classify as having the characteristic (\\(\\text{TP} + \\text{FP}\\)) who actually have the characteristic (\\(\\text{TP}\\)): \\(\\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{86}{86 + 422} = .17\\). NPV is the proportion of those we classify as not having the characteristic (\\(\\text{TN} + \\text{FN}\\)) who actually do not have the characteristic (\\(\\text{TN}\\)): \\(\\frac{\\text{TN}}{\\text{TN} + \\text{FN}} = \\frac{1,478}{1,478 + 14} = .99\\). Sensitivity, specificity, PPV, and NPV are proportions, and their values therefore range from 0 to 1, where higher values reflect greater accuracy. With sensitivity, specificity, PPV, and NPV, we have a good snapshot of how accurate the measure is at a given cutoff. In our case, our measure is good at finding whom to reject (high sensitivity), but it is rejecting too many people who do not need to be rejected (lower PPV due to many FPs). Most people whom we classify as having the characteristic do not actually have the characteristic. However, the fact that we are over-rejecting could be okay depending on our goals, for instance, if we do not care about over-dropping (i.e., the PPV being low). 9.1.2.6.1 Some Accuracy Estimates Depend on the Cutoff Sensitivity, specificity, PPV, and NPV differ based on the cutoff (i.e., threshold) for classification. Consider the following example. Aliens visit Earth, and they develop a test to determine whether a berry is edible or inedible. Figure 9.9 depicts the distributions of scores by berry type. Note how there are clearly two distinct distributions. However, the distributions overlap to some degree. Thus, any cutoff will have at least some inaccurate classifications. The extent of overlap of the distributions reflects the amount of measurement error of the measure with respect to the characteristic of interest. Figure 9.9: Distribution of Test Scores by Berry Type. Figure 9.10 depicts the distributions of scores by berry type with a cutoff. The red line indicates the cutoff—the level above which berries are classified by the test as inedible. There are errors on each side of the cutoff. Below the cutoff, there are some false negatives (blue): inedible berries that are inaccurately classified as edible. Above the cutoff, there are some false positives (green): edible berries that are inaccurately classified as inedible. Costs of false negatives could include sickness or death from eating the inedible berries. Costs of false positives could include taking longer to find food, finding insufficient food, and starvation. Figure 9.10: Classifications Based on a Cutoff. Note that some true negatives and true positives are hidden behind the false positives and false negatives. Based on our assessment goals, we might use a different selection ratio by changing the cutoff. Figure 9.11 depicts the distributions of scores by berry type when we raise the cutoff. There are now more false negatives (blue) and fewer false positives (green). If we raise the cutoff (to be more conservative), the number of false negatives increases and the number of false positives decreases. Consequently, as the cutoff increases, sensitivity and NPV decrease (because we have more false negatives), whereas specificity and PPV increase (because we have fewer false positives). A higher cutoff could be optimal if the costs of false positives are considered greater than the costs of false negatives. For instance, if the aliens cannot risk eating the inedible berries because the berries are fatal, and there are sufficient edible berries that can be found to feed the alien colony. Figure 9.11: Classifications Based on Raising the Cutoff. Note that some true negatives and true positives are hidden behind the false positives and false negatives. Figure 9.12 depicts the distributions of scores by berry type when we lower the cutoff. There are now fewer false negatives (blue) and more false positives (green). If we lower the cutoff (to be more liberal), the number of false negatives decreases and the number of false positives increases. Consequently, as the cutoff decreases, sensitivity and NPV increase (because we have fewer false negatives), whereas specificity and PPV decrease (because we have more false positives). A lower cutoff could be optimal if the costs of false negatives are considered greater than the costs of false positives. For instance, if the aliens cannot risk missing edible berries because they are in short supply relative to the size of the alien colony, and eating the inedible berries would, at worst, lead to minor, temporary discomfort. Figure 9.12: Classifications Based on Lowering the Cutoff. Note that some true negatives and true positives are hidden behind the false positives and false negatives. In sum, sensitivity and specificity differ based on the cutoff for classification. if we raise the cutoff, sensitivity and PPV increase (due to fewer false positives), whereas and sensitivity and NPV decrease (due to more false negatives). If we lower the cutoff, sensitivity and NPV increase (due to fewer false negatives), whereas specificity and PPV decrease (due to more false positives). Thus, the optimal cutoff depends on how costly each type of error is: false negatives and false positives. If false negatives are more costly than false positives, we would set a low cutoff. If false positives are more costly than false negatives, we would set a high cutoff. 9.1.2.7 Signal Detection Theory Signal detection theory (SDT) is a probability-based theory for the detection of a given stimulus (signal) from a stimulus set that includes non-target stimuli (noise). SDT arose through the development of radar (RAdio Detection And Ranging) and sonar (SOund Navigation And Ranging) in World War II based on research on sensory-perception research. The military wanted to determine which objects on radar/sonar were enemy aircraft/submarines, and which were noise (e.g., different object in the environment or even just the weather itself). SDT allowed determining how many errors operators made (how accurate they were) and decomposing errors into different kinds of errors. SDT distinguishes between sensitivity and bias. In SDT, sensitivity (or discriminability) is how well an assessment distinguishes between a target stimulus and non-target stimuli (i.e., how well the assessment detects the target stimulus amid non-target stimuli). Bias is the extent to which the probability of a selection decision from the assessment is higher or lower than the true rate of the target stimulus. Some radar/sonar operators were not as sensitive to the differences between signal and noise, due to factors such as age, ability to distinguish gradations of a signal, etc. People who showed low sensitivity (i.e., who were not as successful at distinguishing between signal and noise) were screened out because the military perceived sensitivity as a skill that was not easily taught. By contrast, other operators could distinguish signal from noise, but their threshold was too low or high—they could take in information, but their decisions tended to be wrong due to systematic bias or poor calibration. That is, they systematically over-rejected or under-rejected stimuli. Over-rejecting leads to many false negatives (i.e., saying that a stimulus is safe when it is not). Under-rejecting leads to many false positives (i.e., saying that a stimulus is harmful when it is not). A person who showed good sensitivity but systematic bias was considered more teach-able than a person who showed low sensitivity. Thus, radar and sonar operators were selected based on their sensitivity to distinguish signal from noise, and then were trained to improve the calibration so they reduce their systematic bias and do not systematically over- or under-reject. Although SDT was originally developed for use in World War II, it now plays an important role in many areas of science and medicine. A medical application of SDT is tumor detection in radiology. SDT also plays an important role in psychology, especially cognitive psychology. For instance, research on social perception of sexual interest has shown that men tend to show lack of sensitivity to differences in women’s affect—i.e., they have relative difficulties discriminating between friendliness and sexual interest (Farris et al., 2008). Men also tend to show systematic bias (poor calibration) such that they tend to over-estimate women’s sexual interest in them—i.e., men tend to have too low of a threshold for determining that a women is showing sexual interest in them (Farris et al., 2006). SDT metrics of sensitivity include \\(d&#39;\\) (“\\(d\\)-prime”), \\(A\\) (or \\(A&#39;\\)), and the area under the receiver operating characteristic (ROC) curve. SDT metrics of bias include \\(\\beta\\) (beta), \\(c\\), and \\(b\\). 9.1.2.7.1 Receiver Operating Characteristic (ROC) Curve The x-axis of the ROC curve is the false alarm rate or false positive rate (\\(1 -\\) specificity). The y-axis is the hit rate or true positive rate (sensitivity). We can trace the ROC curve as the combination between sensitivity and specificity at every possible cutoff. At a cutoff of zero (top right of ROC curve), we calculate sensitivity (1.0) and specificity (0) and plot it. At a cutoff of zero, the assessment tells us to make an action for every stimulus (i.e., it is the most liberal). We then gradually increase the cutoff, and plot sensitivity and specificity at each cutoff. As the cutoff increases, sensitivity decreases and specificity increases. We end at the highest possible cutoff, where the sensitivity is 0 and the specificity is 1.0 (i.e., we never make an action; i.e., it is the most conservative). Each point on the ROC curve corresponds to a pair of hit and false alarm rates (sensitivity and specificity) resulting from a specific cutoff value. Then, we can draw lines or a curve to connect the points. Figure 9.13 depicts an empirical ROC plot where lines are drawn to connect the hit and false alarm rates. Figure 9.13: Empirical Receiver Operating Characteristic Curve. AUC = Area under the receiver operating characteristic curve. Figure 9.14 depicts an ROC curve where a smoothed and fitted curve is drawn to connect the hit and false alarm rates. Figure 9.14: Smooth Receiver Operating Characteristic Curve. AUC = Area under the receiver operating characteristic curve. 9.1.2.7.1.1 Area Under the ROC Curve ROC methods can be used to compare and compute the discriminative power of measurement devices free from the influence of selection ratios, base rates, and costs and benefits. An ROC analysis yields a quantitative index of how well an index predicts a signal of interest or can discriminate between different signals. ROC analysis can help tell us how often our assessment would be correct. If we randomly pick two observations, and we were right once and wrong once, we were 50% accurate. But this would be a useless measure because it reflects chance responding. The geometrical area under the ROC curve reflects the discriminative accuracy of the measure. The index is called the area under the curve (AUC) of an ROC curve. AUC quantifies the discriminative power of an assessment. AUC is the probability that a randomly selected target and a randomly selected non-target is ranked correctly by the assessment method. AUC values range from 0.0 to 1.0, where chance accuracy is 0.5 as indicated by diagonal line in the ROC curve. That is, a measure can be useful to the extent that its ROC curve is above the diagonal line (i.e., its discriminative accuracy is above chance). Figure 9.15: Area Under The Receiver Operating Characteristic Curve (AUC). Figure 9.16: Receiver Operating Characteristic (ROC) Curves for Various Levels of Area Under The ROC Curve (AUC) for Various Measures. As an example, given an AUC of .75, this says that the overall score of an individual who has the characteristic in question will be higher 75% of the time than the overall score of an individual who does not have the characteristic. In lay terms, AUC provides the probability that we will classify correctly based on our instrument if we were to randomly pick one good and one bad outcome. AUC is a stronger index of accuracy than percent accuracy, because you can have high percent accuracy just by going with the base rate. AUC tells us how much better than chance a measure is at discriminating outcomes. AUC is useful as a measure of general discriminative accuracy, and it tells us how accurate a measure is at all possible cutoffs. Knowing the accuracy of a measure at all possible cutoffs can be helpful for selecting the optimal cutoff, given the goals of the assessment. In reality, however, we may not be interested in all cutoffs because not all errors are equal in their costs. If we lower the base rate, we would need a larger sample to get enough people to classify into each group. SDT/ROC methods are traditionally about dichotomous decisions (yes/no), not graded judgments. SDT/ROC methods can get messy with ordinal data that are more graded because you would have an AUC curve for each ordinal grouping. 9.2 Getting Started 9.2.1 Load Libraries Code library(&quot;petersenlab&quot;) #to install: install.packages(&quot;remotes&quot;); remotes::install_github(&quot;DevPsyLab/petersenlab&quot;) library(&quot;pROC&quot;) library(&quot;ROCR&quot;) library(&quot;rms&quot;) library(&quot;ResourceSelection&quot;) library(&quot;PredictABEL&quot;) library(&quot;uroc&quot;) #to install: install.packages(&quot;remotes&quot;); remotes::install_github(&quot;evwalz/uroc&quot;) library(&quot;rms&quot;) library(&quot;gridExtra&quot;) library(&quot;grid&quot;) library(&quot;ggpubr&quot;) library(&quot;msir&quot;) library(&quot;car&quot;) library(&quot;viridis&quot;) library(&quot;ggrepel&quot;) library(&quot;MOTE&quot;) library(&quot;tidyverse&quot;) library(&quot;here&quot;) library(&quot;tinytex&quot;) library(&quot;rmarkdown&quot;) 9.2.2 Prepare Data 9.2.2.1 Load Data aSAH is a data set from the pROC package (Robin et al., 2021) that contains test scores (s100b) and clinical outcomes (outcome) for patients. Code data(aSAH) mydataSDT &lt;- aSAH 9.2.2.2 Simulate Data For reproducibility, I set the seed below. Using the same seed will yield the same answer every time. There is nothing special about this particular seed. Code set.seed(52242) mydataSDT$testScore &lt;- mydataSDT$s100b mydataSDT &lt;- mydataSDT %&gt;% mutate(testScoreSimple = ntile(testScore, 10)) mydataSDT$predictedProbability &lt;- (mydataSDT$s100b - min(mydataSDT$s100b, na.rm = TRUE)) / (max(mydataSDT$s100b, na.rm = TRUE) - min(mydataSDT$s100b, na.rm = TRUE)) mydataSDT$continuousOutcome &lt;- mydataSDT$testScore + rnorm(nrow(mydataSDT), mean = 0.20, sd = 0.20) mydataSDT$disorder &lt;- NA mydataSDT$disorder[mydataSDT$outcome == &quot;Good&quot;] &lt;- 0 mydataSDT$disorder[mydataSDT$outcome == &quot;Poor&quot;] &lt;- 1 9.2.2.3 Add Missing Data Adding missing data to dataframes helps make examples more realistic to real-life data and helps you get in the habit of programming to account for missing data. Code mydataSDT$testScore[c(5,10)] &lt;- NA mydataSDT$disorder[c(10,15)] &lt;- NA 9.3 Receiver Operating Characteristic (ROC) Curve The receiver operating characteristic (ROC) curve shows the combination of hit rate (sensitivity) and false alarm rate (\\(1 - \\text{specificity}\\)) at every possible cutoff. It depicts that, as the cutoff increases (i.e., becomes more conservative), sensitivity decreases and specificity increases. It also depicts that, as the cutoff decreases (i.e., becomes more liberal), sensitivity increases and specificity decreases. Receiver operating characteristic (ROC) curves were generated using the pROC package (Robin et al., 2021). The examples depict ROC curves that demonstrate that the measure is moderately accurate—the measure is more accurate than chance but there remains considerable room for improvement in predictive accuracy. 9.3.1 Empirical ROC Curve The syntax used to generate an empirical ROC plot is below, and the plot is in Figure 9.13. Code rocCurve &lt;- roc(data = mydataSDT, response = disorder, predictor = testScore, smooth = FALSE) Code plot(rocCurve, legacy.axes = TRUE, print.auc = TRUE) Figure 9.17: Empirical Receiver Operating Characteristic Curve. AUC = Area under the receiver operating characteristic curve. An empirical ROC plot with cutoffs overlaid is in Figure 9.18. Code pred &lt;- prediction(na.omit(mydataSDT[,c( &quot;testScoreSimple&quot;,&quot;disorder&quot;)])$testScoreSimple, na.omit(mydataSDT[,c(&quot;testScoreSimple&quot;,&quot;disorder&quot;)])$disorder) perf &lt;- performance(pred, &quot;tpr&quot;, &quot;fpr&quot;) plot( perf, print.cutoffs.at = 1:11, text.adj = c(1, -1), ylim = c(0, 1.05)) abline(coef = c(0,1)) Figure 9.18: Empirical Receiver Operating Characteristic Curve With Cutoffs Overlaid. 9.3.2 Smooth ROC Curve Code rocCurveSmooth &lt;- roc(data = mydataSDT, response = disorder, predictor = testScore, smooth = TRUE) Code plot(rocCurveSmooth, legacy.axes = TRUE, print.auc = TRUE) Figure 9.19: Smooth Receiver Operating Characteristic Curve. AUC = Area under the receiver operating characteristic curve. 9.3.3 Youden’s J Statistic The threshold at the Youden’s J statistic is the threshold where the test has the maximum combination (i.e., sum) of sensitivity and specificity: \\(\\text{max}(\\text{sensitivity} + \\text{specificity} - 1)\\) Code youdenJ &lt;- coords( rocCurve, x = &quot;best&quot;, best.method = &quot;youden&quot;) youdenJthreshold &lt;- youdenJ$threshold youdenJspecificity &lt;- youdenJ$specificity youdenJsensitivity &lt;- youdenJ$sensitivity youdenJ For this test, the Youden’s J Statistic is at a threshold of \\(0.205\\), where sensitivity is \\(0.65\\) and specificity is \\(0.8\\). 9.3.4 The point closest to the top-left part of the ROC curve with perfect sensitivity and specificity The point closest to the top-left part of the ROC plot with perfect sensitivity and specificity: \\(\\text{min}[(1 - \\text{sensitivity})^2 + (1 - \\text{specificity})^2]\\) Code closestTopLeft &lt;- coords( rocCurve, x = &quot;best&quot;, best.method = &quot;closest.topleft&quot;) closestTopLeftthreshold &lt;- closestTopLeft$threshold closestTopLeftspecificity &lt;- closestTopLeft$specificity closestTopLeftsensitivity &lt;- closestTopLeft$sensitivity closestTopLeft For this test, the combination of sensitivity and specificity is closest to the top left of the ROC plot at a threshold of \\(0.205\\), where sensitivity is \\(0.65\\) and specificity is \\(0.8\\). 9.4 Prediction Accuracy Across Cutoffs There are two primary dimensions of accuracy: (1) discrimination (e.g., sensitivity, specificity, area under the ROC curve) and (2) calibration. Some general indexes of accuracy combine discrimination and calibration, as described in Section 9.4.1. This section (9.4) describes indexes of accuracy that span all possible cutoffs. That is, each index of accuracy described in this section provides a single numerical index of accuracy that aggregates the accuracy across all possible cutoffs. Aspects of accuracy at a particular cutoff are described in Section 9.5. The petersenlab package (Petersen, 2024b) contains the accuracyOverall() function that estimates the prediction accuracy across cutoffs. Code accuracyOverall( predicted = mydataSDT$testScore, actual = mydataSDT$disorder) %&gt;% t %&gt;% round(., 2) [,1] ME -0.11 MAE 0.34 MSE 0.21 RMSE 0.46 MPE -Inf MAPE Inf sMAPE 82.46 MASE 0.74 RMSLE 0.30 rsquared 0.18 rsquaredAdj 0.17 rsquaredPredictive 0.12 Code accuracyOverall( predicted = mydataSDT$testScore, actual = mydataSDT$disorder, dropUndefined = TRUE) %&gt;% t %&gt;% round(., 2) [,1] ME -0.11 MAE 0.34 MSE 0.21 RMSE 0.46 MPE 59.62 MAPE 64.97 sMAPE 82.46 MASE 0.74 RMSLE 0.30 rsquared 0.18 rsquaredAdj 0.17 rsquaredPredictive 0.12 9.4.1 General Prediction Accuracy There are many metrics of general prediction accuracy. When thinking about which metric(s) may be best for a given problem, it is important to consider the purpose of the assessment. The estimates of general prediction accuracy are separated below into scale-dependent and scale-independent accuracy estimates. 9.4.1.1 Scale-Dependent Accuracy Estimates The estimates of prediction accuracy described in this section are scale-dependent. These accuracy estimates depend on the unit of measurement and therefore cannot be compared across measures with different scales or across data sets. 9.4.1.1.1 Mean Error Here, “error” (\\(e\\)) is the difference between the predicted and observed value for a given individual (\\(i\\)). Mean error (ME; also known as bias, see Section 4.5.1.3.2) is the mean difference between the predicted and observed values across individuals (\\(i\\)), that is, the mean of the errors across individuals (\\(e_i\\)). Values closer to zero reflect greater accuracy. If mean error is above zero, it indicates that predicted values are, on average, greater than observed values (i.e., over-estimating errors). If mean error is below zero, it indicates that predicted values are, on average, less than observed values (i.e., under-estimating errors). If both over-estimating and under-estimating errors are present, however, they can cancel each other out. As a result, even with a mean error of zero, there can still be considerable error present. Thus, although mean error can be helpful for examining whether predictions systematically under- or over-estimate the actual scores, other forms of accuracy are necessary to examine the extent of error. The formula for mean error is in Equation (9.20): \\[ \\begin{aligned} \\text{mean error} &amp;= \\frac{\\sum\\limits_{i = 1}^n(\\text{predicted}_i - \\text{observed}_i)}{n} \\\\ &amp;= \\text{mean}(e_i) \\end{aligned} \\tag{9.20} \\] Code meanError &lt;- function(predicted, actual){ value &lt;- mean(predicted - actual, na.rm = TRUE) return(value) } Code meanError(mydataSDT$testScore, mydataSDT$disorder) [1] -0.1123636 In this case, the mean error is negative, so the predictions systematically under-estimate the actual scores. 9.4.1.1.2 Mean Absolute Error (MAE) Mean absolute error (MAE) is the mean of the absolute value of differences between the predicted and observed values across individuals, that is, the mean of the absolute value of errors. Smaller MAE values (closer to zero) reflect greater accuracy. MAE is preferred over root mean squared error (RMSE) when you want to give equal weight to all errors and when the outliers have considerable impact. The formula for MAE is in Equation (9.21): \\[ \\begin{aligned} \\text{mean absolute error (MAE)} &amp;= \\frac{\\sum\\limits_{i = 1}^n|\\text{predicted}_i - \\text{observed}_i|}{n} \\\\ &amp;= \\text{mean}(|e_i|) \\end{aligned} \\tag{9.21} \\] Code meanAbsoluteError &lt;- function(predicted, actual){ value &lt;- mean(abs(predicted - actual), na.rm = TRUE) return(value) } Code meanAbsoluteError(mydataSDT$testScore, mydataSDT$disorder) [1] 0.3407273 9.4.1.1.3 Mean Squared Error (MSE) Mean squared error (MSE) is the mean of the square of the differences between the predicted and observed values across individuals, that is, the mean of the squared value of errors. Smaller MSE values (closer to zero) reflect greater accuracy. MSE penalizes larger errors more heavily than smaller errors (unlike MAE). However, MSE is sensitive to outliers and can be impacted if the errors are skewed. The formula for MSE is in Equation (9.22): \\[ \\begin{aligned} \\text{mean squared error (MSE)} &amp;= \\frac{\\sum\\limits_{i = 1}^n(\\text{predicted}_i - \\text{observed}_i)^2}{n} \\\\ &amp;= \\text{mean}(e_i^2) \\end{aligned} \\tag{9.22} \\] Code meanSquaredError = function(predicted, actual){ value &lt;- mean((predicted - actual)^2, na.rm = TRUE) return(value) } Code meanSquaredError(mydataSDT$testScore, mydataSDT$disorder) [1] 0.2078273 9.4.1.1.4 Root Mean Squared Error (RMSE) Root mean squared error (RMSE) is the square root of the mean of the square of the differences between the predicted and observed values across individuals, that is, the root mean squared value of errors. Smaller RMSE values (closer to zero) reflect greater accuracy. RMSE penalizes larger errors more heavily than smaller errors (unlike MAE). However, RMSE is sensitive to outliers and can be impacted if the errors are skewed. The formula for RMSE is in Equation (9.23): \\[ \\begin{aligned} \\text{root mean squared error (RMSE)} &amp;= \\sqrt{\\frac{\\sum\\limits_{i = 1}^n(\\text{predicted}_i - \\text{observed}_i)^2}{n}} \\\\ &amp;= \\sqrt{\\text{mean}(e_i^2)} \\end{aligned} \\tag{9.23} \\] Code rootMeanSquaredError = function(predicted, actual){ value &lt;- sqrt(mean((predicted - actual)^2, na.rm = TRUE)) return(value) } Code rootMeanSquaredError(mydataSDT$testScore, mydataSDT$disorder) [1] 0.4558808 9.4.1.2 Scale-Independent Accuracy Estimates The estimates of prediction accuracy described in this section are intended to be scale-independent (unit-free) so the accuracy estimates can be compared across measures with different scales or across data sets (Hyndman &amp; Athanasopoulos, 2018). 9.4.1.2.1 Mean Percentage Error (MPE) Mean percentage error (MPE) values closer to zero reflect greater accuracy. The formula for percentage error is in Equation (9.24): \\[ \\begin{aligned} \\text{percentage error }(p_i) = \\frac{100\\% \\times (\\text{observed}_i - \\text{predicted}_i)}{\\text{observed}_i} \\end{aligned} \\tag{9.24} \\] We then take the mean of the percentage errors to get MPE. The formula for MPE is in Equation (9.25): \\[ \\begin{aligned} \\text{mean percentage error (MPE)} &amp;= \\frac{100\\%}{n} \\sum\\limits_{i = 1}^n \\frac{\\text{observed}_i - \\text{predicted}_i}{\\text{observed}_i} \\\\ &amp;= \\text{mean(percentage error)} \\\\ &amp;= \\text{mean}(p_i) \\end{aligned} \\tag{9.25} \\] Note: MPE is undefined when one or more of the observed values equals zero, due to division by zero. I provide the option in the function to drop undefined values so you can still generate an estimate of accuracy despite undefined values, but use this option at your own risk. Code meanPercentageError = function(predicted, actual, dropUndefined = FALSE){ percentageError &lt;- 100 * (actual - predicted) / actual if(dropUndefined == TRUE){ percentageError[!is.finite(percentageError)] &lt;- NA } value &lt;- mean(percentageError, na.rm = TRUE) return(value) } Code meanPercentageError( mydataSDT$testScore, mydataSDT$disorder) [1] -Inf Code meanPercentageError( mydataSDT$testScore, mydataSDT$disorder, dropUndefined = TRUE) [1] 59.625 9.4.1.2.2 Mean Absolute Percentage Error (MAPE) Smaller mean absolute percentage error (MAPE) values (closer to zero) reflect greater accuracy. The formula for MAPE is in Equation (9.26): MAPE is asymmetric because it overweights underestimates and underweights overestimates. MAPE can be preferable to symmetric mean absolute percentage error (sMAPE) if there are no observed values of zero and if you want to emphasize the importance of underestimates (relative to overestimates). \\[ \\begin{aligned} \\text{mean absolute percentage error (MAPE)} &amp;= \\frac{100\\%}{n} \\sum\\limits_{i = 1}^n \\Bigg|\\frac{\\text{observed}_i - \\text{predicted}_i}{\\text{observed}_i}\\Bigg| \\\\ &amp;= \\text{mean(|percentage error|)} \\\\ &amp;= \\text{mean}(|p_i|) \\end{aligned} \\tag{9.26} \\] Note: MAPE is undefined when one or more of the observed values equals zero, due to division by zero. I provide the option in the function to drop undefined values so you can still generate an estimate of accuracy despite undefined values, but use this option at your own risk. Code meanAbsolutePercentageError = function(predicted, actual, dropUndefined = FALSE){ percentageError &lt;- 100 * (actual - predicted) / actual if(dropUndefined == TRUE){ percentageError[!is.finite(percentageError)] &lt;- NA } value &lt;- mean(abs(percentageError), na.rm = TRUE) return(value) } Code meanAbsolutePercentageError( mydataSDT$testScore, mydataSDT$disorder) [1] Inf Code meanAbsolutePercentageError( mydataSDT$testScore, mydataSDT$disorder, dropUndefined = TRUE) [1] 64.975 9.4.1.2.3 Symmetric Mean Absolute Percentage Error (sMAPE) Unlike MAPE, symmetric mean absolute percentage error (sMAPE) is symmetric because it equally weights underestimates and overestimates. Smaller sMAPE values (closer to zero) reflect greater accuracy. The formula for sMAPE is in Equation (9.27): \\[ \\small \\begin{aligned} \\text{symmetric mean absolute percentage error (sMAPE)} = \\frac{100\\%}{n} \\sum\\limits_{i = 1}^n \\frac{|\\text{predicted}_i - \\text{observed}_i|}{|\\text{predicted}_i| + |\\text{observed}_i|} \\end{aligned} \\tag{9.27} \\] Note: sMAPE is undefined when one or more of the individuals has a prediction–observed combination such that the sum of the absolute value of the predicted value and the absolute value of the observed value equals zero (\\(|\\text{predicted}_i| + |\\text{observed}_i|\\)), due to division by zero. I provide the option in the function to drop undefined values so you can still generate an estimate of accuracy despite undefined values, but use this option at your own risk. Code symmetricMeanAbsolutePercentageError = function(predicted, actual, dropUndefined = FALSE){ relativeError &lt;- abs(predicted - actual)/(abs(predicted) + abs(actual)) if(dropUndefined == TRUE){ relativeError[!is.finite(relativeError)] &lt;- NA } value &lt;- 100 * mean(abs(relativeError), na.rm = TRUE) return(value) } Code symmetricMeanAbsolutePercentageError( mydataSDT$testScore, mydataSDT$disorder) [1] 82.45553 9.4.1.2.4 Mean Absolute Scaled Error (MASE) Mean absolute scaled error (MASE) is described by (Hyndman &amp; Athanasopoulos, 2018). Values closer to zero reflect greater accuracy. The adapted formula for MASE with non-time series data is described here (https://stats.stackexchange.com/a/108963/20338) (archived at https://perma.cc/G469-8NAJ). Scaled errors are calculated using Equation (9.28): \\[ \\begin{aligned} \\text{scaled error}(q_i) &amp;= \\frac{\\text{observed}_i - \\text{predicted}_i}{\\text{scaling factor}} \\\\ &amp;= \\frac{\\text{observed}_i - \\text{predicted}_i}{\\frac{1}{n} \\sum\\limits_{i = 1}^n |\\text{observed}_i - \\overline{\\text{observed}}|} \\end{aligned} \\tag{9.28} \\] Then, we calculate the mean of the absolute value of the scaled errors to get MASE, as in Equation (9.29): \\[ \\begin{aligned} \\text{mean absolute scaled error (MASE)} &amp;= \\frac{1}{n} \\sum\\limits_{i = 1}^n |q_i| \\\\ &amp;= \\text{mean(|scaled error|)} \\\\ &amp;= \\text{mean}(|q_i|) \\end{aligned} \\tag{9.29} \\] Note: MASE is undefined when the scaling factor is zero, due to division by zero. With non-time series data, the scaling factor is the average of the absolute value of individuals’ observed scores minus the average observed score (\\(\\frac{1}{n} \\sum\\limits_{i = 1}^n |\\text{observed}_i - \\overline{\\text{observed}}|\\)). Code meanAbsoluteScaledError &lt;- function(predicted, actual){ mydata &lt;- data.frame(na.omit(cbind(predicted, actual))) errors &lt;- mydata$actual - mydata$predicted scalingFactor &lt;- mean(abs(mydata$actual - mean(mydata$actual))) scaledErrors &lt;- errors/scalingFactor value &lt;- mean(abs(scaledErrors)) return(value) } Code meanAbsoluteScaledError(mydataSDT$testScore, mydataSDT$disorder) [1] 0.7362143 9.4.1.2.5 Root Mean Squared Log Error (RMSLE) The squared log of the accuracy ratio is described by Tofallis (2015). The accuracy ratio is in Equation (9.30): \\[ \\begin{aligned} \\text{accuracy ratio} &amp;= \\frac{\\text{predicted}_i}{\\text{observed}_i} \\end{aligned} \\tag{9.30} \\] However, the accuracy ratio is undefined with observed or predicted values of zero, so it is common to modify it by adding 1 to the predictor and denominator, as in Equation (9.31): \\[ \\begin{aligned} \\text{accuracy ratio} &amp;= \\frac{\\text{predicted}_i + 1}{\\text{observed}_i + 1} \\end{aligned} \\tag{9.31} \\] Squaring the log values keeps the values positive, such that smaller values (values closer to zero) reflect greater accuracy. Then we take the mean of the squared log values, which keeps the values positive, and calculate the square root of the mean squared log values to put them back on the (pre-squared) log metric. This is known as the root mean squared log error (RMSLE). Division inside the log is equal to subtraction outside the log. So, the formula can be reformulated with the subtraction of two logs, as in Equation (9.32): \\[ \\begin{aligned} \\text{root mean squared log error (RMSLE)} &amp;= \\sqrt{\\sum\\limits_{i = 1}^n log\\bigg(\\frac{\\text{predicted}_i + 1}{\\text{observed}_i + 1}\\bigg)^2} \\\\ &amp;= \\sqrt{\\text{mean}\\Bigg[log\\bigg(\\frac{\\text{predicted}_i + 1}{\\text{observed}_i + 1}\\bigg)^2\\Bigg]} \\\\ &amp;= \\sqrt{\\text{mean}\\big[log(\\text{accuracy ratio})^2\\big]} = \\sqrt{\\text{mean}\\Big\\{\\big[log(\\text{predicted}_i + 1) - log(\\text{actual}_i + 1)\\big]^2\\Big\\}} \\end{aligned} \\tag{9.32} \\] RMSLE can be preferable when the scores have a wide range of values and are skewed. RMSLE can help to reduce the impact of outliers. RMSLE gives more weight to smaller errors in the prediction of small observed values, while also penalizing larger errors in the prediction of larger observed values. It overweights underestimates and underweights overestimates. There are other variations of prediction accuracy metrics that use the log of the accuracy ratio. One variation makes it similar to median symmetric percentage error (Morley et al., 2018). Note: Root mean squared log error is undefined when one or more predicted values or actual values equals -1. When predicted or actual values are -1, this leads to \\(log(0)\\), which is undefined. I provide the option in the function to drop undefined values so you can still generate an estimate of accuracy despite undefined values, but use this option at your own risk. Code rootMeanSquaredLogError &lt;- function(predicted, actual, dropUndefined = FALSE){ logError &lt;- log(predicted + 1) - log(actual + 1) if(dropUndefined == TRUE){ logError[!is.finite(logError)] &lt;- NA } value &lt;- sqrt(mean(logError^2, na.rm = TRUE)) return(value) } Code rootMeanSquaredLogError( mydataSDT$testScore, mydataSDT$disorder) [1] 0.303727 Code rootMeanSquaredLogError( mydataSDT$testScore, mydataSDT$disorder, dropUndefined = TRUE) [1] 0.303727 9.4.1.2.6 Coefficient of Determination (\\(R^2\\)) The coefficient of determination (\\(R^2\\)) is a general index of accuracy that combines both discrimination and calibration. It reflects the proportion of variance in the outcome (dependent) variable that is explained by the model predictions: \\(R^2 = \\frac{\\text{variance explained in }Y}{\\text{total variance in }Y}\\). Larger values indicate greater accuracy. \\(R^2\\) is commonly estimated in multiple regression, in which multiple predictors are allowed to predict one outcome. Multiple regression can be conceptualized with overlapping circles in what is called a Ballantine graph (archived at https://perma.cc/C7CU-KFVG). \\(R^2\\) in multiple regression is depicted conceptually with a Ballantine graph in Figure 9.20. Figure 9.20: Conceptual Depiction of Proportion of Variance Explained (\\(R^2\\)) in an Outcome Variable (\\(Y\\)) by Multiple Predictors (\\(X1\\) and \\(X2\\)) in Multiple Regression. The size of each circle represents the variable’s variance. The proportion of variance in \\(Y\\) that is explained by the predictors is depicted by the areas in orange. The dark orange space (\\(G\\)) is where multiple predictors explain overlapping variance in the outcome. Overlapping variance that is explained in the outcome (\\(G\\)) will not be recovered in the regression coefficients when both predictors are included in the regression model. \\(R^2\\): Code summary(lm( disorder ~ testScore, data = mydataSDT))$r.squared [1] 0.1778746 The predictor (testScore) explains \\(17.79\\)% of the variance \\((R^2 = .1779)\\) in the outcome (disorder status). 9.4.1.2.6.1 Adjusted \\(R^2\\) (\\(R^2_{adj}\\)) Adjusted \\(R^2\\) is similar to the coefficient of determination, but it accounts for the number of predictors included in the regression model to penalize overfitting. Adjusted \\(R^2\\) reflects the proportion of variance in the outcome (dependent) variable that is explained by the model predictions over and above what would be expected to be accounted for by chance, given the number of predictors in the model. Larger values indicate greater accuracy. The formula for adjusted \\(R^2\\) is in Equation (9.33): \\[\\begin{equation} R^2_{adj} = 1 - (1 - R^2) \\frac{n - 1}{n - p - 1} \\tag{9.33} \\end{equation}\\] where \\(p\\) is the number of predictors in the model, and \\(n\\) is the sample size. Code summary(lm( disorder ~ testScore, data = mydataSDT))$adj.r.squared [1] 0.1702623 Adjusted \\(R^2\\) is described further in Section 9.9. 9.4.1.2.6.2 Predictive \\(R^2\\) Predictive \\(R^2\\) is described here: https://tomhopper.me/2014/05/16/can-we-do-better-than-r-squared/ (archived at https://perma.cc/BK8J-HFUK). Predictive \\(R^2\\) penalizes overfitting, unlike traditional \\(R^2\\). Larger values indicate greater accuracy. Code #predictive residual sum of squares (PRESS) PRESS &lt;- function(linear.model) { #calculate the predictive residuals pr &lt;- residuals(linear.model)/(1-lm.influence(linear.model)$hat) #calculate the PRESS PRESS &lt;- sum(pr^2) return(PRESS) } predictiveRSquared &lt;- function(predicted, actual){ #fit linear model linear.model &lt;- lm(actual ~ predicted) #use anova() to get the sum of squares for the linear model lm.anova &lt;- anova(linear.model) #calculate the total sum of squares tss &lt;- sum(lm.anova$&#39;Sum Sq&#39;) #calculate the predictive R^2 value &lt;- 1 - PRESS(linear.model)/(tss) return(value) } Code predictiveRSquared(mydataSDT$testScore, mydataSDT$disorder) [1] 0.1190976 9.4.2 Discrimination When dealing with a categorical outcome, discrimination is the ability to separate events from non-events. When dealing with a continuous outcome, discrimination is the strength of the association between the predictor and the outcome. Aspects of discrimination at a particular cutoff (e.g., sensitivity, specificity) are described in Section 9.5. 9.4.2.1 Area under the ROC curve (AUC) The area under the ROC curve (AUC) is a general index of discrimination accuracy for a categorical outcome. It is also called the concordance (\\(c\\)) statistic. Larger values reflect greater discrimination accuracy. AUC was estimated using the pROC package (Robin et al., 2021). Code rocCurve$auc Area under the curve: 0.7312 9.4.2.2 Coefficient of Predictive Ability (CPA) The coefficient of predictive ability (CPA) is a generalization of AUC to handle non-binary outcomes including ordinal and continuous outcomes (Gneiting &amp; Walz, 2021). Larger values reflect greater accuracy. It was calculated using the uroc package (Gneiting &amp; Walz, 2021). Code cpaNoMissing &lt;- na.omit( mydataSDT[,c(&quot;testScore&quot;,&quot;continuousOutcome&quot;)]) cpa( response = cpaNoMissing$continuousOutcome, predictor = cpaNoMissing$testScore) [1] 0.8380923 9.4.2.3 Spearman’s Rho (\\(\\rho\\)) Rank Correlation When the coefficient of predictive ability is applied to an ordinal or continuous outcome, it is linearly related to Spearman’s rho (\\(\\rho\\)) rank correlation (Gneiting &amp; Walz, 2021). Larger values reflect greater accuracy. Spearman’s rho was estimated in the rms package (Harrell, Jr., 2021). Code cor( x = mydataSDT$testScore, y = mydataSDT$continuousOutcome, use = &quot;pairwise.complete.obs&quot;, method = &quot;spearman&quot;) [1] 0.6768502 Code orm( continuousOutcome ~ testScore, data = mydataSDT)$stats[&quot;rho&quot;] rho 0.6768502 9.4.2.4 Somers’ \\(D_{xy}\\) Rank Correlation Somers \\(D_{xy}\\) is an index of discrimination for an ordinal outcome (Harrell, 2015). Larger values reflect greater accuracy. Somers’ \\(D_{xy}\\) was estimated in the rms package (Harrell, Jr., 2021). Code lrm( continuousOutcome ~ testScore, data = mydataSDT)$stats[&quot;Dxy&quot;] Dxy 0.4977887 Code rms::validate(lrm( continuousOutcome ~ testScore, data = mydataSDT, x = TRUE, y = TRUE), group = mydataSDT$continuousOutcome)[&quot;Dxy&quot;,&quot;index.corrected&quot;] [1] 0.4977887 9.4.2.5 Kendall’s Tau-a (\\(\\tau_A\\)) Rank Correlation Kendall’s tau-a (\\(\\tau_A\\)) is an index of the discrimination for an ordinal outcome (Harrell, 2015). Larger values reflect greater accuracy. Kendall’s tau-a was estimated in the rms package (Harrell, Jr., 2021). Code cor( x = mydataSDT$testScore, y = mydataSDT$continuousOutcome, use = &quot;pairwise.complete.obs&quot;, method = &quot;kendall&quot;) [1] 0.5050378 Code lrm(continuousOutcome ~ testScore, data = mydataSDT)$stats[&quot;Tau-a&quot;] Tau-a 0.4977887 9.4.2.6 \\(c\\) Index The \\(c\\) index, also called the concordance probability, is a generalization of the area under the ROC curve that applies to a continuous outcome (Harrell, 2015). The \\(c\\) index is the proportion of all pairs of predicted-actual values whose actual value can be ordered such that the pair with the higher predicted value is the one who had the higher actual value. Larger values reflect greater accuracy. The Brown–Hollander–Korwar non-parametric test of association was estimated in the rms package (Harrell, Jr., 2021). Code lrm( continuousOutcome ~ testScore, data = mydataSDT)$stats[&quot;C&quot;] C 0.7488943 9.4.2.7 Effect Size (\\(\\beta\\)) of Regression The effect size of a predictor, i.e., the standardized regression coefficient is called a beta (\\(\\beta\\)) coefficient, is a general index of discrimination accuracy for a continuous outcome. Larger values reflect greater accuracy. We can obtain standardized regression coefficients by standardizing the predictors and outcome using the scale() function in R. \\(\\beta\\): Code lm( scale(continuousOutcome) ~ scale(testScore), data = mydataSDT)$coef[&quot;scale(testScore)&quot;] scale(testScore) 0.8284925 9.4.3 Calibration When dealing with a categorical outcome, calibration is the degree to which a probabilistic estimate of an event reflects the true underlying probability of the event. When dealing with a continuous outcome, calibration is the degree to which the predicted values are close in value to the outcome values. The importance of examining calibration (in addition to discrimination) is described by Lindhiem et al. (2020). Calibration came to be considered a central aspect of weather forecast accuracy. For instance, on the days that the meteorologist says there is a 60% chance of rain, it should rain about 60% of the time. Through improvements in scientific understanding of weather systems, rain forecasts have become more accurate. For instance, rain forecasts from the National Weather Service are well calibrated (see Figure 9.22, reprinted from Charba &amp; Klein, 1980). However, forecasts of rain may be exaggerated by local TV meteorologists to boost ratings (Silver, 2012). Interestingly, rain forecasts from The Weather Channel are somewhat miscalibrated under certain conditions. For instance, on days when they forecast a 20% chance of rain, the actual chance of rain is around 5% (see Figure 9.21, Bickel &amp; Kim, 2008). However, this miscalibration is deliberate. People tend to be more angry when the meteorologist says it will not rain when it actually does (false negative) compared to when the meteorologist says it will rain when it actually does not (false positive). As Silver (2012) notes, “If it rains when it isn’t supposed to, [people] curse the weatherman for ruining their picnic, whereas an unexpectedly sunny day is taken as a serendipitous bonus. It isn’t good science, but as Dr. Rose at The Weather Channel acknowledged to [Mr. Silver]: ‘If the forecast was objective, if it has zero bias in precipitation, we’d probably be in trouble.’” (p. 135). Figure 9.21: Calibration Plot Of Same-Day Probability Of Precipitation (PoP) Forecasts From The Weather Channel. The plot depicts that the rain forecasts are generally well calibrated, but shows some miscalibration. For instance, the forecasted probability of rain is lower than the actual probability of rain when the forecasted probability of rain is 20%. (Figure reprinted from Bickel &amp; Kim (2008), Figure 2, p. 4872. Bickel, J. E., &amp; Kim, S. D. (2008). Verification of The Weather Channel probability of precipitation forecasts. Monthly Weather Review, 136(12), 4867–4881. doi: https://doi.org/10.1175/2008MWR2547.1 Copyright (c) American Meteorological Society. Used with permission.) Figure 9.22: Calibration Plot Of Local Probability Of Precipitation (PoP) Forecasts for 87 Stations From the United States National Weather Service. Numbers next to the plotted points are the sample sizes. (Figure reprinted from Charba &amp; Klein (1980), Figure 6, p. 1550. Charba, J. P., &amp; Klein, W. H. (1980). Skill in precipitation forecasting in the National Weather Service. Bulletin of the American Meteorological Society, 61(12), 1546–1555. https://doi.org/10.1175/1520-0477(1980)061&lt;1546:SIPFIT&gt;2.0.CO;2. Copyright (c) American Meteorological Society. Used with permission.) Calibration is not just important for weather forecasts. It is also important for psychological assessment. Calibration can be examined in several ways, including Brier Scores (see Section 9.4.3.2), the Hosler-Lemeshow test (see Section 9.4.3.3), Spiegelhalter’s \\(z\\) (see Section 9.4.3.4), and the mean difference between predicted and observed values at different binned thresholds as depicted graphically with a calibration plot (see Figure 9.24). 9.4.3.1 Calibration Plot Calibration plots can be helpful for identifying miscalibration. A calibration plot depicts the predicted probability of an event on the x-axis, and the actual (observed) probability of the event on the y-axis. The predictions are binned into a certain number of groups (commonly 10). The diagonal line reflects predictions that are perfectly calibrated. To the extent that predictions deviate from the diagonal line, the predictions are miscalibrated. There are four general patterns of miscalibration: overextremity, underextremity, overprediction, and underprediction (see Figure 9.23). Overextremity exists when the predicted probabilites are too close to the extremes (zero or one). Underextremity exists when the predicted probabilities are too far away from the extremes. Overprediction exists when the predicted probabilities are consistently greater than the observed probabilities. Underprediction exists when the predicted probabilities are consistently less than the observed probabilities. For a more thorough description of these types of miscalibration, see Lindhiem et al. (2020). Figure 9.23: Types Of Miscalibration. This calibration plot was generated using the PredictABEL package (Kundu et al., 2020), and is depicted in Figure 9.24. Code colNumberOutcome &lt;- which(names(mydataSDT) == &quot;disorder&quot;) myDataNoMissing &lt;- na.omit(mydataSDT) Code plotCalibration( data = na.omit(myDataNoMissing), cOutcome = colNumberOutcome, predRisk = myDataNoMissing$predictedProbability, groups = 10) Figure 9.24: Calibration Plot 1. $Table_HLtest total meanpred meanobs predicted observed [0.0000,0.0245) 19 0.013 0.211 0.25 4 0.0245 7 0.025 0.143 0.17 1 0.0294 8 0.029 0.250 0.24 2 [0.0343,0.0441) 13 0.036 0.231 0.47 3 [0.0441,0.0588) 10 0.051 0.300 0.51 3 [0.0588,0.0735) 10 0.063 0.100 0.63 1 [0.0735,0.1324) 10 0.099 0.500 0.99 5 [0.1324,0.2059) 12 0.165 0.583 1.98 7 [0.2059,0.2598) 10 0.222 0.300 2.22 3 [0.2598,1.0000] 11 0.408 1.000 4.49 11 $Chi_square [1] 152.274 $df [1] 8 $p_value [1] 0 This calibration plot (and ROC curve) was generated using the rms package (Harrell, Jr., 2021), and is depicted in Figure 9.25. The calibration plot is consistent with underprediction. That is, the predicted probabilities were consistently lower than the actual probabilities. To re-calibrate the predicted probabilities, it would be necessary to increase them so they are consistent with observed probabilties. Code val.prob(mydataSDT$predictedProbability, mydataSDT$disorder) Figure 9.25: Calibration Plot 2. Dxy C (ROC) R2 D D:Chi-sq 4.769231e-01 7.384615e-01 -6.156954e-01 -3.797827e-01 -4.039632e+01 D:p U U:Chi-sq U:p Q NA Inf Inf 0.000000e+00 -Inf Brier Intercept Slope Emax E90 2.659086e-01 1.682881e+00 8.857501e-01 7.146778e-01 3.719585e-01 Eavg S:z S:p 2.618115e-01 1.052663e+01 6.512514e-26 This calibration plot was adapted from Darren Dahly’s example, and is depicted in Figure 9.26: https://darrendahly.github.io/post/homr/ (archived at https://perma.cc/6J3J-69G7) Code g1 &lt;- mutate(mydataSDT, bin = cut_number(predictedProbability, 10)) %&gt;% # Bin prediction into 10ths group_by(bin) %&gt;% mutate(n = length(na.omit(predictedProbability)), # Get ests and CIs bin_pred = mean(predictedProbability, na.rm = TRUE), bin_prob = mean(disorder, na.rm = TRUE), se = sd(disorder, na.rm = TRUE) / sqrt(n), ul = bin_prob + qnorm(.975) * se, ll = bin_prob - qnorm(.975) * se) %&gt;% ungroup() %&gt;% ggplot(aes(x = bin_pred, y = bin_prob, ymin = ll, ymax = ul)) + geom_pointrange(size = 0.5, color = &quot;black&quot;) + scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.1)) + scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.1)) + geom_abline() + # 45 degree line indicating perfect calibration geom_smooth(method = &quot;lm&quot;, se = FALSE, linetype = &quot;dashed&quot;, color = &quot;black&quot;, formula = y~-1 + x) + # straight line fit through estimates geom_smooth(aes(x = predictedProbability, y = disorder), color = &quot;red&quot;, se = FALSE, method = &quot;loess&quot;) + # loess fit through estimates xlab(&quot;&quot;) + ylab(&quot;Observed Probability&quot;) + theme_minimal()+ xlab(&quot;Predicted Probability&quot;) g2 &lt;- ggplot(mydataSDT, aes(x = predictedProbability)) + geom_histogram(fill = &quot;black&quot;, bins = 200) + scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.1)) + xlab(&quot;Histogram of Predicted Probability&quot;) + ylab(&quot;&quot;) + theme_minimal() + theme(panel.grid.minor = element_blank()) g &lt;- arrangeGrob(g1, g2, respect = TRUE, heights = c(1, 0.25), ncol = 1) Code grid.arrange(g) Figure 9.26: Calibration Plot 3. 9.4.3.2 Brier Scores Brier scores were calculated using the rms package (Harrell, Jr., 2021). Smaller values reflect greater calibration accuracy. Code val.prob( mydataSDT$predictedProbability, mydataSDT$disorder, pl = FALSE)[&quot;Brier&quot;] Brier 0.2659086 9.4.3.3 Hosler-Lemeshow Test The Hosler-Lemeshow goodness of fit (GOF) test evaluates the null hypothesis that there is no difference between the predicted versus observed frequencies of an event. The test requires specifying how many groups/bins (\\(g\\)) to use to calculate quantiles. Smaller \\(\\chi^2\\) values (and larger p-values) reflect greater calibration accuracy. A statistically significant \\(\\chi^2\\) (p &lt; .05) indicates a significant degree of miscalibration. The Hosler-Lemeshow GOF test was calculated using the ResourceSelection package (Lele et al., 2019). The table of predicted versus observed frequencies was generated using the PredictABEL package (Kundu et al., 2020). For each number of bins (\\(g\\)) specified below, the predictions show statistically significant miscalibration. \\(g = 2\\) Code gValue &lt;- 2 hoslem.test( mydataSDT$disorder, mydataSDT$predictedProbability, g = gValue) Hosmer and Lemeshow goodness of fit (GOF) test data: mydataSDT$disorder, mydataSDT$predictedProbability X-squared = NA, df = 0, p-value = NA Code plotCalibration( data = na.omit(myDataNoMissing), cOutcome = colNumberOutcome, predRisk = myDataNoMissing$predictedProbability, groups = gValue)$Table_HLtest total meanpred meanobs predicted observed [0.0000,0.0588) 57 0.029 0.228 1.64 13 [0.0588,1.0000] 53 0.194 0.509 10.29 27 \\(g = 4\\) Code gValue &lt;- 4 hoslem.test( mydataSDT$disorder, mydataSDT$predictedProbability, g = gValue) Hosmer and Lemeshow goodness of fit (GOF) test data: mydataSDT$disorder, mydataSDT$predictedProbability X-squared = NA, df = 2, p-value = NA Code plotCalibration( data = na.omit(myDataNoMissing), cOutcome = colNumberOutcome, predRisk = myDataNoMissing$predictedProbability, groups = gValue)$Table_HLtest total meanpred meanobs predicted observed [0.0000,0.0343) 34 0.019 0.206 0.66 7 [0.0343,0.0588) 23 0.043 0.261 0.98 6 [0.0588,0.1569) 26 0.095 0.346 2.48 9 [0.1569,1.0000] 27 0.289 0.667 7.81 18 \\(g = 6\\) Code gValue &lt;- 6 hoslem.test( mydataSDT$disorder, mydataSDT$predictedProbability, g = gValue) Hosmer and Lemeshow goodness of fit (GOF) test data: mydataSDT$disorder, mydataSDT$predictedProbability X-squared = NA, df = 4, p-value = NA Code plotCalibration( data = na.omit(myDataNoMissing), cOutcome = colNumberOutcome, predRisk = myDataNoMissing$predictedProbability, groups = gValue)$Table_HLtest total meanpred meanobs predicted observed [0.0000,0.0245) 19 0.013 0.211 0.25 4 [0.0245,0.0392) 23 0.030 0.217 0.68 5 [0.0392,0.0588) 15 0.047 0.267 0.71 4 [0.0588,0.1127) 17 0.074 0.235 1.26 4 [0.1127,0.2206) 19 0.167 0.474 3.18 9 [0.2206,1.0000] 17 0.344 0.824 5.85 14 \\(g = 8\\) Code gValue &lt;- 8 hoslem.test( mydataSDT$disorder, mydataSDT$predictedProbability, g = gValue) Hosmer and Lemeshow goodness of fit (GOF) test data: mydataSDT$disorder, mydataSDT$predictedProbability X-squared = NA, df = 6, p-value = NA Code plotCalibration( data = na.omit(myDataNoMissing), cOutcome = colNumberOutcome, predRisk = myDataNoMissing$predictedProbability, groups = gValue)$Table_HLtest total meanpred meanobs predicted observed [0.0000,0.0245) 19 0.013 0.211 0.25 4 [0.0245,0.0343) 15 0.027 0.200 0.41 3 0.0343 8 0.034 0.250 0.27 2 [0.0392,0.0588) 15 0.047 0.267 0.71 4 [0.0588,0.0931) 13 0.066 0.077 0.86 1 [0.0931,0.1569) 13 0.124 0.615 1.62 8 [0.1569,0.2402) 15 0.206 0.400 3.09 6 [0.2402,1.0000] 12 0.394 1.000 4.73 12 \\(g = 10\\) Code gValue &lt;- 10 hoslem.test( mydataSDT$disorder, mydataSDT$predictedProbability, g = gValue) Hosmer and Lemeshow goodness of fit (GOF) test data: mydataSDT$disorder, mydataSDT$predictedProbability X-squared = NA, df = 8, p-value = NA Code plotCalibration( data = na.omit(myDataNoMissing), cOutcome = colNumberOutcome, predRisk = myDataNoMissing$predictedProbability, groups = gValue)$Table_HLtest total meanpred meanobs predicted observed [0.0000,0.0245) 19 0.013 0.211 0.25 4 0.0245 7 0.025 0.143 0.17 1 0.0294 8 0.029 0.250 0.24 2 [0.0343,0.0441) 13 0.036 0.231 0.47 3 [0.0441,0.0588) 10 0.051 0.300 0.51 3 [0.0588,0.0735) 10 0.063 0.100 0.63 1 [0.0735,0.1324) 10 0.099 0.500 0.99 5 [0.1324,0.2059) 12 0.165 0.583 1.98 7 [0.2059,0.2598) 10 0.222 0.300 2.22 3 [0.2598,1.0000] 11 0.408 1.000 4.49 11 9.4.3.4 Spiegelhalter’s z Spiegelhalter’s \\(z\\) was calculated using the rms package (Harrell, Jr., 2021). Smaller \\(z\\) values (and larger associated \\(p\\)-values) reflect greater calibration accuracy. A statistically significant Spiegelhalter’s \\(z\\) (p &lt; .05) indicates a significant degree of miscalibration. In this case, the predictions show statistically significant miscalibration. Code val.prob( mydataSDT$predictedProbability, mydataSDT$disorder, pl = FALSE)[&quot;S:z&quot;] S:z 10.52663 Code val.prob( mydataSDT$predictedProbability, mydataSDT$disorder, pl = FALSE)[&quot;S:p&quot;] S:p 6.512514e-26 9.4.3.5 Calibration for predicting a continuous outcome When predicting a continuous outcome, calibration of the predicted values in relation to the outcome values can be examined in multiple ways including: in a calibration plot, the extent to which the intercept is near zero and the slope is near one in a calibration plot, the extent to which the 95% confidence interval of the observed value, across all values of the predicted values, includes the diagonal reference line with an intercept of zero and a slope of one. mean error mean absolute error mean squared error root mean squared error With a plot of the predictions on the x-axis, and the outcomes on the y-axis (i.e., a calibration plot), calibration can be examined graphically as the extent to which the best-fit regression line has an intercept (alpha) close to zero and a slope (beta) close to one (Stevens &amp; Poppe, 2020; Steyerberg &amp; Vergouwe, 2014). The intercept is also called “calibration-in-the-large”, whereas “calibration-in-the-small” refers to the extent to which the predicted values match the observed values at a specific predicted value (e.g., when the weather forecaster says that there is a 10% chance of rain, does it actually rain 10% of the time?). For predictions to be well calibrated, the intercept should be close to zero and the slope should be close to one. If the slope is close to one but the intercept is not close to zero (or the intercept is close to zero but the slope is not close to one), the predictions would not be considered well calibrated. The 95% confidence interval of the observed value, across all values of the predicted values, should include the diagonal reference line whose intercept is zero and whose slope is one. For instance, based on the intercept and slope of the calibration plot in Figure 9.27, the predictions are not well calibrated, despite having a slope near one, because the 95% confidence interval of the intercept does not include zero. The best-fit line is the yellow line. The intercept from the best-fit line is positive, as shown in the regression equation. This is a case of underprediction, where the predicted values are consistently less than the observed values. The confidence interval of the observed value (i.e., the purple band) is the interval within which we have 95% confidence that the true observed value would lie for a given predicted value, based on the model. The 95% prediction interval of the observed value (i.e., the dashed red lines) is the interval within which we would expect that 95% of future observations would lie for a given predicted value. The black diagonal line indicates the reference line with an intercept of zero and a slope of one. The predictions would be significantly miscalibrated at a given level of the predicted values if the 95% confidence interval of the observed value does not include the reference line at that level of the predicted value. In this case, the 95% confidence interval of the observed value does not include the reference line (i.e., the actual observed value) at lower levels of the predicted values, so the predictions are miscalibrated lower levels of the predicted values. Code #95 prediction interval based on linear model calibrationModel &lt;- lm( continuousOutcome ~ testScore, data = mydataSDT) calibrationModelPredictionInterval &lt;- expand.grid( testScore = seq(from = min(mydataSDT$testScore, na.rm = TRUE), to = max(mydataSDT$testScore, na.rm = TRUE), length.out = 1000)) calibrationModelPredictionInterval &lt;- cbind( calibrationModelPredictionInterval, data.frame(predict( calibrationModel, newdata = calibrationModelPredictionInterval, interval = &quot;predict&quot;, level = 0.95))) ggplot( mydataSDT, aes( x = testScore, y = continuousOutcome)) + geom_point() + geom_line( data = calibrationModelPredictionInterval, aes(y = lwr), color = &quot;red&quot;, linetype = &quot;dashed&quot;) + #Lower estimate of 95% prediction interval of linear model geom_line( data = calibrationModelPredictionInterval, aes(y = upr), color = &quot;red&quot;, linetype = &quot;dashed&quot;) + #Upper estimate of 95% prediction interval of linear model #geom_ribbon(data = calibrationModelPredictionInterval, aes(y = fit, ymin = lwr, ymax = upr), fill = viridis(3)[1], alpha = 0.7) + #95% prediction interval of linear model geom_smooth( method = &quot;lm&quot;, color = viridis(3)[3], fill = viridis(3)[1], alpha = 0.7) + #95% confidence interval of linear model geom_abline( slope = 1, intercept = 0) + xlim(0,2.4) + ylim(0,2.4) + xlab(&quot;Predicted Value&quot;) + ylab(&quot;Observed Value&quot;) + stat_cor( label.y = 2.2, aes(label = paste(..rr.label..))) + stat_regline_equation(label.y = 2.0) + theme_bw() Figure 9.27: Calibration Plot for Predictions of a Continuous Outcome, With Best-Fit Line. The black diagonal line indicates the reference line with an intercept of zero and a slope of one. The yellow line is the best-fit line. The purple band is the 95% confidence interval of the observed value. The dashed red lines are the 95% prediction interval of the observed value. The predictions are not well calibrated because the 95% confidence interval of the intercept does not include zero (even though the 95% confidence interval of the slope includes one). The intercept from the best-fit line is positive, as shown in the regression equation. This is a case of underprediction, where the predicted values are consistently less than the observed values. The 95% confidence interval of the observed value does not include the reference line (i.e., the actual observed value) at lower levels of the predicted values, so the predictions are miscalibrated at lower levels of the predicted values. Gold-standard recommendations include examining the predicted values in relation to the observed values using locally estimated scatterplot smoothing (LOESS) (Austin &amp; Steyerberg, 2014), such as in Figure 9.28. We can examine whether the LOESS-based 95% confidence interval of the observed value at every level of the predicted values includes the diagonal reference line (i.e., the actual observed value). In this case, the 95% confidence interval of the observed value does not include the reference line at lower levels of the predicted values, so the predictions are miscalibrated at lower levels of the predicted values. Code #95 prediction interval based on LOESS model calibrationLoessModel &lt;- loess.sd( x = mydataSDT$testScore, y = mydataSDT$continuousOutcome, nsigma = qnorm(.975), na.action = &quot;na.exclude&quot;) calibrationLoessPredictionInterval &lt;- data.frame( x = calibrationLoessModel$x, y = calibrationLoessModel$y, lower = calibrationLoessModel$lower, upper = calibrationLoessModel$upper) ggplot( mydataSDT, aes( x = testScore, y = continuousOutcome)) + geom_point() + geom_line( data = calibrationLoessPredictionInterval, aes(x = x, y = lower), color = &quot;red&quot;, linetype = &quot;dashed&quot;) + #Lower estimate of 95% prediction interval of linear model geom_line( data = calibrationLoessPredictionInterval, aes(x = x, y = upper), color = &quot;red&quot;, linetype = &quot;dashed&quot;) + #Upper estimate of 95% prediction interval of linear model #geom_ribbon(data = calibrationLoessPredictionInterval, aes(x = x, y = y, ymin = lower, ymax = upper), fill = viridis(3)[1], alpha = 0.7) + #95% prediction interval of linear model geom_smooth( method = &quot;loess&quot;, color = viridis(3)[3], fill = viridis(3)[1], alpha = 0.7) + #95% confidence interval of LOESS model geom_abline( slope = 1, intercept = 0) + xlim(0,2.4) + ylim(0,2.4) + xlab(&quot;Predicted Value&quot;) + ylab(&quot;Observed Value&quot;) + theme_bw() Figure 9.28: Calibration Plot for Predictions of a Continuous Outcome, With LOESS Best-Fit Line. The yellow line is the best-fit line based on LOESS. The purple band is the 95% confidence interval of the observed value. The dashed red lines are the 95% prediction interval of the observed value. The 95% confidence interval of the observed value does not include the reference line (i.e., the actual observed value) at lower levels of the predicted values, so the predictions are miscalibrated at lower levels of the predicted values. 9.5 Prediction Accuracy at a Given Cutoff 9.5.1 Set a Cutoff Here, I set a cutoff at the Youden’s J Statistic to calculate the accuracy statistics at that cutoff: Code cutoff &lt;- 0.205 mydataSDT$diagnosis &lt;- NA mydataSDT$diagnosis[mydataSDT$testScore &lt; cutoff] &lt;- 0 mydataSDT$diagnosis[mydataSDT$testScore &gt;= cutoff] &lt;- 1 mydataSDT$diagnosisFactor &lt;- factor( mydataSDT$diagnosis, levels = c(1, 0), labels = c(&quot;Decision: Diagnosis&quot;, &quot;Decision: No Diagnosis&quot;)) mydataSDT$disorderFactor &lt;- factor( mydataSDT$disorder, levels = c(1, 0), labels = c(&quot;Truth: Disorder&quot;, &quot;Truth: No Disorder&quot;)) 9.5.2 Accuracy at a Given Cutoff The petersenlab package (Petersen, 2024b) contains the accuracyAtCutoff() function that estimates the prediction accuracy at a given cutoff. Code accuracyAtCutoff( predicted = mydataSDT$testScore, actual = mydataSDT$disorder, cutoff = cutoff) %&gt;% t %&gt;% round(., 2) [,1] cutoff 0.20 TP 26.00 TN 56.00 FP 14.00 FN 14.00 SR 0.36 BR 0.36 percentAccuracy 74.55 percentAccuracyByChance 53.72 percentAccuracyPredictingFromBaseRate 63.64 RIOC 0.45 relativeImprovementOverPredictingFromBaseRate 0.15 SN 0.65 SP 0.80 TPrate 0.65 TNrate 0.80 FNrate 0.35 FPrate 0.20 HR 0.65 FAR 0.20 PPV 0.65 NPV 0.80 FDR 0.35 FOR 0.20 youdenJ 0.45 balancedAccuracy 0.73 f1Score 0.65 mcc 0.45 diagnosticOddsRatio 7.43 positiveLikelihoodRatio 3.25 negativeLikelihoodRatio 0.44 dPrimeSDT 1.23 betaSDT 1.32 cSDT 0.23 aSDT 0.79 bSDT 1.33 differenceBetweenPredictedAndObserved -0.27 informationGain 0.15 There are also test calculators available online: http://araw.mede.uic.edu/cgi-bin/testcalc.pl https://dlrs.shinyapps.io/shinyDLRs/ 9.5.3 Confusion Matrix aka 2x2 Accuracy Table aka Cross-Tabulation aka Contingency Table A confusion matrix (aka 2x2 accuracy table, cross-tabulation table, or contigency table) is a matrix for categorical data that presents the predicted outcome on one dimension and the actual outcome (truth) on the other dimension. If the predictions and outcomes are dichotomous, the confusion matrix is a 2x2 matrix with two rows and two columns that represent four possible predicted-actual combinations (decision outcomes). In such a case, the confusion matrix provides a tabular count of each type of accurate cases (true positives and true negatives) versus the number of each type of error (false positives and false negatives), as shown in Figure 9.29. An example of a confusion matrix is in Figure 9.4. Figure 9.29: Confusion Matrix. 9.5.3.1 Number Code table(mydataSDT$diagnosisFactor, mydataSDT$disorderFactor) Truth: Disorder Truth: No Disorder Decision: Diagnosis 26 14 Decision: No Diagnosis 14 56 9.5.3.2 Number with margins added Code addmargins(table(mydataSDT$diagnosisFactor, mydataSDT$disorderFactor)) Truth: Disorder Truth: No Disorder Sum Decision: Diagnosis 26 14 40 Decision: No Diagnosis 14 56 70 Sum 40 70 110 9.5.3.3 Proportions Code prop.table(table(mydataSDT$diagnosisFactor, mydataSDT$disorderFactor)) Truth: Disorder Truth: No Disorder Decision: Diagnosis 0.2363636 0.1272727 Decision: No Diagnosis 0.1272727 0.5090909 9.5.3.4 Proportions with margins added Code addmargins(prop.table(table(mydataSDT$diagnosisFactor, mydataSDT$disorderFactor))) Truth: Disorder Truth: No Disorder Sum Decision: Diagnosis 0.2363636 0.1272727 0.3636364 Decision: No Diagnosis 0.1272727 0.5090909 0.6363636 Sum 0.3636364 0.6363636 1.0000000 9.5.4 True Positives (TP) True positives (TPs) are instances in which a positive classification (e.g., a disorder present) is correct—that is, the test says that a classification is present, and the classification is present. True positives are also called valid positives (VPs) or hits. Higher values (relative to the same sample size) reflect greater accuracy. The formula for true positives is in Equation (9.34): \\[ \\begin{aligned} \\text{TP} &amp;= \\text{BR} \\times \\text{SR} \\times N \\end{aligned} \\tag{9.34} \\] Code TPvalue &lt;- length(which( mydataSDT$diagnosis == 1 &amp; mydataSDT$disorder == 1)) TPvalue [1] 26 9.5.5 True Negatives (TN) True negatives (TNs) are instances in which a negative classification (e.g., absence of a disorder) is correct—that is, the test says that a classification is not present, and the classification is actually not present. True negatives are also called valid negatives (VNs) or correct rejections. Higher values (relative to the same sample size) reflect greater accuracy. The formula for true negatives is in Equation (9.35): \\[ \\begin{aligned} \\text{TN} &amp;= (1 - \\text{BR}) \\times (1 - \\text{SR}) \\times N \\end{aligned} \\tag{9.35} \\] Code TNvalue &lt;- length(which( mydataSDT$diagnosis == 0 &amp; mydataSDT$disorder == 0)) TNvalue [1] 56 9.5.6 False Positives (FP) False positives (FPs) are instances in which a positive classification (e.g., a disorder present) is incorrect—that is, the test says that a classification is present, and the classification is not present. False positives are also called false alarms (FAs). Lower values (relative to the same sample size) reflect greater accuracy. The formula for false positives is in Equation (9.36): \\[ \\begin{aligned} \\text{FP} &amp;= (1 - \\text{BR}) \\times \\text{SR} \\times N \\end{aligned} \\tag{9.36} \\] Code FPvalue &lt;- length(which( mydataSDT$diagnosis == 1 &amp; mydataSDT$disorder == 0)) FPvalue [1] 14 9.5.7 False Negatives (FN) False negatives (FNs) are instances in which a negative classification (e.g., absence of a disorder) is incorrect—that is, the test says that a classification is not present, and the classification is present. False negatives are also called misses. Lower values (relative to the same sample size) reflect greater accuracy. The formula for false negatives is in Equation (9.37): \\[ \\begin{aligned} \\text{FN} &amp;= \\text{BR} \\times (1 - \\text{SR}) \\times N \\end{aligned} \\tag{9.37} \\] Code FNvalue &lt;- length(which( mydataSDT$diagnosis == 0 &amp; mydataSDT$disorder == 1)) FNvalue [1] 14 9.5.8 Sample Size (N) Code sampleSize &lt;- function(TP, TN, FP, FN){ value &lt;- TP + TN + FP + FN return(value) } Code sampleSize( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue) [1] 110 9.5.9 Selection Ratio (SR) The selection ratio (SR) is the marginal probability of selection, independent of other things: \\(P(R_i)\\). In clinical psychology, the selection ratio is the proportion of people who test positive for the disorder, as in Equation (9.38): \\[ \\begin{aligned} \\text{SR} &amp;= P(R_i) \\\\ &amp;= \\frac{\\text{TP} + \\text{FP}}{N} \\end{aligned} \\tag{9.38} \\] Code selectionRatio &lt;- function(TP, TN, FP, FN){ N &lt;- TP + TN + FP + FN value &lt;- (TP + FP)/N return(value) } Code selectionRatio( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue) [1] 0.3636364 9.5.10 Base Rate (BR) The base rate (BR) of a classification is its marginal probability, independent of other things: \\(P(C_i)\\). In clinical psychology, the base rate of a disorder is its prevalence in the population, as in Equation (9.39). Without additional information, the base rate is used as the initial pretest probability. \\[ \\begin{aligned} \\text{BR} &amp;= P(C_i) \\\\ &amp;= \\frac{\\text{TP} + \\text{FN}}{N} \\end{aligned} \\tag{9.39} \\] Code baseRate &lt;- function(TP, TN, FP, FN){ N &lt;- TP + TN + FP + FN value &lt;- (TP + FN)/N return(value) } Code baseRate( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue) [1] 0.3636364 9.5.11 Pretest Odds The pretest odds of a classification can be estimated using the pretest probability (i.e., base rate). To convert a probability to odds, divide the probability by one minus that probability, as in Equation (9.40). \\[ \\begin{aligned} \\text{pretest odds} &amp;= \\frac{\\text{pretest probability}}{1 - \\text{pretest probability}} \\\\ \\end{aligned} \\tag{9.40} \\] Code pretestOdds &lt;- function(TP, TN, FP, FN, pretestProb = NULL){ if(!is.null(pretestProb)){ pretestProbability &lt;- pretestProb } else { N &lt;- TP + TN + FP + FN pretestProbability &lt;- (TP + FN)/N } value &lt;- pretestProbability / (1 - pretestProbability) return(value) } Code pretestOdds( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue) [1] 0.5714286 Code pretestOdds(pretestProb = baseRate( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue)) [1] 0.5714286 9.5.12 Percent Accuracy Percent Accuracy is also called overall accuracy. Higher values reflect greater accuracy. The formula for percent accuracy is in Equation (9.41). Percent accuracy has several problems. First, it treats all errors (FP and FN) as equally important. However, in practice, it is rarely the case that false positives and false negatives are equally important. Second, percent accuracy can be misleading because it is highly influenced by base rates. You can have a high percent accuracy by predicting from the base rate and saying that no one has the characteristic (if the base rate is low) or that everyone has the characteristic (if the base rate is high). Thus, it is also important to consider other aspects of accuracy. \\[\\begin{equation} \\text{Percent Accuracy} = 100\\% \\times \\frac{\\text{TP} + \\text{TN}}{N} \\tag{9.41} \\end{equation}\\] Code percentAccuracy &lt;- function(TP, TN, FP, FN){ N &lt;- TP + TN + FP + FN value &lt;- 100 * ((TP + TN)/N) return(value) } Code percentAccuracy( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue) [1] 74.54545 9.5.13 Percent Accuracy by Chance The formula for calculating percent accuracy by chance is in Equation (9.42). \\[ \\begin{aligned} \\text{Percent Accuracy by Chance} &amp;= 100\\% \\times [P(\\text{TP}) + P(\\text{TN})] \\\\ &amp;= 100\\% \\times \\{(\\text{BR} \\times {\\text{SR}}) + [(1 - \\text{BR}) \\times (1 - \\text{SR})]\\} \\end{aligned} \\tag{9.42} \\] Code percentAccuracyByChance &lt;- function(TP, TN, FP, FN){ N &lt;- TP + TN + FP + FN BR &lt;- (TP + FN)/N SR &lt;- (TP + FP)/N value &lt;- 100 * ((BR * SR) + ((1 - BR) * (1 - SR))) return(value) } Code percentAccuracyByChance( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue) [1] 53.71901 9.5.14 Percent Accuracy Predicting from the Base Rate Predicting from the base rate is going with the most likely outcome in every prediction. It is also called “betting from the base rate”. If the base rate is less than .50, it would involve predicting that the condition is absent for every case. If the base rate is .50 or above, it would involve predicting that the condition is present for every case. Predicting from the base rate is a special case of percent accuracy by chance when the selection ratio is set to either one (if the base rate \\(\\geq\\) .5) or zero (if the base rate &lt; .5). Code percentAccuracyPredictingFromBaseRate &lt;- function(TP, TN, FP, FN){ N &lt;- TP + TN + FP + FN BR &lt;- (TP + FN)/N ifelse(BR &gt;= .5, SR &lt;- 1, NA) ifelse(BR &lt; .5, SR &lt;- 0, NA) value &lt;- 100 * ((BR * SR) + ((1 - BR) * (1 - SR))) return(value) } Code percentAccuracyPredictingFromBaseRate( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue) [1] 63.63636 9.5.15 Relative Improvement Over Chance (RIOC) Relative improvement over chance (RIOC) is a prediction’s improvement over chance as a proportion of the maximum possible improvement over chance, as described by Farrington &amp; Loeber (1989). Higher values reflect greater accuracy. The formula for calculating RIOC is in Equation (9.43). \\[ \\begin{aligned} \\text{relative improvement over chance (RIOC)} &amp;= \\frac{\\text{total correct} - \\text{chance correct}}{\\text{maximum correct} - \\text{chance correct}} \\\\ \\end{aligned} \\tag{9.43} \\] Code relativeImprovementOverChance &lt;- function(TP, TN, FP, FN){ N &lt;- TP + TN + FP + FN actualYes &lt;- TP + FN predictedYes &lt;- TP + FP value &lt;- ((N * (TP + TN)) - (actualYes * predictedYes + (N - predictedYes) * (N - actualYes))) / ((N * (actualYes + N - predictedYes)) - (actualYes * predictedYes + (N - predictedYes) * (N - actualYes))) return(value) } Code relativeImprovementOverChance( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue) [1] 0.45 9.5.16 Relative Improvement Over Predicting from the Base Rate Relative improvement over predicting from the base rate is a prediction’s improvement over predicting from the base rate as a proportion of the maximum possible improvement over predicting from the base rate. Higher values reflect greater accuracy. The formula for calculating relative improvement over predicting from the base rate is in Equation (9.44). \\[ \\scriptsize \\begin{aligned} \\text{relative improvement over predicting from base rate} &amp;= \\frac{\\text{total correct} - \\text{correct by predicting from base rate}}{\\text{maximum correct} - \\text{correct by predicting from base rate}} \\\\ \\end{aligned} \\tag{9.44} \\] Code relativeImprovementOverPredictingFromBaseRate &lt;- function(TP, TN, FP, FN){ N &lt;- TP + TN + FP + FN BR &lt;- (TP + FN)/N ifelse(BR &gt;= .5, SR &lt;- 1, NA) ifelse(BR &lt; .5, SR &lt;- 0, NA) actualYes &lt;- TP + FN predictedYes &lt;- SR * N value &lt;- ((N * (TP + TN)) - (actualYes * predictedYes + (N - predictedYes) * (N - actualYes))) / ((N * (actualYes + N - predictedYes)) - (actualYes * predictedYes + (N - predictedYes) * (N - actualYes))) return(value) } Code relativeImprovementOverPredictingFromBaseRate( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue) [1] 0.15 9.5.17 Sensitivity (SN) Sensitivity (SN) is also called true positive rate (TPR), hit rate (HR), or recall. Sensitivity is the conditional probability of a positive test given that the person has the condition: \\(P(R|C)\\). Higher values reflect greater accuracy. The formula for calculating sensitivity is in Equation (9.45). As described in Section 9.1.2.6.1 and as depicted in Figure 9.30, as the cutoff increases (becomes more conservative), sensitivity decreases. As the cutoff decreases, sensitivity increases. \\[ \\begin{aligned} \\text{sensitivity (SN)} &amp;= P(R|C) \\\\ &amp;= \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{\\text{TP}}{N \\times \\text{BR}} = 1 - \\text{FNR} \\end{aligned} \\tag{9.45} \\] Code sensitivity &lt;- function(TP, TN, FP, FN){ value &lt;- TP/(TP + FN) return(value) } Code sensitivity( TP = TPvalue, FN = FNvalue) [1] 0.65 Below I compute sensitivity and specificity at every possible cutoff. Code possibleCutoffs &lt;- unique(na.omit(mydataSDT$testScore)) possibleCutoffs &lt;- possibleCutoffs[order(possibleCutoffs)] possibleCutoffs &lt;- c( possibleCutoffs, max(possibleCutoffs, na.rm = TRUE) + 0.01) specificity &lt;- function(TP, TN, FP, FN){ value &lt;- TN/(TN + FP) return(value) } accuracyVariables &lt;- c(&quot;cutoff&quot;, &quot;TP&quot;, &quot;TN&quot;, &quot;FP&quot;, &quot;FN&quot;) accuracyStats &lt;- data.frame(matrix( nrow = length(possibleCutoffs), ncol = length(accuracyVariables))) names(accuracyStats) &lt;- accuracyVariables for(i in 1:length(possibleCutoffs)){ newCutoff &lt;- possibleCutoffs[i] mydataSDT$diagnosis &lt;- NA mydataSDT$diagnosis[mydataSDT$testScore &lt; newCutoff] &lt;- 0 mydataSDT$diagnosis[mydataSDT$testScore &gt;= newCutoff] &lt;- 1 accuracyStats[i, &quot;cutoff&quot;] &lt;- newCutoff accuracyStats[i, &quot;TP&quot;] &lt;- length(which( mydataSDT$diagnosis == 1 &amp; mydataSDT$disorder == 1)) accuracyStats[i, &quot;TN&quot;] &lt;- length(which( mydataSDT$diagnosis == 0 &amp; mydataSDT$disorder == 0)) accuracyStats[i, &quot;FP&quot;] &lt;- length(which( mydataSDT$diagnosis == 1 &amp; mydataSDT$disorder == 0)) accuracyStats[i, &quot;FN&quot;] &lt;- length(which( mydataSDT$diagnosis == 0 &amp; mydataSDT$disorder == 1)) } accuracyStats$sensitivity &lt;- accuracyStats$TPrate &lt;- sensitivity( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$specificity &lt;- accuracyStats$TNrate &lt;- specificity( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) sensitivitySpecificityData &lt;- pivot_longer( accuracyStats, cols = all_of(c(&quot;sensitivity&quot;,&quot;specificity&quot;))) Figure 9.30: Sensitivity and Specificity as a Function of the Cutoff. 9.5.18 Specificity (SP) Specificity (SP) is also called true negative rate (TNR) or selectivity. Specificity is the conditional probability of a negative test given that the person does not have the condition: \\(P(\\text{not } R|\\text{not } C)\\). Higher values reflect greater accuracy. The formula for calculating specificity is in Equation (9.46). As described in Section 9.1.2.6.1 and as depicted in Figure 9.30, as the cutoff increases (becomes more conservative), specificity increases. As the cutoff decreases, specificity decreases. \\[ \\begin{aligned} \\text{specificity (SP)} &amp;= P(\\text{not } R|\\text{not } C) \\\\ &amp;= \\frac{\\text{TN}}{\\text{TN} + \\text{FP}} = \\frac{\\text{TN}}{N (1 - \\text{BR})} = 1 - \\text{FPR} \\end{aligned} \\tag{9.46} \\] Code specificity &lt;- function(TP, TN, FP, FN){ value &lt;- TN/(TN + FP) return(value) } Code specificity(TN = TNvalue, FP = FPvalue) [1] 0.8 9.5.19 False Negative Rate (FNR) The false negative rate (FNR) is also called the miss rate. The false negative rate is the conditional probability of a negative test given that the person has the condition: \\(P(\\text{not } R|C)\\). Lower values reflect greater accuracy. The formula for calculating false negative rate is in Equation (9.47). \\[ \\begin{aligned} \\text{false negative rate (FNR)} &amp;= P(\\text{not } R|C) \\\\ &amp;= \\frac{\\text{FN}}{\\text{FN} + \\text{TP}} = \\frac{\\text{FN}}{N \\times \\text{BR}} = 1 - \\text{TPR} \\end{aligned} \\tag{9.47} \\] Code falseNegativeRate &lt;- function(TP, TN, FP, FN){ value &lt;- FN/(FN + TP) return(value) } Code falseNegativeRate( TP = TPvalue, FN = FNvalue) [1] 0.35 9.5.20 False Positive Rate (FPR) The false positive rate (FPR) is also called the false alarm rate (FAR) or fall-out. The false positive rate is the conditional probability of a positive test given that the person does not have the condition: \\(P(R|\\text{not } C)\\). Lower values reflect greater accuracy. The formula for calculating false positive rate is in Equation (9.48). \\[ \\begin{aligned} \\text{false positive rate (FPR)} &amp;= P(R|\\text{not } C) \\\\ &amp;= \\frac{\\text{FP}}{\\text{FP} + \\text{TN}} = \\frac{\\text{FP}}{N (1 - \\text{BR})} = 1 - \\text{TNR} \\end{aligned} \\tag{9.48} \\] Code falsePositiveRate &lt;- function(TP, TN, FP, FN){ value &lt;- FP/(FP + TN) return(value) } Code falsePositiveRate( TN = TNvalue, FP = FPvalue) [1] 0.2 9.5.21 Positive Predictive Value (PPV) The positive predictive value (PPV) is also called the positive predictive power (PPP) or precision. Many people confuse sensitivity (\\(P(R|C)\\)) with its inverse conditional probability, PPV (\\(P(C|R)\\)). PPV is the conditional probability of having the condition given a positive test: \\(P(C|R)\\). Higher values reflect greater accuracy. The formula for calculating positive predictive value is in Equation (9.49). PPV can be low even when sensitivity is high because it depends not only on sensitivity, but also on specificity and the base rate. Because PPV depends on the base rate, PPV is not an intrinsic property of a measure. The same measure will have a different PPV in different contexts with different base rates (Treat &amp; Viken, 2023). As described in Section 9.1.2.6.1 and as depicted in Figure 9.31, as the base rate increases, PPV increases. As the base rate decreases, PPV decreases. PPV also differs as a function of the cutoff. As described in Section 9.1.2.6.1 and as depicted in Figure 9.32, as the cutoff increases (becomes more conservative), PPV increases. As the cutoff decreases (becomes more liberal), PPV decreases. \\[ \\small \\begin{aligned} \\text{positive predictive value (PPV)} &amp;= P(C|R) \\\\ &amp;= \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{\\text{TP}}{N \\times \\text{SR}}\\\\ &amp;= \\frac{\\text{sensitivity} \\times {\\text{BR}}}{\\text{sensitivity} \\times {\\text{BR}} + [(1 - \\text{specificity}) \\times (1 - \\text{BR})]} \\end{aligned} \\tag{9.49} \\] Code positivePredictiveValue &lt;- function(TP, TN, FP, FN, BR = NULL, SN, SP){ if(is.null(BR)){ value &lt;- TP/(TP + FP) } else{ value &lt;- (SN * BR)/(SN * BR + (1 - SP) * (1 - BR)) } return(value) } Code positivePredictiveValue( TP = TPvalue, FP = FPvalue) [1] 0.65 Code positivePredictiveValue( BR = baseRate( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue), SN = sensitivity( TP = TPvalue, FN = FNvalue), SP = specificity( TN = TNvalue, FP = FPvalue)) [1] 0.65 Below I compute PPV and NPV at every possible base rate given the sensitivity and specificity at the current cutoff. Code negativePredictiveValue &lt;- function(TP, TN, FP, FN, BR = NULL, SN, SP){ if(is.null(BR)){ value &lt;- TN/(TN + FN) } else{ value &lt;- (SP * (1 - BR))/(SP * (1 - BR) + (1 - SN) * BR) } return(value) } ppvNPVbaseRateData &lt;- data.frame( BR = seq(from = 0, to = 1, by = .01), SN = sensitivity( TP = TPvalue, FN = FNvalue), SP = specificity( TN = TNvalue, FP = FPvalue)) ppvNPVbaseRateData$positivePredictiveValue &lt;- positivePredictiveValue( BR = ppvNPVbaseRateData$BR, SN = ppvNPVbaseRateData$SN, SP = ppvNPVbaseRateData$SP) ppvNPVbaseRateData$negativePredictiveValue &lt;- negativePredictiveValue( BR = ppvNPVbaseRateData$BR, SN = ppvNPVbaseRateData$SN, SP = ppvNPVbaseRateData$SP) ppvNPVbaseRateData_long &lt;- pivot_longer( ppvNPVbaseRateData, cols = all_of(c( &quot;positivePredictiveValue&quot;,&quot;negativePredictiveValue&quot;))) Figure 9.31: Positive Predictive Value and Negative Predictive Value as a Function of the Base Rate. Below I compute PPV and NPV at every possible cutoff. Code accuracyStats$positivePredictiveValue &lt;- positivePredictiveValue( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$negativePredictiveValue &lt;- negativePredictiveValue( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) ppvNPVcutoffData &lt;- pivot_longer( accuracyStats, cols = all_of(c( &quot;positivePredictiveValue&quot;,&quot;negativePredictiveValue&quot;))) Figure 9.32: Positive Predictive Value and Negative Predictive Value as a Function of the Cutoff. 9.5.22 Negative Predictive Value (NPV) The negative predictive value (NPV) is also called the negative predictive power (NPP). Many people confuse specificity (\\(P(\\text{not } R|\\text{not } C)\\)) with its inverse conditional probability, NPV (\\(P(\\text{not } C| \\text{not } R)\\)). NPV is the conditional probability of not having the condition given a negative test: \\(P(\\text{not } C| \\text{not } R)\\). Higher values reflect greater accuracy. The formula for calculating negative predictive value is in Equation (9.50). NPV can be low even when specificity is high because it depends not only on specificity, but also on sensitivity and the base rate. Because NPV depends on the base rate, NPV is not an intrinsic property of a measure. The same measure will have a different NPV in different contexts with different base rates (Treat &amp; Viken, 2023). As described in Section 9.1.2.6.1 and as depicted in Figure 9.31, as the base rate increases, NPV decreases. As the base rate decreases, NPV increases. NPV also differs as a function of the cutoff. As described in Section 9.1.2.6.1 and as depicted in Figure 9.32, as the cutoff increases (becomes more conservative), NPV decreases. As the cutoff decreases (becomes more liberal), NPV decreases. \\[ \\small \\begin{aligned} \\text{negative predictive value (NPV)} &amp;= P(\\text{not } C|\\text{not } R) \\\\ &amp;= \\frac{\\text{TN}}{\\text{TN} + \\text{FN}} = \\frac{\\text{TN}}{N(\\text{1 - SR})}\\\\ &amp;= \\frac{\\text{specificity} \\times (1-{\\text{BR}})}{\\text{specificity} \\times (1-{\\text{BR}}) + [(1 - \\text{sensitivity}) \\times \\text{BR})]} \\end{aligned} \\tag{9.50} \\] Code negativePredictiveValue &lt;- function(TP, TN, FP, FN, BR = NULL, SN, SP){ if(is.null(BR)){ value &lt;- TN/(TN + FN) } else{ value &lt;- (SP * (1 - BR))/(SP * (1 - BR) + (1 - SN) * BR) } return(value) } Code negativePredictiveValue( TN = TNvalue, FN = FNvalue) [1] 0.8 Code negativePredictiveValue( BR = baseRate( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue), SN = sensitivity( TP = TPvalue, FN = FNvalue), SP = specificity( TN = TNvalue, FP = FPvalue)) [1] 0.8 9.5.23 False Discovery Rate (FDR) Many people confuse the false positive rate (\\(P(R|\\text{not } C)\\)) with its inverse conditional probability, the false discovery rate (\\(P(\\text{not } C| R)\\)). The false discovery rate (FDR) is the conditional probability of not having the condition given a positive test: \\(P(\\text{not } C| R)\\). Lower values reflect greater accuracy. The formula for calculating false discovery rate is in Equation (9.51). \\[ \\begin{aligned} \\text{false discovery rate (FDR)} &amp;= P(\\text{not } C|R) \\\\ &amp;= \\frac{\\text{FP}}{\\text{FP} + \\text{TP}} = 1 - \\text{PPV} \\end{aligned} \\tag{9.51} \\] Code falseDiscoveryRate &lt;- function(TP, TN, FP, FN){ value &lt;- FP/(FP + TP) return(value) } Code falseDiscoveryRate( TP = TPvalue, FP = FPvalue) [1] 0.35 9.5.24 False Omission Rate (FOR) Many people confuse the false negative rate (\\(P(\\text{not } R|C)\\)) with its inverse conditional probability, the false omission rate (\\(P(C|\\text{not } R)\\)). The false omission rate (FOR) is the conditional probability of having the condition given a negative test: \\(P(C|\\text{not } R)\\). Lower values reflect greater accuracy. The formula for calculating false omission rate is in Equation (9.52). \\[ \\begin{aligned} \\text{false omission rate (FOR)} &amp;= P(C|\\text{not } R) \\\\ &amp;= \\frac{\\text{FN}}{\\text{FN} + \\text{TN}} = 1 - \\text{NPV} \\end{aligned} \\tag{9.52} \\] Code falseOmissionRate &lt;- function(TP, TN, FP, FN){ value &lt;- FN/(FN + TN) return(value) } Code falseOmissionRate( TN = TNvalue, FN = FNvalue) [1] 0.2 9.5.25 Youden’s J Statistic Youden’s J statistic is also called Youden’s Index or informedness. Youden’s J statistic is the sum of sensitivity and specificity (and subtracting one). Higher values reflect greater accuracy. The formula for calculating Youden’s J statistic is in Equation (9.53). \\[ \\begin{aligned} \\text{Youden&#39;s J statistic} &amp;= \\text{sensitivity} + \\text{specificity} - 1 \\end{aligned} \\tag{9.53} \\] Code youdenJ &lt;- function(TP, TN, FP, FN){ SN &lt;- TP/(TP + FN) SP &lt;- TN/(TN + FP) value &lt;- SN + SP - 1 return(value) } Code youdenJ( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue) [1] 0.45 9.5.26 Balanced Accuracy Balanced accuracy is the average of sensitivity and specificity. Higher values reflect greater accuracy. The formula for calculating balanced accuracy is in Equation (9.54). \\[ \\begin{aligned} \\text{balanced accuracy} &amp;= \\frac{\\text{sensitivity} + \\text{specificity}}{2} \\end{aligned} \\tag{9.54} \\] Code balancedAccuracy &lt;- function(TP, TN, FP, FN){ SN &lt;- TP/(TP + FN) SP &lt;- TN/(TN + FP) value &lt;- (SN + SP) / 2 return(value) } Code balancedAccuracy( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue) [1] 0.725 9.5.27 F-Score The F-score combines precision (positive predictive value) and recall (sensitivity), where \\(\\beta\\) indicates how many times more important sensitivity is than the positive predictive value. If sensitivity and the positive predictive value are equally important, \\(\\beta = 1\\), and the F-score is called the \\(F_1\\) score. Higher values reflect greater accuracy. The formula for calculating the F-score is in Equation (9.55). \\[ \\begin{aligned} F_\\beta &amp;= (1 + \\beta^2) \\cdot \\frac{\\text{positive predictive value} \\cdot \\text{sensitivity}}{(\\beta^2 \\cdot \\text{positive predictive value}) + \\text{sensitivity}} \\\\ &amp;= \\frac{(1 + \\beta^2) \\cdot \\text{TP}}{(1 + \\beta^2) \\cdot \\text{TP} + \\beta^2 \\cdot \\text{FN} + \\text{FP}} \\end{aligned} \\tag{9.55} \\] The formula for calculating the \\(F_1\\) score is in Equation (9.56). \\[ \\begin{aligned} F_1 &amp;= \\frac{2 \\cdot \\text{positive predictive value} \\cdot \\text{sensitivity}}{(\\text{positive predictive value}) + \\text{sensitivity}} \\\\ &amp;= \\frac{2 \\cdot \\text{TP}}{2 \\cdot \\text{TP} + \\text{FN} + \\text{FP}} \\end{aligned} \\tag{9.56} \\] Code fScore &lt;- function(TP, TN, FP, FN, beta = 1){ value &lt;- ((1 + beta^2) * TP) / ((1 + beta^2) * TP + beta^2 * FN + FP) return(value) } Code fScore( TP = TPvalue, FP = FPvalue, FN = FNvalue) [1] 0.65 Code fScore( TP = TPvalue, FP = FPvalue, FN = FNvalue, beta = 2) [1] 0.65 Code fScore( TP = TPvalue, FP = FPvalue, FN = FNvalue, beta = 0.5) [1] 0.65 9.5.28 Matthews Correlation Coefficient (MCC) The Matthews correlation coefficient (MCC) is also called the phi coefficient. It is a correlation coefficient between predicted and observed values from a binary classification. Higher values reflect greater accuracy. The formula for calculating the MCC is in Equation (9.57). \\[ \\begin{aligned} \\text{MCC} &amp;= \\frac{\\text{TP} \\times \\text{TN} - \\text{FP} \\times \\text{FN}}{\\sqrt{(\\text{TP} + \\text{FP})(\\text{TP} + \\text{FN})(\\text{TN} + \\text{FP})(\\text{TN} + \\text{FN})}} \\end{aligned} \\tag{9.57} \\] Code mcc &lt;- function(TP, TN, FP, FN){ TP &lt;- as.double(TP) TN &lt;- as.double(TN) FP &lt;- as.double(FP) FN &lt;- as.double(FN) value &lt;- ((TP * TN) - (FP * FN)) / sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)) return(value) } Code mcc( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue) [1] 0.45 9.5.29 Diagnostic Odds Ratio The diagnostic odds ratio is the odds of a positive test among people with the condition relative to the odds of a positive test among people without the condition. Higher values reflect greater accuracy. The formula for calculating the diagnostic odds ratio is in Equation (9.58). If the predictor is bad, the diagnostic odds ratio could be less than one, and values can go up from there. If the diagnostic odds ratio is greater than 2, we take the odds ratio seriously because we are twice as likely to predict accurately than inaccurately. However, the diagnostic odds ratio ignores/hides base rates. When interpreting the diagnostic odds ratio, it is important to keep in mind the clinical significance, because otherwise it is not very meaningful. Consider a risk factor that has a diagnostic odds ratio of 3 for tuberculosis, i.e., it puts you at 3 times as likely to develop tuberculosis. The prevalence of tuberculosis is relatively low. Assuming the prevalence of tuberculosis is less than 1/10th of 1%, your risk of developing tuberculosis is still very low even if the risk factor (with a diagnostic odds ratio of 3) is present. \\[ \\begin{aligned} \\text{diagnostic odds ratio} &amp;= \\frac{\\text{TP} \\times \\text{TN}}{\\text{FP} \\times \\text{FN}} \\\\ &amp;= \\frac{\\text{sensitivity} \\times \\text{specificity}}{(1 - \\text{sensitivity}) \\times (1 - \\text{specificity})} \\\\ &amp;= \\frac{\\text{PPV} \\times \\text{NPV}}{(1 - \\text{PPV}) \\times (1 - \\text{NPV})} \\\\ &amp;= \\frac{\\text{LR+}}{\\text{LR}-} \\end{aligned} \\tag{9.58} \\] Code diagnosticOddsRatio &lt;- function(TP, TN, FP, FN){ value &lt;- (TP * TN) / (FP * FN) return(value) } Code diagnosticOddsRatio( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue) [1] 7.428571 9.5.30 Diagnostic Likelihood Ratio A likelihood ratio is the ratio of two probabilities. It can be used to compare the likelihood of two possibilities. The diagnostic likelihood ratio is an index of the predictive validity of an instrument: it is the ratio of the probability that a test result is correct to the probability that the test result is incorrect. The diagnostic likelihood ratio is also called the risk ratio. There are two types of diagnostic likelihood ratios: the positive likelihood ratio and the negative likelihood ratio. 9.5.30.1 Positive Likelihood Ratio (LR+) The positive likelihood ratio (LR+) compares the true positive rate to the false positive rate. Higher values reflect greater accuracy, because it indicates the degree to which a true positive is more likely than a false positive. The formula for calculating the positive likelihood ratio is in Equation (9.59). \\[ \\begin{aligned} \\text{positive likelihood ratio (LR+)} &amp;= \\frac{\\text{TPR}}{\\text{FPR}} \\\\ &amp;= \\frac{P(R|C)}{P(R|\\text{not } C)} \\\\ &amp;= \\frac{P(R|C)}{1 - P(\\text{not } R|\\text{not } C)} \\\\ &amp;= \\frac{\\text{sensitivity}}{1 - \\text{specificity}} \\end{aligned} \\tag{9.59} \\] Code positiveLikelihoodRatio &lt;- function(TP, TN, FP, FN){ SN &lt;- TP/(TP + FN) SP &lt;- TN/(TN + FP) value &lt;- SN/(1 - SP) return(value) } Code positiveLikelihoodRatio( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue) [1] 3.25 9.5.30.2 Negative Likelihood Ratio (LR−) The negative likelihood ratio (LR−) compares the false negative rate to the true negative rate. Smaller values reflect greater accuracy, because it indicates that a false negative is less likely than a true negative. The formula for calculating the negative likelihood ratio is in Equation (9.60). \\[ \\begin{aligned} \\text{negative likelihood ratio } (\\text{LR}-) &amp;= \\frac{\\text{FNR}}{\\text{TNR}} \\\\ &amp;= \\frac{P(\\text{not } R|C)}{P(\\text{not } R|\\text{not } C)} \\\\ &amp;= \\frac{1 - P(R|C)}{P(\\text{not } R|\\text{not } C)} \\\\ &amp;= \\frac{1 - \\text{sensitivity}}{\\text{specificity}} \\end{aligned} \\tag{9.60} \\] Code negativeLikelihoodRatio &lt;- function(TP, TN, FP, FN){ SN &lt;- TP/(TP + FN) SP &lt;- TN/(TN + FP) value &lt;- (1 - SN)/SP return(value) } Code negativeLikelihoodRatio( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue) [1] 0.4375 9.5.31 Posttest Odds As presented in Equation (9.16), the posttest (or posterior) odds are equal to the pretest odds multiplied by the likelihood ratio. The posttest odds and posttest probability can be useful to calculate when the pretest probability is different from the pretest probability (or prevalence) of the classification. For instance, you might use a different pretest probability if a test result is already known and you want to know the updated posttest probability after conducting a second test. The formula for calculating posttest odds is in Equation (9.61). \\[ \\begin{aligned} \\text{posttest odds} &amp;= \\text{pretest odds} \\times \\text{likelihood ratio} \\\\ \\end{aligned} \\tag{9.61} \\] For calculating the posttest odds of a true positive compared to a false positive, we use the positive likelihood ratio, described later. We would use the negative likelihood ratio if we wanted to calculate the posttest odds of a false negative compared to a true negative. Code posttestOdds &lt;- function(TP, TN, FP, FN, pretestProb = NULL, SN = NULL, SP = NULL, likelihoodRatio = NULL){ if(!is.null(pretestProb) &amp; !is.null(SN) &amp; !is.null(SP)){ pretestProbability &lt;- pretestProb pretestOdds &lt;- pretestProbability / (1 - pretestProbability) likelihoodRatio &lt;- SN/(1 - SP) } else if(!is.null(pretestProb) &amp; !is.null(likelihoodRatio)){ pretestProbability &lt;- pretestProb pretestOdds &lt;- pretestProbability / (1 - pretestProbability) likelihoodRatio &lt;- likelihoodRatio } else { N &lt;- TP + TN + FP + FN pretestProbability &lt;- (TP + FN)/N pretestOdds &lt;- pretestProbability / (1 - pretestProbability) SN &lt;- TP/(TP + FN) SP &lt;- TN/(TN + FP) likelihoodRatio &lt;- SN/(1 - SP) } value &lt;- pretestOdds * likelihoodRatio return(value) } Code posttestOdds( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue) [1] 1.857143 Code posttestOdds( pretestProb = baseRate( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue), SN = sensitivity( TP = TPvalue, FN = FNvalue), SP = specificity( TN = TNvalue, FP = FPvalue)) [1] 1.857143 Code posttestOdds( pretestProb = baseRate( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue), likelihoodRatio = positiveLikelihoodRatio( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue)) [1] 1.857143 9.5.32 Posttest Probability The posttest probability is the probability of having the disorder given a test result. When the base rate is used as the pretest probability, the posttest probability given a positive test is equal to positive predictive value. To convert odds to a probability, divide the odds by one plus the odds, as is in Equation (9.62). \\[ \\begin{aligned} \\text{posttest probability} &amp;= \\frac{\\text{posttest odds}}{1 + \\text{posttest odds}} \\end{aligned} \\tag{9.62} \\] Code posttestProbability &lt;- function(TP, TN, FP, FN, pretestProb = NULL, SN = NULL, SP = NULL, likelihoodRatio = NULL){ if(!is.null(pretestProb) &amp; !is.null(SN) &amp; !is.null(SP)){ pretestProbability &lt;- pretestProb pretestOdds &lt;- pretestProbability / (1 - pretestProbability) likelihoodRatio &lt;- SN/(1 - SP) } else if(!is.null(pretestProb) &amp; !is.null(likelihoodRatio)){ pretestProbability &lt;- pretestProb pretestOdds &lt;- pretestProbability / (1 - pretestProbability) likelihoodRatio &lt;- likelihoodRatio } else { N &lt;- TP + TN + FP + FN pretestProbability &lt;- (TP + FN)/N pretestOdds &lt;- pretestProbability / (1 - pretestProbability) SN &lt;- TP/(TP + FN) SP &lt;- TN/(TN + FP) likelihoodRatio &lt;- SN/(1 - SP) } posttestOdds &lt;- pretestOdds * likelihoodRatio value &lt;- posttestOdds / (1 + posttestOdds) return(value) } Code posttestProbability( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue) [1] 0.65 Code posttestProbability( pretestProb = baseRate( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue), SN = sensitivity( TP = TPvalue, FN = FNvalue), SP = specificity( TN = TNvalue, FP = FPvalue)) [1] 0.65 Code posttestProbability( pretestProb = baseRate( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue), likelihoodRatio = positiveLikelihoodRatio( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue)) [1] 0.65 Code posttestProbabilityValue &lt;- posttestProbability(TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue) Consider the following example: Assume the base rate of the condition is .03%. We have two tests. Test A has a sensitivity of .95 and a specificity of .80. Test B has a sensitivity of .70 and a specificity of .90. What is the probability of having the condition if a person has a positive test on Test A? Assuming the errors of the two tests are independent, what is the probability of having the condition if the person has a positive test on Test B after having a positive test on Test A? Code probGivenTestA &lt;- posttestProbability( pretestProb = .003, SN = .95, SP = .80) probGivenTestAthenB &lt;- posttestProbability( pretestProb = probGivenTestA, SN = .70, SP = .90) probGivenTestA [1] 0.01409147 Code probGivenTestAthenB [1] 0.09095054 The probability of having the condition if a person has a positive test on Test A is \\(1.4\\)%. The probability of having the condition if the person has a positive test on Test B after having a positive test on Test A is \\(9.1\\)%. 9.5.33 Probability Nomogram The petersenlab package (Petersen, 2024b) contains the nomogrammer() function that creates a probability nomogram plot, adapted from https://github.com/achekroud/nomogrammer. In Figure 9.33, the probability nomogram is generated using the number of true positives, true negatives, false positives, and false negatives at a given cutoff. Code nomogrammer( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue ) Figure 9.33: Probability Nomogram. The blue line indicates the posterior probability of the condition given a positive test. The pink line indicates the posterior probability of the condition given a negative test. One can also generate the probability nomogram from the base rate and the sensitivity and specificity of the test at a given cutoff: Code nomogrammer( pretestProb = baseRate(TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue), SN = sensitivity(TP = TPvalue, FN = FNvalue), SP = specificity(TN = TNvalue, FP = FPvalue) ) One can also generate the probability nomogram from the base rate, positive likelihood ratio, and negative likelihood ratio at a given cutoff: Code nomogrammer( pretestProb = baseRate(TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue), PLR = positiveLikelihoodRatio( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue), NLR = negativeLikelihoodRatio( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue) ) 9.5.34 \\(d&#39;\\) Sensitivity from Signal Detection Theory \\(d&#39;\\) (\\(d\\) prime) is an index of sensitivity from signal detection theory, as described by Stanislaw &amp; Todorov (1999). Higher values reflect greater accuracy. The formula for calculating \\(d&#39;\\) is in Equation (9.63). \\[\\begin{equation} d&#39; = z(\\text{hit rate}) - z(\\text{false alarm rate}) \\tag{9.63} \\end{equation}\\] Code dPrimeSDT &lt;- function(TP, TN, FP, FN){ HR &lt;- TP/(TP + FN) FAR &lt;- FP/(FP + TN) value &lt;- qnorm(HR) - qnorm(FAR) return(value) } Code dPrimeSDT( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue) [1] 1.226942 9.5.35 \\(A\\) (Non-Parametric) Sensitivity from Signal Detection Theory \\(A\\) is a non-parametric index of sensitivity from signal detection theory, as described by Zhang &amp; Mueller (2005). Higher values reflect greater accuracy. The formula for calculating \\(A\\) is in Equation (9.64). https://sites.google.com/a/mtu.edu/whynotaprime/ (archived at https://perma.cc/W2M2-39TJ) \\[\\begin{equation} A = \\begin{cases} \\frac{3}{4} + \\frac{H - F}{4} - F(1 - H) &amp; \\text{if } F \\leq 0.5 \\leq H ; \\\\ \\frac{3}{4} + \\frac{H - F}{4} - \\frac{F}{4H} &amp; \\text{if } F \\leq H \\leq 0.5 ;\\\\ \\frac{3}{4} + \\frac{H - F}{4} - \\frac{1 - H}{4(1 - F)} &amp; \\text{if } 0.5 \\leq F \\leq H . \\end{cases} \\tag{9.64} \\end{equation}\\] where \\(H\\) is the hit rate and \\(F\\) is the false alarm rate. Code aSDT &lt;- function(TP, TN, FP, FN){ HR &lt;- TP/(TP + FN) FAR &lt;- FP/(FP + TN) ifelse(FAR &lt;= .5 &amp; HR &gt;= .5, value &lt;- (3/4) + ((HR - FAR)/4) - (FAR * (1 - HR)), NA) ifelse(FAR &lt;= HR &amp; HR &lt;= .5, value &lt;- (3/4) + ((HR - FAR)/4) - (FAR/(4 * HR)), NA) ifelse(FAR &gt;= .5 &amp; FAR &lt;= HR, value &lt;- (3/4) + ((HR - FAR)/4) - ((1 - HR)/(4 * (1 - FAR))), NA) return(value) } Code aSDT( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue) [1] 0.7925 9.5.36 \\(\\beta\\) Bias from Signal Detection Theory \\(\\beta\\) is an index of bias from signal detection theory, as described by Stanislaw &amp; Todorov (1999). Smaller values reflect greater accuracy. The formula for calculating \\(\\beta\\) is in Equation (9.65). \\[\\begin{equation} \\beta = e^{\\bigg\\{\\frac{\\big[\\phi^{-1}(F)\\big]^2 - \\big[\\phi^{-1}(H)\\big]}{2}\\bigg\\}^2} \\tag{9.65} \\end{equation}\\] where \\(H\\) is the hit rate, \\(F\\) is the false alarm rate, and \\(\\phi\\) (phi) is a mathematical function that converts a z score to a probability by determining the portion of the normal distribution that lies to the left of the z score. Code betaSDT &lt;- function(TP, TN, FP, FN){ HR &lt;- TP/(TP + FN) FAR &lt;- FP/(FP + TN) value &lt;- exp(qnorm(FAR)^2/2 - qnorm(HR)^2/2) return(value) } Code betaSDT( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue) [1] 1.323034 9.5.37 \\(c\\) Bias from Signal Detection Theory \\(c\\) is an index of bias from signal detection theory, as described by Stanislaw &amp; Todorov (1999). Smaller values reflect greater accuracy. The formula for calculating \\(c\\) is in Equation (9.66). \\[\\begin{equation} c = - \\frac{\\phi^{-1}(H) + \\phi^{-1}(F)}{2} \\tag{9.66} \\end{equation}\\] where \\(H\\) is the hit rate, \\(F\\) is the false alarm rate, and \\(\\phi\\) (phi) is a mathematical function that converts a z score to a probability by determining the portion of the normal distribution that lies to the left of the z score. Code cSDT &lt;- function(TP, TN, FP, FN){ HR &lt;- TP/(TP + FN) FAR &lt;- FP/(FP + TN) value &lt;- -(qnorm(HR) + qnorm(FAR))/2 return(value) } Code cSDT( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue) [1] 0.2281504 9.5.38 \\(b\\) (Non-Parametric) Bias from Signal Detection Theory \\(b\\) is a non-parametric index of bias from signal detection theory, as described by Zhang &amp; Mueller (2005). Smaller values reflect greater accuracy. The formula for calculating \\(b\\) is in Equation (9.67). \\[\\begin{equation} b = \\begin{cases} \\frac{5 - 4H}{1 + 4F} &amp; \\text{if } F \\leq 0.5 \\leq H ; \\\\ \\frac{H^2 + H}{H^2 + F} &amp; \\text{if } F \\leq H \\leq 0.5 ;\\\\ \\frac{(1 - F)^2 + (1 - H)}{(1 - F)^2 + (1 - F)} &amp; \\text{if } 0.5 \\leq F \\leq H . \\end{cases} \\tag{9.67} \\end{equation}\\] Code bSDT &lt;- function(TP, TN, FP, FN){ HR &lt;- TP/(TP + FN) FAR &lt;- FP/(FP + TN) ifelse(FAR &lt;= .5 &amp; HR &gt;= .5, value &lt;-(5 - (4 * HR))/(1 + (4 * FAR)), NA) ifelse(FAR &lt;= HR &amp; HR &lt;= .5, value &lt;- (HR^2 + HR)/(HR^2 + FAR), NA) ifelse(FAR &gt;= .5 &amp; FAR &lt;= HR, value &lt;- ((1 - FAR)^2 + (1 - HR))/((1 - FAR)^2 + (1 - FAR)), NA) return(value) } Code bSDT( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue) [1] 1.333333 9.5.39 Mean Difference between Predicted Versus Observed Values (Miscalibration) The mean difference between predicted values versus observed values at a given cutoff is an index of miscalibration of predictions at that cutoff. It is called “calibration-in-the-small” (as opposed to calibration-in-the-large, which spans all cutoffs). Values closer to zero reflect greater accuracy. Values above zero indicate that the predicted values are, on average, greater than the observed values. Values below zero indicate that the observed values are, on average, greater than the predicted values. Code miscalibration &lt;- function(predicted, actual, cutoff, bins = 10){ data &lt;- data.frame(na.omit(cbind(predicted, actual))) calibrationTable &lt;- mutate( data, bin = cut_number( predicted, n = 10)) %&gt;% group_by(bin) %&gt;% summarise( n = length(predicted), meanPredicted = mean(predicted, na.rm = TRUE), meanObserved = mean(actual, na.rm = TRUE), .groups = &quot;drop&quot;) calibrationTable$cutoffMin &lt;- as.numeric(str_replace_all(str_split( calibrationTable$bin, pattern = &quot;,&quot;, simplify = TRUE)[,1], &quot;[^[:alnum:]\\\\-\\\\.]&quot;, &quot;&quot;)) calibrationTable$cutoffMax &lt;- as.numeric(str_replace_all(str_split( calibrationTable$bin, pattern = &quot;,&quot;, simplify = TRUE)[,2], &quot;[^[:alnum:]\\\\-\\\\.]&quot;, &quot;&quot;)) calibrationTable$inRange &lt;- with( calibrationTable, cutoff &gt;= cutoffMin &amp; cutoff &lt;= cutoffMax) if(length(which(calibrationTable$inRange == TRUE)) &gt; 0){ nearestCutoff &lt;- calibrationTable$bin[min(which( calibrationTable$inRange == TRUE))] calibrationAtNearestCutoff &lt;- calibrationTable[which( calibrationTable$bin == nearestCutoff),] calibrationAtNearestCutoff &lt;- as.data.frame(calibrationTable[max(which( calibrationTable$inRange == TRUE)),]) meanPredicted &lt;- calibrationAtNearestCutoff[, &quot;meanPredicted&quot;] meanObserved &lt;- calibrationAtNearestCutoff[, &quot;meanObserved&quot;] differenceBetweenPredictedAndObserved &lt;- meanPredicted - meanObserved } else{ differenceBetweenPredictedAndObserved &lt;- NA } return(differenceBetweenPredictedAndObserved) } Code miscalibration( predicted = mydataSDT$predictedProbability, actual = mydataSDT$disorder, cutoff = cutoff) [1] -0.07843137 9.6 Optimal Cutoff Specification There are two ways to improve diagnostic performance (Swets et al., 2000). One way is to increase the diagnostic accuracy of the assessment. The second way is to increase the utility of the diagnostic decisions that are made, based on where we set the cutoff. The optimal cutoff depends on the differential costs of false positives versus false negatives, as applied in decision theory. When differential costs of false positives versus false negatives cannot be specified, an alternative approach to specifying the optimal cutoff is to use information theory. 9.6.1 Decision Theory According to the decision theory approach to picking the optimal cutoff, the optimal cutoff depends on the value/importance placed on each of the four decision outcomes [(true positives, true negatives, false positives, false negatives); Treat &amp; Viken (2023)]. Utility is the relative value placed on a specific decision-making outcome (i.e., user-perceived benefit or cost): utilities typically range between zero and one, where a value of zero represents the least desired outcome, and a value of one indicates the most desired outcome. According to the decision theory approach, the optimal cutoff is the cutoff with the highest overall utility. 9.6.1.1 Overall utility of a specific cutoff value The overall utility of a specific cutoff value is a utilities-weighted sum of the probabilities of the four decision-making outcomes (hits, misses, correct rejections, false alarms). That is, overall utility is the sum of the product of the probability of a particular outcome (TP, TN, FP, FN; e.g., \\(\\text{BR} \\times \\text{TP rate}\\)) and the utility of that outcome (e.g., how much we value TPs relative to other outcomes). Higher values reflect greater utility, so you would pick the cutoff with the highest overall utility. The formula for calculating overall utility is in Equation (9.68): \\[ \\begin{aligned} U_\\text{overall} = \\ &amp; (\\text{BR})(\\text{HR})(U_\\text{H}) \\\\ &amp;+ (\\text{BR})(1 - \\text{HR})(U_\\text{M}) \\\\ &amp;+ (1 - \\text{BR})(\\text{FAR})(U_\\text{FA}) \\\\ &amp;+ (1 - \\text{BR})(1 - \\text{FAR})(U_\\text{CR}) \\end{aligned} \\tag{9.68} \\] where \\(\\text{BR} = \\text{base rate}\\), \\(\\text{HR} = \\text{hit rate (true positive rate)}\\), \\(\\text{FAR} = \\text{false alarm rate (false positive rate)}\\), \\(U_\\text{H} = \\text{utility of hits (true positives)}\\), \\(U_\\text{M} = \\text{utility of misses (false negatives)}\\), \\(U_\\text{FA} = \\text{utility of false alarms (false positives)}\\), \\(U_\\text{CR} = \\text{utility of correct rejections (true negatives)}\\). Code Uoverall &lt;- function(BR, HR, FAR, UH, UM, UCR, UFA){ (BR*HR*UH) + (BR*(1 - HR)*UM) + ((1 - BR)*FAR*UFA) + ((1 - BR)*(1 - FAR)*(UCR)) } Code Uoverall( BR = baseRate( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue), HR = sensitivity( TP = TPvalue, FN = FNvalue), FAR = falsePositiveRate( TN = TNvalue, FP = FPvalue), UH = 1, UM = 0, UCR = 0.75, UFA = 0.25) [1] 0.65 Code Uoverall( BR = baseRate( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue), HR = sensitivity( TP = TPvalue, FN = FNvalue), FAR = falsePositiveRate( TN = TNvalue, FP = FPvalue), UH = 1, UM = 0, UCR = 1, UFA = 0) [1] 0.7454545 Code Uoverall( BR = baseRate( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue), HR = sensitivity( TP = TPvalue, FN = FNvalue), FAR = falsePositiveRate( TN = TNvalue, FP = FPvalue), UH = 0.75, UM = 0.25, UCR = 1, UFA = 0) [1] 0.7181818 9.6.1.2 Utility ratio The utility ratio is the user-perceived relative importance of decisions about negative versus positive cases. If the utility ratio value is one, it indicates that identifying negative cases and positive cases is equally important. Values above one indicate greater relative importance of identifying negative cases than positive cases. Values below one indicate greater relative importance of identifying positive cases than negative cases. Values of one indicate that you are maximizing percent accuracy. The formula for calculating the utility ratio is in Equation (9.69): \\[\\begin{equation} \\text{Utility Ratio} = \\frac{U_\\text{CR} - U_\\text{FA}}{U_\\text{H} - U_\\text{M}} \\tag{9.69} \\end{equation}\\] where \\(U_\\text{H} = \\text{utility of hits (true positives)}\\), \\(U_\\text{M} = \\text{utility of misses (false negatives)}\\), \\(U_\\text{FA} = \\text{utility of false alarms (false positives)}\\), \\(U_\\text{CR} = \\text{utility of correct rejections (true negatives)}\\). Code utilityRatio &lt;- function(UH, UM, UCR, UFA){ (UCR - UFA) / (UH - UM) } Code utilityRatio(UH = 1, UM = 0, UCR = 0.75, UFA = 0.25) [1] 0.5 Code utilityRatio(UH = 1, UM = 0, UCR = 1, UFA = 0) [1] 1 Code utilityRatio(UH = 0.75, UM = 0.25, UCR = 1, UFA = 0) [1] 2 Decision theory has key advantages, because it identifies the cutoff that would help you best achieve the goals/purpose of the assessment. However, it can be challenging to specify the relative costs of errors. If you cannot decide values for outcomes (relative importance between FP and FN), you can use information theory to identify the optimal cutoff. 9.6.2 Information Theory When the user does not differentially weigh the value/importance of the four decision-making outcomes (hits, misses, correct rejections, false alarms), the information theory approach can be useful for specifying the optimal cutoff. According to the information theory approach, the optimal cutoff is the cutoff that provides the greatest information gain (Treat &amp; Viken, 2023). 9.6.2.1 Information Gain Information gain (\\(I_\\text{gain}\\)) is the reduction of uncertainty about the true classification of a case that results from administering an assessment or prediction measure (Treat &amp; Viken, 2023). Greater values reflect greater reduction of uncertainty, so the optimal cutoff can be specified as the cutoff with the highest information gain. 9.6.2.1.1 Formula from Treat &amp; Viken (2023) The formula from Treat &amp; Viken (2023) for calculating information gain is in Equation (9.70): \\[ \\begin{aligned} I_\\text{gain} = \\ &amp; (\\text{BR})(\\text{HR})\\bigg[\\log_2\\bigg(\\frac{\\text{HR}}{G}\\bigg)\\bigg] \\\\ &amp;+ (\\text{BR})(1 - \\text{HR})\\bigg[\\log_2\\bigg(\\frac{1 - \\text{HR}}{1 - G}\\bigg)\\bigg] \\\\ &amp;+ (1 - \\text{BR})(\\text{FAR})\\bigg[\\log_2\\bigg(\\frac{\\text{FAR}}{G}\\bigg)\\bigg] \\\\ &amp;+ (1 - \\text{BR})(1 - \\text{FAR})\\bigg[\\log_2\\bigg(\\frac{1 - \\text{FAR}}{1 - G}\\bigg)\\bigg] \\end{aligned} \\tag{9.70} \\] where \\(\\text{BR} =\\) base rate, \\(\\text{HR} =\\) hit rate (true positive rate), \\(\\text{FAR} =\\) false alarm rate (false positive rate), \\(G =\\) selection ratio \\(= \\text{BR} (\\text{HR}) + (1 - \\text{BR}) (\\text{FAR})\\), as reported in Somoza et al. (1989) (see below). Code Igain &lt;- function(BR, HR, FAR){ G &lt;- BR*(HR) + (1 - BR)*(FAR) (BR*HR*log2(HR/G)) + (BR*(1 - HR)*(log2((1 - HR)/(1 - G)))) + ((1 - BR)*FAR*(log2(FAR/G))) + ((1 - BR)*(1 - FAR)*(log2((1 - FAR)/(1 - G)))) } Code Igain( BR = baseRate( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue), HR = sensitivity( TP = TPvalue, FN = FNvalue), FAR = falsePositiveRate( TN = TNvalue, FP = FPvalue)) [1] 0.1465904 9.6.2.1.2 Alternative formula from Metz et al. (1973) The alternative formula from Metz et al. (1973) for calculating information gain is in Equation (9.71): \\[ \\begin{aligned} I_\\text{gain} = \\ &amp; p(S|s) \\cdot p(s) \\cdot log_2\\Bigg\\{\\frac{p(S|s)}{p(S|s) \\cdot p(s) + p(S|n)[1 - p(s)]}\\Bigg\\} \\\\ &amp;+ p(S|n)[1 - p(s)] \\times log_2\\Bigg\\{\\frac{p(S|n)}{p(S|s) \\cdot p(s) + p(S|n)[1 - p(s)]}\\Bigg\\} \\\\ &amp;+ [1 - p(S|s)] \\cdot p(s) \\times log_2\\Bigg\\{\\frac{1 - p(S|s)}{1 - p(S|s) \\cdot p(s) - p(S|n)[1 - p(s)]}\\Bigg\\} \\\\ &amp;+ [1 - p(S|n)][1 - p(s)] \\times log_2\\Bigg\\{\\frac{1 - p(S|n)}{1 - p(S|s) \\cdot p(s) - p(S|n)[1 - p(s)]}\\Bigg\\} \\end{aligned} \\tag{9.71} \\] where \\(p(S|s) =\\) sensitivity (hit rate or true positive rate); i.e., the conditional probability of deciding a signal is present (\\(S\\)) when the signal is in fact present(\\(s\\)), \\(p(S|n) =\\) false positive rate (false alarm rate); i.e., the conditional probability of deciding a signal is present (\\(S\\)) when the signal is in fact absent (\\(n\\)), \\(p(s) =\\) base rate, i.e., the probability that the signal is in fact present (\\(s\\)). Code Igain2 &lt;- function(BR, HR, FAR){ HR * BR * log2(HR / ((HR * BR) + (FAR * (1 - BR)))) + FAR * (1 - BR) * log2(FAR / ((HR * BR) + (FAR * (1 - BR)))) + (1 - HR) * BR * log2((1 - HR) / (1 - (HR * BR) - (FAR * (1 - BR)))) + (1 - FAR) * (1 - BR) * log2((1 - FAR) / (1 - (HR * BR) - (FAR * (1 - BR)))) } Code Igain2( BR = baseRate( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue), HR = sensitivity( TP = TPvalue, FN = FNvalue), FAR = falsePositiveRate( TN = TNvalue, FP = FPvalue)) [1] 0.1465904 9.6.2.1.3 Alternative formula from Somoza et al. (1989) The alternative formula from Somoza et al. (1989) for calculating information gain is in Equation (9.72): \\[ \\begin{aligned} I_\\text{gain} = \\ &amp; [(\\text{TPR})(\\text{Pr})] \\times \\log_2(\\text{TPR}/G) \\\\ &amp;+ [(\\text{FPR})(1 - \\text{Pr})] \\times \\log_2(\\text{FPR}/G) \\\\ &amp;+ [(1 - \\text{TPR})(\\text{Pr})] \\times \\log_2\\bigg(\\frac{1 - \\text{TPR}}{1 - G}\\bigg) \\\\ &amp;+ [(1 - \\text{FPR})(1 - \\text{Pr})] \\times \\log_2\\bigg(\\frac{1 - \\text{FPR}}{1 - G}\\bigg) \\end{aligned} \\tag{9.72} \\] where \\(\\text{TP} =\\) true positive rate (hit rate), \\(\\text{Pr} =\\) prevalence (base rate), \\(\\text{FP} =\\) false positive rate (false alarm rate), \\(G = \\text{Pr} (\\text{TP}) + (1 - \\text{Pr}) (\\text{FP}) =\\) selection ratio Code Igain3 &lt;- function(BR, HR, FAR){ G &lt;- BR*(HR) + (1 - BR)*(FAR) ((HR)*(BR))*log2((HR/G)) + ((FAR)*(1-BR))*log2((FAR/G)) + ((1-HR)*(BR))*log2((1-HR)/(1-G)) + ((1-FAR)*(1-BR))*log2((1-FAR)/(1-G)) } Code Igain3( BR = baseRate( TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue), HR = sensitivity( TP = TPvalue, FN = FNvalue), FAR = falsePositiveRate( TN = TNvalue, FP = FPvalue)) [1] 0.1465904 9.6.2.1.4 Examples Case A from Exhibit 38.2 (Treat &amp; Viken, 2023): Code Igain(HR = (911/1899), FAR = (509/4757), BR = (1899/6656)) [1] 0.112081 Case B from Exhibit 38.2 (Treat &amp; Viken, 2023): Code Igain(HR = (1597/3328), FAR = (356/3328), BR = (3328/6656)) [1] 0.1283265 Case C from Exhibit 38.2 (Treat &amp; Viken, 2023): Code Igain(HR = (2040/3328), FAR = (654/3328), BR = (3328/6656)) [1] 0.1347846 Case B from Exhibit 38.3 (Treat &amp; Viken, 2023): Code Igain(HR = (1164/1899), FAR = (935/4757), BR = (1899/6656)) [1] 0.1135549 9.6.2.1.5 Effect of Base Rate Information gain depends on the base rate (Treat &amp; Viken, 2023), as depicted in Figure 9.34. The maximum reduction of uncertainty (i.e., greatest information) occurs when the base rate is 0.5. A base rate tells us little a priori about the condition if the probability of the condition is 50/50, so the measure can provide more benefit. If the base rate is 0.3 or 0.7, we can do better than going with the base rate. If the base rate is 0.9 or 0.1, it is difficult for our measure to do better than going with the base rate. If the base rate is 0.05 of 0.95 (or more extreme), it is likely that our measure will do almost nothing in terms of information gain. Figure 9.34: Information Gain as a Function of the Base Rate (BR). 9.7 Accuracy at Every Possible Cutoff 9.7.1 Specify utility of each outcome Code utilityHits &lt;- 1 utilityMisses &lt;- 0 utilityCorrectRejections &lt;- 0.75 utilityFalseAlarms &lt;- 0.25 9.7.2 Calculate Accuracy Code possibleCutoffs &lt;- unique(na.omit(mydataSDT$testScore)) possibleCutoffs &lt;- possibleCutoffs[order(possibleCutoffs)] possibleCutoffs &lt;- c( possibleCutoffs, max(possibleCutoffs, na.rm = TRUE) + 0.01) accuracyVariables &lt;- c( &quot;cutoff&quot;, &quot;TP&quot;, &quot;TN&quot;, &quot;FP&quot;, &quot;FN&quot;, &quot;differenceBetweenPredictedAndObserved&quot;) accuracyStats &lt;- data.frame( matrix( nrow = length(possibleCutoffs), ncol = length(accuracyVariables))) names(accuracyStats) &lt;- accuracyVariables for(i in 1:length(possibleCutoffs)){ cutoff &lt;- possibleCutoffs[i] mydataSDT$diagnosis &lt;- NA mydataSDT$diagnosis[mydataSDT$testScore &lt; cutoff] &lt;- 0 mydataSDT$diagnosis[mydataSDT$testScore &gt;= cutoff] &lt;- 1 accuracyStats[i, &quot;cutoff&quot;] &lt;- cutoff accuracyStats[i, &quot;TP&quot;] &lt;- length(which( mydataSDT$diagnosis == 1 &amp; mydataSDT$disorder == 1)) accuracyStats[i, &quot;TN&quot;] &lt;- length(which( mydataSDT$diagnosis == 0 &amp; mydataSDT$disorder == 0)) accuracyStats[i, &quot;FP&quot;] &lt;- length(which( mydataSDT$diagnosis == 1 &amp; mydataSDT$disorder == 0)) accuracyStats[i, &quot;FN&quot;] &lt;- length(which( mydataSDT$diagnosis == 0 &amp; mydataSDT$disorder == 1)) accuracyStats[i, &quot;differenceBetweenPredictedAndObserved&quot;] &lt;- miscalibration( predicted = mydataSDT$testScore, actual = mydataSDT$disorder, cutoff = cutoff) } accuracyStats$N &lt;- sampleSize( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$selectionRatio &lt;- selectionRatio( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$baseRate &lt;- baseRate( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$percentAccuracy &lt;- percentAccuracy( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$percentAccuracyByChance &lt;- percentAccuracyByChance( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$relativeImprovementOverChance &lt;- relativeImprovementOverChance( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$relativeImprovementOverPredictingFromBaseRate &lt;- relativeImprovementOverPredictingFromBaseRate( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$sensitivity &lt;- accuracyStats$TPrate &lt;- sensitivity( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$specificity &lt;- accuracyStats$TNrate &lt;- specificity( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$FNrate &lt;- falseNegativeRate( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$FPrate &lt;- falsePositiveRate( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$youdenJ &lt;- youdenJ( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$positivePredictiveValue &lt;- positivePredictiveValue( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$negativePredictiveValue &lt;- negativePredictiveValue( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$falseDiscoveryRate &lt;- falseDiscoveryRate( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$falseOmissionRate &lt;- falseOmissionRate( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$balancedAccuracy &lt;- balancedAccuracy( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$f1Score &lt;- fScore( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$mcc &lt;- mcc( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$diagnosticOddsRatio &lt;- diagnosticOddsRatio( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$positiveLikelihoodRatio &lt;- positiveLikelihoodRatio( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$negativeLikelihoodRatio &lt;- negativeLikelihoodRatio( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$dPrimeSDT &lt;- dPrimeSDT( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$betaSDT &lt;- betaSDT( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$cSDT &lt;- cSDT( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$ASDT &lt;- aSDT( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$bSDT &lt;- bSDT( TP = accuracyStats$TP, TN = accuracyStats$TN, FP = accuracyStats$FP, FN = accuracyStats$FN) accuracyStats$overallUtility &lt;- Uoverall( BR = accuracyStats$baseRate, HR = accuracyStats$TPrate, FAR = accuracyStats$FPrate, UH = utilityHits, UM = utilityMisses, UCR = utilityCorrectRejections, UFA = utilityFalseAlarms) accuracyStats$utilityRatio &lt;- utilityRatio( UH = utilityHits, UM = utilityMisses, UCR = utilityCorrectRejections, UFA = utilityFalseAlarms) accuracyStats$informationGain &lt;- Igain( BR = accuracyStats$baseRate, HR = accuracyStats$TPrate, FAR = accuracyStats$FPrate) #Replace NaN and INF values with NA is.nan.data.frame &lt;- function(x) do.call(cbind, lapply(x, is.nan)) accuracyStats[is.nan.data.frame(accuracyStats)] &lt;- NA accuracyStats &lt;- do.call( data.frame, lapply( accuracyStats, function(x) replace(x, is.infinite(x), NA))) 9.7.3 All Accuracy Statistics The petersenlab package (Petersen, 2024b) contains an R function that estimates the prediction accuracy at every possible cutoff. Code accuracyAtEachCutoff( predicted = mydataSDT$testScore, actual = mydataSDT$disorder, UH = utilityHits, UM = utilityMisses, UCR = utilityCorrectRejections, UFA = utilityFalseAlarms) Code paged_table(accuracyStats) 9.7.4 Youden’s J Statistic 9.7.4.1 Threshold Threshold at maximum combination of sensitivity and specificity: \\(\\text{max}(\\text{sensitivity} + \\text{specificity})\\) Code youdenIndex &lt;- coords(roc(data = mydataSDT, response = disorder, predictor = testScore, smooth = FALSE), x = &quot;best&quot;, best.method = &quot;youden&quot;)[[1]] youdenIndex [1] 0.205 9.7.4.2 Accuracy statistics at cutoff of Youden’s J Statistic Code accuracyStats[head(which( accuracyStats$cutoff &gt;= youdenIndex), 1),] Code accuracyStats[which( accuracyStats$youdenJ == max(accuracyStats$youdenJ, na.rm = TRUE)),] 9.7.5 Closest to the Top Left of the ROC Curve 9.7.5.1 Threshold Threshold where the ROC plot is closest to the Top Left: Code closestToTheTopLeft &lt;- coords(roc( data = mydataSDT, response = disorder, predictor = testScore, smooth = FALSE), x = &quot;best&quot;, best.method = &quot;closest.topleft&quot;)[[1]] 9.7.5.2 Accuracy stats at cutoff where the ROC plot is closest to the Top Left Code accuracyStats[head(which( accuracyStats$cutoff &gt;= closestToTheTopLeft), 1),] 9.7.6 Cutoff that optimizes each of the following criteria: The petersenlab package (Petersen, 2024b) contains an R function that identifies the cutoff that optimizes each of various accuracy estimates. Code optimalCutoff( predicted = mydataSDT$testScore, actual = mydataSDT$disorder, UH = utilityHits, UM = utilityMisses, UCR = utilityCorrectRejections, UFA = utilityFalseAlarms) [[1]] percentAccuracyCutoff percentAccuracyOptimal 1 0.22 74.54545 2 0.52 74.54545 [[2]] percentAccuracyByChanceCutoff percentAccuracyByChanceOptimal 1 2.08 63.63636 [[3]] RIOCCutoff RIOCOptimal 1 0.07 0.725 [[4]] relativeImprovementOverPredictingFromBaseRateCutoff 1 0.22 2 0.52 relativeImprovementOverPredictingFromBaseRateOptimal 1 0.15 2 0.15 [[5]] PPVCutoff PPVOptimal 1 0.52 1 2 0.56 1 3 0.58 1 4 0.70 1 5 0.71 1 6 0.74 1 7 0.77 1 8 0.82 1 9 0.86 1 10 0.96 1 11 2.07 1 [[6]] NPVCutoff NPVOptimal 1 0.07 0.9 [[7]] youdenJCutoff youdenJOptimal 1 0.22 0.45 [[8]] balancedAccuracyCutoff balancedAccuracyOptimal 1 0.22 0.725 [[9]] f1ScoreCutoff f1ScoreOptimal 1 0.22 0.65 [[10]] mccCutoff mccOptimal 1 0.52 0.46291 [[11]] diagnosticOddsRatioCutoff diagnosticOddsRatioOptimal 1 0.49 16.37037 [[12]] positiveLikelihoodRatioCutoff positiveLikelihoodRatioOptimal 1 0.49 11.375 [[13]] negativeLikelihoodRatioCutoff negativeLikelihoodRatioOptimal 1 0.07 0.1944444 [[14]] dPrimeSDTCutoff dPrimeSDTOptimal 1 0.49 1.448454 [[15]] betaSDTCutoff betaSDTOptimal 1 0.07 0.2784011 [[16]] cSDTCutoff cSDTOptimal 1 0.16 0.01498818 [[17]] aSDTCutoff aSDTOptimal 1 0.52 0.825 [[18]] bSDTCutoff bSDTOptimal 1 0.07 0.2862166 [[19]] differenceBetweenPredictedAndObservedCutoff 1 0.14 2 0.15 3 0.16 differenceBetweenPredictedAndObservedOptimal 1 0.058 2 0.058 3 0.058 [[20]] informationGainCutoff informationGainOptimal 1 0.22 0.1465904 [[21]] overallUtilityCutoff overallUtilityOptimal 1 0.22 0.65 9.7.6.1 Percent Accuracy Code accuracyStats$cutoff[which( accuracyStats$percentAccuracy == max( accuracyStats$percentAccuracy, na.rm = TRUE))] [1] 0.22 0.52 9.7.6.2 Percent Accuracy by Chance Code accuracyStats$cutoff[which( accuracyStats$percentAccuracyByChance == max( accuracyStats$percentAccuracyByChance, na.rm = TRUE))] [1] 2.08 9.7.6.3 Relative Improvement Over Chance (ROIC) Code accuracyStats$cutoff[which( accuracyStats$relativeImprovementOverChance == max( accuracyStats$relativeImprovementOverChance, na.rm = TRUE))] [1] 0.07 9.7.6.4 Relative Improvement Over Predicting from the Base Rate Code accuracyStats$cutoff[which( accuracyStats$relativeImprovementOverPredictingFromBaseRate == max( accuracyStats$relativeImprovementOverPredictingFromBaseRate, na.rm = TRUE))] [1] 0.22 0.52 9.7.6.5 Sensitivity Code accuracyStats$cutoff[which( accuracyStats$sensitivity == max( accuracyStats$sensitivity, na.rm = TRUE))] [1] 0.03 9.7.6.6 Specificity Code accuracyStats$cutoff[which( accuracyStats$specificity == max( accuracyStats$specificity, na.rm = TRUE))] [1] 0.52 0.56 0.58 0.70 0.71 0.74 0.77 0.82 0.86 0.96 2.07 2.08 9.7.6.7 Positive Predictive Value Code accuracyStats$cutoff[which( accuracyStats$positivePredictiveValue == max( accuracyStats$positivePredictiveValue, na.rm = TRUE))] [1] 0.52 0.56 0.58 0.70 0.71 0.74 0.77 0.82 0.86 0.96 2.07 9.7.6.8 Negative Predictive Value Code accuracyStats$cutoff[which( accuracyStats$negativePredictiveValue == max( accuracyStats$negativePredictiveValue, na.rm = TRUE))] [1] 0.07 9.7.6.9 Youden’s J Statistic Code accuracyStats$cutoff[which( accuracyStats$youdenJ == max( accuracyStats$youdenJ, na.rm = TRUE))] [1] 0.22 9.7.6.10 Balanced Accuracy Code accuracyStats$cutoff[which( accuracyStats$balancedAccuracy == max( accuracyStats$balancedAccuracy, na.rm = TRUE))] [1] 0.22 9.7.6.11 F1 Score Code accuracyStats$cutoff[which( accuracyStats$f1Score == max( accuracyStats$f1Score, na.rm = TRUE))] [1] 0.22 9.7.6.12 Matthews Correlation Coefficient Code accuracyStats$cutoff[which( accuracyStats$mcc == max( accuracyStats$mcc, na.rm = TRUE))] [1] 0.52 9.7.6.13 Diagnostic Odds Ratio Code accuracyStats$cutoff[which( accuracyStats$diagnosticOddsRatio == max( accuracyStats$diagnosticOddsRatio, na.rm = TRUE))] [1] 0.49 9.7.6.14 Positive Likelihood Ratio Code accuracyStats$cutoff[which( accuracyStats$positiveLikelihoodRatio == max( accuracyStats$positiveLikelihoodRatio, na.rm = TRUE))] [1] 0.49 9.7.6.15 Negative Likelihood Ratio Code accuracyStats$cutoff[which( accuracyStats$negativeLikelihoodRatio == min( accuracyStats$negativeLikelihoodRatio, na.rm = TRUE))] [1] 0.07 9.7.6.16 \\(d&#39;\\) Sensitivity Code accuracyStats$cutoff[which( accuracyStats$dPrimeSDT == max( accuracyStats$dPrimeSDT, na.rm = TRUE))] [1] 0.49 9.7.6.17 \\(A\\) (Non-Parametric) Sensitivity Code accuracyStats$cutoff[which( accuracyStats$ASDT == max( accuracyStats$ASDT, na.rm = TRUE))] [1] 0.22 9.7.6.18 \\(\\beta\\) Bias Code accuracyStats$cutoff[which(abs( accuracyStats$betaSDT) == min(abs( accuracyStats$betaSDT), na.rm = TRUE))] [1] 0.07 9.7.6.19 \\(c\\) Bias Code accuracyStats$cutoff[which(abs( accuracyStats$cSDT) == min(abs( accuracyStats$cSDT), na.rm = TRUE))] [1] 0.16 9.7.6.20 \\(b\\) (Non-Parametric) Bias Code accuracyStats$cutoff[which(abs( accuracyStats$bSDT) == min(abs( accuracyStats$bSDT), na.rm = TRUE))] [1] 0.07 9.7.6.21 Mean difference between predicted and observed values (Miscalibration) Code accuracyStats$cutoff[which(abs( accuracyStats$differenceBetweenPredictedAndObserved) == min(abs( accuracyStats$differenceBetweenPredictedAndObserved), na.rm = TRUE))] [1] 0.14 0.15 0.16 9.7.6.22 Overall Utility Code accuracyStats$cutoff[which( accuracyStats$overallUtility == max( accuracyStats$overallUtility, na.rm = TRUE))] [1] 0.22 9.7.6.23 Information Gain Code accuracyStats$cutoff[which( accuracyStats$informationGain == max( accuracyStats$informationGain, na.rm = TRUE))] [1] 0.22 9.8 Regression for Prediction of Continuous Outcomes When predicting a continuous outcome, regression is particularly relevant (or multiple regression, when dealing with multiple predictors). Regression takes the general form in Equation (9.73): \\[\\begin{equation} y = b_0 + b_1 \\cdot x_1 + e \\tag{9.73} \\end{equation}\\] where \\(y\\) is the outcome, \\(b_0\\) is the intercept, \\(b_1\\) is the slope of the association between the predictor (\\(x_1\\)) and outcome, and \\(e\\) is the error term. 9.9 Pseudo-Prediction Consider the following example where you have one predictor and one outcome, as shown in Table 9.1. Table 9.1: Example Data of Predictor (x1) and Outcome (y) Used for Regression Model. y x1 7 1 13 2 29 7 10 2 Using the data, the best fitting regression model is: \\(y = 3.98 + 3.59 \\cdot x_1\\). In this example, the \\(R^2\\) is \\(0.98\\). The equation is not a perfect prediction, but with a single predictor, it captures the majority of the variance in the outcome. Now consider the following example where you add a second predictor to the data above, as shown in Table 9.2. Table 9.2: Example Data of Predictors (x1 and x2) and Outcome (y) Used for Regression Model. y x1 x2 7 1 3 13 2 5 29 7 1 10 2 2 With the second predictor, the best fitting regression model is: \\(y = 0.00 + 4.00 \\cdot x_1 + 1.00 \\cdot x_2\\). In this example, the \\(R^2\\) is \\(1.00\\). The equation with the second predictor provides a perfect prediction of the outcome. Providing perfect prediction with the right set of predictors is the dream of multiple regression. So, in psychology, we often add predictors to incrementally improve prediction. Knowing how much variance would be accounted for by random chance follows Equation (9.74): \\[\\begin{equation} E(R^2) = \\frac{K}{n-1} \\tag{9.74} \\end{equation}\\] where \\(E(R^2)\\) is the expected value of \\(R^2\\) (the proportion of variance explained), \\(K\\) is the number of predictors, and \\(n\\) is the sample size. The formula demonstrates that the more predictors in the regression model, the more variance will be accounted for by chance. With many predictors and a small sample, you can account for a large share of the variance merely by chance—this would be an example of pseudo-prediction. As an example, consider that we have 13 predictors to predict behavior problems for 43 children. Assume that, with 13 predictors, we explain 38% of the variance (\\(R^2 = .38; r = .62\\)). Explaining more than 20–30% of the variance can be a big deal in psychology. We explained a lot of the variance in the outcome, but it is important to consider how much variance could have been explained by random chance: \\(E(R^2) = \\frac{K}{n-1} = \\frac{13}{43 - 1} = .31\\). We expect to explain 31% of the variance, by chance, in the outcome. So, 82% of the variance explained was likely spurious. As the sample size increases, the spuriousness decreases. Adjusted \\(R^2\\) accounts for the number of predictors in the model, based on how much would be expected to be accounted for by chance. But adjusted \\(R^2\\) also has its problems. 9.9.1 Multicollinearity Multicollinearity occurs when two or more predictors in a regression model are highly correlated. The problem is that it makes it challenging to estimate the regression coefficients accurately. Multicollinearity in multiple regression is depicted conceptually in Figure 9.35. Figure 9.35: Conceptual Depiction of Multicollinearity in Multiple Regression. Consider the following example where you have two predictors and one outcome, as shown in Table 9.3. Table 9.3: Example Data of Predictors (x1 and x2) and Outcome (y) Used for Regression Model. y x1 x2 9 2.0 4 11 3.0 6 17 4.0 8 3 1.0 2 21 5.0 10 13 3.5 7 The second measure is not very good—it is exactly twice the value of the first measure. This means that there are different prediction equation possibilities that are equally good—see Equations in (9.75): \\[ \\begin{aligned} 2x_2 &amp;= y \\\\ 0x_1 + 2x_2 &amp;= y \\\\ 4x_1 &amp;= y \\\\ 4x_1 + 0x_2 &amp;= y \\\\ 2x_1 + 1x_2 &amp;= y \\\\ 5x_1 - 0.5x_2 &amp;= y \\\\ ... &amp;= y \\end{aligned} \\tag{9.75} \\] Then, what are the regression coefficients? We do not know, and we could come up with arbitrary estimates with an enormous standard error around each estimate. Any predictors that have a correlation above ~ \\(r = .30\\) with each other could have an impact on the confidence interval of the regression coefficient. As the correlations among the predictors increase, the chance of getting an arbitrary answer increases, sometimes called “bouncing betas.” So, it is important to examine a correlation matrix of the predictors before putting them in the same regression model. You can also examine indices such as variance inflation factor (VIF). Generalized VIF (GVIF) values are estimated below using the car package (Fox et al., 2022). Code set.seed(52242) mydataSDT$collinearPredictor &lt;- mydataSDT$ndka + rnorm(nrow(mydataSDT), sd = 20) collinearRegression_model &lt;- lm( s100b ~ ndka + gender + age + wfns + collinearPredictor, data = mydataSDT) vif(collinearRegression_model) GVIF Df GVIF^(1/(2*Df)) ndka 6.300087 1 2.509997 gender 1.149601 1 1.072194 age 1.127608 1 1.061889 wfns 1.207583 4 1.023858 collinearPredictor 6.405020 1 2.530814 To address multicollinearity, you can drop a redundant predictor or you can also use principal component analysis or factor analysis of the predictors to reduce the predictors down to a smaller number of meaningful predictors. For a meaningful answer in a regression framework that is precise and confident, you need a low level of intercorrelation among predictors, unless you have a very large sample size. However, multicollinearity does not bias parameter estimates (i.e., multicollinearity does not lead to mean error). Instead, multicollinearity increases the uncertainty (i.e., standard errors) around the parameter estimates (archived at https://perma.cc/DJ7L-TCUK), which makes it more challenging to detect an effect as statistically significant. Some forms of multicollinearity are ignorable (archived at https://perma.cc/2JV5-2QEZ), including when the multicollinearity is among (a) the control variables rather than the variables of interest, (b) the powers (e.g., quadratic term) or products (e.g., interaction term) of other variables, or (c) dummy-coded categories. Ultimately, it is important to examine the question of interest, even if that means inclusion of predictors that are inter-correlated in a regression model (archived at https://perma.cc/DJ7L-TCUK). However, it would not make sense to include two predictors that are perfectly correlated, because they are redundant. 9.10 Ways to Improve Prediction Accuracy On the whole, experts’ predictions are inaccurate. Experts’ predictions from many different domains tend to be inaccurate, including political scientists (Tetlock, 2017), physicians (Koehler et al., 2002), clinical psychologists (Oskamp, 1965), stock market traders and corporate financial officers (Skala, 2008), seismologists’ predictions of earthquakes (Hough, 2016), economists’ predictions about the economy (Makridakis et al., 2009), lawyers (Koehler et al., 2002), and business managers (Russo &amp; Schoemaker, 1992). The most common pattern of experts’ predictions is that they show overextremity, that is, their predictions have probability judgments that tend to be too extreme, as described in Section 9.4.3. Overextremity of experts’ predictions likely reflects over-confidence. The degree of confidence of a person’s predictions is often not a good indicator of the accuracy of their predictions [and confidence and prediction accuracy are sometimes inversely associated; Silver (2012)]. Cognitive biases including the anchoring bias (Tversky &amp; Kahneman, 1974), the confirmation bias (Hoch, 1985; Koriat et al., 1980), and base rate neglect (Eddy, 1982; Koehler et al., 2002) could contribute to over-confidence of predictions. Poorly calibrated predictions are especially likely when the base rate is very low (e.g., suicide), as is often the case in clinical psychology, or when the base rate is very high (Koehler et al., 2002). Nevertheless, there are some domains that have shown greater predictive accuracy, from which we may learn what practices may lead to greater accuracy. For instance, experts have shown stronger predictive accuracy in weather forecasting (Murphy &amp; Winkler, 1984), horse race betting (J. E. V. Johnson &amp; Bruce, 2001), and playing the card game of bridge (Keren, 1987), but see Koehler et al. (2002) for exceptions. Here are some potential ways to improve the accuracy (and honesty) of predictions and judgments: Provide appropriate anchoring of your predictions to the base rate of the phenomenon you are predicting. To the extent that the base rate of the event you are predicting is low, more extreme evidence should be necessary to consistently and accurately predict that the event will occur. Applying Bayes’ theorem and Bayesian approaches can help you appropriately weigh base rate and evidence. Include multiple predictors, ideally from different measures and measurement methods. Include the predictors with the strongest validity based on theory of the causal process and based on criterion-related validity. When possible, aggregate multiple perspectives of predictions, especially predictions made independently (from different people/methods/etc.). The “wisdom of the crowd” is often more accurate than individuals’ predictions, including predictions by so-called “experts” (Silver, 2012). A goal of prediction is to capture as much signal as possible and as little noise (error) as possible (Silver, 2012). Parsimony (i.e., not having too many predictors) can help reduce the amount of error variance captured by the prediction model. However, to accurately model complex systems like human behavior, the brain, etc., complex models may be necessary. Nevertheless, strong theory of the causal processes and dynamics may be necessary to develop accurate complex models. Although incorporating theory can be helpful, provide more weight to empiricism than to theory, until our theories and measures are stronger. Ideally, we would use theory to design a model that mirrors the causal system, with accurate measures of each process in the system, so we could make accurate predictions. However, as described in Section 5.3.1.3.3, our psychological theories of the causal processes that influence outcomes are not yet very strong. Until we have stronger theories that specify the causal process for a given outcome, and until we have accurate measures of those causal processes, actuarial approaches are likely to be most accurate, as discussed in Chapter 10. At the same time, keep in mind that measures in psychology, and their resulting data, are often noisy. As a result, theoretically (conceptually) informed empirical approaches may lead to more accuracy than empiricism alone. Use an empirically validated and cross-validated statistical algorithm to combine information from the predictors in a formalized way. Give each predictor appropriate weight in the statistical algorithm, according to its strength of association with the outcome. Use measures with strong reliability and validity for assessing these processes to be used in the algorithm. Cross-validation will help reduce the likelihood that your model is fitting to noise and will maximize the likelihood that the model predicts accurately when applied to new data (i.e., the model’s predictions accurately generalize), as described in Section 10.4. When presenting your predictions, acknowledge what you do not know. Express your predictions in terms of probabilistic estimates and present the uncertainty in your predictions with confidence intervals [even though bolder, more extreme predictions tend to receive stronger television ratings; Silver (2012)]. Qualify your predictions by identifying and noting counter-examples that would not be well fit by your prediction model, such as extreme cases, edge cases, and “broken leg” (Meehl, 1957) cases. Provide clear, consistent, and timely feedback on the outcomes of the predictions to the people making the predictions (Bolger &amp; Önkal-Atay, 2004). Be self-critical about your predictions. Update your judgments based on their accuracy, rather than trying to confirm your beliefs (Atanasov et al., 2020). In addition to considering the accuracy of the prediction, consider the quality of the prediction process, especially when random chance is involved to a degree (such as in poker) (Silver, 2012). Work to identify and mitigate potential blindspots; be aware of cognitive biases, such as confirmation bias and base rate neglect. 9.11 Conclusion Human behavior is challenging to predict. People commonly make cognitive pseudo-prediction errors, such as the confusion of inverse probabilities. People also tend to ignore base rates when making predictions. When the base rate of a behavior is very low or very high, you can be highly accurate in predicting the behavior by predicting from the base rate. Thus, you cannot judge how accurate your prediction is until you know how accurate your predictions would be by random chance. Moreover, maximizing percent accuracy may not be the ultimate goal because different errors have different costs. Though there are many types of accuracy, there are two broad types: discrimination and calibration—and they are orthogonal. Discrimination accuracy is frequently evaluated with the area under the receiver operating characteristic curve, or with sensitivity and specificity, or standardized regression coefficients. Calibration accuracy is frequently evaluated graphically and with various indices. Sensitivity and specificity depend on the cutoff. Therefore, the optimal cutoff depends on the purposes of the assessment and how much one weights the various costs of the different types of errors: false negatives and false positives. It is important to evaluate both discrimination and calibration when evaluating prediction accuracy. 9.12 Suggested Readings Steyerberg et al. (2010); Meehl &amp; Rosen (1955); Treat &amp; Viken (2023); Wiggins (1973) 9.13 Exercises 9.13.1 Questions Note: Several of the following questions use data from the survivorship of the Titanic accident. The data are publicly available (https://hbiostat.org/data/; archived at https://perma.cc/B4AV-YH4V). The Titanic data file for these exercises is located on the book’s page of the Open Science Framework (https://osf.io/3pwza) and in the GitHub repo (https://github.com/isaactpetersen/Principles-Psychological-Assessment/tree/main/Data). Every record in the data set represents a passenger—including the passenger’s age, sex, passenger class, number of siblings/spouses aboard (sibsp), number of parents/children aboard (parch) and, whether the passenger survived the accident. I used these variables to create a prediction model (based on a logistic regression model using Leave-10-out cross-validation) for whether the passenger survived the accident. The model’s prediction for the passenger’s likelihood of survival are in the variable called “prediction”. What are the two main types of prediction accuracy? Define each. How can you quantify each? What is an example of an index that combines both main types of prediction accuracy? Provide the following indexes of overall prediction accuracy for the prediction model in predicting whether a passenger survived the Titanic: Mean error (bias) Mean absolute error (MAE) Mean squared error (MSE) Root mean squared error (RMSE) Mean percentage error (MPE) Mean absolute percentage error (MAPE) Symmetric mean absolute percentage error (sMAPE) Mean absolute scaled error (MASE) Root mean squared log error (RMSLE) Coefficient of determination (\\(R^2\\)) Adjusted \\(R^2\\) (\\(R^2_{adj}\\)) Predictive \\(R^2\\) Based on the mean error for the prediction model you found in 2a, what does this indicate? Create two receiver operating characteristic (ROC) curves for the prediction model in predicting whether a passenger survived the Titanic: one empirical ROC curve and one smooth ROC curve. What is the area under the ROC curve (AUC) for the prediction model in predicting whether a passenger survived the Titanic? What does this indicate? Create a calibration plot. Are the predictions well-calibrated? Provide empirical evidence and support your inferences with interpretation of the calibration plot. If the predictions are miscalibrated, describe the type of miscalibration present. You predict that a passenger survived the Titanic if their predicted probability of survival is .50 or greater. Create a confusion matrix (2x2 matrix of prediction accuracy) for Titanic survival using this threshold. Make sure to include the marginal cells. Label each cell. Enter the number and proportion in each cell. Using the 2x2 prediction matrix, identify or calculate the following: Selection ratio Base rate Percent accuracy Percent accuracy by chance Percent accuracy predicting from the base rate Relative improvement over chance (ROIC) Relative improvement over predicting from the base rate Sensitivity (true positive rate [TPR]) Specificity (true negative rate [TNR]) False negative rate (FNR) False positive rate (FPR) Positive predictive value (PPV) Negative predictive value (NPV) False discovery rate (FDR) False omission rate (FOR) Diagnostic odds ratio Youden’s J statistic Balanced accuracy \\(F_1\\) score Matthews correlation coefficient (MCC) Positive likelihood ratio Negative likelihood ratio \\(d&#39;\\) sensitivity \\(A\\) (non-parametric) sensitivity \\(\\beta\\) bias \\(c\\) bias aa. \\(b\\) (non-parametric) bias ab. Miscalibration (mean difference between predicted and observed values; based on 10 groups) ac. Information gain In terms of percent accuracy, how much more accurate are the predictions compared to predicting from the base rate? What would happen to sensitivity and specificity if you raise the selection ratio? What would happen if you lower the selection ratio? For your assessment goals, it is 4 times more important to identify survivors than to identify non-survivors. Consistent with these assessment goals, you specify the following utility for each of the four outcomes: hits: 1; misses: 0; correct rejections: 0.25; false alarms: 0. What is the utility ratio? What is the overall utility (\\(U_\\text{overall}\\)) of the current cutoff? What cutoff has the highest overall utility? Find the optimal cutoff threshold that optimizes each of the following criteria: Youden’s J statistic Closest to the top left of the ROC curve Percent accuracy Percent accuracy by chance Relative improvement over chance (ROIC) Relative improvement over predicting from the base rate Sensitivity (true positive rate [TPR]) Specificity (true negative rate [TNR]) Positive predictive value (PPV) Negative predictive value (NPV) Diagnostic odds ratio Balanced accuracy \\(F_1\\) score Matthews correlation coefficient (MCC) Positive likelihood ratio Negative likelihood ratio \\(d&#39;\\) sensitivity \\(A\\) (non-parametric) sensitivity \\(\\beta\\) bias \\(c\\) bias \\(b\\) (non-parametric) bias Miscalibration (mean difference between predicted and observed values; based on 10 groups) Overall utility Information gain A company develops a test that seeks to determine whether someone has been previously infected with a novel coronavirus (COVID-75) based on the presence of antibodies in their blood. You take the test and your test result is negative (i.e., the test says that you have not been infected). Your friend takes the test and their test result is positive for coronavirus (i.e., the test says that your friend has been infected). Assume the prevalence of the coronavirus is 5%, the sensitivity of the test is .90, and the specificity of the test is .95. What is the probability that you actually had the coronavirus? What is the probability that your friend actually had the coronavirus? Your friend thinks that, given their positive test, that they have the antibodies (and thus may have immunity). What is the problem with your friend’s logic? What logical fallacy is your friend demonstrating? Why is it challenging to interpret a positive test in this situation? You just took a screening test for a genetic disease. Your test result is positive (i.e., the tests says that you have the disease). Assume the probability of having the disease is 0.5%, the probability of a positive test is 2%, and the probability of a positive test if you have the disease is 99%. What is the probability that you have the genetic disease? You just took a screening test (Test A) for a virus. Your test result is positive. Assume the base rate of the virus is .05%. Test A has a sensitivity of .95 and a specificity of .90. What is your probability of having the virus after testing positive on Test A? After getting the results back from Test A, the physician wants greater confidence regarding whether you have the virus given its low base rate, so the physician orders a second test, Test B. You test positive on Test B. Test B has a sensitivity of .80 and a specificity of .95. Assuming the errors of the Tests A and B are independent, what is your updated probability of having the virus? 9.13.2 Answers The two main types of prediction accuracy are discrimination and calibration. Discrimination refers to the ability of a prediction model to separate/differentiate data into classes (e.g., survived versus died). Calibration for a prediction model refers to how well the predicted probability of an event (e.g., survival) matches the true probability of an event. You can quantify discrimination with AUC; you can quantify various aspects of discrimination with sensitivity, specificity, positive predictive value, and negative predictive value). You can quantify degree of (poor) calibration with Spiegelhalter’s \\(z\\), though other metrics also exist (e.g., Brier scores and the Hosmer-Lemeshow goodness-of-fit statistic). \\(R^2\\) is an overall index of accuracy that combines both discrimination and calibration. Mean error (bias): \\(0.0006\\) Mean absolute error (MAE): \\(0.30\\) Mean squared error (MSE): \\(0.15\\) Root mean squared error (RMSE): \\(0.39\\) Mean percentage error (MPE): undefined, but when dropping undefined values: \\(36.74\\%\\) Mean absolute percentage error (MAPE): undefined, but when dropping undefined values: \\(36.74\\%\\) Symmetric mean absolute percentage error (sMAPE): \\(70.16\\%\\) Mean absolute scaled error (MASE): \\(0.62\\) Root mean squared log error (RMSLE): \\(0.27\\) Coefficient of determination (\\(R^2\\)): \\(.37\\) Adjusted \\(R^2\\) (\\(R^2_{adj}\\)): \\(.37\\) Predictive \\(R^2\\): \\(.37\\) The small mean error/bias \\((0.0006)\\) indicates that predictions did not consistently under- or over-estimate the actual values to a considerable degree. Empirical ROC curve: Figure 9.36: Exercise 4: Empirical Receiver Operating Characteristic Curve. Smooth ROC curve: Figure 9.37: Exercise 4: Smooth Receiver Operating Characteristic Curve. The AUC is \\(.84\\). AUC indicates the probability that a randomly selected case has a higher test result than a randomly selected control. Thus, the probability is \\(84\\%\\) that a randomly selected passenger who survived had a higher predicted probability of survival (based on the prediction model) than a randomly selected passenger who died. The AUC of \\(.84\\) indicates that the prediction model was moderately accurate in terms of discrimination. Calibration plot: Figure 9.38: Exercise 5: Calibration Plot of Predicted Probability Versus Observed Probability. In general, the predictions are well-calibrated. There is not significant miscalibration according to Spiegehalter’s \\(z\\) \\((0.31, p = .76)\\). This is verified graphically in the calibration plot, in which the predicted probabilities fall mostly near the actual/observed probabilities. However, based on the calibration plot, there does appear to be some miscalibration. When the predicted probability of survival was ~60%, the actual probability of survival was lower (~40%). This pattern of miscalibration is known as overprediction, as depicted in Figure 1 of Lindhiem et al. (2020). The 2x2 prediction matrix is below: Figure 9.39: Exercise 6: 2x2 Prediction Matrix. TP = true positives; TN = true negatives; FP = false positives; FN = false negatives; BR = base rate; SR = selection ratio. Selection ratio: \\(.39\\) Base rate: \\(.41\\) Percent accuracy: \\(77\\%\\) Percent accuracy by chance: \\(52\\%\\) Percent accuracy predicting from the base rate: \\(59\\%\\) Relative improvement over chance (ROIC): \\(.51\\) Relative improvement over predicting from the base rate: \\(.22\\) Sensitivity (true positive rate [TPR]): \\(.70\\) Specificity (true negative rate [TNR]): \\(.83\\) False negative rate (FNR): \\(.30\\) False positive rate (FPR): \\(.17\\) Positive predictive value (PPV): \\(.73\\) Negative predictive value (NPV): \\(.80\\) False discovery rate (FDR): \\(.27\\) False omission rate (FOR): \\(.20\\) Diagnostic odds ratio: \\(11.05\\) Youden’s J statistic: \\(.53\\) Balanced accuracy: \\(.76\\) \\(F_1\\) score: \\(0.72\\) Matthews correlation coefficient (MCC): \\(.53\\) Positive likelihood ratio: \\(4.01\\) Negative likelihood ratio: \\(0.36\\) \\(d&#39;\\) sensitivity: \\(1.46\\) \\(A\\) (non-parametric) sensitivity: \\(0.83\\) \\(\\beta\\) bias: \\(1.35\\) \\(c\\) bias: \\(0.21\\) aa. \\(b\\) (non-parametric) bias: \\(1.30\\) ab. Miscalibration (mean difference between predicted and observed values): \\(0.16\\) ac. Information gain: \\(0.21\\) Predicting from the base rate would have a percent accuracy of \\(59\\%\\). So, the predictions increase the percent accuracy by \\(18\\%\\). If you raise the selection ratio (i.e., predict more people survived) sensitivity will increase whereas specificity will decrease. If you lower the selection ratio, specificity will increase and sensitivity will decrease. The utility ratio is \\(0.25\\). The overall utility (\\(U_\\text{overall}\\)) of the cutoff is \\(0.41\\). The cutoff with the highest overall utility is \\(0.266\\). The cutoff that optimizes each of the following criteria: Youden’s J statistic: \\(0.556\\) Closest to the top left of the ROC curve: \\(0.391\\) Percent accuracy: \\(0.618\\) Percent accuracy by chance: 1 (i.e., predicting from the base rate—that nobody will survive) Relative improvement over chance (ROIC): \\(.017, .023, .027, .031, .032, .035, .035, .035, .038, .039\\) Relative improvement over predicting from the base rate: \\(.618\\) Sensitivity (true positive rate [TPR]): 0 (i.e., predicting that everyone will survive will minimize false negatives) Specificity: 1 (i.e., predicting that nobody will survive will minimize false positives) Positive predictive value (PPP): \\(0.875\\) Negative predictive value (NPV): \\(0.017, 0.023, 0.027, 0.031, 0.032, 0.035, 0.035, 0.035, 0.038, 0.039\\) Diagnostic odds ratio: \\(0.875\\) Balanced accuracy: \\(0.556\\) \\(F_1\\) score: \\(0.391\\) Matthews correlation coefficient (MCC): \\(0.618\\) Positive likelihood ratio: \\(0.875\\) Negative likelihood ratio: \\(0.017, 0.023, 0.027, 0.031, 0.032, 0.035, 0.035, 0.035, 0.038, 0.039\\) Miscalibration (mean difference between predicted and observed values): \\(0.017–0.085\\) \\(d&#39;\\) sensitivity: \\(0.833\\) \\(A\\) (non-parametric) sensitivity \\(0.316\\) \\(\\beta\\) bias \\(0.017, 0.023, 0.027, 0.031, 0.032, 0.035, 0.035, 0.035, 0.038, 0.039, 0.979\\) \\(c\\) bias \\(0.387\\) \\(b\\) (non-parametric) bias \\(0.017\\) Overall utility: \\(0.266\\) Information gain: \\(0.631\\) Based on negative predictive value (i.e., the probability of no disease given a negative test), the probability that you actually had the coronavirus is less than 1 in 100 \\((.006)\\). Based on positive predictive value (i.e., the probability of disease given a positive test), the probability that your friend actually had the coronavirus is less than 50% \\((.49)\\). The problem with your friend’s logic is that your friend is assuming they have the antibodies when chances are more likely that they do not. Your friend is likely demonstrating the fallacy of base rate neglect. A positive test on a screening device is hard to interpret in this situation because of the low base rate. “Even with a very accurate test, the fewer people in a population who have a condition, the more likely it is that an individual’s positive result is wrong.” For more info, see here: https://www.scientificamerican.com/article/coronavirus-antibody-tests-have-a-mathematical-pitfall/ (archived at https://perma.cc/GL9F-EVPH) According to Bayes’ theorem, the probability that you have the disease is \\(24.75\\%\\). For more info, see here: https://www.scientificamerican.com/article/what-is-bayess-theorem-an/ (archived at https://perma.cc/GM6B-5MEP) According to Bayes’ theorem, the probability that you have the virus after testing positive on Test A is \\(4.6\\%\\). According to Bayes’ theorem, the updated probability that you have the virus after testing positive on both Tests A and B is \\(43.3\\%\\). References Atanasov, P., Witkowski, J., Ungar, L., Mellers, B., &amp; Tetlock, P. (2020). Small steps to accuracy: Incremental belief updaters are better forecasters. Organizational Behavior and Human Decision Processes, 160, 19–35. https://doi.org/10.1016/j.obhdp.2020.02.001 Austin, P. C., &amp; Steyerberg, E. W. (2014). Graphical assessment of internal and external calibration of logistic regression models by using loess smoothers. Statistics in Medicine, 33(3), 517–535. https://doi.org/10.1002/sim.5941 Ballesteros-Pérez, P., González-Cruz, M. C., &amp; Mora-Melià, D. (2018). Explaining the Bayes’ theorem graphically. Proceedings of the International Technology, Education and Development Conference. Bickel, J. E., &amp; Kim, S. D. (2008). Verification of The Weather Channel probability of precipitation forecasts. Monthly Weather Review, 136(12), 4867–4881. https://doi.org/10.1175/2008MWR2547.1 Bolger, F., &amp; Önkal-Atay, D. (2004). The effects of feedback on judgmental interval predictions. International Journal of Forecasting, 20(1), 29–39. https://doi.org/10.1016/S0169-2070(03)00009-8 Charba, J. P., &amp; Klein, W. H. (1980). Skill in precipitation forecasting in the National Weather Service. Bulletin of the American Meteorological Society, 61(12), 1546–1555. https://doi.org/10.1175/1520-0477(1980)061&lt;1546:SIPFIT&gt;2.0.CO;2 Eddy, D. M. (1982). Probabilistic reasoning in clinical medicine: Problems and opportunities. In D. Kahneman, P. Slovic, &amp; A. Tversky (Eds.), Judgment under uncertainty: Heuristics and biases (pp. 249–267). Cambridge University Press. Farrington, D. P., &amp; Loeber, R. (1989). Relative improvement over chance (RIOC) and phi as measures of predictive efficiency and strength of association in 2×2 tables. Journal of Quantitative Criminology, 5(3), 201–213. https://doi.org/10.1007/BF01062737 Farris, C., Treat, T. A., Viken, R. J., &amp; McFall, R. M. (2008). Perceptual mechanisms that characterize gender differences in decoding women’s sexual intent. Psychological Science, 19(4), 348–354. https://doi.org/10.1111/j.1467-9280.2008.02092.x Farris, C., Viken, R. J., Treat, T. A., &amp; McFall, R. M. (2006). Heterosocial perceptual organization: Application of the choice model to sexual coercion. Psychological Science (0956-7976), 17(10), 869–875. https://doi.org/10.1111/j.1467-9280.2006.01796.x Fox, J., Weisberg, S., &amp; Price, B. (2022). Car: Companion to applied regression. https://CRAN.R-project.org/package=car Gneiting, T., &amp; Walz, E.-M. (2021). Receiver operating characteristic (ROC) movies, universal ROC (UROC) curves, and coefficient of predictive ability (CPA). Machine Learning. https://doi.org/10.1007/s10994-021-06114-3 Harrell, F. (2015). Regression modeling strategies: With applications to linear models, logistic and ordinal regression, and survival analysis. Springer. Harrell, Jr., F. E. (2021). rms: Regression modeling strategies. https://CRAN.R-project.org/package=rms Hoch, S. J. (1985). Counterfactual reasoning and accuracy in predicting personal events. Journal of Experimental Psychology: Learning, Memory, and Cognition, 11(4), 719–731. https://doi.org/10.1037/0278-7393.11.1-4.719 Hough, S. E. (2016). Predicting the unpredictable: The tumultuous science of earthquake prediction. Princeton University Press. Hyndman, R. J., &amp; Athanasopoulos, G. (2018). Forecasting: Principles and practice (2nd ed.). OTexts. Johnson, J. E. V., &amp; Bruce, A. C. (2001). Calibration of subjective probability judgments in a naturalistic setting. Organizational Behavior and Human Decision Processes, 85(2), 265–290. https://doi.org/10.1006/obhd.2000.2949 Keren, G. (1987). Facing uncertainty in the game of bridge: A calibration study. Organizational Behavior and Human Decision Processes, 39(1), 98–114. https://doi.org/10.1016/0749-5978(87)90047-1 Kessler, R. C., Bossarte, R. M., Luedtke, A., Zaslavsky, A. M., &amp; Zubizarreta, J. R. (2020). Suicide prediction models: A critical review of recent research with recommendations for the way forward. Molecular Psychiatry, 25(1), 168–179. https://doi.org/10.1038/s41380-019-0531-0 Koehler, D. J., Brenner, L., &amp; Griffin, D. (2002). The calibration of expert judgment: Heuristics and biases beyond the laboratory. In T. Gilovich, D. Griffin, &amp; D. Kahneman (Eds.), Heuristics and biases: The psychology of intuitive judgment. Cambridge University Press. Koriat, A., Lichtenstein, S., &amp; Fischhoff, B. (1980). Reasons for confidence. Journal of Experimental Psychology: Human Learning and Memory, 6(2), 107–118. https://doi.org/10.1037/0278-7393.6.2.107 Kundu, S., Aulchenko, Y. S., &amp; Janssens, A. C. J. W. (2020). PredictABEL: Assessment of risk prediction models. https://CRAN.R-project.org/package=PredictABEL Lele, S. R., Keim, J. L., &amp; Solymos, P. (2019). ResourceSelection: Resource selection (probability) functions for use-availability data. https://github.com/psolymos/ResourceSelection Lindhiem, O., Petersen, I. T., Mentch, L. K., &amp; Youngstrom, E. A. (2020). The importance of calibration in clinical psychology. Assessment, 27(4), 840–854. https://doi.org/10.1177/1073191117752055 Makridakis, S., Hogarth, R. M., &amp; Gaba, A. (2009). Forecasting and uncertainty in the economic and business world. International Journal of Forecasting, 25(4), 794–812. https://doi.org/10.1016/j.ijforecast.2009.05.012 Meehl, P. E. (1957). When shall we use our heads instead of the formula? Journal of Counseling Psychology, 4(4), 268–273. https://doi.org/10.1037/h0047554 Meehl, P. E., &amp; Rosen, A. (1955). Antecedent probability and the efficiency of psychometric signs, patterns, or cutting scores. Psychological Bulletin, 52(3), 194–216. https://doi.org/10.1037/h0048070 Metz, C. E., Goodenough, D. J., &amp; Rossmann, K. (1973). Evaluation of receiver operating characteristic curve data in terms of information theory, with applications in radiography. Radiology, 109(2), 297–303. https://doi.org/10.1148/109.2.297 Morley, S. K., Brito, T. V., &amp; Welling, D. T. (2018). Measures of model performance based on the log accuracy ratio. Space Weather, 16(1), 69–88. https://doi.org/10.1002/2017SW001669 Murphy, A. H., &amp; Winkler, R. L. (1984). Probability forecasting in meterology. Journal of the American Statistical Association, 79(387), 489–500. https://doi.org/10.2307/2288395 Oskamp, S. (1965). Overconfidence in case-study judgments. Journal of Consulting Psychology, 29(3), 261–265. https://doi.org/10.1037/h0022125 Petersen, I. T. (2024b). petersenlab: Package of R functions for the Petersen Lab. https://doi.org/10.5281/zenodo.7602890 Robin, X., Turck, N., Hainard, A., Tiberti, N., Lisacek, F., Sanchez, J.-C., &amp; Müller, M. (2021). pROC: Display and analyze ROC curves. http://expasy.org/tools/pROC/ Russo, J. E., &amp; Schoemaker, P. J. (1992). Managing overconfidence. Sloan Management Review, 33(2), 7. Silver, N. (2012). The signal and the noise: Why so many predictions fail–but some don’t. Penguin. Skala, D. (2008). Overconfidence in psychology and finance–an interdisciplinary literature review. Bank i Kredyt, 4, 33–50. Somoza, E., Soutullo-Esperon, L., &amp; Mossman, D. (1989). Evaluation and optimization of diagnostic tests using receiver operating characteristic analysis and information theory. International Journal of Bio-Medical Computing, 24(3), 153–189. https://doi.org/10.1016/0020-7101(89)90029-9 Stanislaw, H., &amp; Todorov, N. (1999). Calculation of signal detection theory measures. Behavior Research Methods, Instruments, &amp; Computers, 31(1), 137–149. https://doi.org/10.3758/bf03207704 Stevens, R. J., &amp; Poppe, K. K. (2020). Validation of clinical prediction models: What does the “calibration slope” really measure? Journal of Clinical Epidemiology, 118, 93–99. https://doi.org/10.1016/j.jclinepi.2019.09.016 Steyerberg, E. W., &amp; Vergouwe, Y. (2014). Towards better clinical prediction models: Seven steps for development and an ABCD for validation. European Heart Journal, 35(29), 1925–1931. https://doi.org/10.1093/eurheartj/ehu207 Steyerberg, E. W., Vickers, A. J., Cook, N. R., Gerds, T., Gonen, M., Obuchowski, N., Pencina, M. J., &amp; Kattan, M. W. (2010). Assessing the performance of prediction models: A framework for traditional and novel measures. Epidemiology, 21(1), 128–138. https://doi.org/10.1097/EDE.0b013e3181c30fb2 Swets, J. A., Dawes, R. M., &amp; Monahan, J. (2000). Psychological science can improve diagnostic decisions. Psychological Science in the Public Interest, 1, 1–26. https://doi.org/10.1111/1529-1006.001 Tetlock, P. E. (2017). Expert political judgment: How good is it? How can we know? - New edition. Princeton University Press. Tofallis, C. (2015). A better measure of relative prediction accuracy for model selection and model estimation. Journal of the Operational Research Society, 66(8), 1352–1362. https://doi.org/10.1057/jors.2014.103 Treat, T. A., &amp; Viken, R. J. (2023). Measuring test performance with signal detection theory techniques. In H. Cooper, M. N. Coutanche, L. M. McMullen, A. T. Panter, D. Rindskopf, &amp; K. J. Sher (Eds.), APA handbook of research methods in psychology: Foundations, planning, measures, and psychometrics (2nd ed., Vol. 1, pp. 837–858). American Psychological Association. Tversky, A., &amp; Kahneman, D. (1974). Judgment under uncertainty: Heuristics and biases. Science, 185(4157), 1124–1131. https://doi.org/10.1126/science.185.4157.1124 Wiggins, J. S. (1973). Personality and prediction: Principles of personality assessment. Addison-Wesley. Zhang, J., &amp; Mueller, S. T. (2005). A note on ROC analysis and non-parametric estimate of sensitivity. Psychometrika, 70(1), 203–212. https://doi.org/10.1007/s11336-003-1119-8 Please note that the areas in the figure are not drawn to scale; otherwise, some regions would be too small to include text.↩︎ "],["actuarial.html", "Chapter 10 Clinical Judgment Versus Algorithmic Prediction 10.1 Approaches to Prediction 10.2 Errors in Clinical Judgment 10.3 Humans Versus Computers 10.4 Accuracy of Different Statistical Models 10.5 Getting Started 10.6 Fitting the Statistical Models 10.7 Examples of Using Actuarial Methods 10.8 Why Clinical Judgment is More Widely Used Than Statistical Formulas 10.9 Conclusion 10.10 Suggested Readings", " Chapter 10 Clinical Judgment Versus Algorithmic Prediction 10.1 Approaches to Prediction There are two primary approaches to prediction: human (i.e., clinical) judgment and the actuarial (i.e., statistical) method. 10.1.1 Human/Clinical Judgment Using the clinical judgment method of prediction, all gathered information is collected and formulated into a diagnosis or prediction in the clinician’s mind. The clinician selects, measures, and combines risk factors and produces risk estimate solely according to clinical experience and judgment. 10.1.2 Actuarial/Statistical Method In the actuarial or statistical method of prediction (i.e., statistical prediction rules), information is gathered and combined systematically in an evidence-based statistical prediction formula, and established cutoffs are used. The method is based on equations and data, so both are needed. An example of a statistical method of prediction is the Violence Risk Appraisal Guide (Rice et al., 2013). The Violence Risk Appraisal Guide is used in an attempt to predict violence and is used for parole decisions. For instance, the equation might be something like Equation (10.1): \\[\\begin{equation} \\scriptsize \\text{Violence risk} = \\beta \\cdot \\text{conduct disorder} + \\beta \\cdot \\text{substance use} + \\beta \\cdot \\text{suspended from school} + \\beta \\cdot \\text{childhood aggression} + ... \\tag{10.1} \\end{equation}\\] Then, based on their score and the established cutoffs, a person is given a “low risk”, “medium risk”, or “high risk” designation. 10.1.3 Combining Human Judgment and Statistical Algorithms There are numerous ways in which humans and statistical algorithms could be involved. On one extreme, humans make all judgments (consistent with the clinical judgment approach). On the other extreme, although humans may be involved in data collection, a statistical formula makes all decisions based on the input data, consistent with an actuarial approach. However, the clinical judgment and actuarial approaches can be combined in a hybrid way (J. Dana &amp; Thomas, 2006). For example, to save time and money, a clinician might use an actuarial approach in all cases, but might only use a clinical approach when the actuarial approach gives a “positive” test. Or, the clinician might use both human judgment and an actuarial approach independently to see whether they agree. That is, the clinician may make a prediction based on their judgment and might also generate a prediction from an actuarial approach. The challenge is what to do when the human and the algorithm disagree. Hypothetically, humans reviewing and adjusting the results from the statistical algorithm could lead to more accurate predictions. However, human input also could lead to the possibility or exacerbation of biased predictions. In general, with very few exceptions, actuarial approaches are as accurate or more accurate than clinical judgment (Ægisdóttir et al., 2006; Baird &amp; Wagner, 2000; Dawes et al., 1989; Grove et al., 2000; Grove &amp; Meehl, 1996). Moreover, the superiority of actuarial approaches to clinical judgment tends to hold even when the clinician is given more information than the actuarial approach (Dawes et al., 1989). Allowing clinicians to override actuarial predictions consistently leads to lower predictive accuracy (Garb &amp; Wood, 2019). In general, validity tends to increase with greater structure (e.g., structured or semi-structured interviews as opposed to free-flowing, unstructured interviews), in terms of administration, responses, scoring, interpretation, etc. Unstructured interviews are susceptible to confirmatory bias, such as exploring only the diagnoses that confirm the clinician’s hypotheses rather than attempting to explore diagnoses that disconfirm the clinician’s hypotheses, which leads to fewer diagnoses. Unstructured interviews are also susceptible to bias relating to race, gender, class, etc. (Garb, 1997, 2005). There is sometimes a misconception that formulas cannot account for qualitative information. However, that is not true. Qualitative information can be scored or coded to be quantified so that it can be included in statistical formulas. That said, the quality of predictions rests on the quality and relevance of the assessment information for the particular prediction/judgment decision. If the assessment data are lousy, it is unlikely that a statistical algorithm (or a human for that matter) will make an accurate prediction: “Garbage in, garbage out”. A statistical formula cannot rescue inaccurate assessment data. 10.2 Errors in Clinical Judgment Clinical judgment is naturally subject to errors. Below, I describe a few errors to which clinical judgment seems particularly prone. When operating freely, clinicians tend to over-estimate exceptions to the established rules (i.e., the broken leg syndrome). Meehl (1957) acknowledged that there may be some situations where it is glaringly obvious that the statistical formula would be incorrect because it fails to account for an important factor. He called these special cases “broken leg” cases, in which the human should deviate from the formula (i.e., broken leg countervailing). The example goes like this: “If a sociologist were predicting whether Professor X would go to the movies on a certain night, he might have an equation involving age, academic specialty, and introversion score. The equation might yield a probability of .90 that Professor X goes to the movie tonight. But if the family doctor announced that Professor X had just broken his leg, no sensible sociologist would stick with the equation. Why didn’t the factor of ‘broken leg’ appear in the formula? Because broken legs are very rare, and in the sociologist’s entire sample of 500 criterion cases plus 250 cross-validating cases, he did not come upon a single instance of it. He uses the broken leg datum confidently, because ‘broken leg’ is a subclass of a larger class we may crudely denote as ‘relatively immobilizing illness or injury,’ and movie-attending is a subclass of a larger class of ‘actions requiring moderate mobility.’” (Meehl, 1957, pp. 269–270) However, people too often think that cases where they disagree with the statistical algorithm are broken leg cases. People too often think their case is an exception to the rule. As a result, they too often change the result of the statistical algorithm and are more likely to be wrong than right in doing so. Because actuarial methods are based on actual population levels (i.e., base rates), unique exceptions are not over-estimated. Actuarial predictions are perfectly reliable—they will always return the same conclusion given an identical set of data. The human judge is likely to both disagree with others and with themselves given the same set of symptoms. A clinician’s decision is likely to be influenced by past experiences, and given the sample of humanity that the clinician is exposed to, the clinician is likely (based on prior experience) to over-estimate the likelihood of occurrence of infrequent phenomena. Actuarial methods are based on objective algorithms, and past personal experience and personal biases do not factor into any decisions. Clinicians give weight to less relevant information, and often give too much weight to singular variables (e.g., Graduate Record Examination scores). Actuarial formulas do a better job of focusing on relevant variables. Computers are good at factoring in base rates, inverse conditional probabilities, etc. Humans ignore base rates (base rate neglect), and tend to show confusion of the inverse. Computers are better at accurately weighing risk factors and calculating unbiased risk estimates. In an actuarial formula, the relevant risk factors are weighted according to their predictive power. This stands in contrast to the Diagnostic and Statistical Manual of Mental Disorders (DSM), in which each symptom is equally weighted. Humans are typically given no feedback on their judgments. To improve accuracy of judgments, it is important for feedback to be clear, consistent, and timely. It is especially unlikely for feedback in clinical psychology to be timely because we will have to wait a long time to see the outcomes of predictions. Clinicians are susceptible to representative schema biases (Dawes, 1986). Clinicians are exposed to a skewed sample of humanity, and they make judgments based on a prototype from their (biased) experiences. This is known as the representativeness heuristic. Different clinicians may have different prototypes, leading to lower inter-rater reliability. 10.3 Humans Versus Computers 10.3.1 Advantages of Computers Here are some advantages of computers over humans: Computers can process lots of information simultaneously. So can humans. But computers can to an even greater degree. Computers are faster at making calculations. Computations by computers are error-free (as long as the computations are programmed correctly). Computers’ judgments will not be biased by fatigue or emotional responses. Computers’ judgments will tend not to be biased in the way that humans’ cognitive biases are, such as with anchoring bias, representativeness bias, confirmation bias, or recency bias. Computers are less likely to be over-confident in their judgments. Computers can more accurately weight the set of predictors based on large data sets. Humans tend to give too much weight to singular predictors. 10.3.2 Advantages of Humans Computers are bad at some things too. Here are some advantages of humans over computers (as of now): Humans can be better at identifying patterns in data (but also can mistakenly identify patterns where there are none). Humans can be flexible and take a different approach if a given approach is not working. Humans are better at tasks requiring creativity and imagination, such as developing theories that explain phenomena. Humans have the ability to reason, which is especially important when dealing with complex, abstract, or open-ended problems, or problems that have not been faced before (or for which we have insufficient data). Humans are better able to learn. Humans are better at holistic, gestalt processing, including facial and linguistic processing. There may be situations in which a clinical judgment would do better than an actuarial judgment. One situation where clinical judgment would be important is when no actuarial method exists for the judgment or prediction. For instance, when no actuarial method exists for the diagnosis or disorder (e.g., suicide), it is up to the clinical judge. However, we could collect data on the outcomes or on clinicians’ judgments to develop an actuarial method that will be more reliable than the clinicians’ judgments. That is, an actuarial method developed based on clinicians’ judgments will be more accurate than clinicians’ judgments. In other words, we do not necessarily need clients’ outcome data to develop an actuarial method. We could use the client’s data as predictors of the clinicians’ judgments to develop a structured approach to prediction that weighs factors similarly to clinicians, but with more reliable predictions. Another situation in which clinical judgment could outperform a statistical algorithm is in true “broken leg” cases, e.g., important and rare events (edge cases) that are not yet accounted for by the algorithm. Another situation in which clinical judgment could be preferable is if advanced, complex theories exist. Computers have a difficult time adhering to complex theories, so clinicians may be better suited. However, we do not have any of these complex theories in psychology that are accurate. We would need strong theory informed by data regarding causal influences, and accurate measures to assess them. If theories alone were true and could explain everything, the psychoanalytic tradition would give us all the answers, as depicted in Figure 10.1. However, no theories in psychology are that good. Nevertheless, predictive accuracy can be improved when considering theory (Garb &amp; Wood, 2019; Silver, 2012). Figure 10.1: Conceptual Depiction of the Psychoanalytic Tradition. If the diagnosis/prediction requires complex configural relations that a computer will have a difficult time replicating, a clinician’s judgment may be preferred. Although the likelihood that the clinician can accurately work through these complex relations is theoretically possible, it is highly unlikely. Holistic pattern recognition (such as language and faces) tends to be better by humans than computers. But computers are getting better with holistic pattern recognition through machine learning. An example of a simple decision (as opposed to a complex decision) would be whether to provide treatment A (e.g., cognitive behavior therapy) or treatment B (e.g., medication) for someone who shows melancholy. In sum, the clinician seeks to integrate all information to make a decision, but is biased. 10.3.3 Comparison of Evidence Hundreds of studies have examined clinical versus actuarial prediction methods across many disciplines. Findings consistently show that actuarial methods are as accurate or more accurate than clinical prediction methods. “There is no controversy in social science that shows such a large body of qualitatively diverse studies coming out so uniformly…as this one” (Meehl, 1986, pp. 373–374). Actuarial methods are particularly valuable for criterion-referenced assessment tasks, in which the aim is to predict specific events or outcomes (Garb &amp; Wood, 2019). For instance, actuarial methods have shown promise in predicting violence, criminal recidivism, psychosis onset, course of mental disorders, treatment selection, treatment failure, suicide attempts, and suicide (Garb &amp; Wood, 2019). Psychometric methods of scale construction such as factor analysis may be preferred to statistical prediction rules for norm-referenced assessment tasks such as describing personality and psychopathology (Garb &amp; Wood, 2019). Moreover, actuarial methods are explicit; they can be transparent and lead to informed scientific criticism to improve them. By contrast, clinical judgment methods are not typically transparent; clinical judgment relies on mental processes that are often difficult to specify. 10.4 Accuracy of Different Statistical Models It is important to evaluate the accuracy of different types of statistical models to provide guidance on which types of models may be most accurate for a given prediction problem. Simpler statistical formulas are more likely to generalize to new samples than complex statistical models due to model over-fitting. As described in Section 5.3.1.3.3.3, over-fitting occurs when the statistical model accounts for error variance (an overly specific prediction), which will not generalize to future samples. Youngstrom et al. (2018) discussed the benefits of the probability nomogram. The probability nomogram combines the risk ratio with the base rate (according to Bayes’ theorem) to generate a prediction. The risk ratio (diagnostic likelihood ratio) is an index of the predictive validity of an instrument: it is the ratio of the probability that a test result is correct to the probability that the test result is incorrect. There are two types of diagnostic likelihood ratios: the positive likelihood ratio and the negative likelihood ratio. As described in Equation (9.59), the positive likelihood ratio is the probability that a person with the disease tested positive for the disease (true positive rate) divided by the probability that a person without the disease tested positive for the disease (false positive rate). As described in Equation (9.60), the negative likelihood ratio is the probability that a person with the disease tested negative for the disease (false negative rate) divided by the probability that a person without the disease tested negative for the disease (true negative rate). Using a probability nomogram, you start with the base rate, then plot the diagnostic likelihood ratio corresponding to a second source of information (e.g., positive test), then connect the base rate (pretest probability) to the posttest probability through the likelihood ratio. These approaches are described in greater detail in Section 12.5. Youngstrom et al. (2018) ordered actuarial approaches to diagnostic classification from less to more complex: Predicting from the base rate Take the best The probability nomogram Naïve Bayesian algorithms Logistic regression with one predictor Logistic regression with multiple predictors Least absolute shrinkage and selection option (LASSO) As described in Section 9.1.2.4, predicting from the base rate is selecting the most likely outcome in every case. “Take the best” refers to focusing on the single variable with the largest validity coefficient, and making a decision based on it (based on some threshold). The probability nomogram combines the base rate (prior probability) with the information offered by an assessment finding to revise the probability. Naïve Bayesian algorithms use the probability nomogram with multiple assessment findings. Basically, you continue to calculate a new posttest probability based on each assessment finding, revising the pretest probability for the next assessment finding based on the posttest probability of the last assessment finding. However, this approach assumes that all predictors are uncorrelated, which is probably not true in practice. Logistic regression is useful for classification problems because it deals with a dichotomous dependent variable, but one could extrapolate it to a continuous dependent variable with multiple regression. The effect size from logistic regression is equivalent to receiver operating characteristic (ROC) curve analysis. Logistic regression with multiple predictors combines multiple predictors into one regression model. Including multiple predictors in the same model optimizes the weights of multiple predictors, but it is important for predictors not to be collinear. LASSO is a form of regression that can handle multiple predictors (like multiple regression). However, it can handle more predictors than multiple regression. For instance, LASSO can accommodate situations where there are more predictors than there are participants. Moreover, unlike multiple regression, it can handle collinear predictors. LASSO uses internal cross-validation to avoid over-fitting. Youngstrom et al. (2018) found that model complexity improves accuracy but only to a point; some of the simpler models (Naïve Bayesian and Logistic regression) did just as well and in some cases better than LASSO models in classifying bipolar disorder. Although LASSO models showed the highest discrimination accuracy in the internal sample, they showed the largest shrinkage in the external sample. Moreover, the LASSO models showed poor calibration in the internal sample; they over-diagnosed bipolar disorder. By contrast, simpler models showed better calibration in the internal sample. The probability nomogram with one or multiple assessment findings showed better calibration than LASSO models because they accounted for the original base rate, so they did not over-diagnose bipolar disorder. In summary, the best models are those that are relatively simple (parsimonious), that can account for one or several of the most important predictors and their optimal weightings, and that account for the base rate of the phenomenon. Even unit-weighted formulas (formulas whose predictors are equally weighted with a weight of one) can sometimes generalize better to other samples than complex weightings (Garb &amp; Wood, 2019). Differential weightings sometimes capture random variance and over-fit the model, thus leading to predictive accuracy shrinkage in cross-validation samples (Garb &amp; Wood, 2019), as described below. The choice of predictive variables often matters more than their weighting. In general, there is often shrinkage of estimates from training data set to a test data set. Shrinkage is when variables with stronger predictive power in the original data set tend to show somewhat smaller predictive power (smaller validity coefficients) when applied to new groups. Shrinkage reflects a model over-fitting (i.e., fitting to error by capitalizing on chance). Shrinkage is especially likely when the original sample is small and/or unrepresentative and the number of variables considered for inclusion is large. Cross-validation with large, representative samples can help evaluate the amount of shrinkage of estimates, particularly for more complex models such as machine learning models (Ursenbach et al., 2019). Ideally, cross-validation would be conducted with a separate sample (external cross-validation) to see the generalizability of estimates. However, you can also do internal cross-validation. For example, you can perform k-fold cross-validation, where you: split the data set into k groups for each unique group: take the group as a hold-out data set (also called a test data set) take the remaining groups as a training data set fit a model on the training data set and evaluate it on the test data set after all k-folds have been used as the test data set, and all models have been fit, you average the estimates across the models, which presumably yields more robust, generalizable estimates An emerging technique that holds promise for increasing predictive accuracy of actuarial methods is machine learning (Garb &amp; Wood, 2019). However, one challenge of some machine learning techniques is that they are like a “black box” and are not transparent, which raises ethical issues (Garb &amp; Wood, 2019). Machine learning may be most valuable when the data available are complex and there are many predictive variables (Garb &amp; Wood, 2019). 10.5 Getting Started 10.5.1 Load Libraries Code library(&quot;petersenlab&quot;) #to install: install.packages(&quot;remotes&quot;); remotes::install_github(&quot;DevPsyLab/petersenlab&quot;) library(&quot;pROC&quot;) library(&quot;caret&quot;) library(&quot;MASS&quot;) library(&quot;randomForest&quot;) library(&quot;e1071&quot;) library(&quot;ranger&quot;) library(&quot;ordinalForest&quot;) library(&quot;elasticnet&quot;) library(&quot;LiblineaR&quot;) library(&quot;glmnet&quot;) library(&quot;viridis&quot;) library(&quot;MOTE&quot;) library(&quot;tidyverse&quot;) library(&quot;here&quot;) library(&quot;tinytex&quot;) library(&quot;rmarkdown&quot;) 10.5.2 Prepare Data 10.5.2.1 Load Data aSAH is a data set from the pROC package (Robin et al., 2021) that contains test scores (s100b) and clinical outcomes (outcome) for patients. Code data(aSAH) mydataActuarial &lt;- aSAH mydataActuarial$disorder &lt;- NA mydataActuarial$disorder[which( mydataActuarial$outcome == &quot;Poor&quot;)] &lt;- 1 mydataActuarial$disorder[which( mydataActuarial$outcome == &quot;Good&quot;)] &lt;- 0 10.5.2.2 Simulate Data For reproducibility, I set the seed below. Using the same seed will yield the same answer every time. There is nothing special about this particular seed. Code set.seed(52242) mydataActuarial$ordinal &lt;- sample( 0:2, nrow(mydataActuarial), replace = TRUE) mydataActuarial$ordinalFactor &lt;- factor(mydataActuarial$ordinal, ordered = TRUE) 10.6 Fitting the Statistical Models You can find an array of statistical models for classification and prediction available in the caret package (Kuhn, 2022) at the following link: https://topepo.github.io/caret/available-models.html (archived at https://perma.cc/E45U-YE6S) 10.6.1 Regression with One Predictor Dichotomous Outcome: Code summary( glm(disorder ~ s100b, data = mydataActuarial)) Call: glm(formula = disorder ~ s100b, data = mydataActuarial) Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.1796 0.0561 3.202 0.00178 ** s100b 0.7417 0.1530 4.848 4.09e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for gaussian family taken to be 0.1942322) Null deviance: 26.124 on 112 degrees of freedom Residual deviance: 21.560 on 111 degrees of freedom AIC: 139.49 Number of Fisher Scoring iterations: 2 Ordinal Outcome: Code summary( polr(ordinalFactor ~ s100b, data = mydataActuarial)) Call: polr(formula = ordinalFactor ~ s100b, data = mydataActuarial) Coefficients: Value Std. Error t value s100b 0.8485 0.6905 1.229 Intercepts: Value Std. Error t value 0|1 -0.6055 0.2570 -2.3565 1|2 0.8514 0.2621 3.2480 Residual Deviance: 246.405 AIC: 252.405 Code lrm( ordinal ~ s100b, data = mydataActuarial, x = TRUE, y = TRUE) Logistic Regression Model lrm(formula = ordinal ~ s100b, data = mydataActuarial, x = TRUE, y = TRUE) Model Likelihood Discrimination Rank Discrim. Ratio Test Indexes Indexes Obs 113 LR chi2 1.59 R2 0.016 C 0.525 0 35 d.f. 1 R2(1,113)0.005 Dxy 0.050 1 39 Pr(&gt; chi2) 0.2067 R2(1,100.4)0.006 gamma 0.051 2 39 Brier 0.213 tau-a 0.033 max |deriv| 1e-09 Coef S.E. Wald Z Pr(&gt;|Z|) y&gt;=1 0.6055 0.2570 2.36 0.0184 y&gt;=2 -0.8514 0.2621 -3.25 0.0012 s100b 0.8485 0.6905 1.23 0.2191 Code ordinalModel &lt;- train( ordinalFactor ~ s100b, data = mydataActuarial, method = &quot;polr&quot;) print(ordinalModel) Ordered Logistic or Probit Regression 113 samples 1 predictor 3 classes: &#39;0&#39;, &#39;1&#39;, &#39;2&#39; No pre-processing Resampling: Bootstrapped (25 reps) Summary of sample sizes: 113, 113, 113, 113, 113, 113, ... Resampling results across tuning parameters: method Accuracy Kappa cauchit 0.2999569 0.002188742 cloglog 0.3092742 0.014863850 logistic 0.3070254 0.011181887 loglog 0.3050676 0.008781209 probit 0.3090156 0.014176878 Accuracy was used to select the optimal model using the largest value. The final value used for the model was method = cloglog. Continuous Outcome: Code summary( lm(s100b ~ ndka, data = mydataActuarial)) Call: lm(formula = s100b ~ ndka, data = mydataActuarial) Residuals: Min 1Q Median 3Q Max -0.37261 -0.14000 -0.09024 0.10732 0.76057 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.1705939 0.0234559 7.273 5.28e-11 *** ndka 0.0038861 0.0005259 7.390 2.94e-11 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.2238 on 111 degrees of freedom Multiple R-squared: 0.3298, Adjusted R-squared: 0.3237 F-statistic: 54.61 on 1 and 111 DF, p-value: 2.936e-11 10.6.2 Regression with Multiple Predictors Dichotomous Outcome: Code summary( glm(disorder ~ s100b + ndka + gender + age + wfns, data = mydataActuarial)) Call: glm(formula = disorder ~ s100b + ndka + gender + age + wfns, data = mydataActuarial) Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.1888383 0.1527337 1.236 0.219099 s100b 0.1537913 0.2338580 0.658 0.512231 ndka 0.0006123 0.0012259 0.499 0.618515 genderFemale -0.1713339 0.0824500 -2.078 0.040168 * age 0.0052547 0.0028551 1.840 0.068550 . wfns.L 0.4182090 0.1078207 3.879 0.000184 *** wfns.Q 0.0261311 0.1295000 0.202 0.840479 wfns.C 0.1669874 0.0834770 2.000 0.048064 * wfns^4 -0.0932508 0.1550179 -0.602 0.548784 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for gaussian family taken to be 0.1549053) Null deviance: 26.124 on 112 degrees of freedom Residual deviance: 16.110 on 104 degrees of freedom AIC: 120.56 Number of Fisher Scoring iterations: 2 Ordinal Outcome: Code summary( polr(ordinalFactor ~ s100b + ndka + gender + age + wfns, data = mydataActuarial)) Call: polr(formula = ordinalFactor ~ s100b + ndka + gender + age + wfns, data = mydataActuarial) Coefficients: Value Std. Error t value s100b 1.8489542 1.164022 1.58842 ndka 0.0003385 0.006903 0.04903 genderFemale -0.6508492 0.398431 -1.63353 age 0.0015384 0.013144 0.11705 wfns.L -0.6994631 0.530547 -1.31838 wfns.Q -1.3813985 0.613950 -2.25002 wfns.C -0.1869263 0.429721 -0.43499 wfns^4 0.6399225 0.731127 0.87526 Intercepts: Value Std. Error t value 0|1 -0.9359 0.7340 -1.2751 1|2 0.6024 0.7295 0.8257 Residual Deviance: 238.5788 AIC: 258.5788 Code lrm( ordinal ~ s100b + ndka + gender + age + wfns, data = mydataActuarial, x = TRUE, y = TRUE) Logistic Regression Model lrm(formula = ordinal ~ s100b + ndka + gender + age + wfns, data = mydataActuarial, x = TRUE, y = TRUE) Model Likelihood Discrimination Rank Discrim. Ratio Test Indexes Indexes Obs 113 LR chi2 9.42 R2 0.090 C 0.637 0 35 d.f. 8 R2(8,113)0.012 Dxy 0.274 1 39 Pr(&gt; chi2) 0.3080 R2(8,100.4)0.014 gamma 0.274 2 39 Brier 0.205 tau-a 0.184 max |deriv| 3e-10 Coef S.E. Wald Z Pr(&gt;|Z|) y&gt;=1 0.4489 0.8968 0.50 0.6167 y&gt;=2 -1.0894 0.9016 -1.21 0.2269 s100b 1.8487 1.1637 1.59 0.1122 ndka 0.0003 0.0069 0.05 0.9609 gender=Female -0.6508 0.3984 -1.63 0.1024 age 0.0015 0.0131 0.12 0.9068 wfns 0.3266 0.4408 0.74 0.4587 wfns=3 0.7044 1.1576 0.61 0.5428 wfns=4 -0.8591 1.2771 -0.67 0.5011 wfns=5 -2.3094 1.6651 -1.39 0.1655 Code ordinalModelPredictors &lt;- train( ordinalFactor ~ s100b + ndka + gender + age + wfns, data = mydataActuarial, method = &quot;polr&quot;) print(ordinalModelPredictors) Ordered Logistic or Probit Regression 113 samples 5 predictor 3 classes: &#39;0&#39;, &#39;1&#39;, &#39;2&#39; No pre-processing Resampling: Bootstrapped (25 reps) Summary of sample sizes: 113, 113, 113, 113, 113, 113, ... Resampling results across tuning parameters: method Accuracy Kappa cauchit 0.2051282 -0.194664032 cloglog 0.3177155 0.019400327 logistic 0.3105615 0.005977777 loglog 0.3080244 0.007859259 probit 0.3074942 0.005253176 Accuracy was used to select the optimal model using the largest value. The final value used for the model was method = cloglog. Continuous Outcome: Code summary( lm(s100b ~ ndka + gender + age + wfns, data = mydataActuarial)) Call: lm(formula = s100b ~ ndka + gender + age + wfns, data = mydataActuarial) Residuals: Min 1Q Median 3Q Max -0.45724 -0.08047 -0.00746 0.03834 0.60901 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.0870834 0.0631673 1.379 0.1709 ndka 0.0033368 0.0003945 8.458 1.72e-13 *** genderFemale 0.0333867 0.0342521 0.975 0.3319 age 0.0017750 0.0011788 1.506 0.1351 wfns.L 0.3124368 0.0330875 9.443 1.09e-15 *** wfns.Q 0.1156324 0.0528496 2.188 0.0309 * wfns.C -0.0125489 0.0348138 -0.360 0.7192 wfns^4 -0.0779965 0.0642403 -1.214 0.2274 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.1642 on 105 degrees of freedom Multiple R-squared: 0.6586, Adjusted R-squared: 0.6358 F-statistic: 28.93 on 7 and 105 DF, p-value: &lt; 2.2e-16 10.6.3 Least Absolute Shrinkage and Selection Option (LASSO) The least absolute shrinkage and selection option (LASSO) models were fit in the caret package (Kuhn, 2022). Dichotomous Outcome: Code lassoModelDichotomous &lt;- train( outcome ~ s100b + ndka + gender + age + wfns, data = mydataActuarial, method = &quot;glmnet&quot;) print(lassoModelDichotomous) glmnet 113 samples 5 predictor 2 classes: &#39;Good&#39;, &#39;Poor&#39; No pre-processing Resampling: Bootstrapped (25 reps) Summary of sample sizes: 113, 113, 113, 113, 113, 113, ... Resampling results across tuning parameters: alpha lambda Accuracy Kappa 0.10 0.0005284466 0.7355782 0.4287611 0.10 0.0052844664 0.7287405 0.4099028 0.10 0.0528446638 0.7218622 0.3770501 0.55 0.0005284466 0.7375306 0.4322519 0.55 0.0052844664 0.7286740 0.4091351 0.55 0.0528446638 0.7227115 0.3738905 1.00 0.0005284466 0.7344793 0.4259979 1.00 0.0052844664 0.7336155 0.4208213 1.00 0.0528446638 0.7221166 0.3723118 Accuracy was used to select the optimal model using the largest value. The final values used for the model were alpha = 0.55 and lambda = 0.0005284466. Continuous Outcome: Code lassoModelContinuous &lt;- train( s100b ~ ndka + gender + age + wfns, data = mydataActuarial, method = &quot;lasso&quot;) print(lassoModelContinuous) The lasso 113 samples 4 predictor No pre-processing Resampling: Bootstrapped (25 reps) Summary of sample sizes: 113, 113, 113, 113, 113, 113, ... Resampling results across tuning parameters: fraction RMSE Rsquared MAE 0.1 0.2429079 0.2539792 0.1738286 0.5 0.2108372 0.3378498 0.1324253 0.9 0.2166231 0.3055272 0.1267198 RMSE was used to select the optimal model using the smallest value. The final value used for the model was fraction = 0.5. 10.6.4 Ridge Regression The ridge regression models were fit in the caret package (Kuhn, 2022). Dichotomous Outcome: Code ridgeModelDichotomous &lt;- train( outcome ~ s100b + ndka + gender + age + wfns, data = mydataActuarial, method = &quot;regLogistic&quot;) print(ridgeModelDichotomous) Regularized Logistic Regression 113 samples 5 predictor 2 classes: &#39;Good&#39;, &#39;Poor&#39; No pre-processing Resampling: Bootstrapped (25 reps) Summary of sample sizes: 113, 113, 113, 113, 113, 113, ... Resampling results across tuning parameters: cost loss epsilon Accuracy Kappa 0.5 L1 0.001 0.7325252 0.4103154 0.5 L1 0.010 0.7295235 0.4051628 0.5 L1 0.100 0.7277705 0.3960880 0.5 L2_dual 0.001 0.7248420 0.3938821 0.5 L2_dual 0.010 0.7326213 0.4044132 0.5 L2_dual 0.100 0.7345136 0.4173344 0.5 L2_primal 0.001 0.7345607 0.4114534 0.5 L2_primal 0.010 0.7323873 0.4078716 0.5 L2_primal 0.100 0.6439091 0.1596625 1.0 L1 0.001 0.7355483 0.4187553 1.0 L1 0.010 0.7374291 0.4240174 1.0 L1 0.100 0.7358523 0.4188055 1.0 L2_dual 0.001 0.7259651 0.4116438 1.0 L2_dual 0.010 0.7290618 0.4028366 1.0 L2_dual 0.100 0.7236890 0.3806801 1.0 L2_primal 0.001 0.7436326 0.4349872 1.0 L2_primal 0.010 0.7334694 0.4118764 1.0 L2_primal 0.100 0.6446988 0.1637100 2.0 L1 0.001 0.7452451 0.4415898 2.0 L1 0.010 0.7435028 0.4363186 2.0 L1 0.100 0.7363747 0.4247123 2.0 L2_dual 0.001 0.7400012 0.4239179 2.0 L2_dual 0.010 0.7279734 0.3936125 2.0 L2_dual 0.100 0.7177376 0.3990560 2.0 L2_primal 0.001 0.7485614 0.4486697 2.0 L2_primal 0.010 0.7436215 0.4330285 2.0 L2_primal 0.100 0.6466046 0.1685345 Accuracy was used to select the optimal model using the largest value. The final values used for the model were cost = 2, loss = L2_primal and epsilon = 0.001. Continuous Outcome: Code ridgeModelContinuous &lt;- train( s100b ~ ndka + gender + age + wfns, data = mydataActuarial, method = &quot;ridge&quot;) print(ridgeModelContinuous) Ridge Regression 113 samples 4 predictor No pre-processing Resampling: Bootstrapped (25 reps) Summary of sample sizes: 113, 113, 113, 113, 113, 113, ... Resampling results across tuning parameters: lambda RMSE Rsquared MAE 0e+00 0.2334776 0.3049999 0.1386310 1e-04 0.2330332 0.3068559 0.1380783 1e-01 0.2364670 0.2965017 0.1388237 RMSE was used to select the optimal model using the smallest value. The final value used for the model was lambda = 1e-04. 10.6.5 Elastic Net The elastic net models were fit in the caret package (Kuhn, 2022). Continuous Outcome: Code elasticnetModelContinuous &lt;- train( s100b ~ ndka + gender + age + wfns, data = mydataActuarial, method = &quot;enet&quot;) print(elasticnetModelContinuous) Elasticnet 113 samples 4 predictor No pre-processing Resampling: Bootstrapped (25 reps) Summary of sample sizes: 113, 113, 113, 113, 113, 113, ... Resampling results across tuning parameters: lambda fraction RMSE Rsquared MAE 0e+00 0.050 0.2731440 0.2621499 0.1863863 0e+00 0.525 0.2336049 0.3631764 0.1389417 0e+00 1.000 0.2385990 0.3368319 0.1364719 1e-04 0.050 0.2731444 0.2621501 0.1863868 1e-04 0.525 0.2336066 0.3631744 0.1389431 1e-04 1.000 0.2386016 0.3368277 0.1364723 1e-01 0.050 0.2734316 0.2622984 0.1867288 1e-01 0.525 0.2348413 0.3605836 0.1397894 1e-01 1.000 0.2412255 0.3321961 0.1369182 RMSE was used to select the optimal model using the smallest value. The final values used for the model were fraction = 0.525 and lambda = 0. 10.6.6 Random Forest Machine Learning The random forest models were fit in the caret package (Kuhn, 2022). Dichotomous Outcome: Code rfModelDichotomous &lt;- train( outcome ~ s100b + ndka + gender + age + wfns, data = mydataActuarial, method = &quot;rf&quot;) print(rfModelDichotomous) Random Forest 113 samples 5 predictor 2 classes: &#39;Good&#39;, &#39;Poor&#39; No pre-processing Resampling: Bootstrapped (25 reps) Summary of sample sizes: 113, 113, 113, 113, 113, 113, ... Resampling results across tuning parameters: mtry Accuracy Kappa 2 0.7369067 0.3953884 5 0.7347013 0.4032376 8 0.7284303 0.3899586 Accuracy was used to select the optimal model using the largest value. The final value used for the model was mtry = 2. Ordinal Outcome: Code rfModelOrdinal &lt;- train( ordinalFactor ~ s100b + ndka + gender + age + wfns, data = mydataActuarial, method = &quot;ordinalRF&quot;) print(rfModelOrdinal) Random Forest 113 samples 5 predictor 3 classes: &#39;0&#39;, &#39;1&#39;, &#39;2&#39; No pre-processing Resampling: Bootstrapped (25 reps) Summary of sample sizes: 113, 113, 113, 113, 113, 113, ... Resampling results across tuning parameters: nsets ntreeperdiv ntreefinal Accuracy Kappa 50 50 200 0.3211389 -0.005466516 50 50 400 0.3022129 -0.031700266 50 50 600 0.3053906 -0.026771454 50 100 200 0.3079423 -0.024898546 50 100 400 0.3067742 -0.025865855 50 100 600 0.3104021 -0.020939333 50 150 200 0.3086011 -0.025645459 50 150 400 0.3040616 -0.029628515 50 150 600 0.3020118 -0.033196001 100 50 200 0.3290558 0.003086972 100 50 400 0.3042518 -0.027140626 100 50 600 0.3138210 -0.014979737 100 100 200 0.3070572 -0.029330252 100 100 400 0.3142540 -0.017303057 100 100 600 0.3048068 -0.034920067 100 150 200 0.3163612 -0.013654494 100 150 400 0.3058112 -0.031364175 100 150 600 0.3091778 -0.025260742 150 50 200 0.3080989 -0.023783202 150 50 400 0.3097436 -0.022047402 150 50 600 0.3088715 -0.027163376 150 100 200 0.3076460 -0.027799395 150 100 400 0.3090770 -0.023510219 150 100 600 0.3130607 -0.018028479 150 150 200 0.3058690 -0.028026053 150 150 400 0.2998348 -0.039127889 150 150 600 0.3062674 -0.031066605 Accuracy was used to select the optimal model using the largest value. The final values used for the model were nsets = 100, ntreeperdiv = 50 and ntreefinal = 200. Continuous Outcome: Code rfModelContinuous &lt;- train( s100b ~ ndka + gender + age + wfns, data = mydataActuarial, method = &quot;rf&quot;) print(rfModelContinuous) Random Forest 113 samples 4 predictor No pre-processing Resampling: Bootstrapped (25 reps) Summary of sample sizes: 113, 113, 113, 113, 113, 113, ... Resampling results across tuning parameters: mtry RMSE Rsquared MAE 2 0.2217943 0.3596668 0.1278765 4 0.2219191 0.3547089 0.1278610 7 0.2222394 0.3476888 0.1318543 RMSE was used to select the optimal model using the smallest value. The final value used for the model was mtry = 2. 10.6.7 k-Fold Cross-Validation The k-fold cross-validation models were fit in the caret package (Kuhn, 2022). Code kFolds &lt;- 10 train.control &lt;- trainControl( method = &quot;cv&quot;, number = kFolds) Dichotomous Outcome: Code cvModelDichotomous &lt;- train( outcome ~ s100b + ndka + gender + age + wfns, data = mydataActuarial, method = &quot;glm&quot;, trControl = train.control) print(cvModelDichotomous) Generalized Linear Model 113 samples 5 predictor 2 classes: &#39;Good&#39;, &#39;Poor&#39; No pre-processing Resampling: Cross-Validated (10 fold) Summary of sample sizes: 102, 102, 102, 102, 102, 102, ... Resampling results: Accuracy Kappa 0.7780303 0.5046079 Continuous Outcome: Code cvModelContinuous &lt;- train( s100b ~ ndka + gender + age + wfns, data = mydataActuarial, method = &quot;lm&quot;, trControl = train.control) print(cvModelContinuous) Linear Regression 113 samples 4 predictor No pre-processing Resampling: Cross-Validated (10 fold) Summary of sample sizes: 102, 103, 101, 102, 101, 102, ... Resampling results: RMSE Rsquared MAE 0.2074522 0.3819308 0.1293988 Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE 10.6.8 Leave-One-Out (LOO) Cross-Validation The leave-one-out (LOO) cross-validation models were fit in the caret package (Kuhn, 2022). Code train.control &lt;- trainControl( method = &quot;LOOCV&quot;) Dichotomous Outcome: Code loocvModelDichotomous &lt;- train( outcome ~ s100b + ndka + gender + age + wfns, data = mydataActuarial, method = &quot;glm&quot;, trControl = train.control) print(loocvModelDichotomous) Generalized Linear Model 113 samples 5 predictor 2 classes: &#39;Good&#39;, &#39;Poor&#39; No pre-processing Resampling: Leave-One-Out Cross-Validation Summary of sample sizes: 112, 112, 112, 112, 112, 112, ... Resampling results: Accuracy Kappa 0.7699115 0.4747944 Continuous Outcome: Code loocvModelContinuous &lt;- train( s100b ~ ndka + gender + age + wfns, data = mydataActuarial, method = &quot;lm&quot;, trControl = train.control) print(loocvModelContinuous) Linear Regression 113 samples 4 predictor No pre-processing Resampling: Leave-One-Out Cross-Validation Summary of sample sizes: 112, 112, 112, 112, 112, 112, ... Resampling results: RMSE Rsquared MAE 0.2313417 0.2764679 0.1274925 Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE 10.7 Examples of Using Actuarial Methods There are many examples of actuarial methods used in research and practice. As just a few examples, actuarial methods have been used to predict violence risk (Rice et al., 2013), to predict treatment outcome [for the treatment of depression; DeRubeis et al. (2014); Cohen &amp; DeRubeis (2018)], and to screen for attention-deficit/hyperactivity disorder (Lindhiem et al., 2015). Machine learning is becoming more widely used as another form of approaches to actuarial methods (Dwyer et al., 2018). For an empirical example, we have examined actuarial methods to predict arrests, illegal drug use, and drunk driving (Petersen et al., 2015). 10.8 Why Clinical Judgment is More Widely Used Than Statistical Formulas Despite actuarial methods being generally more accurate than clinical judgment, clinical judgment is much more widely used. There are several reasons why actuarial methods have not caught on; one reason is professional traditions. Experts in any field do not like to think that a computer could outperform them. Some practitioners argue that judgment/prediction is an “art form” and that using a statistical formula is treating people like a number. However, using an approach (i.e., clinical judgment) that systematically leads to less accurate decisions and predictions is an ethical problem. Some clinicians do not think that group averages (e.g., in terms of which treatment is most effective) apply to an individual client. This invokes the distinction between nomothetic (group-level) inferences and idiographic (individual-level) inferences. However, the scientific evidence and probability theory strongly favor the nomothetic approach to clinical prediction—it is better to generalize from group-level evidence than throwing out all the evidence and taking the approach of “anything goes.” Clinicians frequently believe the broken leg fallacy, i.e., thinking that your client is an exception to the algorithmic prediction. In most cases, deviating from the statistical formula will result in less accurate predictions. People tend to over-estimate the probability of low base rate conditions and events. Another reason why actuarial methods have not caught on is the belief that receiving a treatment is the only thing that matters. But it is an empirical question which treatment is most effective for whom. What if we could do better? For example, we could potentially use a formula to identify the most effective treatment for a client. Some treatments are no better than placebo; other treatments are actually harmful (Lilienfeld, 2007; Williams et al., 2021). Another reason why clinical methods are more widely used than actuarial methods is the over-confidence in clinicians’ predictions—clinicians think they are more accurate than they actually are. We see this when examining their calibration; their predictions tend to be mis-calibrated. For example, things they report with 80% confidence occur less than 80% of the time. Clinicians will sometimes be correct by chance, and they tend to mis-attribute that to their assessment; clinicians tend to remember the successes and forget the failures. Note, however, that it is not just clinicians who are over-confident; humans in general tend to be over-confident in their predictions. Another argument against using actuarial methods is that “no methods exist”. In some cases, that is true—actuarial methods do not yet exist for some prediction problems. However, one can always create an algorithm of the clinicians’ judgments, even if one does not have access to the clients’ outcome information. A model of clinicians’ responses tends to be more accurate than clinicians’ judgments themselves because the model gives the same outcome with the same input data—i.e., it is perfectly reliable. Another argument from some clinicians is that, “My job is to understand, not to predict”. But what kind of understanding does not involve predictions? Accurate predictions help in understanding. Knowing how people would perform in different conditions is the same thing as good understanding. 10.9 Conclusion In general, it is better to develop and use structured, actuarial approaches than informal approaches that rely on human or clinical judgment. Actuarial approaches to prediction tend to be as accurate or more accurate than clinical judgment. Nevertheless, clinical judgment tends to be much more widely used than actuarial approaches, which is a major ethical problem. 10.10 Suggested Readings Dawes et al. (1989); Garb &amp; Wood (2019); Grove et al. (2000) References Ægisdóttir, S., White, M. J., Spengler, P. M., Maugherman, A. S., Anderson, L. A., Cook, R. S., Nichols, C. N., Lampropoulos, G. K., Walker, B. S., Cohen, G., &amp; Rush, J. D. (2006). The meta-analysis of clinical judgment project: Fifty-six years of accumulated research on clinical versus statistical prediction. The Counseling Psychologist, 34(3), 341–382. https://doi.org/10.1177/0011000005285875 Baird, C., &amp; Wagner, D. (2000). The relative validity of actuarial- and consensus-based risk assessment systems. Children and Youth Services Review, 22(11), 839–871. https://doi.org/10.1016/S0190-7409(00)00122-5 Cohen, Z. D., &amp; DeRubeis, R. J. (2018). Treatment selection in depression. Annual Review of Clinical Psychology, 14(1), 209–236. https://doi.org/10.1146/annurev-clinpsy-050817-084746 Dana, J., &amp; Thomas, R. (2006). In defense of clinical judgment … and mechanical prediction. Journal of Behavioral Decision Making, 19(5), 413–428. https://doi.org/10.1002/bdm.537 Dawes, R. M. (1986). Representative thinking in clinical judgment. Clinical Psychology Review, 6, 425–441. https://doi.org/10.1016/0272-7358(86)90030-9 Dawes, R. M., Faust, D., &amp; Meehl, P. E. (1989). Clinical versus actuarial judgment. Science, 243(4899), 1668–1674. https://doi.org/10.1126/science.2648573 DeRubeis, R. J., Cohen, Z. D., Forand, N. R., Fournier, J. C., Gelfand, L. A., &amp; Lorenzo-Luaces, L. (2014). The personalized advantage index: Translating research on prediction into individualized treatment recommendations. A demonstration. PLoS ONE, 9(1), e83875. https://doi.org/10.1371/journal.pone.0083875 Dwyer, D. B., Falkai, P., &amp; Koutsouleris, N. (2018). Machine learning approaches for clinical psychology and psychiatry. Annual Review of Clinical Psychology, 14(1), 91–118. https://doi.org/10.1146/annurev-clinpsy-032816-045037 Garb, H. N. (1997). Race bias, social class bias, and gender bias in clinical judgment. Clinical Psychology: Science and Practice, 4(2), 99–120. https://doi.org/10.1111/j.1468-2850.1997.tb00104.x Garb, H. N. (2005). Clinical judgment and decision making. Annual Review of Clinical Psychology, 1, 67–89. https://doi.org/10.1146/annurev.clinpsy.1.102803.143810 Garb, H. N., &amp; Wood, J. M. (2019). Methodological advances in statistical prediction. Psychological Assessment, 31(12), 1456–1466. https://doi.org/10.1037/pas0000673 Grove, W. M., &amp; Meehl, P. E. (1996). Comparative efficiency of informal (subjective, impressionistic) and formal (mechanical, algorithmic) prediction procedures: The clinical–statistical controversy. Psychology, Public Policy, and Law, 2(2), 293–323. https://doi.org/10.1037/1076-8971.2.2.293 Grove, W. M., Zald, D. H., Lebow, B. S., Snitz, B. E., &amp; Nelson, C. (2000). Clinical versus mechanical prediction: A meta-analysis. Psychological Assessment, 12(1), 19–30. https://doi.org/10.1037/1040-3590.12.1.19 Kuhn, M. (2022). caret: Classification and regression training. https://github.com/topepo/caret/ Lilienfeld, S. O. (2007). Psychological treatments that cause harm. Perspectives on Psychological Science, 2(1), 53–70. https://doi.org/10.1111/j.1745-6916.2007.00029.x Lindhiem, O., Yu, L., Grasso, D. J., Kolko, D. J., &amp; Youngstrom, E. A. (2015). Adapting the posterior probability of diagnosis index to enhance evidence-based screening: An application to ADHD in primary care. Assessment, 22(2), 198–207. https://doi.org/10.1177/1073191114540748 Meehl, P. E. (1957). When shall we use our heads instead of the formula? Journal of Counseling Psychology, 4(4), 268–273. https://doi.org/10.1037/h0047554 Meehl, P. E. (1986). Causes and effects of my disturbing little book. Journal of Personality Assessment, 50(3), 370–375. https://doi.org/10.1207/s15327752jpa5003_6 Petersen, I. T., Bates, J. E., Dodge, K. A., Lansford, J. E., &amp; Pettit, G. S. (2015). Describing and predicting developmental profiles of externalizing problems from childhood to adulthood. Development and Psychopathology, 27(3), 791–818. https://doi.org/10.1017/S0954579414000789 Rice, M. E., Harris, G. T., &amp; Lang, C. (2013). Validation of and revision to the VRAG and SORAG: The Violence Risk Appraisal Guide—Revised (VRAG-R). Psychological Assessment, 25(3), 951–965. https://doi.org/10.1037/a0032878 Robin, X., Turck, N., Hainard, A., Tiberti, N., Lisacek, F., Sanchez, J.-C., &amp; Müller, M. (2021). pROC: Display and analyze ROC curves. http://expasy.org/tools/pROC/ Silver, N. (2012). The signal and the noise: Why so many predictions fail–but some don’t. Penguin. Ursenbach, J., O’Connell, M. E., Neiser, J., Tierney, M. C., Morgan, D., Kosteniuk, J., &amp; Spiteri, R. J. (2019). Scoring algorithms for a computer-based cognitive screening tool: An illustrative example of overfitting machine learning approaches and the impact on estimates of classification accuracy. Psychological Assessment, 31(11), 1377–1382. https://doi.org/10.1037/pas0000764 Williams, A. J., Botanov, Y., Kilshaw, R. E., Wong, R. E., &amp; Sakaluk, J. K. (2021). Potentially harmful therapies: A meta-scientific review of evidential value. Clinical Psychology: Science and Practice, 28(1), 5–18. https://doi.org/10.1111/cpsp.12331 Youngstrom, E. A., Halverson, T. F., Youngstrom, J. K., Lindhiem, O., &amp; Findling, R. L. (2018). Evidence-based assessment from simple clinical judgments to statistical learning: Evaluating a range of options using pediatric bipolar disorder as a diagnostic challenge. Clinical Psychological Science, 6(2), 243–265. https://doi.org/10.1177/2167702617741845 "],["clinical.html", "Chapter 11 General Issues in Clinical Assessment 11.1 Historical Perspectives on Clinical Assessment 11.2 Contemporary Trends 11.3 Terminology 11.4 Errors of Pseudo-Prediction 11.5 Conclusion 11.6 Suggested Readings", " Chapter 11 General Issues in Clinical Assessment 11.1 Historical Perspectives on Clinical Assessment Historically, assessment has played a key role in clinical psychology. A history of clinical psychology is provided by Benjamin (2005). Many would consider assessment—especially the assessment of intelligence—the beginning of clinical psychology, and the role that distinguished clinical psychology from other fields. Intelligence testing has been one of the real successes of clinical psychology and assessment. For example, several early measures were developed to assess intelligence, including James Cattell’s measure and the Binet-Simon Intelligence scale. Lewis Terman at Stanford extended the Binet-Simon Intelligence scale to study gifted people, which became known as the Stanford-Binet Intelligence Scales. During World War I, instruments were used to select people for various military occupations (e.g., pilot versus submariner). Intellectual assessment, including the Army Alpha and Beta Tests, was used to decide who was psychologically unfit to serve. Then came the development of personality assessments, such as assessment of “shell shock”, which would eventually become known as post-traumatic stress disorder (PTSD). The military conducted personality assessments to identify people who would be susceptible to shell shock, which would be labeled today as a measure of “neuroticism”. Intellectual assessment and personality assessment became the two principal tools for clinical psychologists. Initially, psychologists were primarily psychometricians. They developed assessment devices and helped score and interpret them. Early on, clinical psychologists did not play a large role in making diagnoses or in treatment—instead, psychiatrists did. However, that changed after World War I. Between World War I and World War II, assessment was the dominant role of practicing clinical psychologists, especially projective personality testing—such as word-association tests, the Rorschach Inkblot Test, and the Thematic Apperception Test—and more objective personality testing. The Rorschach (1921) gave clinical psychologists a “voice at the table” with psychiatrists, even though doubts arose about its accuracy. Projective testing dominated personality assessment through the 1940s. The Minnesota Multiphasic Personality Inventory (MMPI), which became a popular objective personality test, was published in 1943. During World War II, many of the modern psychology departments, clinical psychology programs, and VA-funded clinical psychology internships were created, not by the psychology community, but by the federal government to address wartime and post-wartime needs. Likewise, many measures were developed as a need to meet wartime needs. One of the origins of the psychometric movement in psychology was spurred by historical pressures to create tests to classify people during wartime, such as with the Army Alpha and Beta Tests. 11.2 Contemporary Trends There has been a general decline in the frequency with which clinical assessment (and psychotherapy) is conducted by clinical psychologists. Assessment and measurement are crucial to research but are declining in treatment. This is in great part due to managed care, in which a primary goal is cost containment of mental health services. Managed care has resulted in greatly reduced patient access to mental health services, a dramatic reduction in insurance funds for reimbursement of services, more clinician time spent on paperwork and less time seeing clients, having less time for assessment because fewer sessions are reimbursed, tendencies to administer less frequent or less comprehensive assessments, and increased provision of psychological services from master’s degree-level providers because they are less expensive than doctoral providers. Nevertheless, there are contexts in which clinical assessment is more common. There is still considerable assessment in the Department of Veterans Affairs (VA) system, including testing and integrated reports. In addition to the VA, there is still considerable neuropsychological and aptitude testing. What is the future of assessment for treatment? My hope is that it will be evidence-based assessment (along with evidence-based treatment). 11.3 Terminology Several terms are relevant for clinical assessment. Prevalence is the proportion of the population that has the condition at a given point in time, whereas incidence is the rate of occurrence of new cases. Incidence indicates the risk of developing or contracting the condition, whereas prevalence indicates how widespread the condition is. Point prevalence is the proportion of the population that has the condition at a single point in time. Lifetime prevalence is the proportion of the population that will ever have a condition at any point in their lifetime. The positivity rate is the proportion of tests that are positive (the marginal probability or base rate of positive tests; i.e., the selection ratio). The case fatality rate is the proportion of people with a condition who die as a result of that condition. 11.3.1 The Three S’s: Signs, Symptoms, and Syndromes The Three S’s of assessment in clinical psychology are signs, symptoms, and syndromes. Signs are observable features (or manifestations) of a disorder (Lilienfeld et al., 2015). Therefore, signs can be perceived by a clinician. By contrast, symptoms are unobservable manifestations of a disorder that can only be perceived by the client (Lilienfeld et al., 2015). The term “sign” has a connotation of being somewhat more objective than symptom because a symptom is based on the client’s perceptions, but at least in clinical psychology (and also in many medical fields), both signs and symptoms have considerable elements of subjectivity. A syndrome is a collection of signs and symptoms that co-occur and may reflect a particular disorder. 11.4 Errors of Pseudo-Prediction There are many errors of pseudo-prediction that are relevant to clinical assessment. One error of pseudo-prediction is the confusion of inverse probabilities, as described in Section 9.1.1.2. Another error of pseudo-prediction involves, when predicting a low base-rate phenomenon, only capitalizing on chance occurrences. Another form of pseudo-prediction is constructing predictions that seem compelling but that have no basis, for instance using the date and place of birth to predict one’s personality. Several techniques are used by people to make their predictions seem more compelling: make it true, make it positive, make it ambiguous, and make it mysterious or intimidating. Making it “true” means surrounding predictions with true statements, and using Barnum statements. Barnum statements are statements that are true for most people, such as: “You have a tendency to be critical of yourself.” Including, in your prediction, outcomes that are a common occurrence for 80% of respondents (blanket statements) may make it more likely that people believe the prediction. Most of the statements are correct, so people tend to think the whole thing is correct. Making it “positive” means making the statement positive about the person. People tend to believe positive things about the self. However, if the statement is about a third person, people tend to believe it more if the statement is partially negative. Making it “ambiguous” means using vague language such as “it has not been unusual,” “sometimes,” “at times,” “you tend to be,” etc. Making it mysterious or intimidating could involve making the prediction from some organization or process with authority. An example of making statement mysterious is the PaTE report (“Psychodynamic and Therapeutic Evaluation”) (Kriegman &amp; Kriegman, 1965). Unsupported fad treatments are another form of pseudo-prediction. There is unfortunately a large gap between science and practice. Many of the treatments known to work are not commonly implemented in practice; instead, many treatments implemented in practice have been shown not to be helpful (compared to placebo), or have been shown to be harmful (Lilienfeld, 2007), or have insufficient evidence supporting their use. Selecting a given treatment is a prediction by the therapist that the treatment will lead to the most positive outcomes for the client than the alternatives. There are likely a multitude of reasons for the disparity between practice and research. For instance, some practitioners feel that research is useless. Other practitioners are unaware that there are more effective treatments. Others may not have the time or money to learn new treatments on top of their already-busy schedules. Yet other therapists are greedy. In other cases, it is difficult to determine which treatments work due to a variety of effects. For instance, some ineffective treatments may appear to “work” because of placebo effects, whereby merely receiving a treatment and expecting to improve yields positive benefits—not because of the specific treatment per se. In addition to client expectancy effects, there can also be therapist expectancy effects. There can also be demand effects, where participants form an interpretation of the study’s purpose and subconsciously change their behavior to fit that interpretation. In addition, ineffective treatments can appear to work due to maturational effects, especially with children and long treatments. Ineffective treatments can appear to work due to differential dropout or attrition, where the people who are improving are the most likely to stay in treatment. In addition, there are regression effects that can make an ineffective treatment appear to work. As a reminder, the true score formula in classical test theory is: \\(X = T + e\\). There are random fluctuations in an observed score; part of the observed score is stable true score and part of it is random error. As an example, consider that we will use Norwegian acupuncture. We will select clients who are at the extremes in terms of high pain. They will ultimately converge or regress to their mean (i.e., their true score). So, on average, their pain will be lower the next time. In therapy, clients tend to come in at their worst, so they are bound to improve. The clients would have tended (on average) to get better, even without treatment. This is known as regression to the mean. For these reasons, it is important to include a control group to compare how clients would have done had they received no treatment, a placebo, or some other treatment. For example, in experiments that test the efficacy of cognitive behavioral therapy (CBT), it is important to include a control group, such as no treatment control, waitlist control, treatment as usual, or best available treatment. Violating the extension rule (the conjunction fallacy) is another form of pseudo-prediction. The extension rule states that a special outcome cannot be more probable than a general one of which it is part. But in spite of this, we tend to assign high probabilities to prototypical combinations of characteristics, events, or sequences, and irrationally low probabilities to atypical single characteristics, events, or sequences. Here’s an example from Dawes (1986): “Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations.” Which is more probable? Linda is a bank teller. Linda is a bank teller and is active in the feminist movement.” The probability that a woman is a bank teller and feminist is less likely than the probability that she is a bank teller. However, people tend to say that B is more probable. People tend to make judgments based on the representativeness heuristic—how similar to the prototypes the cases are. Another form of pseudo-prediction is imposing structure (non-randomness) on a pattern where there are none. In addition, expecting alternation, known as the “belief in the law of small numbers”, is a form of pseudo-prediction. Expecting alternation is believing that even small sequences should be representative of populations, even though that is false. For instance, it might involve expecting that one will get three heads if flipping a coin six times. Spurious correlations are another form of pseudo-prediction. The expected value of \\(R^2\\) is \\(\\frac{K}{n - 1}\\), where \\(K\\) is the number of predictors. The more predictors in the model, the more variance is accounted for in the outcome. Thus, with many predictors and a small sample, there is pseudo-prediction, as described in Section 9.9. However, as the sample size increases, spurious correlations decrease. Multicollinearity is also a form of pseudo-prediction. As the correlation among the predictors increase, the chance of getting an arbitrary answer increases. Another form of pseudo-prediction is the hot hand fallacy. The hot hand fallacy in basketball is believing that making a shot makes it more likely that you will make the next shot. Most research suggests that there is no such thing as a “hot hand” (Avugos et al., 2013; Bar-Eli et al., 2006; Gilovich et al., 1985). Some recent research, however, has suggested that there may be a small hot hand effect in some contexts (Bocskocsky et al., 2014; J. B. Miller &amp; Sanjurjo, 2014). Capitalizing on regression effects is pseudo-prediction. When underestimating regression effects, a person can capitalize on regression effects by predicting an extremity in one variable (e.g., amount of improvement over time) due to an extremity in another variable (e.g., pain severity), when in fact, regression to the mean is most likely. 11.5 Conclusion Many of the origins of assessment came from attempts to address wartime needs. Since the 1980s, there has been a general decline in the frequency with which clinical assessments (and psychotherapy) are conducted by clinical psychologists, due to managed care and cost containment. Many errors of pseudo-prediction are relevant to clinical assessment. 11.6 Suggested Readings Wood et al. (2002) References Avugos, S., Köppen, J., Czienskowski, U., Raab, M., &amp; Bar-Eli, M. (2013). The “hot hand” reconsidered: A meta-analytic approach. Psychology of Sport and Exercise, 14(1), 21–27. https://doi.org/10.1016/j.psychsport.2012.07.005 Bar-Eli, M., Avugos, S., &amp; Raab, M. (2006). Twenty years of “hot hand” research: Review and critique. Psychology of Sport and Exercise, 7(6), 525–553. https://doi.org/10.1016/j.psychsport.2006.03.001 Benjamin, L. T. (2005). A history of clinical psychology as a profession in America (and a glimpse of its future). Annual Review of Clinical Psychology, 1, 1–30. https://doi.org/10.1146/annurev.clinpsy.1.102803.143758 Bocskocsky, A., Ezekowitz, J., &amp; Stein, C. (2014). The hot hand: A new approach to an old “fallacy.” MIT Sloan Sports Analytics Conference. Dawes, R. M. (1986). Representative thinking in clinical judgment. Clinical Psychology Review, 6, 425–441. https://doi.org/10.1016/0272-7358(86)90030-9 Gilovich, T., Vallone, R., &amp; Tversky, A. (1985). The hot hand in basketball: On the misperception of random sequences. Cognitive Psychology, 17(3), 295–314. https://doi.org/10.1016/0010-0285(85)90010-6 Kriegman, L. S., &amp; Kriegman, G. (1965). The PaTE report: A new psychodynamic and therapeutic evaluative procedure. The Psychiatric Quarterly, 39(1), 646–674. https://doi.org/10.1007/BF01569493 Lilienfeld, S. O. (2007). Psychological treatments that cause harm. Perspectives on Psychological Science, 2(1), 53–70. https://doi.org/10.1111/j.1745-6916.2007.00029.x Lilienfeld, S. O., Sauvigne, K., Lynn, S. J., Latzman, R. D., Cautin, R., &amp; Waldman, I. D. (2015). Fifty psychological and psychiatric terms to avoid: A list of inaccurate, misleading, misused, ambiguous, and logically confused words and phrases. Frontiers in Psychology, 6. https://doi.org/10.3389/fpsyg.2015.01100 Miller, J. B., &amp; Sanjurjo, A. (2014). A cold shower for the hot hand fallacy. Innocenzo Gasparini Institute for Economic Research. https://repec.unibocconi.it/igier/igi/wp/2014/518.pdf Wood, J. M., Garb, H. N., Lilienfeld, S. O., &amp; Nezworski, M. T. (2002). Clinical assessment. Annual Review of Psychology, 53(1), 519. https://doi.org/10.1146/annurev.psych.53.100901.135136 "],["evidence-based.html", "Chapter 12 Evidence-Based Assessment 12.1 Considerations 12.2 Clinically Relevant 12.3 Culturally Sensitive 12.4 Scientifically Sound 12.5 Bayesian Updating 12.6 Dimensional Approaches to Psychopathology 12.7 Reporting Guidelines for Publications 12.8 Many Measures Are Available 12.9 Conclusion 12.10 Suggested Readings", " Chapter 12 Evidence-Based Assessment I was initially hesitant to include a chapter titled “evidence-based assessment” because this whole book is about doing assessment in the strongest, most scientifically supported (i.e., evidence-based) ways. However, I decided to include a chapter on evidence-based assessment to summarize many of the important considerations in ensuring our assessments are strong for their intended uses, especially in clinical psychology, and to discuss the value of some cutting-edge (and some not-so-cutting-edge) techniques to maximize the validity of inferences. A case study in evidence-based assessment is provided by Antony &amp; Rowa (2005). 12.1 Considerations In general, we need to use assessment devices that are: clinically relevant (if used in a clinical domain): they inform decision-making processes culturally sensitive: they are relevant to individuals from various backgrounds, especially the backgrounds in the population of interest scientifically sound: strong psychometrics (reliability and validity) 12.2 Clinically Relevant Effective treatment depends on accurate assessment. Thus, it is important to consider the treatment utility of assessment to determine whether the assessment has value. Several study methods were designed to examine the treatment utility of assessment, including manipulated assessment (compares assessment versus no assessment) and manipulated use (compares use versus no use of data from the assessment for treatment matching). It is also important to consider the ease of dissemination of an assessment: is it time- and money-efficient, especially in terms of its cost–benefit ratio? Brief assessments, computerized testing, and adaptive testing can help assessments be clinically practical. It should be straightforward to score and interpret. And it should provide incremental validity, i.e., additional useful information above and beyond what is gained by other assessment devices. We have already discussed how assessments can yield more effective outcomes indirectly through matching the right treatment approach to the client. It is also worth noting, however, that assessments can also lead directly to improved treatment outcomes. Conducting an assessment is an intervention—it can have carryover effects, and can result in self-reflection, self-awareness, and reactivity, i.e., change due to observing it. Assessment is also an important component of measurement-based care. Measurement-based care is the approach of treatment monitoring and modifying treatment accordingly (Lutz et al., 2022). Assessment does not stop at the beginning of the treatment—it is continuous and ongoing. So, ongoing measurement is recommended throughout treatment so that the clinician can modify the treatment in response to measurements. Assessment of treatment progress needs to be sensitive to change, i.e., the assessment needs to show treatment sensitivity, and it needs to be actionable, i.e., it needs to have utility. In clinical psychological assessment, it is important for assessments to: consider co-occurrence of multiple issues (comorbidity) and differential diagnosis accurately assess conditions that frequently co-occur or covary (e.g., depression and anxiety), and to differentiate between multiple possibilities for what may explain the client’s difficulties. In addition, it is valuable to: pose alternative or competing hypotheses and test them to rule out other potential explanations look for disconfirming evidence, not just evidence that confirms one’s suspicions consider assessments from multiple informants (e.g., parents, teachers, peers) and multiple levels of analysis, including biological, psychological, and social-contextual factors consider functional impairment and not just diagnostic status, and to consider the context of the client’s difficulties, including the onset, duration, and course of the problem, the client’s treatment history, family history, familial and cultural context, and medical conditions. 12.3 Culturally Sensitive It is important to mitigate cultural bias of instruments. It is also important for the measure to be useful across the population of interest. To accomplish this, one may have to modify an assessment approach to account for clinically significant moderating variables. We discuss culturally sensitive assessment more in Chapter 25, and we discuss test bias in Chapter 16. 12.4 Scientifically Sound The development and selection of measures should be based on scientifically supported theories in psychopathology and basic psychological science. Measures should be standardized, with similar procedures across participants, clients, and examiners. Reliability and validity of measures’ scores are specific to a particular use of the test, and are specific for a given population and context. So, it is important to clearly specify the purpose of the assessment. As just a few examples, the purpose of the assessment could be screening for early identification of risk, for diagnosis, for treatment monitoring, for treatment evaluation, or for measuring a phenomenon of interest in research. In general, the purposes of an assessment can be summarized into the 3 Ps (Youngstrom et al., 2017): predict, prescribe, and process. The purpose of an assessment is to predict if the purpose is to relate an assessment to a criterion at a later point in time. An assessment is used to prescribe if it informs decision-making about the participant or client—for example, a decision about which treatment to give, identifying moderators of treatment effectiveness, or specifying potential confounds or alternative explanations that would warrant a different treatment. An assessment is used to understand process if it informs understanding of the participant or client—for example, identifying mediators of treatment effectiveness or tracking treatment progress. Whichever the purpose of the assessment, it is important to test and evaluate the psychometrics of the assessment device for that particular purpose. Only use the assessment device in a test battery if it advances that purpose. Youngstrom et al. (2017) describe the different core psychometric features that are especially relevant for each purpose. For instance, predictive and discriminative validity are crucial for prediction, whereas inter-rater reliability is crucial for prescription, and treatment sensitivity—a form of criterion-related validity—is crucial for understanding process. 12.4.1 Standard for Excellent Tests Additionally, when making norm-referenced judgments, it is important that the assessment has appropriate norms and evidence of accuracy for any cut-scores (e.g., diagnostic thresholds). Per Youngstrom et al. (2017), below are the norms and psychometric standards for excellent tests: If the assessment has norms, the norms provide a mean and standard deviation for the total score (and any subscores) that were determined from multiple, large samples that are representative of the populations to which it is intended for the test to be administered. Internal consistency: Greater than .90 based on Cronbach’s alpha or (better yet) omega. Inter-rater reliability: Cohen’s kappa \\(\\ge\\) .85; intraclass correlation \\(\\ge\\) .90. Test–retest reliability: If the construct is stable, the measure shows stability of individual differences and repeatability. However, note that not all constructs are expected to be stable. For stable constructs, the measure should show stability of individual differences (i.e., relative or rank-order stability): \\(r\\text{s} \\ge .70\\) over a year or longer. For stable constructs, the measure should also show repeatability, i.e., absolute stability in level: A Bland–Altman plot and corresponding regression shows no significant bias or trends, and the repeatability coefficient (also known as Smallest Real Difference, SRD, or limits of agreement, LOA) is small. Content validity: The test developers clearly defined the domain and ensured representation of the entire set of facets. All elements of the assessment are evaluated by quantitative ratings by multiple groups of judges. Construct validity: The assessment has evidence in support of construct validity, including concurrent, predictive, convergent, and discriminant validity, that has been replicated by independent researchers. Incremental validity: The assessment shows incremental utility above other measures. Discriminative validity: for diagnostic assessment, the measure should have an area under the receiver operating characteristic curve (AUC) of .75 to .90 under clinically realistic conditions. AUC is the single best index of discrimination accuracy, but it is also helpful to know the sensitivity, specificity, positive predictive value, and negative predictive value at particular cutoffs of interest to determine whether the assessment is aligned with its purpose. Prescriptive validity: An assessment that has strong prescriptive validity regarding which diagnosis to give would have good inter-rater reliability (kappa) for diagnosis in more than one sample. An assessment that has strong prescriptive validity regarding which treatment to give would have a moderate effect size (or larger) for treatment moderation—i.e., there is evidence for treatment matching based on the characteristic. Validity generalization: The bulk of the evidence supports use of the assessment with more than one demographic group (if it is to be used with more than one group) and in multiple settings. Sensitivity to change and treatment sensitivity: Replications from independent researchers show evidence that the assessment detects (i.e., is sensitive to) change over time, change over the course of treatment, and differences as a function of different types of treatment. Clinical utility: After practical considerations, including costs, respondent burden, ease of administration and scoring, replication from independent researchers shows that the assessment data are likely to be clinically actionable and confer clinical benefits, such as better outcomes, lower attrition, and greater satisfaction in areas that are important to the stakeholders. The whole assessment process should be empirically evaluated, including the selection, administration, scoring, and interpretation of an instrument, the integration of multiple sources of assessment data, and the decision-making process. When possible, it is valuable to evaluate the accuracy of the decision-making, ideally against some gold standard, for quality control improvement. In general, it is better to include a wide range of assessment tools than a single strong measure. For instance, assessment tools could include self- and informant-report questionnaires, interviews, self-monitoring diaries, behavioral observation, performance-based techniques, and psychophysiological techniques. Then, each measure can be weighted according to its diagnostic likelihood ratio—i.e., its concurrent/predictive validity in the form of the positive likelihood ratio. One challenge is how to handle a situation where different instruments yield different answers. For instance, if the results from parent report versus teacher report conflict with each other. To address potential discrepancies, one can weight the different instruments or informant sources according to their predictive accuracy. For instance, you might give more weight to youths’ self-report for assessing anxiety but might give more weight to parent report for assessing ADHD. One way to handle informant discrepancies when there is not a clear primary informant who is best-positioned to rate the person most accurately is to count a symptom as present if it is endorsed by any of the informants (Hinshaw &amp; Nigg, 1999). 12.5 Bayesian Updating As presented in Equation (9.16), the posttest (or posterior) odds are equal to the pretest odds multiplied by the likelihood ratio. Bayes’ theorem is discussed in Section 9.1.1.3. Using this formula, and converting odds to probabilities, we can use a Fagan probability nomogram to determine the posttest probability following a test result. The calculation of posttest probability is described in Section 9.5.32. A probability nomogram is a way of visually applying Bayes’ theorem to determine the posttest probability of having a condition based on the pretest (or prior) probability and likelihood ratio, as depicted in Figure 12.1. To use a probability nomogram, connect the dots from the starting probability (left line) with the likelihood ratio (middle line) to see the updated probability. The updated (posttest) probability is where the connecting line crosses the third, right line. Figure 12.1: Probability Nomogram. (Figure retrieved from https://upload.wikimedia.org/wikipedia/commons/thumb/6/66/Fagan_nomogram.svg/945px-Fagan_nomogram.svg.png). For instance, if the starting probability is .5% and the likelihood ratio is 10 (e.g., sensitivity = .90, specificity = .91: \\(\\text{likelihood ratio} = \\frac{\\text{sensitivity}}{1 - \\text{specificity}} = \\frac{.9}{1-.91} = 10\\)), the updated probability is less than 5%, as depicted in Figure 12.2. The formula and function for computing posttest probability are provided in Section 9.5.32. Code posttestProbability( pretestProb = .005, likelihoodRatio = 10) [1] 0.04784689 Figure 12.2: Probability Nomogram Example. (Figure adapted from https://upload.wikimedia.org/wikipedia/commons/thumb/6/66/Fagan_nomogram.svg/945px-Fagan_nomogram.svg.png) A probability nomogram calculator can be found at the following link: http://araw.mede.uic.edu/cgi-bin/testcalc.pl (archived at https://perma.cc/X8TF-7YBX). 12.6 Dimensional Approaches to Psychopathology The Diagnostic and Statistical Manual of Mental Disorders (DSM) takes primarily a categorical approach to psychopathology—it classifies mental disorders as discrete categories. That is, anxiety is classified as a distinct disorder from depression in the DSM, despite their strong covariation and co-occurrence. Moreover, disorders are generally classified in the DSM in a binary fashion—according to the DSM, you either “have” the disorder or “do not have” the disorder—there is no middle ground, and there is not a finer differentiation among those who are given the disorder. Despite the categorical approach of the DSM, most disorders seem to be more accurately conceptualized as dimensional than categorical (Markon et al., 2011). As described in Section 17.5.2, DSM disorders are fictive categories. They do not carve nature at its joints. Many disorders show high comorbidity with other disorders. Transdiagnostic dimensions can better account for the heterogeneity within and across disorders—and more parsimoniously—than DSM categories. For instance, depression and anxiety may share an underlying construct of negative emotionality. A dimensional approach considers a person’s level on each dimension, even if the person is sub-threshold. Moreover a transdiagnostic approach may afford greater flexibility in treatment across disorders by focusing on dimensions of difficulties that may cut across traditional diagnostic boundaries. For instance, the Unified Protocol is a form of psychotherapy designed to address a range of emotional disorders, not just depression or anxiety (Ellard et al., 2010). 12.6.1 Multi-Stage Approach to Assessment Another way in which a dimensional approach to psychopathology can provide greater treatment flexibility than a categorical approach is in applying a multi-stage approach to assessment. Youngstrom &amp; Van Meter (2016) described a multi-stage approach to assessment in which the treatment provider calculates the posterior probability of disorder based on an instrument score, the extent of risk factors present, and the base rate of the disorder. Based on the posterior probability of disorder, the treatment provider places the client into one of three graded strata: the Green zone, Yellow zone, or Red zone (see Figure 12.3). Figure 12.3: Multi-Stage Approach to Assessment. The Green zone is the “Low Probability/Wait Zone”. If the client is in the Green zone, the course of action would be to either do nothing or to engage the client in primary prevention techniques that are safe and inexpensive. The threshold between the Green zone and Yellow zone is the “wait–test” threshold. If the client exceeds the wait–test threshold, they move to the Yellow zone. The Yellow zone is the “Moderate Probability/Assessment Zone”. If the client is in the Yellow zone, secondary intervention would be indicated, including targeted intervention efforts, or nonspecific and low-risk treatment, such as skills-oriented psychotherapies. More assessment would be indicated until the posterior probability rises into the treatment zone (Red zone) or falls into the wait zone (Green zone). The threshold between the Yellow zone and Red zone is the “test–treat” threshold. If the client exceeds the test–treat threshold, they move to the Red zone. The Red zone is the “High Probability/Acute Treatment Zone”. If the client is in the Red zone, tertiary intervention would be indicated. The locations of the wait–test threshold and the test–treat threshold depend on the risks and benefits and the client’s preferences. To conduct a multi-stage approach to assessment, Youngstrom &amp; Van Meter (2016) recommend starting with an instrument with high sensitivity, such as tests with broad content coverage. Low scores on a measure with high sensitivity rule out diagnosis (i.e., the client is in the Green zone), but high scores are ambiguous (i.e., the client is in the Yellow zone) because the measure does not have high specificity. The mnemonic device that can be helpful for remembering the utility of sensitivity and specificity is “SNout” and “SPin”. Low (i.e., negative) scores on a test with high sensitivity rule OUT a diagnosis, whereas high (i.e., positive) scores on a test with high specificity rule IN a diagnosis. If a person scores high on the test with high sensitivity and goes to the Yellow zone, follow up with tests that have higher specificity, including more focused tests, to reduce the number of false positives. Ideally, use multiple tests that have incremental validity and do not have high inter-correlation (\\(r &lt; .3\\)) so that they are not redundant (e.g., not all self-report) and so they meaningfully update the posterior probability. To avoid redundancy with rating scales, you can “take the best”—i.e., include only the most valid rating scale from each informant to test each hypothesis to avoid high inter-correlations and multicollinearity. To integrate the results of multiple assessments, use a probability nomogram or a diagnostic test calculator (http://araw.mede.uic.edu/cgi-bin/testcalc.pl; archived at https://perma.cc/X8TF-7YBX) to update the posterior probability after each assessment. The locations of the wait–test threshold and test–treat threshold are based on the risks and benefits of diagnosis and treatment, and can also be influenced by the client’s preference for various approaches. 12.7 Reporting Guidelines for Publications When publishing papers, it is important to report effect sizes, including the AUC, diagnostic likelihood ratios, and correlations between test scores and the criterion. Do not just rely on statistical significance. The effect size is important for determining the practical or clinical significance of the effects. Guidelines for reporting about and evaluating assessments in research reports are provided by Kazdin (1995). It is also important to report aspects of reliability and validity, both in the current sample and from prior studies. Describe the current sample in adequate detail to understand the population to whom the findings from the study may best generalize. Describe how many items were included in the assessment, the item content (or a description of the content, if proprietary), how the assessment was scored, how scores were aggregated (e.g., sum or mean), the range of possible values, the mean and standard deviation of scores, and what higher values represent. 12.8 Many Measures Are Available Assessment in psychology is similar to the Wild West. There are many different measures available. Many measures in psychology do not have evidence of strong psychometrics (reliability/validity) and are not based on research in basic psychological science including cognitive psychology, social psychology, etc. Measures in clinical psychology often focus on complex syndromes and conditions rather than basic states or processes, unlike most tests in medicine. Because there are so many measures, it can be challenging to know which ones to use. To learn about good tests or the strengths and weaknesses of assessment devices in psychology, there are resources that provide reviews of measures, including the Buros Mental Measurements Yearbook (Buros Center for Testing, 2021), which is published by the Buros Center for Testing, and the Handbook of Psychiatric Measures (Rush et al., 2009), which is published by the American Psychiatric Association. 12.9 Conclusion Evidence-based assessment involves using assessment devices that are clinically relevant, culturally sensitive, and scientifically sound. Standards are available for excellent tests. Bayesian updating is a key technique for evidence-based assessment. For assessing psychopathology, evidence-based techniques include dimensional and multi-stage assessment approaches. There are many measures available for assessment; the Buros Mental Measurements Yearbook (Buros Center for Testing, 2021) and the Handbook of Psychiatric Measures (Rush et al., 2009) provide reviews of measures. 12.10 Suggested Readings Hunsley &amp; Mash (2007); Youngstrom et al. (2017) References Antony, M. M., &amp; Rowa, K. (2005). Evidence-based assessment of anxiety disorders in adults. Psychological Assessment, 17(3), 256–266. https://doi.org/10.1037/1040-3590.17.3.256 Buros Center for Testing. (2021). The twenty-first mental measurements yearbook. Buros Center for Testing. Ellard, K. K., Fairholme, C. P., Boisseau, C. L., Farchione, T. J., &amp; Barlow, D. H. (2010). Unified protocol for the transdiagnostic treatment of emotional disorders: Protocol development and initial outcome data. Cognitive and Behavioral Practice, 17(1), 88–101. https://doi.org/10.1016/j.cbpra.2009.06.002 Hinshaw, S. P., &amp; Nigg, J. T. (1999). Behavior rating scales in the assessment of disruptive behavior problems in childhood. In D. Shaffer, C. P. Lucas, &amp; J. E. Richters (Eds.), Diagnostic assessment in child and adolescent psychopathology. (pp. 91–126). The Guilford Press. Hunsley, J., &amp; Mash, E. J. (2007). Evidence-based assessment. Annual Review of Clinical Psychology, 3, 29–51. https://doi.org/10.1146/annurev.clinpsy.3.022806.091419 Kazdin, A. E. (1995). Preparing and evaluating research reports. Psychological Assessment, 7(3), 228–237. https://doi.org/10.1037/1040-3590.7.3.228 Lutz, W., Schwartz, B., &amp; Delgadillo, J. (2022). Measurement-based and data-informed psychological therapy. Annual Review of Clinical Psychology, 18(1), 71–98. https://doi.org/10.1146/annurev-clinpsy-071720-014821 Markon, K. E., Chmielewski, M., &amp; Miller, C. J. (2011). The reliability and validity of discrete and continuous measures of psychopathology: A quantitative review. Psychological Bulletin, 137(5), 856–879. https://doi.org/10.1037/a0023678 Rush, A. J., First, M. B., &amp; Blacker, D. (2009). Handbook of psychiatric measures. American Psychiatric Publishing. Youngstrom, E. A., &amp; Van Meter, A. (2016). Empirically supported assessment of children and adolescents. Clinical Psychology: Science and Practice, 23(4), 327–347. https://doi.org/10.1111/cpsp.12172 Youngstrom, E. A., Van Meter, A., Frazier, T. W., Hunsley, J., Prinstein, M. J., Ong, M.-L., &amp; Youngstrom, J. K. (2017). Evidence-based assessment as an integrative model for applying psychological science to guide the voyage of treatment. Clinical Psychology: Science and Practice, 24(4), 331–363. https://doi.org/10.1111/cpsp.12207 "],["ethics.html", "Chapter 13 Ethical Issues in Assessment 13.1 Belmont Report 13.2 My Ethical Advice 13.3 APA Ethics Code 13.4 AERA Guidelines 13.5 Clinical Report Writing 13.6 Open Science 13.7 Conclusion 13.8 Suggested Readings", " Chapter 13 Ethical Issues in Assessment Ethics are relevant to all of our domains as psychologists, including (but not limited to) research, teaching, assessment, intervention, and supervision. 13.1 Belmont Report The Belmont Report informs professional ethics. The principles from the Belmont Report include: respect for persons, beneficence, and justice. 13.1.1 Respect for Persons A key component of respect for persons is informed consent. It also involves special considerations to protect the welfare of vulnerable people, including children, prisoners, pregnant women, people with intellectual disability, and people who are economically or educationally disadvantaged. It is important to get full informed consent from clients or participants before taking actions that will affect them. 13.1.2 Beneficence The principle of beneficence is about maximizing benefit and minimizing harm, similar to the aphorism, “First, do no harm”. Clients and participants have the prerogative to make choices about whether to receive a procedure, so they need to know the benefits and harm of each procedure. Just having the intent to help is not enough—you also need to be using a helpful technique and to have the knowledge of how to implement the technique. 13.1.3 Justice Justice goes beyond the interaction with the individual client or participant; it deals with broader, societal concerns. For instance, justice involves ensuring that the people taking the risk will also benefit. For example, if a study to test the effectiveness of a medication enrolls primarily homeless people, the participant population may be taking the risk of trying the drug but not ultimately benefiting from the knowledge gained. 13.2 My Ethical Advice My ethical advice is summarized in the following two pieces of advice: Don’t do stupid things Don’t do bad things 13.2.1 Don’t Do Stupid Things First, don’t do stupid things. With respect to assessement, for instance, do not use weak assessment instruments. The assessment techniques you use should be reliable and valid for the tasks and population of interest. They should be based on scientific research, not by professional judgment. Reliability and validity have many facets and all facets must be used in evaluating assessment techniques. Reliability and validity are characteristics of uses of tests, not the tests themselves. So, identify what is important, for example test–retest reliability versus internal consistency. Reliability and validity may differ for populations that vary in characteristics (e.g., age, gender, education, ethnicity). 13.2.2 Don’t Do Bad Things Second, don’t do bad things. For instance, do not administer services without first getting the person’s consent. Informed consent for an assessment includes multiple components. Ensure that the person being assessed is informed about important aspects of the testing situation. Say what services will be administered (i.e., the nature of testing), why the testing will be administered (i.e., the purpose of testing), what the evidence is for the assessment devices, the costs and potential side effects of the assessment and the tests, the pros and cons of underdoing the assessment, and other options available. Describe the type of information that the client will receive. Describe any third-party involvement (e.g., insurance companies, schools, employers, court) in accessing the assessment information. Describe the limits of confidentiality, which typically would include when someone is in danger, as defined by unreported child abuse or neglect, elder abuse, or imminent suicidality or homicidality. If the assessment is court-ordered or the results of the assessment are subpoenaed by a court of law, the limits of confidentiality change. It is also important to describe the psychologist’s role relative to the client. Be transparent about what services are experimental; do not pretend to know more than you know. Do not make false or misleading claims or promises. Honor the agreement implied by the explanation provided in the informed consent. For an example of a document that I use with clients that describes the pros and cons of an intervention technique (parent management training) and other options available, see here: https://osf.io/qmnu6. It is also important to maintain the integrity of assessment techniques. Do not teach people how to raise their scores. There has to be integrity to begin assessment. 13.2.3 Consult, Consult, Consult If you are unsure how to handle an ethical dilemma, CONSULT, CONSULT, CONSULT! In notes and reports, provide documentation of the rationale for decisions made. Professional ethics boards want to know that you are acting in good faith of the client’s best interests. 13.3 APA Ethics Code The American Psychological Association (APA) publishes ethics guidelines in its document entitled, Ethical Principles of Psychologists and Code of Conduct (hereafter referred to as the “APA Ethics Code”) (American Psychological Association, 2017). The latest version of the APA Ethics Code is available here: https://www.apa.org/ethics/code (archived at https://perma.cc/6F4Z-WQ57). The current version of the APA Ethics code was published in 2017 and is available here: https://www.apa.org/ethics/code/ethics-code-2017.pdf (archived at: https://perma.cc/PF6F-QZ5C). The APA also published “Guidelines for Psychological Assessment and Evaluation”, available here: https://www.apa.org/about/policy/guidelines-psychological-assessment-evaluation.pdf (archived at https://perma.cc/FRB2-ND58). Here, I emphasize a few of the guidelines from the APA Ethics Code with respect to assessment. However, you should read and follow the entire APA Ethics Code. That said, the APA Ethics Code is not up to date for clinical science. But there are many important components. Case illustrations of ethical guidelines from the APA Ethics Code are provided by L. Campbell et al. (2010). Ethical issues in psychological assessment are also discussed by Bersoff et al. (2012) and Nagy (2011). 13.3.1 Competence Section 2 of the APA Ethics Code notes that you should only act withing the boundaries of your competence. It is also important to be aware of and report conflicts of interest. 13.3.2 Bases for Assessments Section 9 of the APA Ethics Code describes ethics guidelines for assessment. Section 9.01 describes ethics guidelines regarding the bases for assessments. Use techniques that are sufficient to substantiate the findings, based on strong psychometrics. In a professional context, sit down face to face and evaluate the individual, as opposed to just relying on automated reports. If using computerized assessments, you should have the expertise to consider the appropriateness of the interpretations. Work to reduce or eliminate bias. If using record review, explain the sources of information used for conclusions and recommendations. Maintain a high standard when recording assessment information. Psychologists have responsibility for maintenance and retention of their records. Records should describe the nature, delivery, progress, and results of services, and related fees. Take reasonable steps to maintain confidentiality of records. When a client might want to know how records will be maintained, disclose record-keeping procedures in the informed consent. The duration that full records should be kept depends on whether they are adults or children. For adults, keep full records for 7 years after the last date of service. For children, keep full records for 7 years after the last date of service, or for 3 years after they become adults, whichever is later. The latest version of the APA record keeping guidelines is available here: https://www.apa.org/practice/guidelines/record-keeping (archived at https://perma.cc/S37L-5VNX). The current version of the APA Ethics code was published in 2007 and is available here: https://www.apa.org/pubs/journals/features/record-keeping.pdf (archived at: https://perma.cc/7A8U-M4TQ). 13.3.3 Use of Assessments Section 9.02 describes ethics guidelines regarding the use of assessments. It notes to use assessments “in light of research or evidence”. But how would research and evidence not go together?! This phrase from the APA Ethics Code suggests that some forms of non-research-based evidence have equal footing with research in terms of deciding what assessments to use. I would argue that assessments should be selected based on the best available scientific evidence. And if scientific evidence does not yet exist to use an assessment device for a particular purpose with a particular population, the assessment device should be studied and evaluated before it is used to make important decisions. The assessment should be reliable and valid for the population and purpose. When the reliability and validity has not been established, describe the strengths and weaknesses. Use language and competence level that is appropriate for the participant/client. Consider the potential social consequences of testing. High stakes decisions probably should not be made on the basis of the results of a single test. 13.3.4 Informed Consent in Assessments Section 9.03 describes ethics guidelines regarding informed consent in assessments. Informed consent should be obtained unless: the assessment is government mandated (which changes things a bit), it is implied because it is considered voluntary (e.g., when applying for a job), or evaluating decisional capacity and the person is not capable of providing consent; in these cases, you should still inform them of what testing will include and the purpose of assessment. 13.3.5 Release of Test Data Section 9.04 describes ethics guidelines regarding release of test data. “Test data” include raw and scaled scores, a client’s responses to questions or stimuli, and the psychologist’s notes about the participant. You are expected to release test data if the client provides a signed release, unless it would cause substantial harm or would be a misrepresentation of the data or test. You may also have to release test data if there is a law or court order, but you may be able to fight these. 13.3.6 Other Guidelines Section 9.07 notes that you should not promote the use of assessment techniques by unqualified people. Section 9.08 notes that you should not use obsolete tests and outdated results. 13.3.7 Maintain Test Security (Integrity) Section 9.11 describes ethics guidelines regarding maintaining test security (integrity). Maintaining test integrity involves protecting test instruments, protocols, questions or stimuli, and manuals from unauthorized access. If releasing test data, take care to release only the scores of copyrighted tests; do not release the copyrighted tests themselves. For example, if performing intellectual testing, try to release the subscale scores and summary scores (e.g., T scores) without releasing test stimuli. Also, do not coach people on how to perform better on questions, and do not let them take tests home. 13.4 AERA Guidelines 13.5 Clinical Report Writing Write every report as if it will be used in a court of law. You want it to be able to stand up in court. Always identify the source of each inference (e.g., “Per the client’s report, …”). Do not write a self-reported statement as if it were a factual statement. Instead of writing, “The client cut herself”, write “The client reported that she cut herself”. Do not overstate findings. Information from reports can be used to make decisions that significantly impact people’s lives, so it is important not to misrepresent information. Recognize factors, such as language, culture, and the situation, that could have impacted the results, and note them in the report. Recognize limitations in the recommendations. 13.6 Open Science There has been a replication crisis in psychology and social science, in which the findings from many studies have failed to replicate when independent researchers attempt to replicate the studies’ findings (Open Science Collaboration, 2015). Due to the replication crisis, there has been a movement toward open, transparent science where researchers pre-register their methods, and share their data and analysis scripts. Open practices reduce threats to replicability and increase the public accessibility of science. The movement to open practices in science is known as the open science movement. The open science movement seeks to increase the replicability and accessibility of science through three primary methods. First, the open science movement seeks to improve transparency, i.e., what researchers are actually doing in their studies and analyses. Second, especially when researchers have confirmatory hypotheses, the open science movement seeks to restrict researcher degrees of freedom so that the researchers do what they said they would do rather than contingent analysis decisions to get a desired result. Similar to a garden of forking paths (see Figure 1.1), there are so many decision points in a study, and different decisions can lead to drastically different results. Third, the open science movement seeks to prevent ethically questionable research practices. Ethically questionable research practices may include (John et al., 2012): not reporting all dependent variables deciding to collect more data after looking at the data to see whether the results were significant not reporting all of a study’s conditions stopping collecting data earlier than planned because the expected result is detected rounding down a p-value selectively reporting studies that worked deciding whether to exclude data after looking at the impact of doing so on the results reporting an unexpected finding as having been predicted from the start (known as HARK-ing: hypothesizing after the results are known) claiming that results are unaffected by demographic variables when one is actually unsure falsifying data p-Hacking: multiple testing, selective reporting, and misreporting of the true effect size To advance reproducibility and replicability in science, researchers are encouraged to provide greater transparency and sharing of their research. For instance, one way to provide greater transparency and to limit the researcher’s degrees of freedom is to (pre-)register a study. There is a continuum of study registration that includes pre-registration, co-registration, and post-registration, depending on when the registration occurs relative to data collection and analysis (Benning et al., 2019). Study pre-registration involves publicly posting a study’s design, hypotheses, methods, materials, and data analysis plan prior to data collection. Co-registration occurs after data collection starts but before data analysis begins. Post-registration occurs after data analysis has begun. Although pre-registration is most commonly used with confirmatory research in which you have a directional hypothesis, you can also pre-register exploratory research in which you have no directional hypothesis. Pre-registration helps to make it more explicit what tests are confirmatory (based on a priori directional hypotheses) and which are exploratory. It is important to interpret the results of confirmatory and exploratory tests differently, because you are more likely to get false positive results if conducting lots of exploratory tests. You can also pre-register decision trees that allow flexibility for multiple approaches depending on the data. Some journals see the value of pre-registered studies and publish registered reports. A registered report is a study that is evaluated based on the pre-registered methods, hypotheses, and data analysis plan, and if the pre-registered study is accepted, the eventual paper will be accepted (as long as the researchers followed the pre-registered plan) regardless of whether the findings supported the hypotheses. Registered reports of exploratory analyses are sometimes called exploratory reports. There are many tools that aim to advance the open science movement. A key tool is the Open Science Framework (OSF; https://osf.io). The OSF is a free, open source platform that supports the transparency of scientific research. There are numerous ways that researchers can make use of the OSF to advance the aims of open science. Here is a template for a pre-registration of a study on the OSF: https://osf.io/2hrfy. Researchers can (pre-)register their study on the OSF by posting their hypotheses, study methods, and data analysis plan. In addition to the OSF, other website for pre-registration include AsPredicted (https://aspredicted.org), PROSPERO (for systematic reviews; https://www.crd.york.ac.uk/prospero), and ClinicalTrials.gov (https://clinicaltrials.gov). In addition, researchers can use the OSF to post manuals, protocols, lab notebooks, data, data processing syntax, statistical analysis code, detailed results, computational notebooks, and paper preprints (Tackett, Brandes, &amp; Reardon, 2019). A computational notebook (e.g., Jupyter notebook, R Markdown, Quarto) is a document in which the analysis code, output, and text are weaved together, so the reader can see how each statistical result was obtained for reproducibility. An example of an R Markdown computational notebook is on the book’s page on the OSF: https://osf.io/nkyve (the associated .Rmd file that generated the html is here: https://osf.io/fxrzm). An example of a Quarto computational notebook is also on the book’s page on the OSF: https://osf.io/8ybtk (the associated .qmd file that generated the html is here: https://osf.io/zbruf). In addition to the OSF, other website for sharing materials include Databrary (which is particularly useful for sharing videos; https://nyu.databrary.org), Zenodo (https://zenodo.org), the Dataverse Project (https://dataverse.org), and figshare (https://figshare.com). After pre-registration, you can share a preprint of your paper on a preprint server like arXiv, PsyArXiv, bioRxiv, SocArXiv, EdArXiv, MetaArXiv, or medRxiv. For an example of a preprint, see here: https://psyarxiv.com/gtsvw. Posting a preprint disseminates the paper without waiting for peer review and the publishing process. It also provides free access to papers that might otherwise be hidden behind journals’ paywalls. Also, it helps establishes who was first to perform work on a topic. There are now journals dedicated to publishing replication studies, including ReScienceX (http://rescience.org/x) and to publishing null findings, including the Journal of Articles in Support of the Null Hypothesis (https://www.jasnh.com). Nevertheless, open science does not provide a one-size-fits-all solution to the vast diversity of scientific methods. For instance, there may be important ways needed to adapt open science and pre-registration practices to longitudinal research (Petersen et al., 2024). 13.7 Conclusion According to the Belmont Report, professional ethics includes respect for persons, beneficence, and justice. It is also important to follow the APA Ethics Code (American Psychological Association, 2017) and AERA guidelines (American Educational Research Association et al. (2014)). It is also important to produce a replicable science, which requires better science, including use of measures with strong psychometrics (reliability and validity), in addition to not engaging in questionable research practices. The open science movement, including pre-registrations, sharing data, preprints, and research materials may help improve the replicability of science. 13.8 Suggested Readings American Psychological Association (2017); American Educational Research Association et al. (2014); Bersoff et al. (2012); Nagy (2011); L. Campbell et al. (2010) References American Educational Research Association, American Psychological Association, &amp; National Council on Measurement in Education. (2014). Standards for educational and psychological testing. American Educational Research Association. American Psychological Association. (2017). Ethical principles of psychologists and code of conduct. Benning, S. D., Bachrach, R. L., Smith, E. A., Freeman, A. J., &amp; Wright, A. G. C. (2019). The registration continuum in clinical science: A guide toward transparent practices. Journal of Abnormal Psychology, 128(6), 528–540. https://doi.org/10.1037/abn0000451 Bersoff, D. N., DeMatteo, D., &amp; Foster, E. E. (2012). Assessment and testing. In S. J. Knapp (Ed.), APA handbook of ethics in psychology, Vol 2: Practice, teaching, and research (pp. 45–74). American Psychological Association. Campbell, L., Vasquez, M., Behnke, S., &amp; Kinscherff, R. (2010). APA ethics code commentary and case illustrations (pp. v, 392–v, 392). American Psychological Association. John, L. K., Loewenstein, G., &amp; Prelec, D. (2012). Measuring the prevalence of questionable research practices with incentives for truth telling. Psychological Science, 23(5), 524–532. https://doi.org/10.1177/0956797611430953 Nagy, T. F. (2011). Essential ethics for psychologists: A primer for understanding and mastering core issues (pp. x, 252–x, 252). American Psychological Association. Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251). https://doi.org/10.1126/science.aac4716 Petersen, I. T., Apfelbaum, K. S., &amp; McMurray, B. (2024). Adapting open science and pre-registration to longitudinal research. Infant and Child Development, 33(1), e2315. https://doi.org/10.1002/icd.2315 Tackett, J. L., Brandes, C. M., &amp; Reardon, K. W. (2019). Leveraging the open science framework in clinical psychological assessment research. Psychological Assessment, 31(12), 1386–1394. https://doi.org/10.1037/pas0000583 "],["factor-analysis-PCA.html", "Chapter 14 Factor Analysis and Principal Component Analysis 14.1 Overview of Factor Analysis 14.2 Getting Started 14.3 Descriptive Statistics and Correlations 14.4 Factor Analysis 14.5 Principal Component Analysis (PCA) 14.6 Conclusion 14.7 Suggested Readings 14.8 Exercises", " Chapter 14 Factor Analysis and Principal Component Analysis 14.1 Overview of Factor Analysis Factor analysis is a class of latent variable models that is designated to identify the structure of a measure or set of measures, and ideally, a construct or set of constructs. It aims to identify the optimal latent structure for a group of variables. Factor analysis encompasses two general types: confirmatory factor analysis and exploratory factor analysis. Exploratory factor analysis (EFA) is a latent variable modeling approach that is used when the researcher has no a priori hypotheses about how a set of variables is structured. EFA seeks to identify the empirically optimal-fitting model in ways that balance accuracy (i.e., variance accounted for) and parsimony (i.e., simplicity). Confirmatory factor analysis (CFA) is a latent variable modeling approach that is used when a researcher wants to evaluate how well a hypothesized model fits, and the model can be examined in comparison to alternative models. Using a CFA approach, the researcher can pit models representing two theoretical frameworks against each other to see which better accounts for the observed data. Factor analysis is considered to be a “pure” data-driven method for identifying the structure of the data, but the “truth” that we get depends heavily on the decisions we make regarding the parameters of our factor analysis. The goal of factor analysis is to identify simple, parsimonious factors that underlie the “junk” (i.e., scores filled with measurement error) that we observe. It used to take a long time to calculate a factor analysis because it was computed by hand. Now, it is fast to compute factor analysis with computers (e.g., oftentimes less than 30 ms). In the 1920s, Spearman developed factor analysis to understand the factor structure of intelligence. It was a long process—it took Spearman around one year to calculate the first factor analysis! Factor analysis takes a large dimension data set and simplifies it into a smaller set of factors that are thought to reflect underlying constructs. If you believe that nature is simple underneath, factor analysis gives nature a chance to display the simplicity that lives beneath the complexity on the surface. Spearman identified a single factor, g, that accounted for most of the covariation between the measures of intelligence. Factor analysis involves observed (manifest) variables and unobserved (latent) factors. In a reflective model, it is assumed that the latent factor influences the manifest variables, and the latent factor therefore reflects the common (reliable) variance among the variables. A factor model potentially includes factor loadings, residuals (errors or disturbances), intercepts/means, covariances, and regression paths. A regression path indicates a hypothesis that one variable (or factor) influences another. The standardized regression coefficient represents the strength of association between the variables or factors. A factor loading is a regression path from a latent factor to an observed (manifest) variable. The standardized factor loading represents the strength of association between the variable and the latent factor. A residual is variance in a variable (or factor) that is unexplained by other variables or factors. An indicator’s intercept is the expected value of the variable when the factor(s) (onto which it loads) is equal to zero. Covariances are the associations between variables (or factors). In factor analysis, the relation between an indicator (\\(\\text{X}\\)) and its underlying latent factor(s) (\\(\\text{F}\\)) can be represented with a regression formula as in Equation (14.1): \\[\\begin{equation} \\text{X} = \\lambda \\cdot \\text{F} + \\text{Item Intercept} + \\text{Error Term} \\tag{14.1} \\end{equation}\\] where: \\(\\text{X}\\) is the observed value of the indicator \\(\\lambda\\) is the factor loading, indicating the strength of the association between the indicator and the latent factor(s) \\(\\text{F}\\) is the person’s value on the latent factor(s) \\(\\text{Item Intercept}\\) represents the constant term that accounts for the expected value of the indicator when the latent factor(s) are zero \\(\\text{Error Term}\\) is the residual, indicating the extent of variance in the indicator that is not explained by the latent factor(s) When the latent factors are uncorrelated, the (standardized) error term for an indicator is calculated as 1 minus the sum of squared standardized factor loadings for a given item (including cross-loadings). Another class of factor analysis models are higher-order (or hierarchical) factor models and bifactor models. Guidelines in using higher-order factor and bifactor models are discussed by Markon (2019). Factor analysis is a powerful technique to help identify the factor structure that underlies a measure or construct. As discussed in Section 14.1.4, however, there are many decisions to make in factor analysis, in addition to questions about which variables to use, how to scale the variables, etc. If the variables going into a factor analysis are not well assessed, factor analysis will not rescue the factor structure. In such situations, there is likely to be the problem of garbage in, garbage out. Factor analysis depends on the covariation among variables. Given the extensive method variance that measures have, factor analysis (and principal component analysis) tends to extract method factors. Method factors are factors that are related to the methods being assessed rather than the construct of interest. However, multitrait-multimethod approaches to factor analysis (such as in Section 14.4.2.13) help better partition the variance in variables that reflects method variance versus construct variance, to get more accurate estimates of constructs. Floyd &amp; Widaman (1995) provide an overview of factor analysis for the development of clinical assessments. 14.1.1 Example Factor Models from Correlation Matrices Below, I provide some example factor models from various correlation matrices. Analytical examples of factor analysis are presented in Section 14.4. Consider the example correlation matrix in Figure 14.1. Because all of the correlations are the same (\\(r = .60\\)), we expect there is approximately one factor for this pattern of data. Figure 14.1: Example Correlation Matrix 1. In a single-factor model fit to these data, the factor loadings are .77 and the residual error terms are .40, as depicted in Figure 14.2. The amount of common variance (\\(R^2\\)) that is accounted for by an indicator is estimated as the square of the standardized loading: \\(.60 = .77 \\times .77\\). The amount of error for an indicator is estimated as: \\(\\text{error} = 1 - \\text{common variance}\\), so in this case, the amount of error is: \\(.40 = 1 - .60\\). The proportion of the total variance in indicators that is accounted for by the latent factor is the sum of the square of the standardized loadings divided by the number of indicators. That is, to calculate the proportion of the total variance in the variables that is accounted for by the latent factor, you would square the loadings, sum them up, and divide by the number of variables: \\(\\frac{.77^2 + .77^2 + .77^2 + .77^2 + .77^2 + .77^2}{6} = \\frac{.60 + .60 + .60 + .60 + .60 + .60}{6} = .60\\). Thus, the latent factor accounts for 60% of the variance in the indicators. In this model, the latent factor explains the covariance among the variables. If the answer is simple, a small and parsimonious model should be able to obtain the answer. Figure 14.2: Example Confirmatory Factor Analysis Model: Unidimensional Model. Consider a different correlation matrix in Figure 14.3. There is no common variance (correlations between the variables are zero), so there is no reason to believe there is a common factor that influences all of the variables. Variables that are not correlated cannot be related by a third variable, such as a common factor, so a common factor is not the right model. Figure 14.3: Example Correlation Matrix 2. Consider another correlation matrix in Figure 14.4. Figure 14.4: Example Correlation Matrix 3. If you try to fit a single factor to this correlation matrix, it generates a factor model depicted in Figure 14.5. In this model, the first three variables have a factor loading of .77, but the remaining three variables have a factor loading of zero. This indicates that three remaining factors likely do not share a common factor with the first three variables. Figure 14.5: Example Confirmatory Factor Analysis Model: Multidimensional Model. Therefore, a one-factor model is probably not correct; instead, the structure of the data is probably best represented by a two-factor model, as depicted in Figure 14.6. In the model, Factor 1 explains why measures 1, 2, and 3 are correlated, whereas Factor 2 explains why measures 4, 5, and 6 are correlated. A two-factor model thus explains why measures 1, 2, and 3 are not correlated with measures 4, 5, and 6. In this model, each latent factor accounts for 60% of the variance in the indicators that load onto it: \\(\\frac{.77^2 + .77^2 + .77^2}{3} = \\frac{.60 + .60 + .60}{3} = .60\\). Each latent factor accounts for 30% of the variance in all of the indicators: \\(\\frac{.77^2 + .77^2 + .77^2 + 0^2 + 0^2 + 0^2}{6} = \\frac{.60 + .60 + .60 + 0 + 0 + 0}{6} = .30\\). Figure 14.6: Example Confirmatory Factor Analysis Model: Two-Factor Model With Uncorrelated Factors. Consider another correlation matrix in Figure 14.7. Figure 14.7: Example Correlation Matrix 4. One way to model these data is depicted in Figure 14.8. In this model, the factor loadings are .77, the residual error terms are .40, and there is a covariance path of .50 for the association between Factor 1 and Factor 2. Going from the model to the correlation matrix is deterministic. If you know the model, you can calculate the correlation matrix. For instance, using path tracing rules (described in Section 4.1), the correlation of measures within a factor in this model is calculated as: \\(0.60 = .77 \\times .77\\). Using path tracing rules, the correlation of measures across factors in this model is calculated as: \\(.30 = .77 \\times .50 \\times .77\\). In this model, each latent factor accounts for 60% of the variance in the indicators that load onto it: \\(\\frac{.77^2 + .77^2 + .77^2}{3} = \\frac{.60 + .60 + .60}{3} = .60\\). Each latent factor accounts for 37% of the variance in all of the indicators: \\(\\frac{.77^2 + .77^2 + .77^2 + (.50^2 \\times .77^2) + (.50^2 \\times .77^2) + (.50^2 \\times .77^2)}{6} = \\frac{.60 + .60 + .60 + .15 + .15 + .15}{6} = .37\\). Figure 14.8: Example Confirmatory Factor Analysis Model: Two-Factor Model With Correlated Factors. Although going from the model to the correlation matrix is deterministic, going from the correlation matrix to the model is not deterministic. If you know the correlation matrix, there may be many possible models. For instance, the model could also be the one depicted in Figure 14.9, with factor loadings of .77, residual error terms of .40, a regression path of .50, and a disturbance term of .75. The proportion of variance in Factor 2 that is explained by Factor 1 is calculated as: \\(.25 = .50 \\times .50\\). The disturbance term is calculated as \\(.75 = 1 - (.50 \\times .50) = 1 - .25\\). In this model, each latent factor accounts for 60% of the variance in the indicators that load onto it: \\(\\frac{.77^2 + .77^2 + .77^2}{3} = \\frac{.60 + .60 + .60}{3} = .60\\). Factor 1 accounts for 15% of the variance in the indicators that load onto Factor 2: \\(\\frac{(.50^2 \\times .77^2) + (.50^2 \\times .77^2) + (.50^2 \\times .77^2)}{3} = \\frac{.15 + .15 + .15}{3} = .15\\). This model has the exact same fit as the previous model, but it has different implications. Unlike the previous model, in this model, there is a “causal” pathway from Factor 1 to Factor 2. However, the causal effect of Factor 1 does not account for all of the variance in Factor 2 because the correlation is only .50. Figure 14.9: Example Confirmatory Factor Analysis Model: Two-Factor Model With Regression Path. Alternatively, something else (e.g., another factor) could be explaining the data that we have not considered, as depicted in Figure 14.10. This is a higher-order factor model, in which there is a higher-order factor (\\(A_1\\)) that influences both lower-order factors, Factor 1 (\\(F_1\\)) and Factor 2 (\\(F_2\\)). The factor loadings from the lower order factors to the manifest variables are .77, the factor loading from the higher-order factor to the lower-order factors is .71, and the residual error terms are .40. This model has the exact same fit as the previous models. The proportion of variance in a lower-order factor (\\(F_1\\) or \\(F_2\\)) that is explained by the higher-order factor (\\(A_1\\)) is calculated as: \\(.50 = .71 \\times .71\\). The disturbance term is calculated as \\(.50 = 1 - (.71 \\times .71) = 1 - .50\\). Using path tracing rules, the correlation of measures across factors in this model is calculated as: \\(.30 = .77 \\times .71 \\times .71 \\times .77\\). In this model, the higher-order factor (\\(A_1\\)) accounts for 30% of the variance in the indicators: \\(\\frac{(.77^2 \\times .71^2) + (.77^2 \\times .71^2) + (.77^2 \\times .71^2) + (.77^2 \\times .71^2) + (.77^2 \\times .71^2) + (.77^2 \\times .71^2)}{6} = \\frac{.30 + .30 + .30 + .30 + .30 + .30}{6} = .30\\). Figure 14.10: Example Confirmatory Factor Analysis Model: Higher-Order Factor Model. Alternatively, there could be a single factor that ties measures 1, 2, and 3 together and measures 4, 5, and 6 together, as depicted in Figure 14.11. In this model, the measures no longer have merely random error: measures 1, 2, and 3 have correlated residuals—that is, they share error variance (i.e., systematic error); likewise, measures 4, 5, and 6 have correlated residuals. This model has the exact same fit as the previous models. The amount of common variance (\\(R^2\\)) that is accounted for by an indicator is estimated as the square of the standardized loading: \\(.30 = .55 \\times .55\\). The amount of error for an indicator is estimated as: \\(\\text{error} = 1 - \\text{common variance}\\), so in this case, the amount of error is: \\(.70 = 1 - .30\\). Using path tracing rules, the correlation of measures within a factor in this model is calculated as: \\(.60 = (.55 \\times .55) + (.70 \\times .43 \\times .70) + (.70 \\times .43 \\times .43 \\times .70)\\). The correlation of measures across factors in this model is calculated as: \\(.30 = .55 \\times .55\\). In this model, the latent factor accounts for 30% of the variance in the indicators: \\(\\frac{.55^2 + .55^2 + .55^2 + .55^2 + .55^2 + .55^2}{6} = \\frac{.30 + .30 + .30 + .30 + .30 + .30}{6} = .30\\). Figure 14.11: Example Confirmatory Factor Analysis Model: Unidimensional Model With Correlated Residuals. Alternatively, there could be a single factor that influences measures 1, 2, 3, 4, 5, and 6 in addition to a method bias factor (e.g., a particular measurement method, item stem, reverse-worded item, or another method bias) that influences measures 4, 5, and 6 equally, as depicted in Figure 14.12. In this model, measures 4, 5, and 6 have cross-loadings—that is, they load onto more than one latent factor. This model has the exact same fit as the previous models. The amount of common variance (\\(R^2\\)) that is accounted for by an indicator is estimated as the sum of the squared standardized loadings: \\(.60 = .77 \\times .77 = (.39 \\times .39) + (.67 \\times .67)\\). The amount of error for an indicator is estimated as: \\(\\text{error} = 1 - \\text{common variance}\\), so in this case, the amount of error is: \\(.40 = 1 - .60\\). Using path tracing rules, the correlation of measures within a factor in this model is calculated as: \\(.60 = (.77 \\times .77) = (.39 \\times .39) + (.67 \\times .67)\\). The correlation of measures across factors in this model is calculated as: \\(.30 = .77 \\times .39\\). In this model, the first latent factor accounts for 37% of the variance in the indicators: \\(\\frac{.77^2 + .77^2 + .77^2 + .39^2 + .39^2 + .39^2}{6} = \\frac{.59 + .59 + .59 + .15 + .15 + .15}{6} = .30\\). The second latent factor accounts for 45% of the variance in its indicators: \\(\\frac{.67^2 + .67^2 + .67^2}{3} = \\frac{.45 + .45 + .45}{3} = .45\\). Figure 14.12: Example Confirmatory Factor Analysis Model: Two-Factor Model With Cross-Loadings. 14.1.2 Indeterminacy There could be many more models that have the same fit to the data. Thus, factor analysis has indeterminacy because all of these models can explain these same data equally well, with all having different theoretical meaning. The goal of factor analysis is for the model to look at the data and induce the model. However, most data matrices in real life are very complicated—much more complicated than in these examples. This is why we do not calculate our own factor analysis by hand; use a stats program! It is important to think about the possibility of other models to determine how confident you can be in your data model. For every fully specified factor model—i.e., where the relevant paths are all defined, there is one and only one predictive data matrix (correlation matrix). However, each data matrix can produce many different factor models. There is no way to distinguish which of these factor models is correct from the data matrix alone. Any given data matrix can predict an infinite number of factor models that accurately represent the data structure (Raykov &amp; Marcoulides, 2001)—so we make decisions that determine what type of factor solution our data will yield. Many models could explain your data, and there are many more models that do not explain the data. For equally good-fitting models, decide based on interpretability. If you have strong theory, decide based on theory and things outside of factor analysis! 14.1.3 Practical Considerations There are important considerations for doing factor analysis in real life with complex data. Traditionally, researchers had to consider what kind of data they have, and they often assumed interval-level data even though data in psychology are often not interval data. In the past, factor analysis was not good with categorical and dichotomous (e.g., True/False) data because the variance then is largely determined by the mean. So, we need something more complicated for dichotomous data. More solutions are available now for factor analysis with ordinal and dichotomous data, but it is generally best to have at least four ordered categories to perform factor analysis. The necessary sample size depends on the complexity of the true factor structure. If there is a strong single factor for 30 items, then \\(N = 50\\) is plenty. But if there are five factors and some correlated errors, then the sample size will need to be closer to ~5,000. Factor analysis can recover the truth when the world is simple. However, nature is often not simple, and it may end in the distortion of nature instead of nature itself. Recommendations for factor analysis are described by Sellbom &amp; Tellegen (2019). 14.1.4 Decisions to Make in Factor Analysis There are many decisions to make in factor analysis. These decisions can have important impacts on the resulting solution. Decisions include things such as: What variables to include in the model and how to scale them Method of factor extraction: factor analysis or PCA If factor analysis, the kind of factor analysis: EFA or CFA How many factors to retain If EFA or PCA, whether and how to rotate factors (factor rotation) Model selection and interpretation 14.1.4.1 1. Variables to Include and their Scaling The first decision when conducting a factor analysis is which variables to include and the scaling of those variables. What factors (or components) you extract can differ widely depending on what variables you include in the analysis. For example, if you include many variables from the same source (e.g., self-report), it is possible that you will extract a factor that represents the common variance among the variables from that source (i.e., the self-reported variables). This would be considered a method factor, which works against the goal of estimating latent factors that represent the constructs of interest (as opposed to the measurement methods used to estimate those constructs). An additional consideration is the scaling of the variables—whether to use the raw scaling or whether to standardize them to be on a more common metric (e.g., z-score metric with a mean of zero and standard deviation of one). 14.1.4.2 2. Method of Factor Extraction The second decision is to select the method of factor extraction. This is the algorithm that is going to try to identify factors. There are two main families of factor or component extraction: analytic or principal components. The principal components approach is called principal component analysis (PCA). PCA is not really a form factor analysis; rather, it is useful for data reduction (Lilienfeld et al., 2015). The analytic family includes factor analysis approaches such as principal axis factoring and maximum likelihood factor analysis. The distinction between factor analysis and PCA is depicted in Figure 14.13. Figure 14.13: Distinction Between Factor Analysis and Principal Component Analysis. 14.1.4.2.1 Principal Component Analysis Principal component analysis (PCA) is used if you want to reduce your data matrix. PCA composites represent the variances of an observed measure in as economical a fashion as possible, with no latent underlying variables. The goal of PCA is to identify a smaller number of components that explain as much variance in a set of variables as possible. It is an atheoretical way to decompose a matrix. PCA involves decomposition of a data matrix into a set of eigenvectors, which are transformations of the old variables. The eigenvectors attempt to simplify the data in the matrix. PCA takes the data matrix and identifies the weighted sum of all variables that does the best job at explaining variance: these are the principal components, also called eigenvectors. Principal components reflect optimally weighted sums. In this way, PCA is a formative model (by contrast, factor analysis applies a reflective model). PCA decomposes the data matrix into any number of components—as many as the number of variables, which will always account for all variance. After the model is fit, you can look at the results and discard the components which likely reflect error variance. Judgments about which factors to retain are based on empirical criteria in conjunction with theory to select a parsimonious number of components that account for the majority of variance. The eigenvalue reflects the amount of variance explained by the component (eigenvector). When using a varimax (orthogonal) rotation, an eigenvalue for a component is calculated as the sum of squared standardized component loadings on that component. When using oblique rotation, however, the items explain more variance than is attributable to their factor loadings because the factors are correlated. PCA pulls the first principal component out (i.e., the eigenvector that explains the most variance) and makes a new data matrix: i.e., new correlation matrix. Then the PCA pulls out the component that explains the next most variance—i.e., the eigenvector with the next largest eigenvalue, and it does this for all components, equal to the same number of variables. For instance, if there are six variables, it will iteratively extract an additional component up to six components. You can extract as many eigenvectors as there are variables. If you extract all six components, the data matrix left over will be the same as the correlation matrix in Figure 14.3. That is, the remaining variables will be entirely uncorrelated with the remaining variables, because six components explain 100% of the variance from six variables. In other words, you can explain (6) variables with (6) new things! However, it does no good if you have to use all (6) components because there is no data reduction from the original number of variables, but hopefully the first few components will explain most of the variance. The sum of all eigenvalues is equal to the number of variables in the analysis. PCA does not have the same assumptions as factor analysis, which assumes that measures are partly from common variance and error. But if you estimate (6) eigenvectors and only keep (2), the model is a two-component model and whatever left becomes error. Therefore, PCA does not have the same assumptions as factor analysis, but it often ends up in the same place. Most people who want to conduct a factor analysis use PCA, but PCA is not really factor analysis (Lilienfeld et al., 2015). PCA is what SPSS can do quickly. But computers are so fast now—just do a real factor analysis! Factor analysis better handles error than PCA—factor analysis assumes that what is in the variable is the combination of common construct variance and error. By contrast, PCA assumes that the measures have no measurement error. 14.1.4.2.2 Factor Analysis Factor analysis is an analytic approach to factor extraction. Factor analysis is a special case of structural equation modeling (SEM). Factor analysis is an analytic technique that is interested in the factor structure of a measure or set of measures. Factor analysis is a theoretical approach that considers that there are latent theoretical constructs that influence the scores on particular variables. It assumes that part of the explanation for each variable is shared between variables, and that part of it is unique variance. The unique variance is considered error. The common variance is called the communality, which is the factor variance. Communality of a factor is estimated using the average variance extracted (AVE). The amount of variance due to error is: \\(1 - \\text{communality}\\). There are several types of factor analysis, including principal axis factoring and maximum likelihood factor analysis. Factor analysis can be used to test measurement/factorial invariance and for multitrait-multimethod designs. One example of a MTMM model in factor analysis is the correlated traits correlated methods model (Tackett, Lang, et al., 2019). There are several differences between (real) factor analysis versus PCA. Factor analysis has greater sophistication than PCA, but greater sophistication often results in greater assumptions. Factor analysis does not always work; the data may not always fit to a factor analysis model; therefore, use PCA as a second/last option. PCA can decompose any data matrix; it always works. PCA is okay if you are not interested in the factor structure. PCA uses all variance of variables and assumes variables have no error, so it does not account for measurement error. PCA is good if you just want to form a linear composite and if the causal structure is formative (rather than reflective). However, if you are interested in the factor structure, use factor analysis, which estimates a latent variable that accounts for the common variance and discards error variance. Factor analysis is useful for the identification of latent constructs—i.e., underlying dimensions or factors that explain (cause) scores on items. 14.1.4.3 3. EFA or CFA A third decision is the kind of factor analysis to use: exploratory factor analysis (EFA) or confirmatory factor analysis (CFA). 14.1.4.3.1 Exploratory Factor Analysis (EFA) Exploratory factor analysis (EFA) is used if you have no a priori hypotheses about the factor structure of the model, but you would like to understand the latent variables represented by your items. EFA is partly induced from the data. You feed in the data and let the program build the factor model. You can set some parameters going in, including how to extract or rotate the factors. The factors are extracted from the data without specifying the number and pattern of loadings between the items and the latent factors (Bollen, 2002). All cross-loadings are freely estimated. 14.1.4.3.2 Confirmatory Factor Analysis (CFA) Confirmatory factor analysis (CFA) is used to confirm a priori hypotheses about the factor structure of the model. CFA is a test of the hypothesis. In CFA, you specify the model and ask how well this model represents the data. The researcher specifies the number, meaning, associations, and pattern of free parameters in the factor loading matrix (Bollen, 2002). A key advantage of CFA is the ability to directly compare alternative models (i.e., factor structures), which is valuable for theory testing (Strauss &amp; Smith, 2009). For instance, you could use CFA to test whether the variance in several measures’ scores is best explained with one factor or two factors. In CFA, cross-loadings are not estimated unless the researcher specifies them. 14.1.4.3.3 Exploratory Structural Equation Modeling (ESEM) In real life, there is not a clear distinction between EFA and CFA. In CFA, researchers often set only half of the constraints, and let the data fill in the rest. In EFA, researchers often set constraints and assumptions. Thus, the line between EFA and CFA is often blurred. EFA and CFA can be considered special cases of exploratory structural equation modeling (ESEM), which combines features of EFA, CFA, and SEM (Marsh et al., 2014). ESEM can include any combination of exploratory (i.e., EFA) and confirmatory (CFA) factors. ESEM, unlike traditional CFA models, typically estimates all cross-loadings—at least for the exploratory factors. If a CFA model without cross-loadings and correlated residuals fits as well as an ESEM model with all cross-loadings, the CFA model should be retained for its simplicity. However, ESEM models often fit better than CFA models because requiring no cross-loadings is an unrealistic expectation of items from many psychological instruments (Marsh et al., 2014). The correlations between factors tend to be positively biased when fitting CFA models without cross-loadings, which leads to challenges in using CFA to establish discriminant validity (Marsh et al., 2014). Thus, compared to CFA, ESEM has the potential to more accurately estimate factor correlations and establish discriminant validity (Marsh et al., 2014). Moreover, ESEM can be useful in a multitrait-multimethod framework. We provide examples of ESEM in Section 14.4.3. 14.1.4.4 4. How Many Factors to Retain A goal of factor analysis and PCA is simplification or parsimony, while still explaining as much variance as possible. The hope is that you can have fewer factors that explain the associations between the variables than the number of observed variables. But how do you decide on the number of factors (in factor analysis) or components (in PCA)? There are a number of criteria that one can use to help determine how many factors/components to keep: Kaiser-Guttman criterion: in PCA, components with eigenvalues greater than 1 or, for factor analysis, factors with eigenvalues greater than zero Cattell’s scree test: the “elbow” in a scree plot minus one; sometimes operationalized with optimal coordinates (OC) or the acceleration factor (AF) Parallel analysis: factors that explain more variance than randomly simulated data Very simple structure (VSS) criterion: larger is better Velicer’s minimum average partial (MAP) test: smaller is better Akaike information criterion (AIC): smaller is better Bayesian information criterion (BIC): smaller is better Sample size-adjusted BIC (SABIC): smaller is better Root mean square error of approximation (RMSEA): smaller is better Chi-square difference test: smaller is better; a significant test indicates that the more complex model is significantly better fitting than the less complex model Standardized root mean square residual (SRMR): smaller is better Comparative Fit Index (CFI): larger is better Tucker Lewis Index (TLI): larger is better There is not necessarily a “correct” criterion to use in determining how many factors to keep, so it is generally recommended that researchers use multiple criteria in combination with theory and interpretability. A scree plot from a factor analysis or PCA provides lots of information. A scree plot has the factor number on the x-axis and the eigenvalue on the y-axis. The eigenvalue is the variance accounted for by a factor; when using a varimax (orthogonal) rotation, an eigenvalue (or factor variance) is calculated as the sum of squared standardized factor (or component) loadings on that factor. An example of a scree plot is in Figure 14.14. Figure 14.14: Example of a Scree Plot. The total variance is equal to the number of variables you have, so one eigenvalue is approximately one variable’s worth of variance. In a factor analysis and PCA, the first factor (or component) accounts for the most variance, the second factor accounts for the second-most variance, and so on. The more factors you add, the less variance is explained by the additional factor. One criterion for how many factors to keep is the Kaiser-Guttman criterion. According to the Kaiser-Guttman criterion, you should keep any factors whose eigenvalue is greater than 1. That is, for the sake of simplicity, parsimony, and data reduction, you should take any factors that explain more than a single variable would explain. According to the Kaiser-Guttman criterion, we would keep three factors from Figure 14.14 that have eigenvalues greater than 1. The default in SPSS is to retain factors with eigenvalues greater than 1. However, keeping factors whose eigenvalue is greater than 1 is not the most correct rule. If you let SPSS do this, you may get many factors with eigenvalues around 1 (e.g., factors with an eigenvalue ~ 1.0001) that are not adding so much that it is worth the added complexity. The Kaiser-Guttman criterion usually results in keeping too many factors. Factors with small eigenvalues around 1 could reflect error shared across variables. For instance, factors with small eigenvalues could reflect method variance (i.e., method factor), such as a self-report factor that turns up as a factor in factor analysis, but that may be useless to you as a conceptual factor of a construct of interest. Another criterion is Cattell’s scree test, which involves selecting the number of factors from looking at the scree plot. “Scree” refers to the rubble of stones at the bottom of a mountain. According to Cattell’s scree test, you should keep the factors before the last steep drop in eigenvalues—i.e., the factors before the rubble, where the slope approaches zero. The beginning of the scree (or rubble), where the slope approaches zero, is called the “elbow” of a scree plot. Using Cattell’s scree test, you retain the number of factors that explain the most variance prior to the explained variance drop-off, because, ultimately, you want to include only as many factors in which you gain substantially more by the inclusion of these factors. That is, you would keep the number of factors at the elbow of the scree plot minus one. If the last steep drop occurs from Factor 4 to Factor 5 and the elbow is at Factor 5, we would keep four factors. In Figure 14.14, the last steep drop in eigenvalues occurs from Factor 3 to Factor 4; the elbow of the scree plot occurs at Factor 4. We would keep the number of factors at the elbow minus one. Thus, using Cattell’s scree test, we would keep three factors based on Figure 14.14. There are more sophisticated ways of using a scree plot, but they usually end up at a similar decision. Examples of more sophisticated tests include parallel analysis and very simple structure (VSS) plots. In a parallel analysis, you examine where the eigenvalues from observed data and random data converge, so you do not retain a factor that explains less variance than would be expected by random chance. A parallel analysis can be helpful when you have many variables and one factor accounts for the majority of the variance such that the elbow is at Factor 2 (which would result in keeping one factor), but you have theoretical reasons to select more than one factor. An example in which parallel analysis may be helpful is with neurophysiological data. For instance, parallel analysis can be helpful when conducting temporo-spatial PCA of event-related potential (ERP) data in which you want to separate multiple time windows and multiple spatial locations despite a predominant signal during a given time window and spatial location (Dien, 2012). In general, my recommendation is to use Cattell’s scree test, and then test the factor solutions with plus or minus one factor. You should never accept PCA components with eigenvalues less than one (or factors with eigenvalues less than zero), because they are likely to be largely composed of error. If you are using maximum likelihood factor analysis, you can compare the fit of various models with model fit criteria to see which model fits best for its parsimony. A model will always fit better when you add additional parameters or factors, so you examine if there is significant improvement in model fit when adding the additional factor—that is, we keep adding complexity until additional complexity does not buy us much. Always try a factor solution that is one less and one more than suggested by Cattell’s scree test to buffer your final solution because the purpose of factor analysis is to explain things and to have interpretability. Even if all rules or indicators suggest to keep X number of factors, maybe \\(\\pm\\) one factor helps clarify things. Even though factor analysis is empirical, theory and interpretatability should also inform decisions. 14.1.4.5 5. Factor Rotation The next step if using EFA or PCA is, possibly, to rotate the factors to make them more interpretable and simple, which is the whole goal. To interpret the results of a factor analysis, we examine the factor matrix. The columns refer to the different factors; the rows refer to the different observed variables. The cells in the table are the factor loadings—they are basically the correlation between the variable and the factor. Our goal is to achieve a model with simple structure because it is easily interpretable. Simple structure means that every variable loads perfectly on one and only one factor, as operationalized by a matrix of factor loadings with values of one and zero and nothing else. An example of a factor matrix that follows simple structure is depicted in Figure 14.15. Figure 14.15: Example of a Factor Matrix That Follows Simple Structure. An example of a measurement model that follows simple structure is depicted in Figure 14.16. Each variable loads onto one and only one factor, which makes it easy to interpret the meaning of each factor, because a given factor represents the common variance among the items that load onto it. Figure 14.16: Example of a Measurement Model That Follows Simple Structure. ‘INT’ = internalizing problems; ‘EXT’ = externalizing problems; ‘TD’ = thought-disordered problems. However, pure simple structure only occurs in simulations, not in real-life data. In reality, our measurement model in an unrotated factor analysis model might look like the model in Figure 14.17. In this example, the measurement model does not show simple structure because the items have cross-loadings—that is, the items load onto more than one factor. The cross-loadings make it difficult to interpret the factors, because all of the items load onto all of the factors, so the factors are not very distinct from each other, which makes it difficult to interpret what the factors mean. Figure 14.17: Example of a Measurement Model That Does Not Follow Simple Structure. ‘INT’ = internalizing problems; ‘EXT’ = externalizing problems; ‘TD’ = thought-disordered problems. As a result of the challenges of intepretability caused by cross-loadings, factor rotations are often performed. An example of an unrotated factor matrix is in Figure 14.18. Figure 14.18: Example of a Factor Matrix. In the example factor matrix in Figure 14.18, the factor analysis is not very helpful—it tells us very little because it did not distinguish between the two factors. The variables have similar loadings on Factor 1 and Factor 2. An example of a unrotated factor solution is in Figure 14.19. In the figure, all of the variables are in the midst of the quadrants—they are not on the factors’ axes. Thus, the factors are not very informative. Figure 14.19: Example of an Unrotated Factor Solution. As a result, to improve the interpretability of the factor analysis, we can do what is called rotation. Rotation involves changing the orientation of the factors by changing the axes so that variables end up with very high (close to one or negative one) or very low (close to zero) loadings, so that it is clear which factors include which variables. That is, it tries to identify the ideal solution (factor) for each variable. It searches for simple structure and keeps searching until it finds a minimum. After rotation, if the rotation was successful for imposing simple structure, each factor will have loadings close to one (or negative one) for some variables and close to zero for other variables. The goal of factor rotation is to achieve simple structure, to help make it easier to interpret the meaning of the factors. To perform factor rotation, orthogonal rotations are often used. Orthogonal rotations make the rotated factors uncorrelated. An example of a commonly used orthogonal rotation is varimax rotation. An example of a factor matrix following an orthogonal rotation is depicted in Figure 14.20. An example of a factor solution following an orthogonal rotation is depicted in Figure 14.21. Figure 14.20: Example of a Rotated Factor Matrix. Figure 14.21: Example of a Rotated Factor Solution. An example of a factor matrix from SPSS following an orthogonal rotation is depicted in Figure 14.22. Figure 14.22: Example of a Rotated Factor Matrix From SPSS. An example of a factor structure from an orthogonal rotation is in Figure 14.23. Figure 14.23: Example of a Factor Structure From an Orthogonal Rotation. Sometimes, however, the two factors and their constituent variables may be correlated. Examples of two correlated factors may be depression and anxiety. When the two factors are correlated in reality, if we make them uncorrelated, this would result in an inaccurate model. Oblique rotation allows for factors to be correlated, but if the factors have low correlation (e.g., .2 or less), you can likely continue with orthogonal rotation. An example of a factor structure from an oblique rotation is in Figure 14.24. Results from an oblique rotation are more complicated than orthogonal rotation—they provide lots of output and are more complicated to interpret. In addition, oblique rotation might not yield a smooth answer if you have a relatively small sample size. Figure 14.24: Example of a Factor Structure From an Oblique Rotation. As an example of rotation based on interpretability, consider the Five-Factor Model of Personality (the Big Five), which goes by the acronym, OCEAN: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. Although the five factors of personality are somewhat correlated, we can use rotation to ensure they are maximally independent. Upon rotation, extraversion and neuroticism are essentially uncorrelated, as depicted in Figure 14.25. The other pole of extraversion is intraversion and the other pole of neuroticism might be emotional stability or calmness. Figure 14.25: Example of a Factor Rotation of Neuroticism and Extraversion. Simple structure is achieved when each variable loads highly onto as few factors as possible (i.e., each item has only one significant or primary loading). Oftentimes this is not the case, so we choose our rotation method in order to decide if the factors can be correlated (an oblique rotation) or if the factors will be uncorrelated (an orthogonal rotation). If the factors are not correlated with each other, use an orthogonal rotation. The correlation between an item and a factor is a factor loading, which is simply a way to ask how much a variable is correlated with the underlying factor. However, its interpretation is more complicated if there are correlated factors! An orthogonal rotation (e.g., varimax) can help with simplicity of interpretation because it seeks to yield simple structure without cross-loadings. Cross-loadings are instances where a variable loads onto multiple factors. My recommendation would always be to use an orthogonal rotation if you have reason to believe that finding simple structure in your data is possible; otherwise, the factors are extremely difficult to interpret—what exactly does a cross-loading even mean? However, you should always try an oblique rotation, too, to see how strongly the factors are correlated. Examples of oblique rotations include oblimin and promax. 14.1.4.6 6. Model Selection and Interpretation The next step of factor analysis is selecting and interpreting the model. One data matrix can lead to many different (correct) models—you must choose one based on the factor structure and theory. Use theory to interpret the model and label the factors. When interpreting others’ findings, do not rely just on the factor labels—look at the actual items to determine what they assess. What they are called matters much less than what the actual items are! 14.1.5 The Downfall of Factor Analysis The downfall of factor analysis is cross-validation. Cross-validating a factor structure would mean getting the same factor structure with a new sample. We want factor structures to show good replicability across samples. However, cross-validation often falls apart. The way to attempt to replicate a factor structure in an independent sample is to use CFA to set everything up and test the hypothesized factor structure in the independent sample. 14.1.6 What to Do with Factors What can you do with factors once you have them? In SEM, factors have meaning. You can use them as predictors, mediators, moderators, or outcomes. And, using latent factors in SEM helps disattenuate associations for measurement error, as described in Section 5.6.3. People often want to use factors outside of SEM, but there is confusion here: When researchers find that three variables load onto Factor A, the researchers often combine those three using a sum or average—but this is not accurate. If you just add or average them, this ignores the factor loadings and the error. Another solution is to form a linear composite by adding and weighting the variables by the factor loadings, which retains the differences in correlations (i.e., a weighted sum), but this still ignores the estimated error, so it still may not be generalizable and meaningful. At the same time, weighted sums may be less generalizable than unit-weighted composites where each variable is given equal weight because some variability in factor loadings likely reflects sampling error. 14.1.7 Missing Data Handling The PCA default in SPSS is listwise deletion of missing data: if a participant is missing data on any variable, the subject gets excluded from the analysis, so you might end up with too few participants. Instead, use a correlation matrix with pairwise deletion for PCA with missing data. Maximum likelihood factor analysis can make use of all available data points for a participant, even if they are missing some data points. Mplus, which is often used for SEM and factor analysis, will notify you if you are removing many participants in CFA/EFA. The lavaan package (Rosseel et al., 2022) in R also notifies you if you are removing participants in CFA/SEM models. 14.2 Getting Started 14.2.1 Load Libraries Code library(&quot;petersenlab&quot;) #to install: install.packages(&quot;remotes&quot;); remotes::install_github(&quot;DevPsyLab/petersenlab&quot;) library(&quot;lavaan&quot;) library(&quot;psych&quot;) library(&quot;corrplot&quot;) library(&quot;nFactors&quot;) library(&quot;semPlot&quot;) library(&quot;lavaan&quot;) library(&quot;semTools&quot;) library(&quot;dagitty&quot;) library(&quot;kableExtra&quot;) library(&quot;MOTE&quot;) library(&quot;tidyverse&quot;) library(&quot;here&quot;) library(&quot;tinytex&quot;) library(&quot;knitr&quot;) library(&quot;rmarkdown&quot;) 14.2.2 Load Data 14.2.3 Prepare Data 14.2.3.1 Add Missing Data Adding missing data to dataframes helps make examples more realistic to real-life data and helps you get in the habit of programming to account for missing data. For reproducibility, I set the seed below. Using the same seed will yield the same answer every time. There is nothing special about this particular seed. HolzingerSwineford1939 is a data set from the lavaan package (Rosseel et al., 2022) that contains mental ability test scores (x1–x9) for seventh- and eighth-grade children. Code set.seed(52242) varNames &lt;- names(HolzingerSwineford1939) dimensionsDf &lt;- dim(HolzingerSwineford1939) unlistedDf &lt;- unlist(HolzingerSwineford1939) unlistedDf[sample( 1:length(unlistedDf), size = .15 * length(unlistedDf))] &lt;- NA HolzingerSwineford1939 &lt;- as.data.frame(matrix( unlistedDf, ncol = dimensionsDf[2])) names(HolzingerSwineford1939) &lt;- varNames vars &lt;- c(&quot;x1&quot;,&quot;x2&quot;,&quot;x3&quot;,&quot;x4&quot;,&quot;x5&quot;,&quot;x6&quot;,&quot;x7&quot;,&quot;x8&quot;,&quot;x9&quot;) 14.3 Descriptive Statistics and Correlations Before conducting a factor analysis, it is important examine descriptive statistics and correlations among variables. 14.3.1 Descriptive Statistics Descriptive statistics are presented in Table ??. Code paged_table(psych::describe(HolzingerSwineford1939[,vars])) Code summaryTable &lt;- HolzingerSwineford1939 %&gt;% dplyr::select(all_of(vars)) %&gt;% summarise(across( everything(), .fns = list( n = ~ length(na.omit(.)), missingness = ~ mean(is.na(.)) * 100, M = ~ mean(., na.rm = TRUE), SD = ~ sd(., na.rm = TRUE), min = ~ min(., na.rm = TRUE), max = ~ max(., na.rm = TRUE), skewness = ~ psych::skew(., na.rm = TRUE), kurtosis = ~ kurtosi(., na.rm = TRUE)), .names = &quot;{.col}.{.fn}&quot;)) %&gt;% pivot_longer( cols = everything(), names_to = c(&quot;variable&quot;,&quot;index&quot;), names_sep = &quot;\\\\.&quot;) %&gt;% pivot_wider( names_from = index, values_from = value) summaryTableTransposed &lt;- summaryTable[-1] %&gt;% t() %&gt;% as.data.frame() %&gt;% setNames(summaryTable$variable) %&gt;% round(., digits = 2) summaryTableTransposed 14.3.2 Correlations Code cor( HolzingerSwineford1939[,vars], use = &quot;pairwise.complete.obs&quot;) x1 x2 x3 x4 x5 x6 x7 x1 1.00000000 0.31671519 0.4592132 0.4009470 0.3060026 0.3204367 0.04336147 x2 0.31671519 1.00000000 0.3373084 0.1474393 0.1925946 0.1686010 -0.08744963 x3 0.45921315 0.33730843 1.0000000 0.1770624 0.1504923 0.1930589 0.08771510 x4 0.40094704 0.14743932 0.1770624 1.0000000 0.7314651 0.6796334 0.14825470 x5 0.30600259 0.19259455 0.1504923 0.7314651 1.0000000 0.6931306 0.10592715 x6 0.32043667 0.16860100 0.1930589 0.6796334 0.6931306 1.0000000 0.09881660 x7 0.04336147 -0.08744963 0.0877151 0.1482547 0.1059272 0.0988166 1.00000000 x8 0.25775028 0.09357234 0.2012953 0.1297963 0.2134497 0.1856636 0.48743045 x9 0.34289304 0.20083617 0.3959389 0.1917355 0.2605835 0.1805793 0.34622036 x8 x9 x1 0.25775028 0.3428930 x2 0.09357234 0.2008362 x3 0.20129535 0.3959389 x4 0.12979634 0.1917355 x5 0.21344970 0.2605835 x6 0.18566357 0.1805793 x7 0.48743045 0.3462204 x8 1.00000000 0.4637719 x9 0.46377193 1.0000000 Correlation matrices of various types using the cor.table() function from the petersenlab package (Petersen, 2024b) are in Tables 14.1, 14.2, and 14.3. Code cor.table( HolzingerSwineford1939[,vars], dig = 2) cor.table( HolzingerSwineford1939[,vars], type = &quot;manuscript&quot;, dig = 2) cor.table( HolzingerSwineford1939[,vars], type = &quot;manuscriptBig&quot;, dig = 2) Table 14.1: Correlation Matrix with r, n, and p-values. x1 x2 x3 x4 x5 x6 x7 x8 x9 1. x1.r 1.00 .32*** .46*** .40*** .31*** .32*** .04 .26*** .34*** 2. sig NA .00 .00 .00 .00 .00 .52 .00 .00 3. n 251 221 211 208 215 205 221 219 215 4. x2.r .32*** 1.00 .34*** .15* .19*** .17* -.09 .09 .20*** 5. sig .00 NA .00 .03 .00 .01 .18 .16 .00 6. n 221 265 225 227 228 216 234 227 229 7. x3.r .46*** .34*** 1.00 .18* .15* .19* .09 .20*** .40*** 8. sig .00 .00 NA .01 .03 .01 .19 .00 .00 9. n 211 225 255 211 215 206 227 219 220 10. x4.r .40*** .15* .18* 1.00 .73*** .68*** .15* .13† .19*** 11. sig .00 .03 .01 NA .00 .00 .03 .06 .00 12. n 208 227 211 252 216 209 224 217 215 13. x5.r .31*** .19*** .15* .73*** 1.00 .69*** .11 .21*** .26*** 14. sig .00 .00 .03 .00 NA .00 .11 .00 .00 15. n 215 228 215 216 259 217 226 222 219 16. x6.r .32*** .17* .19* .68*** .69*** 1.00 .10 .19* .18* 17. sig .00 .01 .01 .00 .00 NA .15 .01 .01 18. n 205 216 206 209 217 248 218 214 209 19. x7.r .04 -.09 .09 .15* .11 .10 1.00 .49*** .35*** 20. sig .52 .18 .19 .03 .11 .15 NA .00 .00 21. n 221 234 227 224 226 218 266 231 226 22. x8.r .26*** .09 .20*** .13† .21*** .19* .49*** 1.00 .46*** 23. sig .00 .16 .00 .06 .00 .01 .00 NA .00 24. n 219 227 219 217 222 214 231 259 222 25. x9.r .34*** .20*** .40*** .19*** .26*** .18* .35*** .46*** 1.00 26. sig .00 .00 .00 .00 .00 .01 .00 .00 NA 27. n 215 229 220 215 219 209 226 222 257 Table 14.2: Correlation Matrix with Asterisks for Significant Associations. x1 x2 x3 x4 x5 x6 x7 x8 x9 1. x1 1.00 2. x2 .32*** 1.00 3. x3 .46*** .34*** 1.00 4. x4 .40*** .15* .18* 1.00 5. x5 .31*** .19*** .15* .73*** 1.00 6. x6 .32*** .17* .19* .68*** .69*** 1.00 7. x7 .04 -.09 .09 .15* .11 .10 1.00 8. x8 .26*** .09 .20*** .13† .21*** .19* .49*** 1.00 9. x9 .34*** .20*** .40*** .19*** .26*** .18* .35*** .46*** 1.00 Table 14.3: Correlation Matrix. x1 x2 x3 x4 x5 x6 x7 x8 x9 1. x1 1.00 2. x2 .32 1.00 3. x3 .46 .34 1.00 4. x4 .40 .15 .18 1.00 5. x5 .31 .19 .15 .73 1.00 6. x6 .32 .17 .19 .68 .69 1.00 7. x7 .04 -.09 .09 .15 .11 .10 1.00 8. x8 .26 .09 .20 .13 .21 .19 .49 1.00 9. x9 .34 .20 .40 .19 .26 .18 .35 .46 1.00 Pairs panel plots were generated using the psych package (Revelle, 2022). Correlation plots were generated using the corrplot package (Wei &amp; Simko, 2021). A pairs panel plot is in Figure 14.26. Code pairs.panels(HolzingerSwineford1939[,vars]) Figure 14.26: Pairs Panel Plot. A correlation plot is in Figure 14.27. Code corrplot(cor( HolzingerSwineford1939[,vars], use = &quot;pairwise.complete.obs&quot;)) Figure 14.27: Correlation Plot. 14.4 Factor Analysis 14.4.1 Exploratory Factor Analysis (EFA) I introduced exploratory factor analysis (EFA) models in Section 14.1.4.3.1. 14.4.1.1 Determine number of factors Determine the number of factors to retain using the Scree plot and Very Simple Structure plot. 14.4.1.1.1 Scree Plot Scree plots were generated using the psych (Revelle, 2022) and nFactors (Raiche &amp; Magis, 2020) packages. The optimal coordinates and the acceleration factor attempt to operationalize the Cattell scree test: i.e., the “elbow” of the scree plot (Ruscio &amp; Roche, 2012). The optimal coordinators factor is quantified using a series of linear equations to determine whether observed eigenvalues exceed the predicted values. The acceleration factor is quantified using the acceleration of the curve, that is, the second derivative. The Kaiser-Guttman rule states to keep principal components whose eigenvalues are greater than 1. However, for exploratory factor analysis (as opposed to PCA), the criterion is to keep the factors whose eigenvalues are greater than zero (i.e., not the factors whose eigenvalues are greater than 1) (Dinno, 2014). The number of factors to keep would depend on which criteria one uses. Based on the rule to keep factors whose eigenvalues are greater than zero and based on the parallel test, we would keep three factors. However, based on the Cattell scree test (as operationalized by the optimal coordinates and acceleration factor), we would keep one factor. Therefore, interpretability of the factors would be important for deciding between whether to keep one, two, or three factors. A scree plot from a parallel analysis is in Figure 14.28. Code fa.parallel( x = HolzingerSwineford1939[,vars], fm = &quot;ml&quot;, fa = &quot;fa&quot;) Figure 14.28: Scree Plot from Parallel Analysis in Exploratory Factor Analysis. Parallel analysis suggests that the number of factors = 3 and the number of components = NA A scree plot from EFA is in Figure 14.29. Code plot( nScree(x = cor( HolzingerSwineford1939[,vars], use = &quot;pairwise.complete.obs&quot;), model = &quot;factors&quot;)) Figure 14.29: Scree Plot in Exploratory Factor Analysis. 14.4.1.1.2 Very Simple Structure (VSS) Plot The very simple structure (VSS) is another criterion that can be used to determine the optimal number of factors or components to retain. Using the VSS criterion, the optimal number of factors to retain is the number of factors that maximizes the VSS criterion (Revelle &amp; Rocklin, 1979). The VSS criterion is evaluated with models in which factor loadings for a given item that are less than the maximum factor loading for that item are suppressed to zero, thus forcing simple structure (i.e., no cross-loadings). The goal is finding a factor structure with interpretability so that factors are clearly distinguishable. Thus, we want to identify the number of factors with the highest VSS criterion (i.e., the highest line). Very simple structure (VSS) plots were generated using the psych package (Revelle, 2022). The output also provides additional criteria by which to determine the optimal number of factors, each for which lower values are better, including the Velicer minimum average partial (MAP) test, the Bayesian information criterion (BIC), the sample size-adjusted BIC (SABIC), and the root mean square error of approximation (RMSEA). 14.4.1.1.2.1 Orthogonal (Varimax) rotation In the example with orthogonal rotation below, the VSS criterion is highest with 3 or 4 factors. A three-factor solution is supported by the lowest BIC, whereas a four-factor solution is supported by the lowest SABIC. A VSS plot is in Figure 14.30. Code vss( HolzingerSwineford1939[,vars], rotate = &quot;varimax&quot;, fm = &quot;ml&quot;) Figure 14.30: Very Simple Structure Plot With Orthogonal Rotation in Exploratory Factor Analysis. Very Simple Structure Call: vss(x = HolzingerSwineford1939[, vars], rotate = &quot;varimax&quot;, fm = &quot;ml&quot;) VSS complexity 1 achieves a maximimum of 0.7 with 4 factors VSS complexity 2 achieves a maximimum of 0.84 with 3 factors The Velicer MAP achieves a minimum of 0.07 with 2 factors BIC achieves a minimum of -32.61 with 3 factors Sample Size adjusted BIC achieves a minimum of -5.65 with 4 factors Statistics by number of factors vss1 vss2 map dof chisq prob sqresid fit RMSEA BIC SABIC complex 1 0.57 0.00 0.078 27 3.2e+02 2.3e-52 7.1 0.57 0.191 168.4 254.05 1.0 2 0.64 0.76 0.067 19 1.4e+02 1.2e-20 3.9 0.76 0.146 32.5 92.75 1.2 3 0.69 0.84 0.071 12 3.6e+01 3.4e-04 2.1 0.87 0.081 -32.6 5.45 1.3 4 0.69 0.83 0.125 6 9.6e+00 1.4e-01 2.0 0.88 0.044 -24.7 -5.65 1.4 5 0.67 0.81 0.199 1 1.8e+00 1.8e-01 1.6 0.90 0.050 -3.9 -0.77 1.5 6 0.70 0.82 0.403 -3 2.6e-08 NA 1.6 0.91 NA NA NA 1.5 7 0.66 0.78 0.447 -6 5.2e-13 NA 1.4 0.91 NA NA NA 1.6 8 0.65 0.74 1.000 -8 0.0e+00 NA 1.3 0.92 NA NA NA 1.7 eChisq SRMR eCRMS eBIC 1 5.5e+02 1.6e-01 0.184 396.3 2 1.6e+02 8.7e-02 0.119 53.8 3 1.2e+01 2.3e-02 0.040 -56.7 4 4.5e+00 1.4e-02 0.035 -29.7 5 8.0e-01 6.1e-03 0.036 -4.9 6 1.3e-08 7.7e-07 NA NA 7 2.1e-13 3.1e-09 NA NA 8 2.1e-16 1.0e-10 NA NA Multiple VSS-related fit indices are in Figure 14.31. Code nfactors( HolzingerSwineford1939[,vars], rotate = &quot;varimax&quot;, fm = &quot;ml&quot;) Figure 14.31: Very Simple Structure-Related Indices With Orthogonal Rotation in Exploratory Factor Analysis. Number of factors Call: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, n.obs = n.obs, plot = FALSE, title = title, use = use, cor = cor) VSS complexity 1 achieves a maximimum of Although the vss.max shows 6 factors, it is probably more reasonable to think about 4 factors VSS complexity 2 achieves a maximimum of 0.84 with 3 factors The Velicer MAP achieves a minimum of 0.07 with 2 factors Empirical BIC achieves a minimum of -56.69 with 3 factors Sample Size adjusted BIC achieves a minimum of -5.65 with 4 factors Statistics by number of factors vss1 vss2 map dof chisq prob sqresid fit RMSEA BIC SABIC complex 1 0.57 0.00 0.078 27 3.2e+02 2.3e-52 7.1 0.57 0.191 168.4 254.05 1.0 2 0.64 0.76 0.067 19 1.4e+02 1.2e-20 3.9 0.76 0.146 32.5 92.75 1.2 3 0.69 0.84 0.071 12 3.6e+01 3.4e-04 2.1 0.87 0.081 -32.6 5.45 1.3 4 0.69 0.83 0.125 6 9.6e+00 1.4e-01 2.0 0.88 0.044 -24.7 -5.65 1.4 5 0.67 0.81 0.199 1 1.8e+00 1.8e-01 1.6 0.90 0.050 -3.9 -0.77 1.5 6 0.70 0.82 0.403 -3 2.6e-08 NA 1.6 0.91 NA NA NA 1.5 7 0.66 0.78 0.447 -6 5.2e-13 NA 1.4 0.91 NA NA NA 1.6 8 0.65 0.74 1.000 -8 0.0e+00 NA 1.3 0.92 NA NA NA 1.7 9 0.66 0.82 NA -9 4.4e+01 NA 2.5 0.85 NA NA NA 1.3 eChisq SRMR eCRMS eBIC 1 5.5e+02 1.6e-01 0.184 396.3 2 1.6e+02 8.7e-02 0.119 53.8 3 1.2e+01 2.3e-02 0.040 -56.7 4 4.5e+00 1.4e-02 0.035 -29.7 5 8.0e-01 6.1e-03 0.036 -4.9 6 1.3e-08 7.7e-07 NA NA 7 2.1e-13 3.1e-09 NA NA 8 2.1e-16 1.0e-10 NA NA 9 2.1e+01 3.1e-02 NA NA 14.4.1.1.2.2 Oblique (Oblimin) rotation In the example with oblique rotation below, the VSS criterion is highest with 3 factors. A three-factor solution is supported by the lowest BIC. A VSS plot is in Figure 14.32. Code vss( HolzingerSwineford1939[,vars], rotate = &quot;oblimin&quot;, fm = &quot;ml&quot;) Figure 14.32: Very Simple Structure Plot With Oblique Rotation in Exploratory Factor Analysis. Very Simple Structure Call: vss(x = HolzingerSwineford1939[, vars], rotate = &quot;oblimin&quot;, fm = &quot;ml&quot;) VSS complexity 1 achieves a maximimum of 0.69 with 3 factors VSS complexity 2 achieves a maximimum of 0.77 with 3 factors The Velicer MAP achieves a minimum of 0.07 with 2 factors BIC achieves a minimum of -32.61 with 3 factors Sample Size adjusted BIC achieves a minimum of -5.65 with 4 factors Statistics by number of factors vss1 vss2 map dof chisq prob sqresid fit RMSEA BIC SABIC complex 1 0.57 0.00 0.078 27 3.2e+02 2.3e-52 7.1 0.57 0.191 168.4 254.05 1.0 2 0.64 0.70 0.067 19 1.4e+02 1.2e-20 4.9 0.70 0.146 32.5 92.75 1.2 3 0.69 0.77 0.071 12 3.6e+01 3.4e-04 3.6 0.78 0.081 -32.6 5.45 1.2 4 0.53 0.69 0.125 6 9.6e+00 1.4e-01 4.8 0.71 0.044 -24.7 -5.65 1.4 5 0.50 0.64 0.199 1 1.8e+00 1.8e-01 5.3 0.68 0.050 -3.9 -0.77 1.5 6 0.41 0.55 0.403 -3 2.6e-08 NA 6.1 0.63 NA NA NA 1.8 7 0.42 0.53 0.447 -6 5.2e-13 NA 6.8 0.59 NA NA NA 1.5 8 0.44 0.49 1.000 -8 0.0e+00 NA 7.5 0.55 NA NA NA 1.5 eChisq SRMR eCRMS eBIC 1 5.5e+02 1.6e-01 0.184 396.3 2 1.6e+02 8.7e-02 0.119 53.8 3 1.2e+01 2.3e-02 0.040 -56.7 4 4.5e+00 1.4e-02 0.035 -29.7 5 8.0e-01 6.1e-03 0.036 -4.9 6 1.3e-08 7.7e-07 NA NA 7 2.1e-13 3.1e-09 NA NA 8 2.1e-16 1.0e-10 NA NA Multiple VSS-related fit indices are in Figure 14.33. Code nfactors( HolzingerSwineford1939[,vars], rotate = &quot;oblimin&quot;, fm = &quot;ml&quot;) Figure 14.33: Very Simple Structure-Related Indices With Oblique Rotation in Exploratory Factor Analysis. Number of factors Call: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, n.obs = n.obs, plot = FALSE, title = title, use = use, cor = cor) VSS complexity 1 achieves a maximimum of 0.69 with 3 factors VSS complexity 2 achieves a maximimum of 0.77 with 3 factors The Velicer MAP achieves a minimum of 0.07 with 2 factors Empirical BIC achieves a minimum of -56.69 with 3 factors Sample Size adjusted BIC achieves a minimum of -5.65 with 4 factors Statistics by number of factors vss1 vss2 map dof chisq prob sqresid fit RMSEA BIC SABIC complex 1 0.57 0.00 0.078 27 3.2e+02 2.3e-52 7.1 0.57 0.191 168.4 254.05 1.0 2 0.64 0.70 0.067 19 1.4e+02 1.2e-20 4.9 0.70 0.146 32.5 92.75 1.2 3 0.69 0.77 0.071 12 3.6e+01 3.4e-04 3.6 0.78 0.081 -32.6 5.45 1.2 4 0.53 0.69 0.125 6 9.6e+00 1.4e-01 4.8 0.71 0.044 -24.7 -5.65 1.4 5 0.50 0.64 0.199 1 1.8e+00 1.8e-01 5.3 0.68 0.050 -3.9 -0.77 1.5 6 0.41 0.55 0.403 -3 2.6e-08 NA 6.1 0.63 NA NA NA 1.8 7 0.42 0.53 0.447 -6 5.2e-13 NA 6.8 0.59 NA NA NA 1.5 8 0.44 0.49 1.000 -8 0.0e+00 NA 7.5 0.55 NA NA NA 1.5 9 0.65 0.72 NA -9 4.4e+01 NA 4.5 0.73 NA NA NA 1.2 eChisq SRMR eCRMS eBIC 1 5.5e+02 1.6e-01 0.184 396.3 2 1.6e+02 8.7e-02 0.119 53.8 3 1.2e+01 2.3e-02 0.040 -56.7 4 4.5e+00 1.4e-02 0.035 -29.7 5 8.0e-01 6.1e-03 0.036 -4.9 6 1.3e-08 7.7e-07 NA NA 7 2.1e-13 3.1e-09 NA NA 8 2.1e-16 1.0e-10 NA NA 9 2.1e+01 3.1e-02 NA NA 14.4.1.1.2.3 No rotation In the example with no rotation below, the VSS criterion is highest with 3 or 4 factors. A three-factor solution is supported by the lowest BIC, whereas a four-factor solution is supported by the lowest SABIC. A VSS plot is in Figure 14.34. Code nfactors( HolzingerSwineford1939[,vars], rotate = &quot;none&quot;, fm = &quot;ml&quot;) Figure 14.34: Very Simple Structure Plot With no Rotation in Exploratory Factor Analysis. Number of factors Call: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, n.obs = n.obs, plot = FALSE, title = title, use = use, cor = cor) VSS complexity 1 achieves a maximimum of 0.62 with 2 factors VSS complexity 2 achieves a maximimum of 0.8 with 5 factors The Velicer MAP achieves a minimum of 0.07 with 2 factors Empirical BIC achieves a minimum of -56.69 with 3 factors Sample Size adjusted BIC achieves a minimum of -5.65 with 4 factors Statistics by number of factors vss1 vss2 map dof chisq prob sqresid fit RMSEA BIC SABIC complex 1 0.57 0.00 0.078 27 3.2e+02 2.3e-52 7.1 0.57 0.191 168.4 254.05 1.0 2 0.62 0.76 0.067 19 1.4e+02 1.2e-20 3.9 0.76 0.146 32.5 92.75 1.5 3 0.61 0.77 0.071 12 3.6e+01 3.4e-04 2.1 0.87 0.081 -32.6 5.45 1.9 4 0.62 0.77 0.125 6 9.6e+00 1.4e-01 2.0 0.88 0.044 -24.7 -5.65 1.8 5 0.44 0.80 0.199 1 1.8e+00 1.8e-01 1.6 0.90 0.050 -3.9 -0.77 2.3 6 0.57 0.69 0.403 -3 2.6e-08 NA 1.6 0.91 NA NA NA 2.4 7 0.61 0.77 0.447 -6 5.2e-13 NA 1.4 0.91 NA NA NA 2.2 8 0.62 0.77 1.000 -8 0.0e+00 NA 1.3 0.92 NA NA NA 2.3 9 0.56 0.74 NA -9 4.4e+01 NA 2.5 0.85 NA NA NA 2.0 eChisq SRMR eCRMS eBIC 1 5.5e+02 1.6e-01 0.184 396.3 2 1.6e+02 8.7e-02 0.119 53.8 3 1.2e+01 2.3e-02 0.040 -56.7 4 4.5e+00 1.4e-02 0.035 -29.7 5 8.0e-01 6.1e-03 0.036 -4.9 6 1.3e-08 7.7e-07 NA NA 7 2.1e-13 3.1e-09 NA NA 8 2.1e-16 1.0e-10 NA NA 9 2.1e+01 3.1e-02 NA NA Multiple VSS-related fit indices are in Figure 14.35. Code nfactors( HolzingerSwineford1939[,vars], rotate = &quot;none&quot;, fm = &quot;ml&quot;) Figure 14.35: Very Simple Structure-Related Indices With no Rotation in Exploratory Factor Analysis. Number of factors Call: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, n.obs = n.obs, plot = FALSE, title = title, use = use, cor = cor) VSS complexity 1 achieves a maximimum of 0.62 with 2 factors VSS complexity 2 achieves a maximimum of 0.8 with 5 factors The Velicer MAP achieves a minimum of 0.07 with 2 factors Empirical BIC achieves a minimum of -56.69 with 3 factors Sample Size adjusted BIC achieves a minimum of -5.65 with 4 factors Statistics by number of factors vss1 vss2 map dof chisq prob sqresid fit RMSEA BIC SABIC complex 1 0.57 0.00 0.078 27 3.2e+02 2.3e-52 7.1 0.57 0.191 168.4 254.05 1.0 2 0.62 0.76 0.067 19 1.4e+02 1.2e-20 3.9 0.76 0.146 32.5 92.75 1.5 3 0.61 0.77 0.071 12 3.6e+01 3.4e-04 2.1 0.87 0.081 -32.6 5.45 1.9 4 0.62 0.77 0.125 6 9.6e+00 1.4e-01 2.0 0.88 0.044 -24.7 -5.65 1.8 5 0.44 0.80 0.199 1 1.8e+00 1.8e-01 1.6 0.90 0.050 -3.9 -0.77 2.3 6 0.57 0.69 0.403 -3 2.6e-08 NA 1.6 0.91 NA NA NA 2.4 7 0.61 0.77 0.447 -6 5.2e-13 NA 1.4 0.91 NA NA NA 2.2 8 0.62 0.77 1.000 -8 0.0e+00 NA 1.3 0.92 NA NA NA 2.3 9 0.56 0.74 NA -9 4.4e+01 NA 2.5 0.85 NA NA NA 2.0 eChisq SRMR eCRMS eBIC 1 5.5e+02 1.6e-01 0.184 396.3 2 1.6e+02 8.7e-02 0.119 53.8 3 1.2e+01 2.3e-02 0.040 -56.7 4 4.5e+00 1.4e-02 0.035 -29.7 5 8.0e-01 6.1e-03 0.036 -4.9 6 1.3e-08 7.7e-07 NA NA 7 2.1e-13 3.1e-09 NA NA 8 2.1e-16 1.0e-10 NA NA 9 2.1e+01 3.1e-02 NA NA 14.4.1.2 Run factor analysis Exploratory factor analysis (EFA) models were fit using the fa() function of the psych package (Revelle, 2022) and the sem() and efa() functions of the lavaan package (Rosseel et al., 2022). 14.4.1.2.1 Orthogonal (Varimax) rotation 14.4.1.2.1.1 psych Fit a different model with each number of possible factors: Code efa1factorOrthogonal &lt;- fa( r = HolzingerSwineford1939[,vars], nfactors = 1, rotate = &quot;varimax&quot;, fm = &quot;ml&quot;) efa2factorOrthogonal &lt;- fa( r = HolzingerSwineford1939[,vars], nfactors = 2, rotate = &quot;varimax&quot;, fm = &quot;ml&quot;) efa3factorOrthogonal &lt;- fa( r = HolzingerSwineford1939[,vars], nfactors = 3, rotate = &quot;varimax&quot;, fm = &quot;ml&quot;) efa4factorOrthogonal &lt;- fa( r = HolzingerSwineford1939[,vars], nfactors = 4, rotate = &quot;varimax&quot;, fm = &quot;ml&quot;) efa5factorOrthogonal &lt;- fa( r = HolzingerSwineford1939[,vars], nfactors = 5, rotate = &quot;varimax&quot;, fm = &quot;ml&quot;) efa6factorOrthogonal &lt;- fa( r = HolzingerSwineford1939[,vars], nfactors = 6, rotate = &quot;varimax&quot;, fm = &quot;ml&quot;) efa7factorOrthogonal &lt;- fa( r = HolzingerSwineford1939[,vars], nfactors = 7, rotate = &quot;varimax&quot;, fm = &quot;ml&quot;) efa8factorOrthogonal &lt;- fa( r = HolzingerSwineford1939[,vars], nfactors = 8, rotate = &quot;varimax&quot;, fm = &quot;ml&quot;) efa9factorOrthogonal &lt;- fa( r = HolzingerSwineford1939[,vars], nfactors = 9, rotate = &quot;varimax&quot;, fm = &quot;ml&quot;) 14.4.1.2.1.2 lavaan Model syntax is specified below: Code efa1factorLavaan_syntax &lt;- &#39; # EFA Factor Loadings efa(&quot;efa1&quot;)*f1 =~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 &#39; efa2factorLavaan_syntax &lt;- &#39; # EFA Factor Loadings efa(&quot;efa1&quot;)*f1 + efa(&quot;efa1&quot;)*f2 =~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 &#39; efa3factorLavaan_syntax &lt;- &#39; # EFA Factor Loadings efa(&quot;efa1&quot;)*f1 + efa(&quot;efa1&quot;)*f2 + efa(&quot;efa1&quot;)*f3 =~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 &#39; efa4factorLavaan_syntax &lt;- &#39; # EFA Factor Loadings efa(&quot;efa1&quot;)*f1 + efa(&quot;efa1&quot;)*f2 + efa(&quot;efa1&quot;)*f3 + efa(&quot;efa1&quot;)*f4 =~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 &#39; efa5factorLavaan_syntax &lt;- &#39; # EFA Factor Loadings efa(&quot;efa1&quot;)*f1 + efa(&quot;efa1&quot;)*f2 + efa(&quot;efa1&quot;)*f3 + efa(&quot;efa1&quot;)*f4 + efa(&quot;efa1&quot;)*f5 =~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 &#39; efa6factorLavaan_syntax &lt;- &#39; # EFA Factor Loadings efa(&quot;efa1&quot;)*f1 + efa(&quot;efa1&quot;)*f2 + efa(&quot;efa1&quot;)*f3 + efa(&quot;efa1&quot;)*f4 + efa(&quot;efa1&quot;)*f5 + efa(&quot;efa1&quot;)*f6 =~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 &#39; efa7factorLavaan_syntax &lt;- &#39; # EFA Factor Loadings efa(&quot;efa1&quot;)*f1 + efa(&quot;efa1&quot;)*f2 + efa(&quot;efa1&quot;)*f3 + efa(&quot;efa1&quot;)*f4 + efa(&quot;efa1&quot;)*f5 + efa(&quot;efa1&quot;)*f6 + efa(&quot;efa1&quot;)*f7 =~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 &#39; efa8factorLavaan_syntax &lt;- &#39; # EFA Factor Loadings efa(&quot;efa1&quot;)*f1 + efa(&quot;efa1&quot;)*f2 + efa(&quot;efa1&quot;)*f3 + efa(&quot;efa1&quot;)*f4 + efa(&quot;efa1&quot;)*f5 + efa(&quot;efa1&quot;)*f6 + efa(&quot;efa1&quot;)*f7 + efa(&quot;efa1&quot;)*f8 =~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 &#39; efa9factorLavaan_syntax &lt;- &#39; # EFA Factor Loadings efa(&quot;efa1&quot;)*f1 + efa(&quot;efa1&quot;)*f2 + efa(&quot;efa1&quot;)*f3 + efa(&quot;efa1&quot;)*f4 + efa(&quot;efa1&quot;)*f5 + efa(&quot;efa1&quot;)*f6 + efa(&quot;efa1&quot;)*f7 + efa(&quot;efa1&quot;)*f8 + efa(&quot;efa1&quot;)*f9 =~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 &#39; The models are fitted below: Code efa1factorOrthogonalLavaan_fit &lt;- sem( efa1factorLavaan_syntax, data = HolzingerSwineford1939, information = &quot;observed&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, rotation = &quot;varimax&quot;, meanstructure = TRUE, rotation.args = list(orthogonal = TRUE)) efa2factorOrthogonalLavaan_fit &lt;- sem( efa2factorLavaan_syntax, data = HolzingerSwineford1939, information = &quot;observed&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, rotation = &quot;varimax&quot;, meanstructure = TRUE, rotation.args = list(orthogonal = TRUE)) efa3factorOrthogonalLavaan_fit &lt;- sem( efa3factorLavaan_syntax, data = HolzingerSwineford1939, information = &quot;observed&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, rotation = &quot;varimax&quot;, meanstructure = TRUE, rotation.args = list(orthogonal = TRUE)) efa4factorOrthogonalLavaan_fit &lt;- sem( efa4factorLavaan_syntax, data = HolzingerSwineford1939, information = &quot;observed&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, rotation = &quot;varimax&quot;, meanstructure = TRUE, rotation.args = list(orthogonal = TRUE)) efa5factorOrthogonalLavaan_fit &lt;- sem( efa5factorLavaan_syntax, data = HolzingerSwineford1939, information = &quot;observed&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, rotation = &quot;varimax&quot;, meanstructure = TRUE, rotation.args = list(orthogonal = TRUE)) efa6factorOrthogonalLavaan_fit &lt;- sem( efa6factorLavaan_syntax, data = HolzingerSwineford1939, information = &quot;observed&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, rotation = &quot;varimax&quot;, meanstructure = TRUE, rotation.args = list(orthogonal = TRUE)) efa7factorOrthogonalLavaan_fit &lt;- sem( efa7factorLavaan_syntax, data = HolzingerSwineford1939, information = &quot;observed&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, rotation = &quot;varimax&quot;, meanstructure = TRUE, rotation.args = list(orthogonal = TRUE)) efa8factorOrthogonalLavaan_fit &lt;- sem( efa8factorLavaan_syntax, data = HolzingerSwineford1939, information = &quot;observed&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, rotation = &quot;varimax&quot;, meanstructure = TRUE, rotation.args = list(orthogonal = TRUE)) efa9factorOrthogonalLavaan_fit &lt;- sem( efa9factorLavaan_syntax, data = HolzingerSwineford1939, information = &quot;observed&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, rotation = &quot;varimax&quot;, meanstructure = TRUE, rotation.args = list(orthogonal = TRUE)) The efa() wrapper can fit multiple orthogonal EFA models in one function call: Code efaOrthogonalLavaan_fit &lt;- efa( data = HolzingerSwineford1939, ov.names = vars, nfactors = 1:5, rotation = &quot;varimax&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, meanstructure = TRUE, rotation.args = list(orthogonal = TRUE) ) The efa() wrapper can also fit individual orthogonal EFA models (with output = \"lavaan\"): Code efaFactor1OrthogonalLavaan_fit &lt;- efa( data = HolzingerSwineford1939, ov.names = vars, nfactors = 1, rotation = &quot;varimax&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, meanstructure = TRUE, rotation.args = list(orthogonal = TRUE), output = &quot;lavaan&quot; ) efaFactor2OrthogonalLavaan_fit &lt;- efa( data = HolzingerSwineford1939, ov.names = vars, nfactors = 2, rotation = &quot;varimax&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, meanstructure = TRUE, rotation.args = list(orthogonal = TRUE), output = &quot;lavaan&quot; ) efaFactor3OrthogonalLavaan_fit &lt;- efa( data = HolzingerSwineford1939, ov.names = vars, nfactors = 3, rotation = &quot;varimax&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, meanstructure = TRUE, rotation.args = list(orthogonal = TRUE), output = &quot;lavaan&quot; ) efaFactor4OrthogonalLavaan_fit &lt;- efa( data = HolzingerSwineford1939, ov.names = vars, nfactors = 4, rotation = &quot;varimax&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, meanstructure = TRUE, rotation.args = list(orthogonal = TRUE), output = &quot;lavaan&quot; ) efaFactor5OrthogonalLavaan_fit &lt;- efa( data = HolzingerSwineford1939, ov.names = vars, nfactors = 5, rotation = &quot;varimax&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, meanstructure = TRUE, rotation.args = list(orthogonal = TRUE), output = &quot;lavaan&quot; ) 14.4.1.2.2 Oblique (Oblimin) rotation 14.4.1.2.2.1 psych Fit a different model with each number of possible factors: Code efa1factorOblique &lt;- fa( r = HolzingerSwineford1939[,vars], nfactors = 1, rotate = &quot;oblimin&quot;, fm = &quot;ml&quot;) efa2factorOblique &lt;- fa( r = HolzingerSwineford1939[,vars], nfactors = 2, rotate = &quot;oblimin&quot;, fm = &quot;ml&quot;) efa3factorOblique &lt;- fa( r = HolzingerSwineford1939[,vars], nfactors = 3, rotate = &quot;oblimin&quot;, fm = &quot;ml&quot;) efa4factorOblique &lt;- fa( r = HolzingerSwineford1939[,vars], nfactors = 4, rotate = &quot;oblimin&quot;, fm = &quot;ml&quot;) efa5factorOblique &lt;- fa( r = HolzingerSwineford1939[,vars], nfactors = 5, rotate = &quot;oblimin&quot;, fm = &quot;ml&quot;) efa6factorOblique &lt;- fa( r = HolzingerSwineford1939[,vars], nfactors = 6, rotate = &quot;oblimin&quot;, fm = &quot;ml&quot;) efa7factorOblique &lt;- fa( r = HolzingerSwineford1939[,vars], nfactors = 7, rotate = &quot;oblimin&quot;, fm = &quot;ml&quot;) efa8factorOblique &lt;- fa( r = HolzingerSwineford1939[,vars], nfactors = 8, rotate = &quot;oblimin&quot;, fm = &quot;ml&quot;) #no convergence efa9factorOblique &lt;- fa( r = HolzingerSwineford1939[,vars], nfactors = 9, rotate = &quot;oblimin&quot;, fm = &quot;ml&quot;) 14.4.1.2.2.2 lavaan The models are fitted below: Code # settings to mimic Mplus mplusRotationArgs &lt;- list( rstarts = 30, row.weights = &quot;none&quot;, algorithm = &quot;gpa&quot;, orthogonal = FALSE, jac.init.rot = TRUE, std.ov = TRUE, # row standard = correlation geomin.epsilon = 0.0001) efa1factorObliqueLavaan_fit &lt;- sem( efa1factorLavaan_syntax, data = HolzingerSwineford1939, information = &quot;observed&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, rotation = &quot;geomin&quot;, meanstructure = TRUE, rotation.args = mplusRotationArgs) efa2factorObliqueLavaan_fit &lt;- sem( efa2factorLavaan_syntax, data = HolzingerSwineford1939, information = &quot;observed&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, rotation = &quot;geomin&quot;, meanstructure = TRUE, rotation.args = mplusRotationArgs) efa3factorObliqueLavaan_fit &lt;- sem( efa3factorLavaan_syntax, data = HolzingerSwineford1939, information = &quot;observed&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, rotation = &quot;geomin&quot;, meanstructure = TRUE, rotation.args = mplusRotationArgs) efa4factorObliqueLavaan_fit &lt;- sem( efa4factorLavaan_syntax, data = HolzingerSwineford1939, information = &quot;observed&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, rotation = &quot;geomin&quot;, meanstructure = TRUE, rotation.args = mplusRotationArgs) efa5factorObliqueLavaan_fit &lt;- sem( efa5factorLavaan_syntax, data = HolzingerSwineford1939, information = &quot;observed&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, rotation = &quot;geomin&quot;, meanstructure = TRUE, rotation.args = mplusRotationArgs) efa6factorObliqueLavaan_fit &lt;- sem( efa6factorLavaan_syntax, data = HolzingerSwineford1939, information = &quot;observed&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, rotation = &quot;geomin&quot;, meanstructure = TRUE, rotation.args = mplusRotationArgs) efa7factorObliqueLavaan_fit &lt;- sem( efa7factorLavaan_syntax, data = HolzingerSwineford1939, information = &quot;observed&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, rotation = &quot;geomin&quot;, meanstructure = TRUE, rotation.args = mplusRotationArgs) efa8factorObliqueLavaan_fit &lt;- sem( efa8factorLavaan_syntax, data = HolzingerSwineford1939, information = &quot;observed&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, rotation = &quot;geomin&quot;, meanstructure = TRUE, rotation.args = mplusRotationArgs) efa9factorObliqueLavaan_fit &lt;- sem( efa9factorLavaan_syntax, data = HolzingerSwineford1939, information = &quot;observed&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, rotation = &quot;geomin&quot;, meanstructure = TRUE, rotation.args = mplusRotationArgs) The efa() wrapper can fit multiple oblique EFA models in one function call: Code efaObliqueLavaan_fit &lt;- efa( data = HolzingerSwineford1939, ov.names = vars, nfactors = 1:5, rotation = &quot;geomin&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, meanstructure = TRUE, rotation.args = mplusRotationArgs ) The efa() wrapper can also fit individual oblique EFA models (with output = \"lavaan\"): Code efaFactor1ObliqueLavaan_fit &lt;- efa( data = HolzingerSwineford1939, ov.names = vars, nfactors = 1, rotation = &quot;geomin&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, meanstructure = TRUE, rotation.args = mplusRotationArgs, output = &quot;lavaan&quot; ) efaFactor2ObliqueLavaan_fit &lt;- efa( data = HolzingerSwineford1939, ov.names = vars, nfactors = 2, rotation = &quot;geomin&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, meanstructure = TRUE, rotation.args = mplusRotationArgs, output = &quot;lavaan&quot; ) efaFactor3ObliqueLavaan_fit &lt;- efa( data = HolzingerSwineford1939, ov.names = vars, nfactors = 3, rotation = &quot;geomin&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, meanstructure = TRUE, rotation.args = mplusRotationArgs, output = &quot;lavaan&quot; ) efaFactor4ObliqueLavaan_fit &lt;- efa( data = HolzingerSwineford1939, ov.names = vars, nfactors = 4, rotation = &quot;geomin&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, meanstructure = TRUE, rotation.args = mplusRotationArgs, output = &quot;lavaan&quot; ) efaFactor5ObliqueLavaan_fit &lt;- efa( data = HolzingerSwineford1939, ov.names = vars, nfactors = 5, rotation = &quot;geomin&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, meanstructure = TRUE, rotation.args = mplusRotationArgs, output = &quot;lavaan&quot; ) 14.4.1.3 Factor Loadings 14.4.1.3.1 Orthogonal (Varimax) rotation 14.4.1.3.1.1 psych The factor loadings and summaries of the model results are below: Code efa1factorOrthogonal Factor Analysis using method = ml Call: fa(r = HolzingerSwineford1939[, vars], nfactors = 1, rotate = &quot;varimax&quot;, fm = &quot;ml&quot;) Standardized loadings (pattern matrix) based upon correlation matrix ML1 h2 u2 com x1 0.45 0.202 0.80 1 x2 0.24 0.056 0.94 1 x3 0.27 0.071 0.93 1 x4 0.84 0.710 0.29 1 x5 0.85 0.722 0.28 1 x6 0.80 0.636 0.36 1 x7 0.17 0.029 0.97 1 x8 0.26 0.068 0.93 1 x9 0.32 0.099 0.90 1 ML1 SS loadings 2.59 Proportion Var 0.29 Mean item complexity = 1 Test of the hypothesis that 1 factor is sufficient. df null model = 36 with the objective function = 3.05 with Chi Square = 904.68 df of the model are 27 and the objective function was 1.09 The root mean square of the residuals (RMSR) is 0.16 The df corrected root mean square of the residuals is 0.18 The harmonic n.obs is 222 with the empirical chi square 406.89 with prob &lt; 2e-69 The total n.obs was 301 with Likelihood Chi Square = 322.51 with prob &lt; 2.3e-52 Tucker Lewis Index of factoring reliability = 0.545 RMSEA index = 0.191 and the 90 % confidence intervals are 0.173 0.21 BIC = 168.42 Fit based upon off diagonal values = 0.75 Measures of factor score adequacy ML1 Correlation of (regression) scores with factors 0.94 Multiple R square of scores with factors 0.88 Minimum correlation of possible factor scores 0.76 Code efa2factorOrthogonal Factor Analysis using method = ml Call: fa(r = HolzingerSwineford1939[, vars], nfactors = 2, rotate = &quot;varimax&quot;, fm = &quot;ml&quot;) Standardized loadings (pattern matrix) based upon correlation matrix ML1 ML2 h2 u2 com x1 0.34 0.43 0.301 0.70 1.9 x2 0.17 0.25 0.092 0.91 1.7 x3 0.13 0.48 0.252 0.75 1.1 x4 0.85 0.14 0.737 0.26 1.1 x5 0.83 0.19 0.726 0.27 1.1 x6 0.79 0.15 0.643 0.36 1.1 x7 0.05 0.44 0.198 0.80 1.0 x8 0.09 0.62 0.388 0.61 1.0 x9 0.11 0.75 0.576 0.42 1.0 ML1 ML2 SS loadings 2.22 1.70 Proportion Var 0.25 0.19 Cumulative Var 0.25 0.43 Proportion Explained 0.57 0.43 Cumulative Proportion 0.57 1.00 Mean item complexity = 1.2 Test of the hypothesis that 2 factors are sufficient. df null model = 36 with the objective function = 3.05 with Chi Square = 904.68 df of the model are 19 and the objective function was 0.48 The root mean square of the residuals (RMSR) is 0.09 The df corrected root mean square of the residuals is 0.12 The harmonic n.obs is 222 with the empirical chi square 120.59 with prob &lt; 8.6e-17 The total n.obs was 301 with Likelihood Chi Square = 140.93 with prob &lt; 1.2e-20 Tucker Lewis Index of factoring reliability = 0.733 RMSEA index = 0.146 and the 90 % confidence intervals are 0.124 0.169 BIC = 32.49 Fit based upon off diagonal values = 0.93 Measures of factor score adequacy ML1 ML2 Correlation of (regression) scores with factors 0.93 0.85 Multiple R square of scores with factors 0.86 0.72 Minimum correlation of possible factor scores 0.73 0.45 Code efa3factorOrthogonal Factor Analysis using method = ml Call: fa(r = HolzingerSwineford1939[, vars], nfactors = 3, rotate = &quot;varimax&quot;, fm = &quot;ml&quot;) Standardized loadings (pattern matrix) based upon correlation matrix ML1 ML3 ML2 h2 u2 com x1 0.28 0.61 0.13 0.47 0.53 1.5 x2 0.11 0.49 -0.04 0.26 0.74 1.1 x3 0.05 0.69 0.16 0.50 0.50 1.1 x4 0.83 0.17 0.08 0.73 0.27 1.1 x5 0.84 0.14 0.13 0.74 0.26 1.1 x6 0.78 0.17 0.09 0.64 0.36 1.1 x7 0.07 -0.08 0.69 0.49 0.51 1.1 x8 0.10 0.17 0.71 0.54 0.46 1.2 x9 0.11 0.42 0.54 0.48 0.52 2.0 ML1 ML3 ML2 SS loadings 2.12 1.38 1.35 Proportion Var 0.24 0.15 0.15 Cumulative Var 0.24 0.39 0.54 Proportion Explained 0.44 0.28 0.28 Cumulative Proportion 0.44 0.72 1.00 Mean item complexity = 1.3 Test of the hypothesis that 3 factors are sufficient. df null model = 36 with the objective function = 3.05 with Chi Square = 904.68 df of the model are 12 and the objective function was 0.12 The root mean square of the residuals (RMSR) is 0.02 The df corrected root mean square of the residuals is 0.04 The harmonic n.obs is 222 with the empirical chi square 8.6 with prob &lt; 0.74 The total n.obs was 301 with Likelihood Chi Square = 35.88 with prob &lt; 0.00034 Tucker Lewis Index of factoring reliability = 0.917 RMSEA index = 0.081 and the 90 % confidence intervals are 0.052 0.113 BIC = -32.61 Fit based upon off diagonal values = 0.99 Measures of factor score adequacy ML1 ML3 ML2 Correlation of (regression) scores with factors 0.93 0.82 0.84 Multiple R square of scores with factors 0.86 0.67 0.71 Minimum correlation of possible factor scores 0.71 0.34 0.41 Code efa4factorOrthogonal Factor Analysis using method = ml Call: fa(r = HolzingerSwineford1939[, vars], nfactors = 4, rotate = &quot;varimax&quot;, fm = &quot;ml&quot;) Standardized loadings (pattern matrix) based upon correlation matrix ML1 ML4 ML3 ML2 h2 u2 com x1 0.26 0.63 0.13 0.13 0.50 0.499 1.5 x2 0.11 0.50 -0.04 -0.08 0.27 0.734 1.2 x3 0.05 0.67 0.17 0.02 0.48 0.522 1.1 x4 0.89 0.17 0.08 0.40 1.00 0.005 1.5 x5 0.93 0.14 0.13 -0.32 1.00 0.005 1.3 x6 0.71 0.19 0.10 0.01 0.55 0.449 1.2 x7 0.07 -0.10 0.74 0.12 0.58 0.417 1.1 x8 0.08 0.19 0.68 -0.07 0.52 0.484 1.2 x9 0.12 0.42 0.53 -0.07 0.48 0.520 2.1 ML1 ML4 ML3 ML2 SS loadings 2.27 1.41 1.38 0.31 Proportion Var 0.25 0.16 0.15 0.03 Cumulative Var 0.25 0.41 0.56 0.60 Proportion Explained 0.42 0.26 0.26 0.06 Cumulative Proportion 0.42 0.68 0.94 1.00 Mean item complexity = 1.4 Test of the hypothesis that 4 factors are sufficient. df null model = 36 with the objective function = 3.05 with Chi Square = 904.68 df of the model are 6 and the objective function was 0.03 The root mean square of the residuals (RMSR) is 0.01 The df corrected root mean square of the residuals is 0.04 The harmonic n.obs is 222 with the empirical chi square 3.28 with prob &lt; 0.77 The total n.obs was 301 with Likelihood Chi Square = 9.57 with prob &lt; 0.14 Tucker Lewis Index of factoring reliability = 0.975 RMSEA index = 0.044 and the 90 % confidence intervals are 0 0.095 BIC = -24.67 Fit based upon off diagonal values = 1 Measures of factor score adequacy ML1 ML4 ML3 ML2 Correlation of (regression) scores with factors 0.99 0.83 0.85 0.99 Multiple R square of scores with factors 0.99 0.68 0.73 0.98 Minimum correlation of possible factor scores 0.97 0.36 0.46 0.96 Code efa5factorOrthogonal Factor Analysis using method = ml Call: fa(r = HolzingerSwineford1939[, vars], nfactors = 5, rotate = &quot;varimax&quot;, fm = &quot;ml&quot;) Standardized loadings (pattern matrix) based upon correlation matrix ML1 ML4 ML5 ML2 ML3 h2 u2 com x1 0.28 0.61 0.02 0.13 -0.15 0.49 0.505 1.6 x2 0.11 0.48 -0.11 0.05 0.06 0.26 0.739 1.3 x3 0.04 0.72 0.14 0.01 0.00 0.54 0.455 1.1 x4 0.89 0.17 0.12 -0.03 -0.31 0.94 0.061 1.4 x5 0.91 0.14 0.05 0.09 0.36 1.00 0.005 1.4 x6 0.72 0.19 0.04 0.08 0.00 0.56 0.437 1.2 x7 0.07 -0.04 0.74 0.22 -0.03 0.60 0.396 1.2 x8 0.09 0.18 0.40 0.89 0.01 1.00 0.005 1.5 x9 0.12 0.46 0.42 0.23 0.12 0.46 0.535 2.8 ML1 ML4 ML5 ML2 ML3 SS loadings 2.27 1.45 0.94 0.93 0.27 Proportion Var 0.25 0.16 0.10 0.10 0.03 Cumulative Var 0.25 0.41 0.52 0.62 0.65 Proportion Explained 0.39 0.25 0.16 0.16 0.05 Cumulative Proportion 0.39 0.64 0.80 0.95 1.00 Mean item complexity = 1.5 Test of the hypothesis that 5 factors are sufficient. df null model = 36 with the objective function = 3.05 with Chi Square = 904.68 df of the model are 1 and the objective function was 0.01 The root mean square of the residuals (RMSR) is 0.01 The df corrected root mean square of the residuals is 0.04 The harmonic n.obs is 222 with the empirical chi square 0.55 with prob &lt; 0.46 The total n.obs was 301 with Likelihood Chi Square = 1.77 with prob &lt; 0.18 Tucker Lewis Index of factoring reliability = 0.968 RMSEA index = 0.05 and the 90 % confidence intervals are 0 0.172 BIC = -3.94 Fit based upon off diagonal values = 1 Measures of factor score adequacy ML1 ML4 ML5 ML2 ML3 Correlation of (regression) scores with factors 0.99 0.84 0.78 0.95 0.93 Multiple R square of scores with factors 0.97 0.70 0.60 0.91 0.87 Minimum correlation of possible factor scores 0.94 0.40 0.20 0.82 0.73 Code efa6factorOrthogonal Factor Analysis using method = ml Call: fa(r = HolzingerSwineford1939[, vars], nfactors = 6, rotate = &quot;varimax&quot;, fm = &quot;ml&quot;) Standardized loadings (pattern matrix) based upon correlation matrix ML1 ML4 ML2 ML6 ML3 ML5 h2 u2 com x1 0.25 0.61 0.13 0.20 0.14 -0.03 0.51 0.489 1.8 x2 0.11 0.47 -0.04 -0.04 0.09 0.05 0.25 0.748 1.2 x3 0.05 0.75 0.15 -0.03 -0.18 -0.06 0.63 0.374 1.2 x4 0.84 0.17 0.08 0.44 -0.06 -0.03 0.94 0.060 1.6 x5 0.90 0.15 0.12 -0.11 0.05 0.32 0.95 0.049 1.4 x6 0.79 0.18 0.09 -0.06 0.04 -0.19 0.70 0.298 1.3 x7 0.07 -0.09 0.75 0.07 -0.21 -0.04 0.62 0.376 1.2 x8 0.09 0.18 0.75 -0.05 0.28 -0.01 0.68 0.321 1.4 x9 0.10 0.43 0.51 -0.01 -0.01 0.13 0.47 0.530 2.2 ML1 ML4 ML2 ML6 ML3 ML5 SS loadings 2.23 1.47 1.45 0.25 0.19 0.16 Proportion Var 0.25 0.16 0.16 0.03 0.02 0.02 Cumulative Var 0.25 0.41 0.57 0.60 0.62 0.64 Proportion Explained 0.39 0.25 0.25 0.04 0.03 0.03 Cumulative Proportion 0.39 0.64 0.89 0.94 0.97 1.00 Mean item complexity = 1.5 Test of the hypothesis that 6 factors are sufficient. df null model = 36 with the objective function = 3.05 with Chi Square = 904.68 df of the model are -3 and the objective function was 0 The root mean square of the residuals (RMSR) is 0 The df corrected root mean square of the residuals is NA The harmonic n.obs is 222 with the empirical chi square 0 with prob &lt; NA The total n.obs was 301 with Likelihood Chi Square = 0 with prob &lt; NA Tucker Lewis Index of factoring reliability = 1.042 Fit based upon off diagonal values = 1 Measures of factor score adequacy ML1 ML4 ML2 ML6 ML3 Correlation of (regression) scores with factors 0.96 0.85 0.88 0.81 0.58 Multiple R square of scores with factors 0.93 0.73 0.78 0.66 0.34 Minimum correlation of possible factor scores 0.86 0.45 0.55 0.32 -0.32 ML5 Correlation of (regression) scores with factors 0.68 Multiple R square of scores with factors 0.46 Minimum correlation of possible factor scores -0.07 Code efa7factorOrthogonal Factor Analysis using method = ml Call: fa(r = HolzingerSwineford1939[, vars], nfactors = 7, rotate = &quot;varimax&quot;, fm = &quot;ml&quot;) Standardized loadings (pattern matrix) based upon correlation matrix ML1 ML3 ML4 ML7 ML5 ML2 ML6 h2 u2 com x1 0.24 0.12 0.50 0.58 0.06 0.00 0.01 0.66 0.3413 2.4 x2 0.12 -0.04 0.52 0.06 -0.01 0.02 0.19 0.33 0.6697 1.4 x3 0.06 0.16 0.70 0.14 0.02 -0.03 -0.19 0.57 0.4300 1.4 x4 0.83 0.09 0.10 0.20 0.50 -0.02 -0.02 0.99 0.0058 1.8 x5 0.91 0.12 0.13 0.02 -0.09 0.32 0.06 0.99 0.0146 1.4 x6 0.79 0.09 0.15 0.08 -0.04 -0.20 -0.01 0.70 0.2999 1.3 x7 0.06 0.69 -0.07 -0.04 0.10 -0.02 -0.14 0.52 0.4786 1.2 x8 0.09 0.77 0.13 0.14 -0.10 -0.03 0.19 0.69 0.3135 1.3 x9 0.11 0.53 0.41 0.09 0.00 0.13 -0.07 0.49 0.5133 2.2 ML1 ML3 ML4 ML7 ML5 ML2 ML6 SS loadings 2.24 1.43 1.24 0.44 0.28 0.16 0.14 Proportion Var 0.25 0.16 0.14 0.05 0.03 0.02 0.02 Cumulative Var 0.25 0.41 0.55 0.59 0.63 0.64 0.66 Proportion Explained 0.38 0.24 0.21 0.07 0.05 0.03 0.02 Cumulative Proportion 0.38 0.62 0.83 0.90 0.95 0.98 1.00 Mean item complexity = 1.6 Test of the hypothesis that 7 factors are sufficient. df null model = 36 with the objective function = 3.05 with Chi Square = 904.68 df of the model are -6 and the objective function was 0 The root mean square of the residuals (RMSR) is 0 The df corrected root mean square of the residuals is NA The harmonic n.obs is 222 with the empirical chi square 0 with prob &lt; NA The total n.obs was 301 with Likelihood Chi Square = 0 with prob &lt; NA Tucker Lewis Index of factoring reliability = 1.042 Fit based upon off diagonal values = 1 Measures of factor score adequacy ML1 ML3 ML4 ML7 ML5 Correlation of (regression) scores with factors 0.97 0.87 0.80 0.65 0.89 Multiple R square of scores with factors 0.94 0.76 0.64 0.42 0.79 Minimum correlation of possible factor scores 0.88 0.52 0.28 -0.16 0.59 ML2 ML6 Correlation of (regression) scores with factors 0.70 0.47 Multiple R square of scores with factors 0.50 0.22 Minimum correlation of possible factor scores -0.01 -0.55 Code efa8factorOrthogonal Factor Analysis using method = ml Call: fa(r = HolzingerSwineford1939[, vars], nfactors = 8, rotate = &quot;varimax&quot;, fm = &quot;ml&quot;) Standardized loadings (pattern matrix) based upon correlation matrix ML1 ML3 ML4 ML7 ML6 ML2 ML5 ML8 h2 u2 com x1 0.23 0.12 0.44 0.62 0.07 0.05 0.00 0.01 0.65 0.3462 2.3 x2 0.11 -0.03 0.57 0.07 0.01 0.00 0.02 -0.10 0.36 0.6438 1.2 x3 0.06 0.14 0.60 0.24 0.18 0.01 -0.05 0.25 0.54 0.4648 2.1 x4 0.82 0.08 0.08 0.22 0.02 0.50 -0.02 0.02 0.99 0.0140 1.9 x5 0.92 0.10 0.13 0.03 0.13 -0.10 0.29 -0.06 0.99 0.0091 1.3 x6 0.79 0.10 0.15 0.10 -0.02 -0.03 -0.22 0.04 0.71 0.2884 1.3 x7 0.06 0.72 -0.09 -0.04 0.04 0.11 0.01 0.16 0.57 0.4309 1.2 x8 0.10 0.74 0.14 0.14 0.06 -0.10 -0.02 -0.16 0.64 0.3580 1.3 x9 0.11 0.49 0.33 0.12 0.48 0.01 0.02 0.02 0.61 0.3912 3.0 ML1 ML3 ML4 ML7 ML6 ML2 ML5 ML8 SS loadings 2.24 1.37 1.07 0.54 0.29 0.28 0.14 0.13 Proportion Var 0.25 0.15 0.12 0.06 0.03 0.03 0.02 0.01 Cumulative Var 0.25 0.40 0.52 0.58 0.61 0.64 0.66 0.67 Proportion Explained 0.37 0.23 0.18 0.09 0.05 0.05 0.02 0.02 Cumulative Proportion 0.37 0.60 0.77 0.86 0.91 0.96 0.98 1.00 Mean item complexity = 1.7 Test of the hypothesis that 8 factors are sufficient. df null model = 36 with the objective function = 3.05 with Chi Square = 904.68 df of the model are -8 and the objective function was 0 The root mean square of the residuals (RMSR) is 0 The df corrected root mean square of the residuals is NA The harmonic n.obs is 222 with the empirical chi square 0 with prob &lt; NA The total n.obs was 301 with Likelihood Chi Square = 0 with prob &lt; NA Tucker Lewis Index of factoring reliability = 1.042 Fit based upon off diagonal values = 1 Measures of factor score adequacy ML1 ML3 ML4 ML7 ML6 Correlation of (regression) scores with factors 0.97 0.86 0.75 0.67 0.55 Multiple R square of scores with factors 0.95 0.74 0.56 0.45 0.30 Minimum correlation of possible factor scores 0.89 0.48 0.12 -0.10 -0.39 ML2 ML5 ML8 Correlation of (regression) scores with factors 0.89 0.67 0.46 Multiple R square of scores with factors 0.79 0.45 0.21 Minimum correlation of possible factor scores 0.59 -0.10 -0.58 Code efa9factorOrthogonal Factor Analysis using method = ml Call: fa(r = HolzingerSwineford1939[, vars], nfactors = 9, rotate = &quot;varimax&quot;, fm = &quot;ml&quot;) Standardized loadings (pattern matrix) based upon correlation matrix ML1 ML3 ML2 ML4 ML6 ML9 ML8 ML7 ML5 h2 u2 com x1 0.29 0.59 0.14 0.07 0 0 0 0 0 0.45 0.55 1.6 x2 0.11 0.49 -0.03 -0.06 0 0 0 0 0 0.26 0.74 1.1 x3 0.07 0.61 0.18 0.03 0 0 0 0 0 0.41 0.59 1.2 x4 0.81 0.17 0.09 0.04 0 0 0 0 0 0.70 0.30 1.1 x5 0.81 0.15 0.14 -0.03 0 0 0 0 0 0.69 0.31 1.1 x6 0.77 0.17 0.09 -0.01 0 0 0 0 0 0.62 0.38 1.1 x7 0.07 -0.06 0.64 0.03 0 0 0 0 0 0.41 0.59 1.1 x8 0.10 0.18 0.66 -0.02 0 0 0 0 0 0.47 0.53 1.2 x9 0.11 0.40 0.53 -0.02 0 0 0 0 0 0.46 0.54 2.0 ML1 ML3 ML2 ML4 ML6 ML9 ML8 ML7 ML5 SS loadings 2.02 1.24 1.20 0.01 0.0 0.0 0.0 0.0 0.0 Proportion Var 0.22 0.14 0.13 0.00 0.0 0.0 0.0 0.0 0.0 Cumulative Var 0.22 0.36 0.50 0.50 0.5 0.5 0.5 0.5 0.5 Proportion Explained 0.45 0.28 0.27 0.00 0.0 0.0 0.0 0.0 0.0 Cumulative Proportion 0.45 0.73 1.00 1.00 1.0 1.0 1.0 1.0 1.0 Mean item complexity = 1.3 Test of the hypothesis that 9 factors are sufficient. df null model = 36 with the objective function = 3.05 with Chi Square = 904.68 df of the model are -9 and the objective function was 0.15 The root mean square of the residuals (RMSR) is 0.03 The df corrected root mean square of the residuals is NA The harmonic n.obs is 222 with the empirical chi square 15.66 with prob &lt; NA The total n.obs was 301 with Likelihood Chi Square = 43.91 with prob &lt; NA Tucker Lewis Index of factoring reliability = 1.249 Fit based upon off diagonal values = 0.99 Measures of factor score adequacy ML1 ML3 ML2 ML4 ML6 ML9 Correlation of (regression) scores with factors 0.90 0.76 0.78 0.14 0 0 Multiple R square of scores with factors 0.81 0.58 0.61 0.02 0 0 Minimum correlation of possible factor scores 0.61 0.17 0.22 -0.96 -1 -1 ML8 ML7 ML5 Correlation of (regression) scores with factors 0 0 0 Multiple R square of scores with factors 0 0 0 Minimum correlation of possible factor scores -1 -1 -1 14.4.1.3.1.2 lavaan The factor loadings and summaries of the model results are below: Code summary( efaOrthogonalLavaan_fit) This is lavaan 0.6.17 -- running exploratory factor analysis Estimator ML Rotation method VARIMAX ORTHOGONAL Rotation algorithm (rstarts) GPA (30) Standardized metric TRUE Row weights Kaiser Number of observations 301 Number of missing patterns 75 Overview models: aic bic sabic chisq df pvalue cfi rmsea nfactors = 1 6655.547 6755.639 6670.010 235.316 27 0.000 0.664 0.190 nfactors = 2 6536.011 6665.760 6554.760 112.999 19 0.000 0.863 0.144 nfactors = 3 6468.608 6624.307 6491.107 23.723 12 0.022 0.980 0.070 nfactors = 4 6464.728 6642.669 6490.440 NA 6 NA 0.999 0.022 nfactors = 5 6469.292 6665.769 6497.683 NA 1 NA 0.999 0.066 Eigenvalues correlation matrix: ev1 ev2 ev3 ev4 ev5 ev6 ev7 ev8 ev9 3.220 1.621 1.380 0.689 0.569 0.542 0.461 0.289 0.229 Number of factors: 1 Standardized loadings: (* = significant at 1% level) f1 unique.var communalities x1 0.447* 0.800 0.200 x2 .* 0.944 0.056 x3 .* 0.928 0.072 x4 0.829* 0.313 0.687 x5 0.851* 0.276 0.724 x6 0.817* 0.332 0.668 x7 .* 0.964 0.036 x8 .* 0.926 0.074 x9 0.335* 0.888 0.112 f1 Sum of squared loadings 2.629 Proportion of total 1.000 Proportion var 0.292 Cumulative var 0.292 Number of factors: 2 Standardized loadings: (* = significant at 1% level) f1 f2 unique.var communalities x1 0.323* 0.427* 0.714 0.286 x2 . . 0.907 0.093 x3 . 0.495* 0.742 0.258 x4 0.840* . 0.279 0.721 x5 0.842* .* 0.259 0.741 x6 0.796* . 0.333 0.667 x7 0.438* 0.804 0.196 x8 0.612* 0.619 0.381 x9 . 0.737* 0.441 0.559 f1 f2 total Sum of sq (ortho) loadings 2.215 1.686 3.900 Proportion of total 0.568 0.432 1.000 Proportion var 0.246 0.187 0.433 Cumulative var 0.246 0.433 0.433 Number of factors: 3 Standardized loadings: (* = significant at 1% level) f1 f2 f3 unique.var communalities x1 0.620* .* . 0.531 0.469 x2 0.497* . 0.741 0.259 x3 0.689* . 0.493 0.507 x4 . 0.830* 0.284 0.716 x5 . 0.854* . 0.241 0.759 x6 .* 0.782* 0.336 0.664 x7 0.706* 0.487 0.513 x8 . 0.689* 0.488 0.512 x9 0.402* . 0.536* 0.533 0.467 f2 f1 f3 total Sum of sq (ortho) loadings 2.143 1.385 1.338 4.865 Proportion of total 0.440 0.285 0.275 1.000 Proportion var 0.238 0.154 0.149 0.541 Cumulative var 0.238 0.392 0.541 0.541 Number of factors: 4 Standardized loadings: (* = significant at 1% level) f1 f2 f3 f4 unique.var communalities x1 0.634* . . 0.516 0.484 x2 0.502 0.733 0.267 x3 0.677* . 0.506 0.494 x4 . 0.435 0.886 0.000 1.000 x5 . . 0.943 . 0.000 1.000 x6 . 0.718 . 0.422 0.578 x7 . . 0.756 0.396 0.604 x8 . 0.657 0.521 0.479 x9 0.414 . 0.527* 0.522 0.478 f3 f1 f4 f2 total Sum of sq (ortho) loadings 2.293 1.417 1.357 0.317 5.384 Proportion of total 0.426 0.263 0.252 0.059 1.000 Proportion var 0.255 0.157 0.151 0.035 0.598 Cumulative var 0.255 0.412 0.563 0.598 0.598 Number of factors: 5 Standardized loadings: (* = significant at 1% level) f1 f2 f3 f4 f5 unique.var communalities x1 0.603 . . . 0.517 0.483 x2 0.473 . 0.748 0.252 x3 0.462 0.871 . 0.000 1.000 x4 . 0.444 0.882* 0.000 1.000 x5 . . 0.942 . 0.000 1.000 x6 . 0.719 . 0.423 0.577 x7 . . 0.761 0.333 0.667 x8 . 0.688 0.475 0.525 x9 0.338 . . 0.534* 0.547 0.453 f4 f5 f1 f2 f3 total Sum of sq (ortho) loadings 2.282 1.418 1.092 0.847 0.316 5.955 Proportion of total 0.383 0.238 0.183 0.142 0.053 1.000 Proportion var 0.254 0.158 0.121 0.094 0.035 0.662 Cumulative var 0.254 0.411 0.533 0.627 0.662 0.662 Code summary( efa1factorOrthogonalLavaan_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 30 iterations Estimator ML Optimization method NLMINB Number of model parameters 27 Rotation method VARIMAX ORTHOGONAL Rotation algorithm (rstarts) GPA (30) Standardized metric TRUE Row weights Kaiser Number of observations 301 Number of missing patterns 75 Model Test User Model: Standard Scaled Test Statistic 239.990 235.316 Degrees of freedom 27 27 P-value (Chi-square) 0.000 0.000 Scaling correction factor 1.020 Yuan-Bentler correction (Mplus variant) Model Test Baseline Model: Test statistic 693.305 646.574 Degrees of freedom 36 36 P-value 0.000 0.000 Scaling correction factor 1.072 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.676 0.659 Tucker-Lewis Index (TLI) 0.568 0.545 Robust Comparative Fit Index (CFI) 0.664 Robust Tucker-Lewis Index (TLI) 0.552 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -3300.774 -3300.774 Scaling correction factor 1.108 for the MLR correction Loglikelihood unrestricted model (H1) -3180.778 -3180.778 Scaling correction factor 1.064 for the MLR correction Akaike (AIC) 6655.547 6655.547 Bayesian (BIC) 6755.639 6755.639 Sample-size adjusted Bayesian (SABIC) 6670.010 6670.010 Root Mean Square Error of Approximation: RMSEA 0.162 0.160 90 Percent confidence interval - lower 0.143 0.142 90 Percent confidence interval - upper 0.181 0.179 P-value H_0: RMSEA &lt;= 0.050 0.000 0.000 P-value H_0: RMSEA &gt;= 0.080 1.000 1.000 Robust RMSEA 0.190 90 Percent confidence interval - lower 0.167 90 Percent confidence interval - upper 0.214 P-value H_0: Robust RMSEA &lt;= 0.050 0.000 P-value H_0: Robust RMSEA &gt;= 0.080 1.000 Standardized Root Mean Square Residual: SRMR 0.129 0.129 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 =~ efa1 x1 0.517 0.087 5.944 0.000 0.517 0.447 x2 0.273 0.083 3.277 0.001 0.273 0.236 x3 0.303 0.087 3.506 0.000 0.303 0.269 x4 0.973 0.067 14.475 0.000 0.973 0.829 x5 1.100 0.066 16.748 0.000 1.100 0.851 x6 0.851 0.068 12.576 0.000 0.851 0.817 x7 0.203 0.078 2.590 0.010 0.203 0.189 x8 0.276 0.077 3.584 0.000 0.276 0.272 x9 0.341 0.082 4.154 0.000 0.341 0.335 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 4.961 0.072 69.035 0.000 4.961 4.290 .x2 6.058 0.071 85.159 0.000 6.058 5.232 .x3 2.222 0.070 31.650 0.000 2.222 1.971 .x4 3.098 0.071 43.846 0.000 3.098 2.639 .x5 4.356 0.078 56.050 0.000 4.356 3.369 .x6 2.188 0.064 34.213 0.000 2.188 2.101 .x7 4.173 0.066 63.709 0.000 4.173 3.895 .x8 5.516 0.063 87.910 0.000 5.516 5.433 .x9 5.407 0.063 85.469 0.000 5.407 5.303 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 1.069 0.116 9.243 0.000 1.069 0.800 .x2 1.266 0.136 9.282 0.000 1.266 0.944 .x3 1.179 0.086 13.758 0.000 1.179 0.928 .x4 0.431 0.062 6.913 0.000 0.431 0.313 .x5 0.461 0.073 6.349 0.000 0.461 0.276 .x6 0.360 0.056 6.458 0.000 0.360 0.332 .x7 1.107 0.085 13.003 0.000 1.107 0.964 .x8 0.955 0.114 8.347 0.000 0.955 0.926 .x9 0.923 0.088 10.486 0.000 0.923 0.888 f1 1.000 1.000 1.000 R-Square: Estimate x1 0.200 x2 0.056 x3 0.072 x4 0.687 x5 0.724 x6 0.668 x7 0.036 x8 0.074 x9 0.112 Code summary( efa2factorOrthogonalLavaan_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 37 iterations Estimator ML Optimization method NLMINB Number of model parameters 35 Rotation method VARIMAX ORTHOGONAL Rotation algorithm (rstarts) GPA (30) Standardized metric TRUE Row weights Kaiser Number of observations 301 Number of missing patterns 75 Model Test User Model: Standard Scaled Test Statistic 104.454 112.999 Degrees of freedom 19 19 P-value (Chi-square) 0.000 0.000 Scaling correction factor 0.924 Yuan-Bentler correction (Mplus variant) Model Test Baseline Model: Test statistic 693.305 646.574 Degrees of freedom 36 36 P-value 0.000 0.000 Scaling correction factor 1.072 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.870 0.846 Tucker-Lewis Index (TLI) 0.754 0.708 Robust Comparative Fit Index (CFI) 0.863 Robust Tucker-Lewis Index (TLI) 0.741 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -3233.006 -3233.006 Scaling correction factor 1.140 for the MLR correction Loglikelihood unrestricted model (H1) -3180.778 -3180.778 Scaling correction factor 1.064 for the MLR correction Akaike (AIC) 6536.011 6536.011 Bayesian (BIC) 6665.760 6665.760 Sample-size adjusted Bayesian (SABIC) 6554.760 6554.760 Root Mean Square Error of Approximation: RMSEA 0.122 0.128 90 Percent confidence interval - lower 0.100 0.105 90 Percent confidence interval - upper 0.146 0.152 P-value H_0: RMSEA &lt;= 0.050 0.000 0.000 P-value H_0: RMSEA &gt;= 0.080 0.999 1.000 Robust RMSEA 0.144 90 Percent confidence interval - lower 0.116 90 Percent confidence interval - upper 0.174 P-value H_0: Robust RMSEA &lt;= 0.050 0.000 P-value H_0: Robust RMSEA &gt;= 0.080 1.000 Standardized Root Mean Square Residual: SRMR 0.073 0.073 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 =~ efa1 x1 0.372 0.108 3.444 0.001 0.372 0.323 x2 0.186 0.104 1.779 0.075 0.186 0.160 x3 0.125 0.095 1.318 0.188 0.125 0.111 x4 0.986 0.069 14.255 0.000 0.986 0.840 x5 1.090 0.068 15.951 0.000 1.090 0.842 x6 0.827 0.061 13.454 0.000 0.827 0.796 x7 0.064 0.061 1.065 0.287 0.064 0.060 x8 0.082 0.056 1.459 0.145 0.082 0.081 x9 0.128 0.072 1.760 0.078 0.128 0.125 f2 =~ efa1 x1 0.492 0.157 3.128 0.002 0.492 0.427 x2 0.301 0.145 2.080 0.038 0.301 0.260 x3 0.558 0.144 3.877 0.000 0.558 0.495 x4 0.151 0.068 2.224 0.026 0.151 0.128 x5 0.231 0.076 3.035 0.002 0.231 0.178 x6 0.189 0.079 2.401 0.016 0.189 0.182 x7 0.470 0.144 3.264 0.001 0.470 0.438 x8 0.619 0.121 5.137 0.000 0.619 0.612 x9 0.751 0.074 10.172 0.000 0.751 0.737 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 ~~ f2 -0.000 -0.000 -0.000 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 4.964 0.072 69.307 0.000 4.964 4.300 .x2 6.058 0.071 85.415 0.000 6.058 5.231 .x3 2.202 0.069 31.736 0.000 2.202 1.955 .x4 3.096 0.071 43.802 0.000 3.096 2.635 .x5 4.348 0.078 55.599 0.000 4.348 3.357 .x6 2.187 0.064 34.263 0.000 2.187 2.105 .x7 4.177 0.065 64.157 0.000 4.177 3.898 .x8 5.516 0.062 89.366 0.000 5.516 5.447 .x9 5.399 0.063 85.200 0.000 5.399 5.297 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 0.951 0.115 8.250 0.000 0.951 0.714 .x2 1.216 0.129 9.398 0.000 1.216 0.907 .x3 0.942 0.143 6.610 0.000 0.942 0.742 .x4 0.385 0.069 5.546 0.000 0.385 0.279 .x5 0.435 0.078 5.558 0.000 0.435 0.259 .x6 0.360 0.056 6.485 0.000 0.360 0.333 .x7 0.924 0.127 7.295 0.000 0.924 0.804 .x8 0.635 0.130 4.869 0.000 0.635 0.619 .x9 0.459 0.090 5.076 0.000 0.459 0.441 f1 1.000 1.000 1.000 f2 1.000 1.000 1.000 R-Square: Estimate x1 0.286 x2 0.093 x3 0.258 x4 0.721 x5 0.741 x6 0.667 x7 0.196 x8 0.381 x9 0.559 Code summary( efa3factorOrthogonalLavaan_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 49 iterations Estimator ML Optimization method NLMINB Number of model parameters 42 Rotation method VARIMAX ORTHOGONAL Rotation algorithm (rstarts) GPA (30) Standardized metric TRUE Row weights Kaiser Number of observations 301 Number of missing patterns 75 Model Test User Model: Standard Scaled Test Statistic 23.051 23.723 Degrees of freedom 12 12 P-value (Chi-square) 0.027 0.022 Scaling correction factor 0.972 Yuan-Bentler correction (Mplus variant) Model Test Baseline Model: Test statistic 693.305 646.574 Degrees of freedom 36 36 P-value 0.000 0.000 Scaling correction factor 1.072 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.983 0.981 Tucker-Lewis Index (TLI) 0.950 0.942 Robust Comparative Fit Index (CFI) 0.980 Robust Tucker-Lewis Index (TLI) 0.939 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -3192.304 -3192.304 Scaling correction factor 1.090 for the MLR correction Loglikelihood unrestricted model (H1) -3180.778 -3180.778 Scaling correction factor 1.064 for the MLR correction Akaike (AIC) 6468.608 6468.608 Bayesian (BIC) 6624.307 6624.307 Sample-size adjusted Bayesian (SABIC) 6491.107 6491.107 Root Mean Square Error of Approximation: RMSEA 0.055 0.057 90 Percent confidence interval - lower 0.018 0.020 90 Percent confidence interval - upper 0.089 0.091 P-value H_0: RMSEA &lt;= 0.050 0.358 0.329 P-value H_0: RMSEA &gt;= 0.080 0.124 0.144 Robust RMSEA 0.070 90 Percent confidence interval - lower 0.017 90 Percent confidence interval - upper 0.115 P-value H_0: Robust RMSEA &lt;= 0.050 0.215 P-value H_0: Robust RMSEA &gt;= 0.080 0.393 Standardized Root Mean Square Residual: SRMR 0.020 0.020 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 =~ efa1 x1 0.718 0.101 7.098 0.000 0.718 0.620 x2 0.574 0.094 6.104 0.000 0.574 0.497 x3 0.776 0.094 8.264 0.000 0.776 0.689 x4 0.159 0.068 2.330 0.020 0.159 0.135 x5 0.166 0.077 2.165 0.030 0.166 0.128 x6 0.216 0.064 3.367 0.001 0.216 0.208 x7 -0.096 0.079 -1.213 0.225 -0.096 -0.090 x8 0.178 0.076 2.329 0.020 0.178 0.176 x9 0.410 0.084 4.899 0.000 0.410 0.402 f2 =~ efa1 x1 0.304 0.075 4.032 0.000 0.304 0.263 x2 0.121 0.067 1.800 0.072 0.121 0.104 x3 0.053 0.056 0.940 0.347 0.053 0.047 x4 0.976 0.071 13.648 0.000 0.976 0.830 x5 1.108 0.072 15.472 0.000 1.108 0.854 x6 0.813 0.060 13.439 0.000 0.813 0.782 x7 0.084 0.052 1.598 0.110 0.084 0.078 x8 0.079 0.052 1.520 0.128 0.079 0.078 x9 0.136 0.056 2.421 0.015 0.136 0.133 f3 =~ efa1 x1 0.145 0.075 1.934 0.053 0.145 0.125 x2 -0.040 0.065 -0.613 0.540 -0.040 -0.034 x3 0.194 0.080 2.421 0.015 0.194 0.172 x4 0.106 0.055 1.919 0.055 0.106 0.090 x5 0.143 0.075 1.901 0.057 0.143 0.110 x6 0.100 0.049 2.035 0.042 0.100 0.097 x7 0.754 0.118 6.383 0.000 0.754 0.706 x8 0.698 0.103 6.783 0.000 0.698 0.689 x9 0.547 0.072 7.552 0.000 0.547 0.536 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 ~~ f2 0.000 0.000 0.000 f3 -0.000 -0.000 -0.000 f2 ~~ f3 0.000 0.000 0.000 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 4.974 0.071 69.866 0.000 4.974 4.297 .x2 6.047 0.070 86.533 0.000 6.047 5.230 .x3 2.221 0.069 32.224 0.000 2.221 1.971 .x4 3.095 0.071 43.794 0.000 3.095 2.634 .x5 4.341 0.078 55.604 0.000 4.341 3.347 .x6 2.188 0.064 34.270 0.000 2.188 2.104 .x7 4.177 0.064 64.895 0.000 4.177 3.914 .x8 5.520 0.062 89.727 0.000 5.520 5.449 .x9 5.401 0.063 86.038 0.000 5.401 5.297 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 0.711 0.121 5.855 0.000 0.711 0.531 .x2 0.991 0.119 8.325 0.000 0.991 0.741 .x3 0.626 0.124 5.040 0.000 0.626 0.493 .x4 0.393 0.073 5.413 0.000 0.393 0.284 .x5 0.406 0.084 4.836 0.000 0.406 0.241 .x6 0.364 0.054 6.681 0.000 0.364 0.336 .x7 0.555 0.174 3.188 0.001 0.555 0.487 .x8 0.501 0.117 4.291 0.000 0.501 0.488 .x9 0.554 0.073 7.638 0.000 0.554 0.533 f1 1.000 1.000 1.000 f2 1.000 1.000 1.000 f3 1.000 1.000 1.000 R-Square: Estimate x1 0.469 x2 0.259 x3 0.507 x4 0.716 x5 0.759 x6 0.664 x7 0.513 x8 0.512 x9 0.467 Code summary( efa4factorOrthogonalLavaan_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 594 iterations Estimator ML Optimization method NLMINB Number of model parameters 48 Rotation method VARIMAX ORTHOGONAL Rotation algorithm (rstarts) GPA (30) Standardized metric TRUE Row weights Kaiser Number of observations 301 Number of missing patterns 75 Model Test User Model: Standard Scaled Test Statistic 5.184 5.276 Degrees of freedom 6 6 P-value (Chi-square) 0.520 0.509 Scaling correction factor 0.983 Yuan-Bentler correction (Mplus variant) Model Test Baseline Model: Test statistic 693.305 646.574 Degrees of freedom 36 36 P-value 0.000 0.000 Scaling correction factor 1.072 User Model versus Baseline Model: Comparative Fit Index (CFI) 1.000 1.000 Tucker-Lewis Index (TLI) 1.007 1.007 Robust Comparative Fit Index (CFI) 1.000 Robust Tucker-Lewis Index (TLI) 1.013 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -3183.370 -3183.370 Scaling correction factor 1.074 for the MLR correction Loglikelihood unrestricted model (H1) -3180.778 -3180.778 Scaling correction factor 1.064 for the MLR correction Akaike (AIC) 6462.741 6462.741 Bayesian (BIC) 6640.682 6640.682 Sample-size adjusted Bayesian (SABIC) 6488.453 6488.453 Root Mean Square Error of Approximation: RMSEA 0.000 0.000 90 Percent confidence interval - lower 0.000 0.000 90 Percent confidence interval - upper 0.069 0.070 P-value H_0: RMSEA &lt;= 0.050 0.844 0.834 P-value H_0: RMSEA &gt;= 0.080 0.021 0.023 Robust RMSEA 0.000 90 Percent confidence interval - lower 0.000 90 Percent confidence interval - upper 0.083 P-value H_0: Robust RMSEA &lt;= 0.050 0.792 P-value H_0: Robust RMSEA &gt;= 0.080 0.059 Standardized Root Mean Square Residual: SRMR 0.013 0.013 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 =~ efa1 x1 0.754 0.101 7.463 0.000 0.754 0.653 x2 0.603 0.087 6.917 0.000 0.603 0.522 x3 0.728 0.083 8.737 0.000 0.728 0.647 x4 0.109 0.157 0.693 0.488 0.109 0.093 x5 0.169 0.110 1.532 0.126 0.169 0.130 x6 0.303 0.116 2.616 0.009 0.303 0.291 x7 -0.112 0.072 -1.561 0.119 -0.112 -0.105 x8 0.204 0.071 2.879 0.004 0.204 0.202 x9 0.430 0.081 5.302 0.000 0.430 0.422 f2 =~ efa1 x1 0.229 0.170 1.349 0.177 0.229 0.198 x2 0.061 0.076 0.794 0.427 0.061 0.052 x3 0.058 0.090 0.640 0.522 0.058 0.051 x4 1.689 0.819 2.062 0.039 1.689 1.440 x5 0.524 0.480 1.094 0.274 0.524 0.404 x6 0.465 0.266 1.746 0.081 0.465 0.447 x7 0.091 0.057 1.592 0.111 0.091 0.085 x8 0.029 0.060 0.485 0.627 0.029 0.029 x9 0.087 0.080 1.080 0.280 0.087 0.085 f3 =~ efa1 x1 0.035 0.285 0.122 0.903 0.035 0.030 x2 0.019 0.153 0.122 0.903 0.019 0.016 x3 -0.000 0.013 -0.012 0.990 -0.000 -0.000 x4 0.040 0.414 0.097 0.923 0.040 0.034 x5 5.127 37.020 0.138 0.890 5.127 3.945 x6 0.127 0.966 0.131 0.896 0.127 0.122 x7 0.002 0.024 0.085 0.932 0.002 0.002 x8 0.017 0.135 0.126 0.900 0.017 0.017 x9 0.037 0.282 0.131 0.896 0.037 0.036 f4 =~ efa1 x1 0.142 0.072 1.968 0.049 0.142 0.122 x2 -0.046 0.059 -0.772 0.440 -0.046 -0.039 x3 0.198 0.076 2.616 0.009 0.198 0.175 x4 0.062 0.083 0.749 0.454 0.062 0.053 x5 0.117 0.055 2.140 0.032 0.117 0.090 x6 0.140 0.067 2.085 0.037 0.140 0.135 x7 0.782 0.097 8.058 0.000 0.782 0.732 x8 0.678 0.084 8.065 0.000 0.678 0.671 x9 0.539 0.071 7.624 0.000 0.539 0.528 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 ~~ f2 -0.000 -0.000 -0.000 f3 -0.000 -0.000 -0.000 f4 -0.000 -0.000 -0.000 f2 ~~ f3 0.000 0.000 0.000 f4 0.000 0.000 0.000 f3 ~~ f4 0.000 0.000 0.000 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 4.975 0.071 70.065 0.000 4.975 4.304 .x2 6.046 0.070 86.350 0.000 6.046 5.227 .x3 2.221 0.069 32.192 0.000 2.221 1.972 .x4 3.092 0.070 43.893 0.000 3.092 2.636 .x5 4.340 0.078 55.459 0.000 4.340 3.340 .x6 2.193 0.064 34.043 0.000 2.193 2.109 .x7 4.178 0.064 64.955 0.000 4.178 3.911 .x8 5.519 0.061 89.828 0.000 5.519 5.455 .x9 5.406 0.062 86.847 0.000 5.406 5.305 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 0.693 0.123 5.631 0.000 0.693 0.519 .x2 0.968 0.117 8.264 0.000 0.968 0.723 .x3 0.696 0.106 6.584 0.000 0.696 0.549 .x4 -1.495 2.775 -0.539 0.590 -1.495 -1.087 .x5 -24.913 380.024 -0.066 0.948 -24.913 -14.754 .x6 0.738 0.227 3.258 0.001 0.738 0.683 .x7 0.509 0.139 3.662 0.000 0.509 0.446 .x8 0.520 0.100 5.216 0.000 0.520 0.508 .x9 0.554 0.071 7.779 0.000 0.554 0.534 f1 1.000 1.000 1.000 f2 1.000 1.000 1.000 f3 1.000 1.000 1.000 f4 1.000 1.000 1.000 R-Square: Estimate x1 0.481 x2 0.277 x3 0.451 x4 NA x5 NA x6 0.317 x7 0.554 x8 0.492 x9 0.466 Code summary( efa5factorOrthogonalLavaan_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 did NOT end normally after 10000 iterations ** WARNING ** Estimates below are most likely unreliable Estimator ML Optimization method NLMINB Number of model parameters 53 Rotation method VARIMAX ORTHOGONAL Rotation algorithm (rstarts) GPA (30) Standardized metric TRUE Row weights Kaiser Number of observations 301 Number of missing patterns 75 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 =~ efa1 x1 30.734 0.002 16167.717 0.000 30.734 26.613 x2 0.007 0.003 2.401 0.016 0.007 0.006 x3 0.007 0.003 2.038 0.042 0.007 0.006 x4 0.007 0.002 3.650 0.000 0.007 0.006 x5 0.004 0.002 2.242 0.025 0.004 0.003 x6 0.003 0.002 1.808 0.071 0.003 0.003 x7 -0.001 0.001 -0.602 0.547 -0.001 -0.001 x8 0.004 0.002 2.232 0.026 0.004 0.004 x9 0.004 0.003 1.676 0.094 0.004 0.004 f2 =~ efa1 x1 0.342 0.160 2.141 0.032 0.342 0.296 x2 0.403 0.212 1.900 0.057 0.403 0.349 x3 1.027 0.448 2.292 0.022 1.027 0.913 x4 0.104 0.070 1.495 0.135 0.104 0.089 x5 0.050 0.064 0.777 0.437 0.050 0.038 x6 0.174 0.109 1.603 0.109 0.174 0.168 x7 -0.080 0.100 -0.795 0.427 -0.080 -0.074 x8 0.093 0.074 1.248 0.212 0.093 0.092 x9 0.296 0.138 2.151 0.031 0.296 0.291 f3 =~ efa1 x1 0.230 0.052 4.390 0.000 0.230 0.199 x2 0.156 0.077 2.014 0.044 0.156 0.135 x3 0.069 0.058 1.200 0.230 0.069 0.062 x4 0.986 0.075 13.232 0.000 0.986 0.841 x5 1.126 0.071 15.960 0.000 1.126 0.867 x6 0.821 0.060 13.626 0.000 0.821 0.790 x7 0.075 0.046 1.647 0.099 0.075 0.071 x8 0.091 0.055 1.673 0.094 0.091 0.090 x9 0.146 0.073 2.009 0.045 0.146 0.144 f4 =~ efa1 x1 -0.024 0.080 -0.297 0.767 -0.024 -0.021 x2 -0.159 0.223 -0.711 0.477 -0.159 -0.137 x3 0.097 0.062 1.568 0.117 0.097 0.086 x4 0.102 0.097 1.048 0.295 0.102 0.087 x5 -0.059 0.121 -0.489 0.625 -0.059 -0.045 x6 0.027 0.066 0.406 0.685 0.027 0.026 x7 1.099 0.971 1.131 0.258 1.099 1.027 x8 0.230 0.107 2.158 0.031 0.230 0.228 x9 0.087 0.067 1.298 0.194 0.087 0.086 f5 =~ efa1 x1 0.201 0.077 2.599 0.009 0.201 0.174 x2 0.171 0.161 1.063 0.288 0.171 0.148 x3 0.182 0.081 2.249 0.025 0.182 0.161 x4 0.030 0.083 0.363 0.717 0.030 0.026 x5 0.255 0.075 3.396 0.001 0.255 0.196 x6 0.107 0.096 1.119 0.263 0.107 0.103 x7 0.430 0.187 2.300 0.021 0.430 0.402 x8 0.599 0.228 2.623 0.009 0.599 0.592 x9 0.673 0.250 2.695 0.007 0.673 0.661 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 ~~ f2 -0.000 -0.000 -0.000 f3 -0.000 -0.000 -0.000 f4 -0.000 -0.000 -0.000 f5 -0.000 -0.000 -0.000 f2 ~~ f3 -0.000 -0.000 -0.000 f4 -0.000 -0.000 -0.000 f5 -0.000 -0.000 -0.000 f3 ~~ f4 0.000 0.000 0.000 f5 0.000 0.000 0.000 f4 ~~ f5 0.000 0.000 0.000 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 4.978 0.071 70.409 0.000 4.978 4.311 .x2 6.047 0.070 86.099 0.000 6.047 5.226 .x3 2.224 0.069 32.249 0.000 2.224 1.977 .x4 3.092 0.070 43.917 0.000 3.092 2.636 .x5 4.340 0.078 55.465 0.000 4.340 3.342 .x6 2.191 0.064 34.071 0.000 2.191 2.107 .x7 4.179 0.064 64.936 0.000 4.179 3.906 .x8 5.520 0.061 89.845 0.000 5.520 5.457 .x9 5.403 0.062 86.798 0.000 5.403 5.306 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 -943.431 0.000 -31143376.464 0.000 -943.431 -707.387 .x2 1.097 0.170 6.458 0.000 1.097 0.820 .x3 0.165 0.908 0.181 0.856 0.165 0.130 .x4 0.382 0.096 3.982 0.000 0.382 0.277 .x5 0.348 0.095 3.678 0.000 0.348 0.207 .x6 0.364 0.059 6.215 0.000 0.364 0.337 .x7 -0.259 1.998 -0.130 0.897 -0.259 -0.226 .x8 0.595 0.258 2.303 0.021 0.595 0.581 .x9 0.467 0.312 1.496 0.135 0.467 0.450 f1 1.000 1.000 1.000 f2 1.000 1.000 1.000 f3 1.000 1.000 1.000 f4 1.000 1.000 1.000 f5 1.000 1.000 1.000 R-Square: Estimate x1 NA x2 0.180 x3 0.870 x4 0.723 x5 0.793 x6 0.663 x7 NA x8 0.419 x9 0.550 Code summary( efa6factorOrthogonalLavaan_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) Error in if (!is.finite(X2) || !is.finite(df) || !is.finite(X2.null) || : missing value where TRUE/FALSE needed Code summary( efa7factorOrthogonalLavaan_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) Error in if (!is.finite(X2) || !is.finite(df) || !is.finite(X2.null) || : missing value where TRUE/FALSE needed Code summary( efa8factorOrthogonalLavaan_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) Error in if (!is.finite(X2) || !is.finite(df) || !is.finite(X2.null) || : missing value where TRUE/FALSE needed Code summary( efa9factorOrthogonalLavaan_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) Error in if (!is.finite(X2) || !is.finite(df) || !is.finite(X2.null) || : missing value where TRUE/FALSE needed 14.4.1.3.2 Oblique (Oblimin) rotation 14.4.1.3.2.1 psych The factor loadings and summaries of the model results are below: Code efa1factorOblique Factor Analysis using method = ml Call: fa(r = HolzingerSwineford1939[, vars], nfactors = 1, rotate = &quot;oblimin&quot;, fm = &quot;ml&quot;) Standardized loadings (pattern matrix) based upon correlation matrix ML1 h2 u2 com x1 0.45 0.202 0.80 1 x2 0.24 0.056 0.94 1 x3 0.27 0.071 0.93 1 x4 0.84 0.710 0.29 1 x5 0.85 0.722 0.28 1 x6 0.80 0.636 0.36 1 x7 0.17 0.029 0.97 1 x8 0.26 0.068 0.93 1 x9 0.32 0.099 0.90 1 ML1 SS loadings 2.59 Proportion Var 0.29 Mean item complexity = 1 Test of the hypothesis that 1 factor is sufficient. df null model = 36 with the objective function = 3.05 with Chi Square = 904.68 df of the model are 27 and the objective function was 1.09 The root mean square of the residuals (RMSR) is 0.16 The df corrected root mean square of the residuals is 0.18 The harmonic n.obs is 222 with the empirical chi square 406.89 with prob &lt; 2e-69 The total n.obs was 301 with Likelihood Chi Square = 322.51 with prob &lt; 2.3e-52 Tucker Lewis Index of factoring reliability = 0.545 RMSEA index = 0.191 and the 90 % confidence intervals are 0.173 0.21 BIC = 168.42 Fit based upon off diagonal values = 0.75 Measures of factor score adequacy ML1 Correlation of (regression) scores with factors 0.94 Multiple R square of scores with factors 0.88 Minimum correlation of possible factor scores 0.76 Code efa2factorOblique Factor Analysis using method = ml Call: fa(r = HolzingerSwineford1939[, vars], nfactors = 2, rotate = &quot;oblimin&quot;, fm = &quot;ml&quot;) Standardized loadings (pattern matrix) based upon correlation matrix ML1 ML2 h2 u2 com x1 0.28 0.38 0.301 0.70 1.8 x2 0.13 0.23 0.092 0.91 1.6 x3 0.05 0.48 0.252 0.75 1.0 x4 0.87 -0.03 0.737 0.26 1.0 x5 0.84 0.03 0.726 0.27 1.0 x6 0.80 -0.01 0.643 0.36 1.0 x7 -0.03 0.46 0.198 0.80 1.0 x8 -0.02 0.63 0.388 0.61 1.0 x9 -0.02 0.77 0.576 0.42 1.0 ML1 ML2 SS loadings 2.25 1.66 Proportion Var 0.25 0.18 Cumulative Var 0.25 0.43 Proportion Explained 0.57 0.43 Cumulative Proportion 0.57 1.00 With factor correlations of ML1 ML2 ML1 1.00 0.36 ML2 0.36 1.00 Mean item complexity = 1.2 Test of the hypothesis that 2 factors are sufficient. df null model = 36 with the objective function = 3.05 with Chi Square = 904.68 df of the model are 19 and the objective function was 0.48 The root mean square of the residuals (RMSR) is 0.09 The df corrected root mean square of the residuals is 0.12 The harmonic n.obs is 222 with the empirical chi square 120.59 with prob &lt; 8.6e-17 The total n.obs was 301 with Likelihood Chi Square = 140.93 with prob &lt; 1.2e-20 Tucker Lewis Index of factoring reliability = 0.733 RMSEA index = 0.146 and the 90 % confidence intervals are 0.124 0.169 BIC = 32.49 Fit based upon off diagonal values = 0.93 Measures of factor score adequacy ML1 ML2 Correlation of (regression) scores with factors 0.94 0.87 Multiple R square of scores with factors 0.88 0.75 Minimum correlation of possible factor scores 0.77 0.50 Code efa3factorOblique Factor Analysis using method = ml Call: fa(r = HolzingerSwineford1939[, vars], nfactors = 3, rotate = &quot;oblimin&quot;, fm = &quot;ml&quot;) Standardized loadings (pattern matrix) based upon correlation matrix ML1 ML3 ML2 h2 u2 com x1 0.20 0.59 0.00 0.47 0.53 1.2 x2 0.05 0.51 -0.14 0.26 0.74 1.2 x3 -0.06 0.71 0.04 0.50 0.50 1.0 x4 0.86 0.01 -0.02 0.73 0.27 1.0 x5 0.86 -0.02 0.04 0.74 0.26 1.0 x6 0.79 0.02 -0.01 0.64 0.36 1.0 x7 0.03 -0.15 0.73 0.49 0.51 1.1 x8 0.01 0.11 0.69 0.54 0.46 1.1 x9 0.01 0.39 0.47 0.48 0.52 1.9 ML1 ML3 ML2 SS loadings 2.19 1.37 1.29 Proportion Var 0.24 0.15 0.14 Cumulative Var 0.24 0.40 0.54 Proportion Explained 0.45 0.28 0.27 Cumulative Proportion 0.45 0.73 1.00 With factor correlations of ML1 ML3 ML2 ML1 1.00 0.35 0.23 ML3 0.35 1.00 0.28 ML2 0.23 0.28 1.00 Mean item complexity = 1.2 Test of the hypothesis that 3 factors are sufficient. df null model = 36 with the objective function = 3.05 with Chi Square = 904.68 df of the model are 12 and the objective function was 0.12 The root mean square of the residuals (RMSR) is 0.02 The df corrected root mean square of the residuals is 0.04 The harmonic n.obs is 222 with the empirical chi square 8.6 with prob &lt; 0.74 The total n.obs was 301 with Likelihood Chi Square = 35.88 with prob &lt; 0.00034 Tucker Lewis Index of factoring reliability = 0.917 RMSEA index = 0.081 and the 90 % confidence intervals are 0.052 0.113 BIC = -32.61 Fit based upon off diagonal values = 0.99 Measures of factor score adequacy ML1 ML3 ML2 Correlation of (regression) scores with factors 0.94 0.85 0.85 Multiple R square of scores with factors 0.88 0.72 0.73 Minimum correlation of possible factor scores 0.77 0.44 0.45 Code efa4factorOblique Factor Analysis using method = ml Call: fa(r = HolzingerSwineford1939[, vars], nfactors = 4, rotate = &quot;oblimin&quot;, fm = &quot;ml&quot;) Standardized loadings (pattern matrix) based upon correlation matrix ML4 ML1 ML2 ML3 h2 u2 com x1 0.63 -0.04 0.25 -0.01 0.50 0.499 1.3 x2 0.50 0.13 -0.06 -0.16 0.27 0.734 1.4 x3 0.70 -0.06 0.01 0.03 0.48 0.522 1.0 x4 0.04 0.07 0.94 0.02 1.00 0.005 1.0 x5 -0.03 0.98 0.04 0.00 1.00 0.005 1.0 x6 0.08 0.43 0.35 0.01 0.55 0.449 2.0 x7 -0.13 -0.08 0.12 0.78 0.58 0.417 1.1 x8 0.18 0.14 -0.12 0.63 0.52 0.484 1.3 x9 0.42 0.14 -0.10 0.43 0.48 0.520 2.3 ML4 ML1 ML2 ML3 SS loadings 1.45 1.39 1.26 1.26 Proportion Var 0.16 0.15 0.14 0.14 Cumulative Var 0.16 0.32 0.46 0.60 Proportion Explained 0.27 0.26 0.24 0.24 Cumulative Proportion 0.27 0.53 0.76 1.00 With factor correlations of ML4 ML1 ML2 ML3 ML4 1.00 0.30 0.25 0.24 ML1 0.30 1.00 0.68 0.18 ML2 0.25 0.68 1.00 0.12 ML3 0.24 0.18 0.12 1.00 Mean item complexity = 1.4 Test of the hypothesis that 4 factors are sufficient. df null model = 36 with the objective function = 3.05 with Chi Square = 904.68 df of the model are 6 and the objective function was 0.03 The root mean square of the residuals (RMSR) is 0.01 The df corrected root mean square of the residuals is 0.04 The harmonic n.obs is 222 with the empirical chi square 3.28 with prob &lt; 0.77 The total n.obs was 301 with Likelihood Chi Square = 9.57 with prob &lt; 0.14 Tucker Lewis Index of factoring reliability = 0.975 RMSEA index = 0.044 and the 90 % confidence intervals are 0 0.095 BIC = -24.67 Fit based upon off diagonal values = 1 Measures of factor score adequacy ML4 ML1 ML2 ML3 Correlation of (regression) scores with factors 0.85 1.00 1.00 0.86 Multiple R square of scores with factors 0.73 0.99 0.99 0.73 Minimum correlation of possible factor scores 0.45 0.99 0.99 0.47 Code efa5factorOblique Factor Analysis using method = ml Call: fa(r = HolzingerSwineford1939[, vars], nfactors = 5, rotate = &quot;oblimin&quot;, fm = &quot;ml&quot;) Standardized loadings (pattern matrix) based upon correlation matrix ML4 ML1 ML3 ML2 ML5 h2 u2 com x1 0.54 -0.08 0.31 0.15 -0.14 0.49 0.505 2.0 x2 0.45 0.13 -0.05 0.05 -0.21 0.26 0.739 1.7 x3 0.76 -0.01 -0.01 -0.06 0.05 0.54 0.455 1.0 x4 0.00 0.06 0.92 -0.02 0.05 0.94 0.061 1.0 x5 -0.01 0.98 0.03 0.00 -0.01 1.00 0.005 1.0 x6 0.04 0.38 0.40 0.07 -0.05 0.56 0.437 2.1 x7 0.00 -0.03 0.09 0.09 0.72 0.60 0.396 1.1 x8 -0.01 0.00 -0.01 1.00 0.02 1.00 0.005 1.0 x9 0.47 0.19 -0.12 0.12 0.30 0.46 0.535 2.4 ML4 ML1 ML3 ML2 ML5 SS loadings 1.37 1.32 1.30 1.13 0.73 Proportion Var 0.15 0.15 0.14 0.13 0.08 Cumulative Var 0.15 0.30 0.44 0.57 0.65 Proportion Explained 0.23 0.23 0.22 0.19 0.12 Cumulative Proportion 0.23 0.46 0.68 0.88 1.00 With factor correlations of ML4 ML1 ML3 ML2 ML5 ML4 1.00 0.25 0.28 0.34 0.08 ML1 0.25 1.00 0.71 0.24 0.08 ML3 0.28 0.71 1.00 0.14 0.07 ML2 0.34 0.24 0.14 1.00 0.54 ML5 0.08 0.08 0.07 0.54 1.00 Mean item complexity = 1.5 Test of the hypothesis that 5 factors are sufficient. df null model = 36 with the objective function = 3.05 with Chi Square = 904.68 df of the model are 1 and the objective function was 0.01 The root mean square of the residuals (RMSR) is 0.01 The df corrected root mean square of the residuals is 0.04 The harmonic n.obs is 222 with the empirical chi square 0.55 with prob &lt; 0.46 The total n.obs was 301 with Likelihood Chi Square = 1.77 with prob &lt; 0.18 Tucker Lewis Index of factoring reliability = 0.968 RMSEA index = 0.05 and the 90 % confidence intervals are 0 0.172 BIC = -3.94 Fit based upon off diagonal values = 1 Measures of factor score adequacy ML4 ML1 ML3 ML2 ML5 Correlation of (regression) scores with factors 0.86 1.00 0.97 1.00 0.82 Multiple R square of scores with factors 0.73 0.99 0.94 0.99 0.67 Minimum correlation of possible factor scores 0.46 0.99 0.88 0.99 0.35 Code efa6factorOblique Factor Analysis using method = ml Call: fa(r = HolzingerSwineford1939[, vars], nfactors = 6, rotate = &quot;oblimin&quot;, fm = &quot;ml&quot;) Standardized loadings (pattern matrix) based upon correlation matrix ML4 ML1 ML3 ML2 ML6 ML5 h2 u2 com x1 0.34 -0.07 0.38 0.30 -0.26 -0.02 0.51 0.489 3.8 x2 0.34 0.14 -0.02 0.11 -0.26 -0.01 0.25 0.748 2.5 x3 0.83 -0.01 -0.03 -0.07 0.04 0.06 0.63 0.374 1.0 x4 -0.04 0.09 0.87 -0.05 0.06 0.08 0.94 0.060 1.1 x5 -0.01 0.92 0.04 0.00 -0.02 0.04 0.95 0.049 1.0 x6 0.08 0.11 0.09 0.08 -0.01 0.65 0.70 0.298 1.2 x7 0.07 -0.04 0.10 0.17 0.69 -0.01 0.62 0.376 1.2 x8 -0.05 0.02 -0.05 0.78 0.12 0.06 0.68 0.321 1.1 x9 0.37 0.25 0.00 0.29 0.17 -0.18 0.47 0.530 3.8 ML4 ML1 ML3 ML2 ML6 ML5 SS loadings 1.19 1.11 1.10 1.02 0.72 0.61 Proportion Var 0.13 0.12 0.12 0.11 0.08 0.07 Cumulative Var 0.13 0.26 0.38 0.49 0.57 0.64 Proportion Explained 0.21 0.19 0.19 0.18 0.13 0.11 Cumulative Proportion 0.21 0.40 0.59 0.77 0.89 1.00 With factor correlations of ML4 ML1 ML3 ML2 ML6 ML5 ML4 1.00 0.19 0.30 0.45 -0.07 0.09 ML1 0.19 1.00 0.69 0.27 0.07 0.74 ML3 0.30 0.69 1.00 0.20 0.03 0.70 ML2 0.45 0.27 0.20 1.00 0.45 0.07 ML6 -0.07 0.07 0.03 0.45 1.00 -0.02 ML5 0.09 0.74 0.70 0.07 -0.02 1.00 Mean item complexity = 1.8 Test of the hypothesis that 6 factors are sufficient. df null model = 36 with the objective function = 3.05 with Chi Square = 904.68 df of the model are -3 and the objective function was 0 The root mean square of the residuals (RMSR) is 0 The df corrected root mean square of the residuals is NA The harmonic n.obs is 222 with the empirical chi square 0 with prob &lt; NA The total n.obs was 301 with Likelihood Chi Square = 0 with prob &lt; NA Tucker Lewis Index of factoring reliability = 1.042 Fit based upon off diagonal values = 1 Measures of factor score adequacy ML4 ML1 ML3 ML2 ML6 ML5 Correlation of (regression) scores with factors 0.86 0.97 0.97 0.87 0.81 0.88 Multiple R square of scores with factors 0.74 0.95 0.93 0.76 0.66 0.77 Minimum correlation of possible factor scores 0.48 0.90 0.86 0.51 0.32 0.53 Code efa7factorOblique Factor Analysis using method = ml Call: fa(r = HolzingerSwineford1939[, vars], nfactors = 7, rotate = &quot;oblimin&quot;, fm = &quot;ml&quot;) Standardized loadings (pattern matrix) based upon correlation matrix ML3 ML2 ML1 ML4 ML6 ML5 ML7 h2 u2 com x1 0.04 0.01 0.06 0.04 0.75 0.00 -0.02 0.66 0.3413 1.0 x2 0.09 0.02 0.12 0.21 -0.01 -0.02 0.49 0.33 0.6697 1.6 x3 -0.06 -0.05 -0.03 0.64 0.17 0.11 0.09 0.57 0.4300 1.3 x4 -0.02 0.02 0.96 -0.02 0.02 0.02 0.01 0.99 0.0058 1.0 x5 -0.01 0.96 0.02 -0.01 0.00 0.03 0.00 0.99 0.0146 1.0 x6 0.06 0.09 0.07 0.04 0.00 0.70 -0.01 0.70 0.2999 1.1 x7 0.50 -0.04 0.16 0.18 -0.14 -0.02 -0.28 0.52 0.4786 2.4 x8 0.84 0.00 -0.04 -0.05 0.07 0.05 0.04 0.69 0.3135 1.0 x9 0.33 0.23 0.01 0.39 0.07 -0.15 -0.01 0.49 0.5133 3.1 ML3 ML2 ML1 ML4 ML6 ML5 ML7 SS loadings 1.19 1.09 1.08 0.81 0.76 0.62 0.39 Proportion Var 0.13 0.12 0.12 0.09 0.08 0.07 0.04 Cumulative Var 0.13 0.25 0.37 0.46 0.55 0.62 0.66 Proportion Explained 0.20 0.18 0.18 0.14 0.13 0.10 0.06 Cumulative Proportion 0.20 0.38 0.57 0.70 0.83 0.94 1.00 With factor correlations of ML3 ML2 ML1 ML4 ML6 ML5 ML7 ML3 1.00 0.24 0.15 0.42 0.23 0.09 -0.27 ML2 0.24 1.00 0.71 0.15 0.32 0.76 0.11 ML1 0.15 0.71 1.00 0.17 0.41 0.74 -0.07 ML4 0.42 0.15 0.17 1.00 0.53 0.10 0.17 ML6 0.23 0.32 0.41 0.53 1.00 0.36 0.46 ML5 0.09 0.76 0.74 0.10 0.36 1.00 0.15 ML7 -0.27 0.11 -0.07 0.17 0.46 0.15 1.00 Mean item complexity = 1.5 Test of the hypothesis that 7 factors are sufficient. df null model = 36 with the objective function = 3.05 with Chi Square = 904.68 df of the model are -6 and the objective function was 0 The root mean square of the residuals (RMSR) is 0 The df corrected root mean square of the residuals is NA The harmonic n.obs is 222 with the empirical chi square 0 with prob &lt; NA The total n.obs was 301 with Likelihood Chi Square = 0 with prob &lt; NA Tucker Lewis Index of factoring reliability = 1.042 Fit based upon off diagonal values = 1 Measures of factor score adequacy ML3 ML2 ML1 ML4 ML6 ML5 Correlation of (regression) scores with factors 0.88 0.99 1.00 0.81 0.85 0.89 Multiple R square of scores with factors 0.77 0.98 0.99 0.66 0.73 0.79 Minimum correlation of possible factor scores 0.55 0.97 0.99 0.32 0.46 0.58 ML7 Correlation of (regression) scores with factors 0.72 Multiple R square of scores with factors 0.52 Minimum correlation of possible factor scores 0.05 Code efa8factorOblique Factor Analysis using method = ml Call: fa(r = HolzingerSwineford1939[, vars], nfactors = 8, rotate = &quot;oblimin&quot;, fm = &quot;ml&quot;) Standardized loadings (pattern matrix) based upon correlation matrix ML1 ML3 ML2 ML4 ML6 ML5 ML7 ML8 h2 u2 com x1 0.03 -0.01 0.06 0.79 0.00 -0.01 -0.01 0.00 0.65 0.3462 1.0 x2 0.04 -0.04 0.09 -0.02 -0.01 -0.03 0.61 0.00 0.36 0.6438 1.1 x3 -0.04 0.05 -0.12 0.27 0.17 0.13 0.26 0.31 0.54 0.4648 4.4 x4 0.01 0.01 0.94 0.03 0.02 0.04 0.02 -0.01 0.99 0.0140 1.0 x5 0.99 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.99 0.0091 1.0 x6 0.02 -0.01 0.04 -0.02 -0.01 0.81 -0.02 0.01 0.71 0.2884 1.0 x7 0.01 0.75 0.08 -0.06 0.01 -0.04 -0.07 0.10 0.57 0.4309 1.1 x8 0.01 0.59 -0.10 0.12 0.11 0.09 0.09 -0.23 0.64 0.3580 1.7 x9 0.01 0.00 0.03 -0.02 0.79 -0.01 -0.02 0.00 0.61 0.3912 1.0 ML1 ML3 ML2 ML4 ML6 ML5 ML7 ML8 SS loadings 1.03 1.00 0.98 0.82 0.77 0.75 0.53 0.18 Proportion Var 0.11 0.11 0.11 0.09 0.09 0.08 0.06 0.02 Cumulative Var 0.11 0.23 0.33 0.43 0.51 0.59 0.65 0.67 Proportion Explained 0.17 0.16 0.16 0.14 0.13 0.12 0.09 0.03 Cumulative Proportion 0.17 0.33 0.50 0.63 0.76 0.88 0.97 1.00 With factor correlations of ML1 ML3 ML2 ML4 ML6 ML5 ML7 ML8 ML1 1.00 0.14 0.72 0.31 0.31 0.81 0.21 -0.10 ML3 0.14 1.00 0.11 0.17 0.63 0.18 -0.01 -0.16 ML2 0.72 0.11 1.00 0.37 0.16 0.76 0.07 0.11 ML4 0.31 0.17 0.37 1.00 0.57 0.45 0.67 0.08 ML6 0.31 0.63 0.16 0.57 1.00 0.30 0.50 0.08 ML5 0.81 0.18 0.76 0.45 0.30 1.00 0.29 -0.01 ML7 0.21 -0.01 0.07 0.67 0.50 0.29 1.00 0.12 ML8 -0.10 -0.16 0.11 0.08 0.08 -0.01 0.12 1.00 Mean item complexity = 1.5 Test of the hypothesis that 8 factors are sufficient. df null model = 36 with the objective function = 3.05 with Chi Square = 904.68 df of the model are -8 and the objective function was 0 The root mean square of the residuals (RMSR) is 0 The df corrected root mean square of the residuals is NA The harmonic n.obs is 222 with the empirical chi square 0 with prob &lt; NA The total n.obs was 301 with Likelihood Chi Square = 0 with prob &lt; NA Tucker Lewis Index of factoring reliability = 1.042 Fit based upon off diagonal values = 1 Measures of factor score adequacy ML1 ML3 ML2 ML4 ML6 ML5 Correlation of (regression) scores with factors 1.00 0.86 0.99 0.87 0.87 0.92 Multiple R square of scores with factors 0.99 0.74 0.98 0.75 0.76 0.84 Minimum correlation of possible factor scores 0.98 0.49 0.97 0.50 0.51 0.68 ML7 ML8 Correlation of (regression) scores with factors 0.78 0.59 Multiple R square of scores with factors 0.61 0.35 Minimum correlation of possible factor scores 0.23 -0.29 Code efa9factorOblique Factor Analysis using method = ml Call: fa(r = HolzingerSwineford1939[, vars], nfactors = 9, rotate = &quot;oblimin&quot;, fm = &quot;ml&quot;) Standardized loadings (pattern matrix) based upon correlation matrix ML1 ML2 ML3 ML4 ML9 ML5 ML6 ML7 ML8 h2 u2 com x1 0.15 0.01 0.59 0.00 0 0 0 0 0 0.45 0.55 1.1 x2 0.07 -0.13 0.38 0.17 0 0 0 0 0 0.26 0.74 1.7 x3 -0.07 0.06 0.60 0.07 0 0 0 0 0 0.41 0.59 1.1 x4 0.81 -0.02 0.06 -0.06 0 0 0 0 0 0.70 0.30 1.0 x5 0.84 0.04 -0.05 0.04 0 0 0 0 0 0.69 0.31 1.0 x6 0.79 -0.01 0.00 0.01 0 0 0 0 0 0.62 0.38 1.0 x7 0.02 0.67 -0.11 -0.06 0 0 0 0 0 0.41 0.59 1.1 x8 0.02 0.65 0.06 0.05 0 0 0 0 0 0.47 0.53 1.0 x9 0.01 0.47 0.28 0.09 0 0 0 0 0 0.46 0.54 1.7 ML1 ML2 ML3 ML4 ML9 ML5 ML6 ML7 ML8 SS loadings 2.07 1.15 1.11 0.14 0.0 0.0 0.0 0.0 0.0 Proportion Var 0.23 0.13 0.12 0.02 0.0 0.0 0.0 0.0 0.0 Cumulative Var 0.23 0.36 0.48 0.50 0.5 0.5 0.5 0.5 0.5 Proportion Explained 0.46 0.26 0.25 0.03 0.0 0.0 0.0 0.0 0.0 Cumulative Proportion 0.46 0.72 0.97 1.00 1.0 1.0 1.0 1.0 1.0 With factor correlations of ML1 ML2 ML3 ML4 ML9 ML5 ML6 ML7 ML8 ML1 1.00 0.26 0.43 0.03 0 0 0 0 0 ML2 0.26 1.00 0.33 0.18 0 0 0 0 0 ML3 0.43 0.33 1.00 0.67 0 0 0 0 0 ML4 0.03 0.18 0.67 1.00 0 0 0 0 0 ML9 0.00 0.00 0.00 0.00 1 0 0 0 0 ML5 0.00 0.00 0.00 0.00 0 1 0 0 0 ML6 0.00 0.00 0.00 0.00 0 0 1 0 0 ML7 0.00 0.00 0.00 0.00 0 0 0 1 0 ML8 0.00 0.00 0.00 0.00 0 0 0 0 1 Mean item complexity = 1.2 Test of the hypothesis that 9 factors are sufficient. df null model = 36 with the objective function = 3.05 with Chi Square = 904.68 df of the model are -9 and the objective function was 0.15 The root mean square of the residuals (RMSR) is 0.03 The df corrected root mean square of the residuals is NA The harmonic n.obs is 222 with the empirical chi square 15.66 with prob &lt; NA The total n.obs was 301 with Likelihood Chi Square = 43.91 with prob &lt; NA Tucker Lewis Index of factoring reliability = 1.249 Fit based upon off diagonal values = 0.99 Measures of factor score adequacy ML1 ML2 ML3 ML4 ML9 ML5 Correlation of (regression) scores with factors 0.92 0.81 0.82 0.61 0 0 Multiple R square of scores with factors 0.84 0.65 0.67 0.38 0 0 Minimum correlation of possible factor scores 0.69 0.30 0.33 -0.25 -1 -1 ML6 ML7 ML8 Correlation of (regression) scores with factors 0 0 0 Multiple R square of scores with factors 0 0 0 Minimum correlation of possible factor scores -1 -1 -1 14.4.1.3.2.2 lavaan The factor loadings and summaries of the model results are below: Code summary( efaObliqueLavaan_fit) This is lavaan 0.6.17 -- running exploratory factor analysis Estimator ML Rotation method GEOMIN OBLIQUE Geomin epsilon 1e-04 Rotation algorithm (rstarts) GPA (30) Standardized metric TRUE Row weights None Number of observations 301 Number of missing patterns 75 Overview models: aic bic sabic chisq df pvalue cfi rmsea nfactors = 1 6655.547 6755.639 6670.010 235.316 27 0.000 0.664 0.190 nfactors = 2 6536.011 6665.760 6554.760 112.999 19 0.000 0.863 0.144 nfactors = 3 6468.608 6624.307 6491.107 23.723 12 0.022 0.980 0.070 nfactors = 4 6464.728 6642.669 6490.440 NA 6 NA 0.999 0.022 nfactors = 5 6469.292 6665.769 6497.683 NA 1 NA 0.999 0.066 Eigenvalues correlation matrix: ev1 ev2 ev3 ev4 ev5 ev6 ev7 ev8 ev9 3.220 1.621 1.380 0.689 0.569 0.542 0.461 0.289 0.229 Number of factors: 1 Standardized loadings: (* = significant at 1% level) f1 unique.var communalities x1 0.447* 0.800 0.200 x2 .* 0.944 0.056 x3 .* 0.928 0.072 x4 0.829* 0.313 0.687 x5 0.851* 0.276 0.724 x6 0.817* 0.332 0.668 x7 .* 0.964 0.036 x8 .* 0.926 0.074 x9 0.335* 0.888 0.112 f1 Sum of squared loadings 2.629 Proportion of total 1.000 Proportion var 0.292 Cumulative var 0.292 Number of factors: 2 Standardized loadings: (* = significant at 1% level) f1 f2 unique.var communalities x1 .* 0.377* 0.714 0.286 x2 . . 0.907 0.093 x3 0.496* 0.742 0.258 x4 0.866* 0.279 0.721 x5 0.860* 0.259 0.741 x6 0.810* 0.333 0.667 x7 0.447* 0.804 0.196 x8 0.625* 0.619 0.381 x9 0.746* 0.441 0.559 f1 f2 total Sum of sq (obliq) loadings 2.270 1.630 3.900 Proportion of total 0.582 0.418 1.000 Proportion var 0.252 0.181 0.433 Cumulative var 0.252 0.433 0.433 Factor correlations: (* = significant at 1% level) f1 f2 f1 1.000 f2 0.362* 1.000 Number of factors: 3 Standardized loadings: (* = significant at 1% level) f1 f2 f3 unique.var communalities x1 0.632* . 0.531 0.469 x2 0.505* . 0.741 0.259 x3 0.753* . 0.493 0.507 x4 0.843* 0.284 0.716 x5 0.869* 0.241 0.759 x6 0.774* 0.336 0.664 x7 0.708* 0.487 0.513 x8 . 0.630* 0.488 0.512 x9 0.487* 0.424* 0.533 0.467 f2 f1 f3 total Sum of sq (obliq) loadings 2.126 1.596 1.143 4.865 Proportion of total 0.437 0.328 0.235 1.000 Proportion var 0.236 0.177 0.127 0.541 Cumulative var 0.236 0.414 0.541 0.541 Factor correlations: (* = significant at 1% level) f1 f2 f3 f1 1.000 f2 0.396* 1.000 f3 0.112 0.107 1.000 Number of factors: 4 Standardized loadings: (* = significant at 1% level) f1 f2 f3 f4 unique.var communalities x1 0.660* . 0.516 0.484 x2 0.523* . 0.733 0.267 x3 0.758* . 0.506 0.494 x4 0.686 0.640 0.000 1.000 x5 1.001 0.000 1.000 x6 . . 0.614 0.422 0.578 x7 . 0.766 0.396 0.604 x8 0.316 0.581 0.521 0.479 x9 0.522 0.392 0.522 0.478 f3 f1 f4 f2 total Sum of sq (obliq) loadings 1.876 1.680 1.137 0.691 5.384 Proportion of total 0.348 0.312 0.211 0.128 1.000 Proportion var 0.208 0.187 0.126 0.077 0.598 Cumulative var 0.208 0.395 0.521 0.598 0.598 Factor correlations: (* = significant at 1% level) f1 f2 f3 f4 f1 1.000 f2 0.065 1.000 f3 0.423 0.137 1.000 f4 0.116 -0.050 0.088 1.000 Number of factors: 5 Standardized loadings: (* = significant at 1% level) f1 f2 f3 f4 f5 unique.var communalities x1 0.555 . . 0.517 0.483 x2 0.493 . 0.748 0.252 x3 1.000 0.000 1.000 x4 0.437 0.730 0.000 1.000 x5 -0.374 1.106 0.000 1.000 x6 0.715 0.423 0.577 x7 -0.633 0.885 0.333 0.667 x8 . . 0.855 0.475 0.525 x9 . 0.619 0.547 0.453 f4 f5 f1 f2 f3 total Sum of sq (obliq) loadings 2.227 1.618 0.979 0.774 0.358 5.955 Proportion of total 0.374 0.272 0.164 0.130 0.060 1.000 Proportion var 0.247 0.180 0.109 0.086 0.040 0.662 Cumulative var 0.247 0.427 0.536 0.622 0.662 0.662 Factor correlations: (* = significant at 1% level) f1 f2 f3 f4 f5 f1 1.000 f2 0.607 1.000 f3 0.104 -0.120 1.000 f4 0.143 0.290 0.436 1.000 f5 0.534 0.462 0.066 0.352 1.000 Code summary( efa1factorObliqueLavaan_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 30 iterations Estimator ML Optimization method NLMINB Number of model parameters 27 Rotation method GEOMIN OBLIQUE Geomin epsilon 1e-04 Rotation algorithm (rstarts) GPA (30) Standardized metric TRUE Row weights None Number of observations 301 Number of missing patterns 75 Model Test User Model: Standard Scaled Test Statistic 239.990 235.316 Degrees of freedom 27 27 P-value (Chi-square) 0.000 0.000 Scaling correction factor 1.020 Yuan-Bentler correction (Mplus variant) Model Test Baseline Model: Test statistic 693.305 646.574 Degrees of freedom 36 36 P-value 0.000 0.000 Scaling correction factor 1.072 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.676 0.659 Tucker-Lewis Index (TLI) 0.568 0.545 Robust Comparative Fit Index (CFI) 0.664 Robust Tucker-Lewis Index (TLI) 0.552 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -3300.774 -3300.774 Scaling correction factor 1.108 for the MLR correction Loglikelihood unrestricted model (H1) -3180.778 -3180.778 Scaling correction factor 1.064 for the MLR correction Akaike (AIC) 6655.547 6655.547 Bayesian (BIC) 6755.639 6755.639 Sample-size adjusted Bayesian (SABIC) 6670.010 6670.010 Root Mean Square Error of Approximation: RMSEA 0.162 0.160 90 Percent confidence interval - lower 0.143 0.142 90 Percent confidence interval - upper 0.181 0.179 P-value H_0: RMSEA &lt;= 0.050 0.000 0.000 P-value H_0: RMSEA &gt;= 0.080 1.000 1.000 Robust RMSEA 0.190 90 Percent confidence interval - lower 0.167 90 Percent confidence interval - upper 0.214 P-value H_0: Robust RMSEA &lt;= 0.050 0.000 P-value H_0: Robust RMSEA &gt;= 0.080 1.000 Standardized Root Mean Square Residual: SRMR 0.129 0.129 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 =~ efa1 x1 0.517 0.087 5.944 0.000 0.517 0.447 x2 0.273 0.083 3.277 0.001 0.273 0.236 x3 0.303 0.087 3.506 0.000 0.303 0.269 x4 0.973 0.067 14.475 0.000 0.973 0.829 x5 1.100 0.066 16.748 0.000 1.100 0.851 x6 0.851 0.068 12.576 0.000 0.851 0.817 x7 0.203 0.078 2.590 0.010 0.203 0.189 x8 0.276 0.077 3.584 0.000 0.276 0.272 x9 0.341 0.082 4.154 0.000 0.341 0.335 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 4.961 0.072 69.035 0.000 4.961 4.290 .x2 6.058 0.071 85.159 0.000 6.058 5.232 .x3 2.222 0.070 31.650 0.000 2.222 1.971 .x4 3.098 0.071 43.846 0.000 3.098 2.639 .x5 4.356 0.078 56.050 0.000 4.356 3.369 .x6 2.188 0.064 34.213 0.000 2.188 2.101 .x7 4.173 0.066 63.709 0.000 4.173 3.895 .x8 5.516 0.063 87.910 0.000 5.516 5.433 .x9 5.407 0.063 85.469 0.000 5.407 5.303 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 1.069 0.116 9.243 0.000 1.069 0.800 .x2 1.266 0.136 9.282 0.000 1.266 0.944 .x3 1.179 0.086 13.758 0.000 1.179 0.928 .x4 0.431 0.062 6.913 0.000 0.431 0.313 .x5 0.461 0.073 6.349 0.000 0.461 0.276 .x6 0.360 0.056 6.458 0.000 0.360 0.332 .x7 1.107 0.085 13.003 0.000 1.107 0.964 .x8 0.955 0.114 8.347 0.000 0.955 0.926 .x9 0.923 0.088 10.486 0.000 0.923 0.888 f1 1.000 1.000 1.000 R-Square: Estimate x1 0.200 x2 0.056 x3 0.072 x4 0.687 x5 0.724 x6 0.668 x7 0.036 x8 0.074 x9 0.112 Code summary( efa2factorObliqueLavaan_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 37 iterations Estimator ML Optimization method NLMINB Number of model parameters 35 Rotation method GEOMIN OBLIQUE Geomin epsilon 1e-04 Rotation algorithm (rstarts) GPA (30) Standardized metric TRUE Row weights None Number of observations 301 Number of missing patterns 75 Model Test User Model: Standard Scaled Test Statistic 104.454 112.999 Degrees of freedom 19 19 P-value (Chi-square) 0.000 0.000 Scaling correction factor 0.924 Yuan-Bentler correction (Mplus variant) Model Test Baseline Model: Test statistic 693.305 646.574 Degrees of freedom 36 36 P-value 0.000 0.000 Scaling correction factor 1.072 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.870 0.846 Tucker-Lewis Index (TLI) 0.754 0.708 Robust Comparative Fit Index (CFI) 0.863 Robust Tucker-Lewis Index (TLI) 0.741 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -3233.006 -3233.006 Scaling correction factor 1.140 for the MLR correction Loglikelihood unrestricted model (H1) -3180.778 -3180.778 Scaling correction factor 1.064 for the MLR correction Akaike (AIC) 6536.011 6536.011 Bayesian (BIC) 6665.760 6665.760 Sample-size adjusted Bayesian (SABIC) 6554.760 6554.760 Root Mean Square Error of Approximation: RMSEA 0.122 0.128 90 Percent confidence interval - lower 0.100 0.105 90 Percent confidence interval - upper 0.146 0.152 P-value H_0: RMSEA &lt;= 0.050 0.000 0.000 P-value H_0: RMSEA &gt;= 0.080 0.999 1.000 Robust RMSEA 0.144 90 Percent confidence interval - lower 0.116 90 Percent confidence interval - upper 0.174 P-value H_0: Robust RMSEA &lt;= 0.050 0.000 P-value H_0: Robust RMSEA &gt;= 0.080 1.000 Standardized Root Mean Square Residual: SRMR 0.073 0.073 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 =~ efa1 x1 0.308 0.115 2.685 0.007 0.308 0.267 x2 0.144 0.116 1.240 0.215 0.144 0.124 x3 0.034 0.102 0.339 0.734 0.034 0.031 x4 1.017 0.080 12.762 0.000 1.017 0.866 x5 1.113 0.069 16.232 0.000 1.113 0.860 x6 0.842 0.065 12.867 0.000 0.842 0.810 x7 -0.014 0.095 -0.149 0.881 -0.014 -0.013 x8 -0.022 0.093 -0.242 0.809 -0.022 -0.022 x9 0.003 0.020 0.159 0.873 0.003 0.003 f2 =~ efa1 x1 0.435 0.167 2.614 0.009 0.435 0.377 x2 0.275 0.159 1.731 0.083 0.275 0.238 x3 0.558 0.157 3.550 0.000 0.558 0.496 x4 -0.058 0.080 -0.729 0.466 -0.058 -0.049 x5 0.003 0.012 0.252 0.801 0.003 0.002 x6 0.017 0.073 0.234 0.815 0.017 0.016 x7 0.479 0.162 2.963 0.003 0.479 0.447 x8 0.633 0.139 4.542 0.000 0.633 0.625 x9 0.761 0.074 10.322 0.000 0.761 0.746 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 ~~ f2 0.362 0.082 4.390 0.000 0.362 0.362 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 4.964 0.072 69.307 0.000 4.964 4.300 .x2 6.058 0.071 85.415 0.000 6.058 5.231 .x3 2.202 0.069 31.736 0.000 2.202 1.955 .x4 3.096 0.071 43.802 0.000 3.096 2.635 .x5 4.348 0.078 55.599 0.000 4.348 3.357 .x6 2.187 0.064 34.263 0.000 2.187 2.105 .x7 4.177 0.065 64.157 0.000 4.177 3.898 .x8 5.516 0.062 89.366 0.000 5.516 5.447 .x9 5.399 0.063 85.200 0.000 5.399 5.297 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 0.951 0.115 8.250 0.000 0.951 0.714 .x2 1.216 0.129 9.398 0.000 1.216 0.907 .x3 0.942 0.143 6.610 0.000 0.942 0.742 .x4 0.385 0.069 5.546 0.000 0.385 0.279 .x5 0.435 0.078 5.558 0.000 0.435 0.259 .x6 0.360 0.056 6.485 0.000 0.360 0.333 .x7 0.924 0.127 7.295 0.000 0.924 0.804 .x8 0.635 0.130 4.869 0.000 0.635 0.619 .x9 0.459 0.090 5.076 0.000 0.459 0.441 f1 1.000 1.000 1.000 f2 1.000 1.000 1.000 R-Square: Estimate x1 0.286 x2 0.093 x3 0.258 x4 0.721 x5 0.741 x6 0.667 x7 0.196 x8 0.381 x9 0.559 Code summary( efa3factorObliqueLavaan_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 49 iterations Estimator ML Optimization method NLMINB Number of model parameters 42 Rotation method GEOMIN OBLIQUE Geomin epsilon 1e-04 Rotation algorithm (rstarts) GPA (30) Standardized metric TRUE Row weights None Number of observations 301 Number of missing patterns 75 Model Test User Model: Standard Scaled Test Statistic 23.051 23.723 Degrees of freedom 12 12 P-value (Chi-square) 0.027 0.022 Scaling correction factor 0.972 Yuan-Bentler correction (Mplus variant) Model Test Baseline Model: Test statistic 693.305 646.574 Degrees of freedom 36 36 P-value 0.000 0.000 Scaling correction factor 1.072 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.983 0.981 Tucker-Lewis Index (TLI) 0.950 0.942 Robust Comparative Fit Index (CFI) 0.980 Robust Tucker-Lewis Index (TLI) 0.939 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -3192.304 -3192.304 Scaling correction factor 1.090 for the MLR correction Loglikelihood unrestricted model (H1) -3180.778 -3180.778 Scaling correction factor 1.064 for the MLR correction Akaike (AIC) 6468.608 6468.608 Bayesian (BIC) 6624.307 6624.307 Sample-size adjusted Bayesian (SABIC) 6491.107 6491.107 Root Mean Square Error of Approximation: RMSEA 0.055 0.057 90 Percent confidence interval - lower 0.018 0.020 90 Percent confidence interval - upper 0.089 0.091 P-value H_0: RMSEA &lt;= 0.050 0.358 0.329 P-value H_0: RMSEA &gt;= 0.080 0.124 0.144 Robust RMSEA 0.070 90 Percent confidence interval - lower 0.017 90 Percent confidence interval - upper 0.115 P-value H_0: Robust RMSEA &lt;= 0.050 0.215 P-value H_0: Robust RMSEA &gt;= 0.080 0.393 Standardized Root Mean Square Residual: SRMR 0.020 0.020 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 =~ efa1 x1 0.732 0.117 6.277 0.000 0.732 0.632 x2 0.584 0.091 6.402 0.000 0.584 0.505 x3 0.848 0.103 8.206 0.000 0.848 0.753 x4 0.006 0.077 0.079 0.937 0.006 0.005 x5 -0.005 0.033 -0.151 0.880 -0.005 -0.004 x6 0.097 0.078 1.231 0.218 0.097 0.093 x7 -0.001 0.002 -0.475 0.635 -0.001 -0.001 x8 0.284 0.114 2.486 0.013 0.284 0.280 x9 0.496 0.095 5.244 0.000 0.496 0.487 f2 =~ efa1 x1 0.139 0.123 1.134 0.257 0.139 0.120 x2 -0.008 0.040 -0.199 0.843 -0.008 -0.007 x3 -0.145 0.118 -1.226 0.220 -0.145 -0.129 x4 0.991 0.082 12.070 0.000 0.991 0.843 x5 1.127 0.073 15.377 0.000 1.127 0.869 x6 0.804 0.066 12.258 0.000 0.804 0.774 x7 0.060 0.103 0.587 0.557 0.060 0.056 x8 -0.008 0.034 -0.220 0.826 -0.008 -0.007 x9 0.008 0.042 0.186 0.853 0.008 0.008 f3 =~ efa1 x1 -0.043 0.122 -0.353 0.724 -0.043 -0.037 x2 -0.180 0.114 -1.569 0.117 -0.180 -0.155 x3 0.006 0.010 0.611 0.541 0.006 0.006 x4 0.007 0.072 0.102 0.919 0.007 0.006 x5 0.034 0.086 0.399 0.690 0.034 0.026 x6 -0.001 0.008 -0.161 0.872 -0.001 -0.001 x7 0.756 0.125 6.062 0.000 0.756 0.708 x8 0.638 0.130 4.913 0.000 0.638 0.630 x9 0.432 0.092 4.700 0.000 0.432 0.424 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 ~~ f2 0.396 0.122 3.255 0.001 0.396 0.396 f3 0.112 0.125 0.897 0.370 0.112 0.112 f2 ~~ f3 0.107 0.135 0.798 0.425 0.107 0.107 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 4.974 0.071 69.866 0.000 4.974 4.297 .x2 6.047 0.070 86.533 0.000 6.047 5.230 .x3 2.221 0.069 32.224 0.000 2.221 1.971 .x4 3.095 0.071 43.794 0.000 3.095 2.634 .x5 4.341 0.078 55.604 0.000 4.341 3.347 .x6 2.188 0.064 34.270 0.000 2.188 2.104 .x7 4.177 0.064 64.895 0.000 4.177 3.914 .x8 5.520 0.062 89.727 0.000 5.520 5.449 .x9 5.401 0.063 86.038 0.000 5.401 5.297 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 0.711 0.121 5.855 0.000 0.711 0.531 .x2 0.991 0.119 8.325 0.000 0.991 0.741 .x3 0.626 0.124 5.040 0.000 0.626 0.493 .x4 0.393 0.073 5.413 0.000 0.393 0.284 .x5 0.406 0.084 4.836 0.000 0.406 0.241 .x6 0.364 0.054 6.681 0.000 0.364 0.336 .x7 0.555 0.174 3.187 0.001 0.555 0.487 .x8 0.501 0.117 4.291 0.000 0.501 0.488 .x9 0.554 0.073 7.638 0.000 0.554 0.533 f1 1.000 1.000 1.000 f2 1.000 1.000 1.000 f3 1.000 1.000 1.000 R-Square: Estimate x1 0.469 x2 0.259 x3 0.507 x4 0.716 x5 0.759 x6 0.664 x7 0.513 x8 0.512 x9 0.467 Code summary( efa4factorObliqueLavaan_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 594 iterations Estimator ML Optimization method NLMINB Number of model parameters 48 Rotation method GEOMIN OBLIQUE Geomin epsilon 1e-04 Rotation algorithm (rstarts) GPA (30) Standardized metric TRUE Row weights None Number of observations 301 Number of missing patterns 75 Model Test User Model: Standard Scaled Test Statistic 5.184 5.276 Degrees of freedom 6 6 P-value (Chi-square) 0.520 0.509 Scaling correction factor 0.983 Yuan-Bentler correction (Mplus variant) Model Test Baseline Model: Test statistic 693.305 646.574 Degrees of freedom 36 36 P-value 0.000 0.000 Scaling correction factor 1.072 User Model versus Baseline Model: Comparative Fit Index (CFI) 1.000 1.000 Tucker-Lewis Index (TLI) 1.007 1.007 Robust Comparative Fit Index (CFI) 1.000 Robust Tucker-Lewis Index (TLI) 1.013 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -3183.370 -3183.370 Scaling correction factor 1.074 for the MLR correction Loglikelihood unrestricted model (H1) -3180.778 -3180.778 Scaling correction factor 1.064 for the MLR correction Akaike (AIC) 6462.741 6462.741 Bayesian (BIC) 6640.682 6640.682 Sample-size adjusted Bayesian (SABIC) 6488.453 6488.453 Root Mean Square Error of Approximation: RMSEA 0.000 0.000 90 Percent confidence interval - lower 0.000 0.000 90 Percent confidence interval - upper 0.069 0.070 P-value H_0: RMSEA &lt;= 0.050 0.844 0.834 P-value H_0: RMSEA &gt;= 0.080 0.021 0.023 Robust RMSEA 0.000 90 Percent confidence interval - lower 0.000 90 Percent confidence interval - upper 0.083 P-value H_0: Robust RMSEA &lt;= 0.050 0.792 P-value H_0: Robust RMSEA &gt;= 0.080 0.059 Standardized Root Mean Square Residual: SRMR 0.013 0.013 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 =~ efa1 x1 0.656 0.125 5.245 0.000 0.656 0.567 x2 0.652 0.121 5.387 0.000 0.652 0.564 x3 0.718 0.115 6.266 0.000 0.718 0.638 x4 -0.985 1.426 -0.690 0.490 -0.985 -0.840 x5 -0.000 0.002 -0.052 0.959 -0.000 -0.000 x6 0.001 0.004 0.343 0.732 0.001 0.001 x7 -0.426 0.166 -2.566 0.010 -0.426 -0.399 x8 0.001 0.001 0.960 0.337 0.001 0.001 x9 0.261 0.099 2.646 0.008 0.261 0.256 f2 =~ efa1 x1 0.215 0.124 1.738 0.082 0.215 0.186 x2 0.045 0.069 0.649 0.517 0.045 0.039 x3 0.000 0.044 0.003 0.998 0.000 0.000 x4 2.077 1.517 1.369 0.171 2.077 1.770 x5 0.000 0.002 0.025 0.980 0.000 0.000 x6 0.523 0.281 1.860 0.063 0.523 0.503 x7 0.002 0.032 0.066 0.947 0.002 0.002 x8 -0.079 0.058 -1.359 0.174 -0.079 -0.078 x9 -0.002 0.032 -0.059 0.953 -0.002 -0.002 f3 =~ efa1 x1 0.002 0.017 0.093 0.926 0.002 0.001 x2 -0.004 0.029 -0.126 0.899 -0.004 -0.003 x3 -0.030 0.230 -0.129 0.897 -0.030 -0.026 x4 -0.000 0.005 -0.028 0.978 -0.000 -0.000 x5 5.158 36.684 0.141 0.888 5.158 3.969 x6 0.105 0.791 0.133 0.894 0.105 0.101 x7 -0.004 0.022 -0.185 0.853 -0.004 -0.004 x8 0.002 0.029 0.071 0.943 0.002 0.002 x9 0.014 0.109 0.131 0.896 0.014 0.014 f4 =~ efa1 x1 -0.000 0.008 -0.033 0.973 -0.000 -0.000 x2 -0.179 0.118 -1.517 0.129 -0.179 -0.155 x3 0.075 0.130 0.577 0.564 0.075 0.067 x4 -0.000 0.002 -0.062 0.951 -0.000 -0.000 x5 -0.000 0.002 -0.045 0.964 -0.000 -0.000 x6 0.083 0.086 0.967 0.333 0.083 0.080 x7 0.919 0.151 6.078 0.000 0.919 0.860 x8 0.736 0.086 8.517 0.000 0.736 0.728 x9 0.527 0.081 6.538 0.000 0.527 0.517 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 ~~ f2 0.589 0.292 2.021 0.043 0.589 0.589 f3 0.081 0.587 0.139 0.890 0.081 0.081 f4 0.502 0.119 4.223 0.000 0.502 0.502 f2 ~~ f3 0.143 1.012 0.141 0.888 0.143 0.143 f4 0.390 0.121 3.213 0.001 0.390 0.390 f3 ~~ f4 0.070 0.483 0.144 0.886 0.070 0.070 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 4.975 0.071 70.065 0.000 4.975 4.304 .x2 6.046 0.070 86.350 0.000 6.046 5.227 .x3 2.221 0.069 32.192 0.000 2.221 1.972 .x4 3.092 0.070 43.893 0.000 3.092 2.636 .x5 4.340 0.078 55.459 0.000 4.340 3.340 .x6 2.193 0.064 34.043 0.000 2.193 2.109 .x7 4.178 0.064 64.955 0.000 4.178 3.911 .x8 5.519 0.061 89.828 0.000 5.519 5.455 .x9 5.406 0.062 86.847 0.000 5.406 5.305 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 0.693 0.123 5.631 0.000 0.693 0.519 .x2 0.968 0.117 8.264 0.000 0.968 0.723 .x3 0.696 0.106 6.588 0.000 0.696 0.549 .x4 -1.495 2.775 -0.539 0.590 -1.495 -1.087 .x5 -24.913 378.408 -0.066 0.948 -24.913 -14.754 .x6 0.738 0.226 3.264 0.001 0.738 0.683 .x7 0.509 0.139 3.662 0.000 0.509 0.446 .x8 0.520 0.100 5.216 0.000 0.520 0.508 .x9 0.554 0.071 7.779 0.000 0.554 0.534 f1 1.000 1.000 1.000 f2 1.000 1.000 1.000 f3 1.000 1.000 1.000 f4 1.000 1.000 1.000 R-Square: Estimate x1 0.481 x2 0.277 x3 0.451 x4 NA x5 NA x6 0.317 x7 0.554 x8 0.492 x9 0.466 Code summary( efa5factorObliqueLavaan_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 did NOT end normally after 10000 iterations ** WARNING ** Estimates below are most likely unreliable Estimator ML Optimization method NLMINB Number of model parameters 53 Rotation method GEOMIN OBLIQUE Geomin epsilon 1e-04 Rotation algorithm (rstarts) GPA (30) Standardized metric TRUE Row weights None Number of observations 301 Number of missing patterns 75 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 =~ efa1 x1 29.053 NA 29.053 25.157 x2 0.006 NA 0.006 0.005 x3 0.000 NA 0.000 0.000 x4 0.335 NA 0.335 0.285 x5 0.332 NA 0.332 0.256 x6 0.257 NA 0.257 0.248 x7 0.152 NA 0.152 0.142 x8 0.008 NA 0.008 0.008 x9 -0.008 NA -0.008 -0.008 f2 =~ efa1 x1 9.281 NA 9.281 8.036 x2 0.173 NA 0.173 0.150 x3 0.462 NA 0.462 0.411 x4 -0.619 NA -0.619 -0.528 x5 -0.598 NA -0.598 -0.461 x6 -0.429 NA -0.429 -0.413 x7 -0.000 NA -0.000 -0.000 x8 0.262 NA 0.262 0.259 x9 0.366 NA 0.366 0.360 f3 =~ efa1 x1 -0.000 NA -0.000 -0.000 x2 0.281 NA 0.281 0.243 x3 0.914 NA 0.914 0.812 x4 0.306 NA 0.306 0.260 x5 0.167 NA 0.167 0.129 x6 0.290 NA 0.290 0.279 x7 0.283 NA 0.283 0.264 x8 0.052 NA 0.052 0.051 x9 0.166 NA 0.166 0.163 f4 =~ efa1 x1 0.000 NA 0.000 0.000 x2 0.286 NA 0.286 0.247 x3 0.247 NA 0.247 0.220 x4 0.613 NA 0.613 0.522 x5 0.897 NA 0.897 0.690 x6 0.591 NA 0.591 0.569 x7 0.192 NA 0.192 0.179 x8 0.494 NA 0.494 0.488 x9 0.625 NA 0.625 0.614 f5 =~ efa1 x1 0.000 NA 0.000 0.000 x2 -0.211 NA -0.211 -0.183 x3 -0.154 NA -0.154 -0.137 x4 0.011 NA 0.011 0.009 x5 -0.050 NA -0.050 -0.039 x6 -0.041 NA -0.041 -0.039 x7 1.152 NA 1.152 1.076 x8 0.377 NA 0.377 0.373 x9 0.211 NA 0.211 0.208 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 ~~ f2 0.027 NA 0.027 0.027 f3 -0.116 NA -0.116 -0.116 f4 -0.085 NA -0.085 -0.085 f5 -0.087 NA -0.087 -0.087 f2 ~~ f3 -0.071 NA -0.071 -0.071 f4 -0.058 NA -0.058 -0.058 f5 0.018 NA 0.018 0.018 f3 ~~ f4 0.082 NA 0.082 0.082 f5 -0.008 NA -0.008 -0.008 f4 ~~ f5 -0.048 NA -0.048 -0.048 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 4.978 NA 4.978 4.311 .x2 6.047 NA 6.047 5.226 .x3 2.224 NA 2.224 1.977 .x4 3.092 NA 3.092 2.636 .x5 4.340 NA 4.340 3.342 .x6 2.191 NA 2.191 2.107 .x7 4.179 NA 4.179 3.906 .x8 5.520 NA 5.520 5.457 .x9 5.403 NA 5.403 5.306 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 -943.431 NA -943.431 -707.387 .x2 1.097 NA 1.097 0.820 .x3 0.165 NA 0.165 0.130 .x4 0.382 NA 0.382 0.277 .x5 0.348 NA 0.348 0.207 .x6 0.364 NA 0.364 0.337 .x7 -0.259 NA -0.259 -0.226 .x8 0.595 NA 0.595 0.581 .x9 0.467 NA 0.467 0.450 f1 1.000 1.000 1.000 f2 1.000 1.000 1.000 f3 1.000 1.000 1.000 f4 1.000 1.000 1.000 f5 1.000 1.000 1.000 R-Square: Estimate x1 NA x2 0.180 x3 0.870 x4 0.723 x5 0.793 x6 0.663 x7 NA x8 0.419 x9 0.550 Code summary( efa6factorObliqueLavaan_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) Error in if (!is.finite(X2) || !is.finite(df) || !is.finite(X2.null) || : missing value where TRUE/FALSE needed Code summary( efa7factorObliqueLavaan_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) Error in if (!is.finite(X2) || !is.finite(df) || !is.finite(X2.null) || : missing value where TRUE/FALSE needed Code summary( efa8factorObliqueLavaan_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) Error in if (!is.finite(X2) || !is.finite(df) || !is.finite(X2.null) || : missing value where TRUE/FALSE needed Code summary( efa9factorObliqueLavaan_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) Error in if (!is.finite(X2) || !is.finite(df) || !is.finite(X2.null) || : missing value where TRUE/FALSE needed 14.4.1.4 Estimates of Model Fit 14.4.1.4.1 Orthogonal (Varimax) rotation Fit indices are generated using the syntax below. Code fitMeasures( efaOrthogonalLavaan_fit, fit.measures = c( &quot;chisq&quot;, &quot;df&quot;, &quot;pvalue&quot;, &quot;chisq.scaled&quot;, &quot;df.scaled&quot;, &quot;pvalue.scaled&quot;, &quot;chisq.scaling.factor&quot;, &quot;baseline.chisq&quot;,&quot;baseline.df&quot;,&quot;baseline.pvalue&quot;, &quot;rmsea&quot;, &quot;cfi&quot;, &quot;tli&quot;, &quot;srmr&quot;, &quot;rmsea.robust&quot;, &quot;cfi.robust&quot;, &quot;tli.robust&quot;)) nfct=1 nfct=2 nfct=3 nfct=4 nfct=5 chisq 239.990 104.454 23.051 7.171 1.736 df 27.000 19.000 12.000 6.000 1.000 pvalue 0.000 0.000 0.027 0.305 0.188 chisq.scaled 235.316 112.999 23.723 NA NA df.scaled 27.000 19.000 12.000 6.000 1.000 pvalue.scaled 0.000 0.000 0.022 NA NA chisq.scaling.factor 1.020 0.924 0.972 NA NA baseline.chisq 693.305 693.305 693.305 693.305 693.305 baseline.df 36.000 36.000 36.000 36.000 36.000 baseline.pvalue 0.000 0.000 0.000 0.000 0.000 rmsea 0.162 0.122 0.055 0.025 0.049 cfi 0.676 0.870 0.983 0.998 0.999 tli 0.568 0.754 0.950 0.989 0.960 srmr 0.129 0.073 0.020 0.012 0.006 rmsea.robust 0.190 0.144 0.070 0.022 0.066 cfi.robust 0.664 0.863 0.980 0.999 0.999 tli.robust 0.552 0.741 0.939 0.994 0.946 14.4.1.4.2 Oblique (Oblimin) rotation Code fitMeasures( efaObliqueLavaan_fit, fit.measures = c( &quot;chisq&quot;, &quot;df&quot;, &quot;pvalue&quot;, &quot;chisq.scaled&quot;, &quot;df.scaled&quot;, &quot;pvalue.scaled&quot;, &quot;chisq.scaling.factor&quot;, &quot;baseline.chisq&quot;,&quot;baseline.df&quot;,&quot;baseline.pvalue&quot;, &quot;rmsea&quot;, &quot;cfi&quot;, &quot;tli&quot;, &quot;srmr&quot;, &quot;rmsea.robust&quot;, &quot;cfi.robust&quot;, &quot;tli.robust&quot;)) nfct=1 nfct=2 nfct=3 nfct=4 nfct=5 chisq 239.990 104.454 23.051 7.171 1.736 df 27.000 19.000 12.000 6.000 1.000 pvalue 0.000 0.000 0.027 0.305 0.188 chisq.scaled 235.316 112.999 23.723 NA NA df.scaled 27.000 19.000 12.000 6.000 1.000 pvalue.scaled 0.000 0.000 0.022 NA NA chisq.scaling.factor 1.020 0.924 0.972 NA NA baseline.chisq 693.305 693.305 693.305 693.305 693.305 baseline.df 36.000 36.000 36.000 36.000 36.000 baseline.pvalue 0.000 0.000 0.000 0.000 0.000 rmsea 0.162 0.122 0.055 0.025 0.049 cfi 0.676 0.870 0.983 0.998 0.999 tli 0.568 0.754 0.950 0.989 0.960 srmr 0.129 0.073 0.020 0.012 0.006 rmsea.robust 0.190 0.144 0.070 0.022 0.066 cfi.robust 0.664 0.863 0.980 0.999 0.999 tli.robust 0.552 0.741 0.939 0.994 0.946 14.4.1.5 Factor Scores 14.4.1.5.1 Orthogonal (Varimax) rotation 14.4.1.5.1.1 psych Code fa3Orthogonal &lt;- efa3factorOrthogonal$scores 14.4.1.5.1.2 lavaan Code fa3Orthogonal_lavaan &lt;- lavPredict(efa3factorOrthogonalLavaan_fit) 14.4.1.5.2 Oblique (Oblimin) rotation 14.4.1.5.2.1 psych Code fa3Oblique &lt;- efa3factorOblique$scores 14.4.1.5.2.2 lavaan Code fa3Oblique_lavaan &lt;- lavPredict(efa3factorObliqueLavaan_fit) 14.4.1.6 Plots Biplots were generated using the psych package (Revelle, 2022). Pairs panel plots were generated using the psych package (Revelle, 2022). Correlation plots were generated using the corrplot package (Wei &amp; Simko, 2021). 14.4.1.6.1 Orthogonal (Varimax) rotation A scree plot from a model with orthogonal rotation is in Figure 14.36. Code plot( efa9factorOrthogonal$e.values, xlab = &quot;Factor&quot;, ylab = &quot;Eigenvalue&quot;) Figure 14.36: Scree Plot With Orthogonal Rotation in Exploratory Factor Analysis: psych. A scree plot based on factor loadings from lavaan is in Figure 14.37. When the factors are uncorrelated (orthogonal rotation), the eigenvalue for a factor is calculated as the sum of squared standardized factor loadings across all items, as described in Section 14.1.4.2.1. Code param1Factor &lt;- parameterEstimates( efa1factorOrthogonalLavaan_fit, standardized = TRUE) param2Factor &lt;- parameterEstimates( efa2factorOrthogonalLavaan_fit, standardized = TRUE) param3Factor &lt;- parameterEstimates( efa3factorOrthogonalLavaan_fit, standardized = TRUE) param4Factor &lt;- parameterEstimates( efa4factorOrthogonalLavaan_fit, standardized = TRUE) param5Factor &lt;- parameterEstimates( efa5factorOrthogonalLavaan_fit, standardized = TRUE) param6Factor &lt;- parameterEstimates( efa6factorOrthogonalLavaan_fit, standardized = TRUE) param7Factor &lt;- parameterEstimates( efa7factorOrthogonalLavaan_fit, standardized = TRUE) param8Factor &lt;- parameterEstimates( efa8factorOrthogonalLavaan_fit, standardized = TRUE) param9Factor &lt;- parameterEstimates( efa9factorOrthogonalLavaan_fit, standardized = TRUE) factorNames &lt;- c(&quot;f1&quot;,&quot;f2&quot;,&quot;f3&quot;,&quot;f4&quot;,&quot;f5&quot;,&quot;f6&quot;,&quot;f7&quot;,&quot;f8&quot;,&quot;f9&quot;) loadings1Factor &lt;- param1Factor[which( param1Factor$lhs %in% factorNames &amp; param1Factor$rhs %in% vars), c(&quot;lhs&quot;,&quot;std.all&quot;)] loadings2Factor &lt;- param2Factor[which( param2Factor$lhs %in% factorNames &amp; param2Factor$rhs %in% vars), c(&quot;lhs&quot;,&quot;std.all&quot;)] loadings3Factor &lt;- param3Factor[which( param3Factor$lhs %in% factorNames &amp; param3Factor$rhs %in% vars), c(&quot;lhs&quot;,&quot;std.all&quot;)] loadings4Factor &lt;- param4Factor[which( param4Factor$lhs %in% factorNames &amp; param4Factor$rhs %in% vars), c(&quot;lhs&quot;,&quot;std.all&quot;)] loadings5Factor &lt;- param5Factor[which( param5Factor$lhs %in% factorNames &amp; param5Factor$rhs %in% vars), c(&quot;lhs&quot;,&quot;std.all&quot;)] loadings6Factor &lt;- param6Factor[which( param6Factor$lhs %in% factorNames &amp; param6Factor$rhs %in% vars), c(&quot;lhs&quot;,&quot;std.all&quot;)] loadings7Factor &lt;- param7Factor[which( param7Factor$lhs %in% factorNames &amp; param7Factor$rhs %in% vars), c(&quot;lhs&quot;,&quot;std.all&quot;)] loadings8Factor &lt;- param8Factor[which( param8Factor$lhs %in% factorNames &amp; param8Factor$rhs %in% vars), c(&quot;lhs&quot;,&quot;std.all&quot;)] loadings9Factor &lt;- param9Factor[which( param9Factor$lhs %in% factorNames &amp; param9Factor$rhs %in% vars), c(&quot;lhs&quot;,&quot;std.all&quot;)] eigenData &lt;- data.frame( Factor = 1:9, Eigenvalue = NA) eigenData$Eigenvalue[which( eigenData$Factor == 1)] &lt;- sum(loadings1Factor$std.all^2) eigenData$Eigenvalue[which(eigenData$Factor == 2)] &lt;- loadings2Factor %&gt;% group_by(lhs) %&gt;% summarise( Eigenvalue = sum(std.all^2), .groups = &quot;drop&quot;) %&gt;% summarise(min(Eigenvalue)) eigenData$Eigenvalue[which(eigenData$Factor == 3)] &lt;- loadings3Factor %&gt;% group_by(lhs) %&gt;% summarise( Eigenvalue = sum(std.all^2), .groups = &quot;drop&quot;) %&gt;% summarise(min(Eigenvalue)) eigenData$Eigenvalue[which(eigenData$Factor == 4)] &lt;- loadings4Factor %&gt;% group_by(lhs) %&gt;% summarise( Eigenvalue = sum(std.all^2), .groups = &quot;drop&quot;) %&gt;% summarise(min(Eigenvalue)) eigenData$Eigenvalue[which(eigenData$Factor == 5)] &lt;- loadings5Factor %&gt;% group_by(lhs) %&gt;% summarise( Eigenvalue = sum(std.all^2), .groups = &quot;drop&quot;) %&gt;% summarise(min(Eigenvalue)) eigenData$Eigenvalue[which(eigenData$Factor == 6)] &lt;- loadings6Factor %&gt;% group_by(lhs) %&gt;% summarise( Eigenvalue = sum(std.all^2), .groups = &quot;drop&quot;) %&gt;% summarise(min(Eigenvalue)) eigenData$Eigenvalue[which(eigenData$Factor == 7)] &lt;- loadings7Factor %&gt;% group_by(lhs) %&gt;% summarise( Eigenvalue = sum(std.all^2), .groups = &quot;drop&quot;) %&gt;% summarise(min(Eigenvalue)) eigenData$Eigenvalue[which(eigenData$Factor == 8)] &lt;- loadings8Factor %&gt;% group_by(lhs) %&gt;% summarise( Eigenvalue = sum(std.all^2), .groups = &quot;drop&quot;) %&gt;% summarise(min(Eigenvalue)) eigenData$Eigenvalue[which(eigenData$Factor == 9)] &lt;- loadings9Factor %&gt;% group_by(lhs) %&gt;% summarise( Eigenvalue = sum(std.all^2), .groups = &quot;drop&quot;) %&gt;% summarise(min(Eigenvalue)) plot( eigenData$Factor, eigenData$Eigenvalue, xlab = &quot;Factor&quot;, ylab = &quot;Eigevalue&quot;) Figure 14.37: Scree Plot With Orthogonal Rotation in Exploratory Factor Analysis: lavaan. A biplot is in Figure 14.38. Code biplot(efa2factorOrthogonal) abline(h = 0, v = 0, lty = 2) Figure 14.38: Biplot Using Orthogonal Rotation in Exploratory Factor Analysis. A factor plot is in Figure 14.39. Code factor.plot(efa3factorOrthogonal, cut = 0.5) Figure 14.39: Factor Plot With Orthogonal Rotation in Exploratory Factor Analysis. A factor diagram is in Figure 14.40. Code fa.diagram(efa3factorOrthogonal, digits = 2) Figure 14.40: Factor Diagram With Orthogonal Rotation in Exploratory Factor Analysis. A pairs panel plot is in Figure 14.41. Code pairs.panels(fa3Orthogonal) Figure 14.41: Pairs Panel Plot With Orthogonal Rotation in Exploratory Factor Analysis. A correlation plot is in Figure 14.42. Code corrplot(cor( fa3Orthogonal, use = &quot;pairwise.complete.obs&quot;)) Figure 14.42: Correlation Plot With Orthogonal Rotation in Exploratory Factor Analysis. 14.4.1.6.2 Oblique (Oblimin) rotation A biplot is in Figure 14.43. Code biplot(efa2factorOblique) Figure 14.43: Biplot Using Oblique Rotation in Exploratory Factor Analysis. A factor plot is in Figure 14.44. Code factor.plot(efa3factorOblique, cut = 0.5) Figure 14.44: Factor Plot With Oblique Rotation in Exploratory Factor Analysis. A factor diagram is in Figure 14.45. Code fa.diagram(efa3factorOblique, digits = 2) Figure 14.45: Factor Diagram With Oblique Rotation in Exploratory Factor Analysis. A pairs panel plot is in Figure 14.46. Code pairs.panels(fa3Oblique) Figure 14.46: Pairs Panel Plot With Oblique Rotation in Exploratory Factor Analysis. A correlation plot is in Figure 14.47. Code corrplot(cor( fa3Oblique, use = &quot;pairwise.complete.obs&quot;)) Figure 14.47: Correlation Plot With Oblique Rotation in Exploratory Factor Analysis. 14.4.2 Confirmatory Factor Analysis (CFA) I introduced confirmatory factor analysis (CFA) models in Section 7.3.3 in the chapter on structural equation models. The confirmatory factor analysis (CFA) models were fit in the lavaan package (Rosseel et al., 2022). The examples were adapted from the lavaan documentation: https://lavaan.ugent.be/tutorial/cfa.html (archived at https://perma.cc/GKY3-9YE4) 14.4.2.1 Specify the model Code cfaModel_syntax &lt;- &#39; #Factor loadings visual =~ x1 + x2 + x3 textual =~ x4 + x5 + x6 speed =~ x7 + x8 + x9 &#39; cfaModel_fullSyntax &lt;- &#39; #Factor loadings (free the factor loading of the first indicator) visual =~ NA*x1 + x2 + x3 textual =~ NA*x4 + x5 + x6 speed =~ NA*x7 + x8 + x9 #Fix latent means to zero visual ~ 0 textual ~ 0 speed ~ 0 #Fix latent variances to one visual ~~ 1*visual textual ~~ 1*textual speed ~~ 1*speed #Estimate covariances among latent variables visual ~~ textual visual ~~ speed textual ~~ speed #Estimate residual variances of manifest variables x1 ~~ x1 x2 ~~ x2 x3 ~~ x3 x4 ~~ x4 x5 ~~ x5 x6 ~~ x6 x7 ~~ x7 x8 ~~ x8 x9 ~~ x9 #Free intercepts of manifest variables x1 ~ int1*1 x2 ~ int2*1 x3 ~ int3*1 x4 ~ int4*1 x5 ~ int5*1 x6 ~ int6*1 x7 ~ int7*1 x8 ~ int8*1 x9 ~ int9*1 &#39; 14.4.2.1.1 Model syntax in table form: Code lavaanify(cfaModel_syntax) Code lavaanify(cfaModel_fullSyntax) 14.4.2.2 Fit the model Code cfaModelFit &lt;- cfa( cfaModel_syntax, data = HolzingerSwineford1939, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;, std.lv = TRUE) cfaModelFit_full &lt;- lavaan( cfaModel_fullSyntax, data = HolzingerSwineford1939, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) 14.4.2.3 Display summary output Code summary( cfaModelFit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 40 iterations Estimator ML Optimization method NLMINB Number of model parameters 30 Number of observations 301 Number of missing patterns 75 Model Test User Model: Standard Scaled Test Statistic 65.643 63.641 Degrees of freedom 24 24 P-value (Chi-square) 0.000 0.000 Scaling correction factor 1.031 Yuan-Bentler correction (Mplus variant) Model Test Baseline Model: Test statistic 693.305 646.574 Degrees of freedom 36 36 P-value 0.000 0.000 Scaling correction factor 1.072 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.937 0.935 Tucker-Lewis Index (TLI) 0.905 0.903 Robust Comparative Fit Index (CFI) 0.930 Robust Tucker-Lewis Index (TLI) 0.894 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -3213.600 -3213.600 Scaling correction factor 1.090 for the MLR correction Loglikelihood unrestricted model (H1) -3180.778 -3180.778 Scaling correction factor 1.064 for the MLR correction Akaike (AIC) 6487.199 6487.199 Bayesian (BIC) 6598.413 6598.413 Sample-size adjusted Bayesian (SABIC) 6503.270 6503.270 Root Mean Square Error of Approximation: RMSEA 0.076 0.074 90 Percent confidence interval - lower 0.054 0.053 90 Percent confidence interval - upper 0.098 0.096 P-value H_0: RMSEA &lt;= 0.050 0.026 0.034 P-value H_0: RMSEA &gt;= 0.080 0.403 0.349 Robust RMSEA 0.092 90 Percent confidence interval - lower 0.064 90 Percent confidence interval - upper 0.120 P-value H_0: Robust RMSEA &lt;= 0.050 0.008 P-value H_0: Robust RMSEA &gt;= 0.080 0.780 Standardized Root Mean Square Residual: SRMR 0.061 0.061 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual =~ x1 0.885 0.108 8.183 0.000 0.885 0.762 x2 0.521 0.086 6.037 0.000 0.521 0.451 x3 0.712 0.088 8.065 0.000 0.712 0.632 textual =~ x4 0.989 0.067 14.690 0.000 0.989 0.842 x5 1.121 0.066 16.890 0.000 1.121 0.866 x6 0.849 0.065 12.991 0.000 0.849 0.818 speed =~ x7 0.596 0.084 7.092 0.000 0.596 0.557 x8 0.742 0.096 7.703 0.000 0.742 0.733 x9 0.686 0.099 6.939 0.000 0.686 0.673 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual ~~ textual 0.430 0.080 5.386 0.000 0.430 0.430 speed 0.465 0.110 4.213 0.000 0.465 0.465 textual ~~ speed 0.314 0.084 3.746 0.000 0.314 0.314 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 4.977 0.071 69.850 0.000 4.977 4.289 .x2 6.050 0.070 85.953 0.000 6.050 5.235 .x3 2.227 0.069 32.218 0.000 2.227 1.976 .x4 3.097 0.071 43.828 0.000 3.097 2.634 .x5 4.344 0.077 56.058 0.000 4.344 3.355 .x6 2.187 0.064 34.391 0.000 2.187 2.108 .x7 4.180 0.065 64.446 0.000 4.180 3.904 .x8 5.517 0.062 89.668 0.000 5.517 5.451 .x9 5.405 0.063 86.063 0.000 5.405 5.298 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 0.564 0.169 3.345 0.001 0.564 0.419 .x2 1.064 0.119 8.935 0.000 1.064 0.797 .x3 0.763 0.118 6.454 0.000 0.763 0.601 .x4 0.403 0.063 6.363 0.000 0.403 0.292 .x5 0.420 0.074 5.664 0.000 0.420 0.251 .x6 0.356 0.056 6.415 0.000 0.356 0.331 .x7 0.790 0.093 8.512 0.000 0.790 0.690 .x8 0.474 0.128 3.693 0.000 0.474 0.463 .x9 0.570 0.122 4.660 0.000 0.570 0.548 visual 1.000 1.000 1.000 textual 1.000 1.000 1.000 speed 1.000 1.000 1.000 R-Square: Estimate x1 0.581 x2 0.203 x3 0.399 x4 0.708 x5 0.749 x6 0.669 x7 0.310 x8 0.537 x9 0.452 Code summary( cfaModelFit_full, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 40 iterations Estimator ML Optimization method NLMINB Number of model parameters 30 Number of observations 301 Number of missing patterns 75 Model Test User Model: Standard Scaled Test Statistic 65.643 63.641 Degrees of freedom 24 24 P-value (Chi-square) 0.000 0.000 Scaling correction factor 1.031 Yuan-Bentler correction (Mplus variant) Model Test Baseline Model: Test statistic 693.305 646.574 Degrees of freedom 36 36 P-value 0.000 0.000 Scaling correction factor 1.072 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.937 0.935 Tucker-Lewis Index (TLI) 0.905 0.903 Robust Comparative Fit Index (CFI) 0.930 Robust Tucker-Lewis Index (TLI) 0.894 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -3213.600 -3213.600 Scaling correction factor 1.090 for the MLR correction Loglikelihood unrestricted model (H1) -3180.778 -3180.778 Scaling correction factor 1.064 for the MLR correction Akaike (AIC) 6487.199 6487.199 Bayesian (BIC) 6598.413 6598.413 Sample-size adjusted Bayesian (SABIC) 6503.270 6503.270 Root Mean Square Error of Approximation: RMSEA 0.076 0.074 90 Percent confidence interval - lower 0.054 0.053 90 Percent confidence interval - upper 0.098 0.096 P-value H_0: RMSEA &lt;= 0.050 0.026 0.034 P-value H_0: RMSEA &gt;= 0.080 0.403 0.349 Robust RMSEA 0.092 90 Percent confidence interval - lower 0.064 90 Percent confidence interval - upper 0.120 P-value H_0: Robust RMSEA &lt;= 0.050 0.008 P-value H_0: Robust RMSEA &gt;= 0.080 0.780 Standardized Root Mean Square Residual: SRMR 0.061 0.061 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual =~ x1 0.885 0.108 8.183 0.000 0.885 0.762 x2 0.521 0.086 6.037 0.000 0.521 0.451 x3 0.712 0.088 8.065 0.000 0.712 0.632 textual =~ x4 0.989 0.067 14.690 0.000 0.989 0.842 x5 1.121 0.066 16.890 0.000 1.121 0.866 x6 0.849 0.065 12.991 0.000 0.849 0.818 speed =~ x7 0.596 0.084 7.092 0.000 0.596 0.557 x8 0.742 0.096 7.703 0.000 0.742 0.733 x9 0.686 0.099 6.939 0.000 0.686 0.673 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual ~~ textual 0.430 0.080 5.386 0.000 0.430 0.430 speed 0.465 0.110 4.213 0.000 0.465 0.465 textual ~~ speed 0.314 0.084 3.746 0.000 0.314 0.314 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual 0.000 0.000 0.000 textual 0.000 0.000 0.000 speed 0.000 0.000 0.000 .x1 (int1) 4.977 0.071 69.850 0.000 4.977 4.289 .x2 (int2) 6.050 0.070 85.953 0.000 6.050 5.235 .x3 (int3) 2.227 0.069 32.218 0.000 2.227 1.976 .x4 (int4) 3.097 0.071 43.828 0.000 3.097 2.634 .x5 (int5) 4.344 0.077 56.058 0.000 4.344 3.355 .x6 (int6) 2.187 0.064 34.391 0.000 2.187 2.108 .x7 (int7) 4.180 0.065 64.446 0.000 4.180 3.904 .x8 (int8) 5.517 0.062 89.668 0.000 5.517 5.451 .x9 (int9) 5.405 0.063 86.063 0.000 5.405 5.298 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual 1.000 1.000 1.000 textual 1.000 1.000 1.000 speed 1.000 1.000 1.000 .x1 0.564 0.169 3.345 0.001 0.564 0.419 .x2 1.064 0.119 8.935 0.000 1.064 0.797 .x3 0.763 0.118 6.454 0.000 0.763 0.601 .x4 0.403 0.063 6.363 0.000 0.403 0.292 .x5 0.420 0.074 5.664 0.000 0.420 0.251 .x6 0.356 0.056 6.415 0.000 0.356 0.331 .x7 0.790 0.093 8.512 0.000 0.790 0.690 .x8 0.474 0.128 3.693 0.000 0.474 0.463 .x9 0.570 0.122 4.660 0.000 0.570 0.548 R-Square: Estimate x1 0.581 x2 0.203 x3 0.399 x4 0.708 x5 0.749 x6 0.669 x7 0.310 x8 0.537 x9 0.452 14.4.2.4 Estimates of model fit Code fitMeasures( cfaModelFit, fit.measures = c( &quot;chisq&quot;, &quot;df&quot;, &quot;pvalue&quot;, &quot;chisq.scaled&quot;, &quot;df.scaled&quot;, &quot;pvalue.scaled&quot;, &quot;chisq.scaling.factor&quot;, &quot;baseline.chisq&quot;,&quot;baseline.df&quot;,&quot;baseline.pvalue&quot;, &quot;rmsea&quot;, &quot;cfi&quot;, &quot;tli&quot;, &quot;srmr&quot;, &quot;rmsea.robust&quot;, &quot;cfi.robust&quot;, &quot;tli.robust&quot;)) chisq df pvalue 65.643 24.000 0.000 chisq.scaled df.scaled pvalue.scaled 63.641 24.000 0.000 chisq.scaling.factor baseline.chisq baseline.df 1.031 693.305 36.000 baseline.pvalue rmsea cfi 0.000 0.076 0.937 tli srmr rmsea.robust 0.905 0.061 0.092 cfi.robust tli.robust 0.930 0.894 14.4.2.5 Residuals Code residuals(cfaModelFit, type = &quot;cor&quot;) $type [1] &quot;cor.bollen&quot; $cov x1 x2 x3 x4 x5 x6 x7 x8 x9 x1 0.000 x2 -0.020 0.000 x3 -0.019 0.053 0.000 x4 0.077 -0.035 -0.083 0.000 x5 0.015 -0.006 -0.119 0.004 0.000 x6 0.044 0.038 -0.002 -0.005 0.002 0.000 x7 -0.178 -0.215 -0.073 0.005 -0.061 -0.032 0.000 x8 -0.017 -0.053 -0.013 -0.088 -0.018 -0.008 0.062 0.000 x9 0.103 0.067 0.192 0.003 0.084 0.043 -0.031 -0.044 0.000 $mean x1 x2 x3 x4 x5 x6 x7 x8 x9 0.001 -0.003 -0.002 -0.004 -0.003 0.006 0.000 0.002 0.000 14.4.2.6 Modification indices Modification indices are generated using the syntax below. Code modificationindices(cfaModelFit, sort. = TRUE) 14.4.2.7 Factor scores Code cfaFactorScores &lt;- lavPredict(cfaModelFit) 14.4.2.7.1 Compare CFA factor scores to EFA factor scores As would be expected, the factor scores from the CFA model are highly correlated with the factor scores from the EFA model. Code cor.test( x = cfaFactorScores[,&quot;visual&quot;], y = fa3Orthogonal[,&quot;ML3&quot;]) Pearson&#39;s product-moment correlation data: cfaFactorScores[, &quot;visual&quot;] and fa3Orthogonal[, &quot;ML3&quot;] t = 15.64, df = 69, p-value &lt; 2.2e-16 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.8185187 0.9257296 sample estimates: cor 0.8831693 Code cor.test( x = cfaFactorScores[,&quot;textual&quot;], y = fa3Orthogonal[,&quot;ML1&quot;]) Pearson&#39;s product-moment correlation data: cfaFactorScores[, &quot;textual&quot;] and fa3Orthogonal[, &quot;ML1&quot;] t = 49.444, df = 69, p-value &lt; 2.2e-16 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.9778619 0.9913860 sample estimates: cor 0.9861797 Code cor.test( x = cfaFactorScores[,&quot;speed&quot;], y = fa3Orthogonal[,&quot;ML2&quot;]) Pearson&#39;s product-moment correlation data: cfaFactorScores[, &quot;speed&quot;] and fa3Orthogonal[, &quot;ML2&quot;] t = 17.78, df = 69, p-value &lt; 2.2e-16 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.8530018 0.9405081 sample estimates: cor 0.9060032 14.4.2.8 Internal Consistency Reliability Internal consistency reliability of items composing the latent factors, as quantified by omega (\\(\\omega\\)) and average variance extracted (AVE), was estimated using the semTools package (Jorgensen et al., 2021). Code compRelSEM(cfaModelFit) visual textual speed 0.652 0.878 0.695 Code AVE(cfaModelFit) visual textual speed 0.397 0.713 0.430 14.4.2.9 Path diagram A path diagram of the model generated using the semPlot package (Epskamp, 2022) is in Figure 14.48. Code semPaths( cfaModelFit, what = &quot;std&quot;, layout = &quot;tree2&quot;, edge.label.cex = 1.3) Figure 14.48: Confirmatory Factor Analysis Model Diagram. 14.4.2.10 Class Examples Code standardDeviations &lt;- rep(1, 6) sampleSize &lt;- 300 14.4.2.10.1 Model 1 14.4.2.10.1.1 Create the covariance matrix Code correlationMatrixModel1 &lt;- matrix(.6, nrow = 6, ncol = 6) diag(correlationMatrixModel1) &lt;- 1 rownames(correlationMatrixModel1) &lt;- colnames(correlationMatrixModel1) &lt;- paste(&quot;V&quot;, 1:6, sep = &quot;&quot;) covarianceMatrixModel1 &lt;- psych::cor2cov( correlationMatrixModel1, sigma = standardDeviations) covarianceMatrixModel1 V1 V2 V3 V4 V5 V6 V1 1.0 0.6 0.6 0.6 0.6 0.6 V2 0.6 1.0 0.6 0.6 0.6 0.6 V3 0.6 0.6 1.0 0.6 0.6 0.6 V4 0.6 0.6 0.6 1.0 0.6 0.6 V5 0.6 0.6 0.6 0.6 1.0 0.6 V6 0.6 0.6 0.6 0.6 0.6 1.0 14.4.2.10.1.2 Specify the model Code cfaModel1 &lt;- &#39; #Factor loadings f1 =~ V1 + V2 + V3 + V4 + V5 + V6 &#39; 14.4.2.10.1.3 Model syntax in table form: Code lavaanify(cfaModel1) 14.4.2.10.1.4 Fit the model Code cfaModel1Fit &lt;- cfa( cfaModel1, sample.cov = covarianceMatrixModel1, sample.nobs = sampleSize, estimator = &quot;ML&quot;, std.lv = TRUE) 14.4.2.10.1.5 Display summary output Code summary( cfaModel1Fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 10 iterations Estimator ML Optimization method NLMINB Number of model parameters 12 Number of observations 300 Model Test User Model: Test statistic 0.000 Degrees of freedom 9 P-value (Chi-square) 1.000 Model Test Baseline Model: Test statistic 958.548 Degrees of freedom 15 P-value 0.000 User Model versus Baseline Model: Comparative Fit Index (CFI) 1.000 Tucker-Lewis Index (TLI) 1.016 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -2071.810 Loglikelihood unrestricted model (H1) -2071.810 Akaike (AIC) 4167.621 Bayesian (BIC) 4212.066 Sample-size adjusted Bayesian (SABIC) 4174.009 Root Mean Square Error of Approximation: RMSEA 0.000 90 Percent confidence interval - lower 0.000 90 Percent confidence interval - upper 0.000 P-value H_0: RMSEA &lt;= 0.050 1.000 P-value H_0: RMSEA &gt;= 0.080 0.000 Standardized Root Mean Square Residual: SRMR 0.000 Parameter Estimates: Standard errors Standard Information Expected Information saturated (h1) model Structured Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 =~ V1 0.773 0.050 15.390 0.000 0.773 0.775 V2 0.773 0.050 15.390 0.000 0.773 0.775 V3 0.773 0.050 15.390 0.000 0.773 0.775 V4 0.773 0.050 15.390 0.000 0.773 0.775 V5 0.773 0.050 15.390 0.000 0.773 0.775 V6 0.773 0.050 15.390 0.000 0.773 0.775 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .V1 0.399 0.039 10.171 0.000 0.399 0.400 .V2 0.399 0.039 10.171 0.000 0.399 0.400 .V3 0.399 0.039 10.171 0.000 0.399 0.400 .V4 0.399 0.039 10.171 0.000 0.399 0.400 .V5 0.399 0.039 10.171 0.000 0.399 0.400 .V6 0.399 0.039 10.171 0.000 0.399 0.400 f1 1.000 1.000 1.000 R-Square: Estimate V1 0.600 V2 0.600 V3 0.600 V4 0.600 V5 0.600 V6 0.600 14.4.2.10.1.6 Estimates of model fit Code fitMeasures( cfaModel1Fit, fit.measures = c( &quot;chisq&quot;, &quot;df&quot;, &quot;pvalue&quot;, &quot;rmsea&quot;, &quot;cfi&quot;, &quot;tli&quot;, &quot;srmr&quot;)) chisq df pvalue rmsea cfi tli srmr 0.000 9.000 1.000 0.000 1.000 1.016 0.000 14.4.2.10.1.7 Residuals Code residuals(cfaModel1Fit, type = &quot;cor&quot;) $type [1] &quot;cor.bollen&quot; $cov V1 V2 V3 V4 V5 V6 V1 0 V2 0 0 V3 0 0 0 V4 0 0 0 0 V5 0 0 0 0 0 V6 0 0 0 0 0 0 14.4.2.10.1.8 Modification indices Code modificationindices(cfaModel1Fit, sort. = TRUE) 14.4.2.10.1.9 Internal Consistency Reliability Internal consistency reliability of items composing the latent factors, as quantified by omega (\\(\\omega\\)) and average variance extracted (AVE), was estimated using the semTools package (Jorgensen et al., 2021). Code compRelSEM(cfaModel1Fit) f1 0.9 Code AVE(cfaModel1Fit) f1 0.6 14.4.2.10.1.10 Path diagram A path diagram of the model generated using the semPlot package (Epskamp, 2022) is in Figure 14.49. Code semPaths( cfaModel1Fit, what = &quot;std&quot;, layout = &quot;tree2&quot;, edge.label.cex = 1.5) Figure 14.49: Confirmatory Factor Analysis Model 1 Diagram. 14.4.2.10.2 Model 2 14.4.2.10.2.1 Create the covariance matrix Code correlationMatrixModel2 &lt;- matrix(0, nrow = 6, ncol = 6) diag(correlationMatrixModel2) &lt;- 1 rownames(correlationMatrixModel2) &lt;- colnames(correlationMatrixModel2) &lt;- paste(&quot;V&quot;, 1:6, sep = &quot;&quot;) covarianceMatrixModel2 &lt;- psych::cor2cov( correlationMatrixModel2, sigma = standardDeviations) covarianceMatrixModel2 V1 V2 V3 V4 V5 V6 V1 1 0 0 0 0 0 V2 0 1 0 0 0 0 V3 0 0 1 0 0 0 V4 0 0 0 1 0 0 V5 0 0 0 0 1 0 V6 0 0 0 0 0 1 14.4.2.10.2.2 Specify the model Code cfaModel2 &lt;- &#39; #Factor loadings f1 =~ V1 + V2 + V3 + V4 + V5 + V6 &#39; 14.4.2.10.2.3 Model syntax in table form: Code lavaanify(cfaModel2) 14.4.2.10.2.4 Fit the model Code cfaModel2Fit &lt;- cfa( cfaModel2, sample.cov = covarianceMatrixModel2, sample.nobs = sampleSize, estimator = &quot;ML&quot;, std.lv = TRUE) 14.4.2.10.2.5 Display summary output Code summary( cfaModel2Fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 23 iterations Estimator ML Optimization method NLMINB Number of model parameters 12 Number of observations 300 Model Test User Model: Test statistic 0.000 Degrees of freedom 9 P-value (Chi-square) 1.000 Model Test Baseline Model: Test statistic 0.000 Degrees of freedom 15 P-value 1.000 User Model versus Baseline Model: Comparative Fit Index (CFI) 1.000 Tucker-Lewis Index (TLI) 0.000 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -2551.084 Loglikelihood unrestricted model (H1) -2551.084 Akaike (AIC) 5126.169 Bayesian (BIC) 5170.614 Sample-size adjusted Bayesian (SABIC) 5132.557 Root Mean Square Error of Approximation: RMSEA 0.000 90 Percent confidence interval - lower 0.000 90 Percent confidence interval - upper 0.000 P-value H_0: RMSEA &lt;= 0.050 1.000 P-value H_0: RMSEA &gt;= 0.080 0.000 Standardized Root Mean Square Residual: SRMR 0.000 Parameter Estimates: Standard errors Standard Information Expected Information saturated (h1) model Structured Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 =~ V1 0.000 227.294 0.000 1.000 0.000 0.000 V2 0.000 227.294 0.000 1.000 0.000 0.000 V3 0.000 227.294 0.000 1.000 0.000 0.000 V4 0.000 227.294 0.000 1.000 0.000 0.000 V5 0.000 227.294 0.000 1.000 0.000 0.000 V6 0.000 227.294 0.000 1.000 0.000 0.000 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .V1 0.997 0.098 10.171 0.000 0.997 1.000 .V2 0.997 0.098 10.171 0.000 0.997 1.000 .V3 0.997 0.098 10.171 0.000 0.997 1.000 .V4 0.997 0.098 10.171 0.000 0.997 1.000 .V5 0.997 0.098 10.171 0.000 0.997 1.000 .V6 0.997 0.098 10.171 0.000 0.997 1.000 f1 1.000 1.000 1.000 R-Square: Estimate V1 0.000 V2 0.000 V3 0.000 V4 0.000 V5 0.000 V6 0.000 14.4.2.10.2.6 Estimates of model fit Code fitMeasures( cfaModel2Fit, fit.measures = c( &quot;chisq&quot;, &quot;df&quot;, &quot;pvalue&quot;, &quot;rmsea&quot;, &quot;cfi&quot;, &quot;tli&quot;, &quot;srmr&quot;)) chisq df pvalue rmsea cfi tli srmr 0 9 1 0 1 0 0 14.4.2.10.2.7 Residuals Code residuals(cfaModel2Fit, type = &quot;cor&quot;) $type [1] &quot;cor.bollen&quot; $cov V1 V2 V3 V4 V5 V6 V1 0 V2 0 0 V3 0 0 0 V4 0 0 0 0 V5 0 0 0 0 0 V6 0 0 0 0 0 0 14.4.2.10.2.8 Modification indices Code modificationindices(cfaModel1Fit, sort. = TRUE) 14.4.2.10.2.9 Internal Consistency Reliability Internal consistency reliability of items composing the latent factors, as quantified by omega (\\(\\omega\\)) and average variance extracted (AVE), was estimated using the semTools package (Jorgensen et al., 2021). Code compRelSEM(cfaModel2Fit) f1 0 Code AVE(cfaModel2Fit) f1 0 14.4.2.10.2.10 Path diagram A path diagram of the model generated using the semPlot package (Epskamp, 2022) is in Figure 14.50. Code semPaths( cfaModel2Fit, what = &quot;std&quot;, layout = &quot;tree2&quot;, edge.label.cex = 1.5) Figure 14.50: Confirmatory Factor Analysis Model 2 Diagram. 14.4.2.10.3 Model 3 14.4.2.10.3.1 Create the covariance matrix Code correlationMatrixModel3 &lt;- matrix(c( 1,.6,.6,0,0,0, .6,1,.6,0,0,0, 0,0,1,0,0,0, 0,0,0,1,.6,.6, 0,0,0,.6,1,.6, 0,0,0,.6,.6,1), nrow = 6, ncol = 6) rownames(correlationMatrixModel3) &lt;- colnames(correlationMatrixModel3) &lt;- paste(&quot;V&quot;, 1:6, sep = &quot;&quot;) covarianceMatrixModel3 &lt;- psych::cor2cov( correlationMatrixModel3, sigma = standardDeviations) covarianceMatrixModel3 V1 V2 V3 V4 V5 V6 V1 1.0 0.6 0 0.0 0.0 0.0 V2 0.6 1.0 0 0.0 0.0 0.0 V3 0.6 0.6 1 0.0 0.0 0.0 V4 0.0 0.0 0 1.0 0.6 0.6 V5 0.0 0.0 0 0.6 1.0 0.6 V6 0.0 0.0 0 0.6 0.6 1.0 14.4.2.10.3.2 Specify the model Code cfaModel3A &lt;- &#39; #Factor loadings f1 =~ V1 + V2 + V3 + V4 + V5 + V6 &#39; cfaModel3B &lt;- &#39; #Factor loadings f1 =~ V1 + V2 + V3 f2 =~ V4 + V5 + V6 &#39; 14.4.2.10.3.3 Model syntax in table form: Code lavaanify(cfaModel3A) Code lavaanify(cfaModel3B) 14.4.2.10.3.4 Fit the model Code cfaModel3AFit &lt;- cfa( cfaModel3A, sample.cov = covarianceMatrixModel3, sample.nobs = sampleSize, estimator = &quot;ML&quot;, std.lv = TRUE) cfaModel3BFit &lt;- cfa( cfaModel3B, sample.cov = covarianceMatrixModel3, sample.nobs = sampleSize, estimator = &quot;ML&quot;, std.lv = TRUE) 14.4.2.10.3.5 Display summary output Code summary( cfaModel3AFit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 10 iterations Estimator ML Optimization method NLMINB Number of model parameters 12 Number of observations 300 Model Test User Model: Test statistic 313.237 Degrees of freedom 9 P-value (Chi-square) 0.000 Model Test Baseline Model: Test statistic 626.474 Degrees of freedom 15 P-value 0.000 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.502 Tucker-Lewis Index (TLI) 0.171 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -2394.466 Loglikelihood unrestricted model (H1) -2237.847 Akaike (AIC) 4812.931 Bayesian (BIC) 4857.377 Sample-size adjusted Bayesian (SABIC) 4819.320 Root Mean Square Error of Approximation: RMSEA 0.336 90 Percent confidence interval - lower 0.304 90 Percent confidence interval - upper 0.368 P-value H_0: RMSEA &lt;= 0.050 0.000 P-value H_0: RMSEA &gt;= 0.080 1.000 Standardized Root Mean Square Residual: SRMR 0.227 Parameter Estimates: Standard errors Standard Information Expected Information saturated (h1) model Structured Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 =~ V1 0.773 0.055 14.142 0.000 0.773 0.775 V2 0.773 0.055 14.142 0.000 0.773 0.775 V3 0.773 0.055 14.142 0.000 0.773 0.775 V4 0.000 0.064 0.000 1.000 0.000 0.000 V5 0.000 0.064 0.000 1.000 0.000 0.000 V6 0.000 0.064 0.000 1.000 0.000 0.000 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .V1 0.399 0.051 7.746 0.000 0.399 0.400 .V2 0.399 0.051 7.746 0.000 0.399 0.400 .V3 0.399 0.051 7.746 0.000 0.399 0.400 .V4 0.997 0.081 12.247 0.000 0.997 1.000 .V5 0.997 0.081 12.247 0.000 0.997 1.000 .V6 0.997 0.081 12.247 0.000 0.997 1.000 f1 1.000 1.000 1.000 R-Square: Estimate V1 0.600 V2 0.600 V3 0.600 V4 0.000 V5 0.000 V6 0.000 Code summary( cfaModel3BFit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 8 iterations Estimator ML Optimization method NLMINB Number of model parameters 13 Number of observations 300 Model Test User Model: Test statistic 0.000 Degrees of freedom 8 P-value (Chi-square) 1.000 Model Test Baseline Model: Test statistic 626.474 Degrees of freedom 15 P-value 0.000 User Model versus Baseline Model: Comparative Fit Index (CFI) 1.000 Tucker-Lewis Index (TLI) 1.025 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -2237.847 Loglikelihood unrestricted model (H1) -2237.847 Akaike (AIC) 4501.694 Bayesian (BIC) 4549.843 Sample-size adjusted Bayesian (SABIC) 4508.615 Root Mean Square Error of Approximation: RMSEA 0.000 90 Percent confidence interval - lower 0.000 90 Percent confidence interval - upper 0.000 P-value H_0: RMSEA &lt;= 0.050 1.000 P-value H_0: RMSEA &gt;= 0.080 0.000 Standardized Root Mean Square Residual: SRMR 0.000 Parameter Estimates: Standard errors Standard Information Expected Information saturated (h1) model Structured Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 =~ V1 0.773 0.055 14.142 0.000 0.773 0.775 V2 0.773 0.055 14.142 0.000 0.773 0.775 V3 0.773 0.055 14.142 0.000 0.773 0.775 f2 =~ V4 0.773 0.055 14.142 0.000 0.773 0.775 V5 0.773 0.055 14.142 0.000 0.773 0.775 V6 0.773 0.055 14.142 0.000 0.773 0.775 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 ~~ f2 0.000 0.071 0.000 1.000 0.000 0.000 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .V1 0.399 0.051 7.746 0.000 0.399 0.400 .V2 0.399 0.051 7.746 0.000 0.399 0.400 .V3 0.399 0.051 7.746 0.000 0.399 0.400 .V4 0.399 0.051 7.746 0.000 0.399 0.400 .V5 0.399 0.051 7.746 0.000 0.399 0.400 .V6 0.399 0.051 7.746 0.000 0.399 0.400 f1 1.000 1.000 1.000 f2 1.000 1.000 1.000 R-Square: Estimate V1 0.600 V2 0.600 V3 0.600 V4 0.600 V5 0.600 V6 0.600 14.4.2.10.3.6 Estimates of model fit Code fitMeasures( cfaModel3AFit, fit.measures = c( &quot;chisq&quot;, &quot;df&quot;, &quot;pvalue&quot;, &quot;rmsea&quot;, &quot;cfi&quot;, &quot;tli&quot;, &quot;srmr&quot;)) chisq df pvalue rmsea cfi tli srmr 313.237 9.000 0.000 0.336 0.502 0.171 0.227 Code fitMeasures( cfaModel3BFit, fit.measures = c( &quot;chisq&quot;, &quot;df&quot;, &quot;pvalue&quot;, &quot;rmsea&quot;, &quot;cfi&quot;, &quot;tli&quot;, &quot;srmr&quot;)) chisq df pvalue rmsea cfi tli srmr 0.000 8.000 1.000 0.000 1.000 1.025 0.000 14.4.2.10.3.7 Compare model fit Below is a test of whether the two-factor model fits better than the one-factor model. A significant chi-square difference test indicates that the two-factor model fits significantly better than the one-factor model. Code anova(cfaModel3BFit, cfaModel3AFit) 14.4.2.10.3.8 Residuals Code residuals(cfaModel3AFit, type = &quot;cor&quot;) $type [1] &quot;cor.bollen&quot; $cov V1 V2 V3 V4 V5 V6 V1 0.0 V2 0.0 0.0 V3 0.0 0.0 0.0 V4 0.0 0.0 0.0 0.0 V5 0.0 0.0 0.0 0.6 0.0 V6 0.0 0.0 0.0 0.6 0.6 0.0 Code residuals(cfaModel3BFit, type = &quot;cor&quot;) $type [1] &quot;cor.bollen&quot; $cov V1 V2 V3 V4 V5 V6 V1 0 V2 0 0 V3 0 0 0 V4 0 0 0 0 V5 0 0 0 0 0 V6 0 0 0 0 0 0 14.4.2.10.3.9 Modification indices Code modificationindices(cfaModel3AFit, sort. = TRUE) Code modificationindices(cfaModel3BFit, sort. = TRUE) 14.4.2.10.3.10 Internal Consistency Reliability Internal consistency reliability of items composing the latent factors, as quantified by omega (\\(\\omega\\)) and average variance extracted (AVE), was estimated using the semTools package (Jorgensen et al., 2021). Code compRelSEM(cfaModel3AFit) f1 0.818 Code compRelSEM(cfaModel3BFit) f1 f2 0.818 0.818 Code AVE(cfaModel3AFit) f1 0.6 Code AVE(cfaModel3BFit) f1 f2 0.6 0.6 14.4.2.10.3.11 Path diagram Path diagrams of the models generated using the semPlot package (Epskamp, 2022) are in Figures 14.51 and 14.52. Code semPaths(cfaModel3AFit, what = &quot;std&quot;, layout = &quot;tree2&quot;, edge.label.cex = 1.5) Figure 14.51: Confirmatory Factor Analysis Model 3a Diagram. Code semPaths(cfaModel3BFit, what = &quot;std&quot;, layout = &quot;tree2&quot;, edge.label.cex = 1.5) Figure 14.52: Confirmatory Factor Analysis Model 3b Diagram. 14.4.2.10.4 Model 4 14.4.2.10.5 Create the covariance matrix Code correlationMatrixModel4 &lt;- matrix(c( 1,.6,.6,.3,.3,.3, .6,1,.6,.3,.3,.3, .3,.3,1,.3,.3,.3, .3,.3,.3,1,.6,.6, .3,.3,.3,.6,1,.6, .3,.3,.3,.6,.6,1), nrow = 6, ncol = 6) rownames(correlationMatrixModel4) &lt;- colnames(correlationMatrixModel4) &lt;- paste(&quot;V&quot;, 1:6, sep = &quot;&quot;) covarianceMatrixModel4 &lt;- psych::cor2cov( correlationMatrixModel4, sigma = standardDeviations) covarianceMatrixModel4 V1 V2 V3 V4 V5 V6 V1 1.0 0.6 0.3 0.3 0.3 0.3 V2 0.6 1.0 0.3 0.3 0.3 0.3 V3 0.6 0.6 1.0 0.3 0.3 0.3 V4 0.3 0.3 0.3 1.0 0.6 0.6 V5 0.3 0.3 0.3 0.6 1.0 0.6 V6 0.3 0.3 0.3 0.6 0.6 1.0 14.4.2.10.5.1 Specify the model Code cfaModel4A &lt;- &#39; #Factor loadings f1 =~ V1 + V2 + V3 f2 =~ V4 + V5 + V6 &#39; cfaModel4B &lt;- &#39; #Factor loadings f1 =~ V1 + V2 + V3 f2 =~ V4 + V5 + V6 #Regression path f2 ~ f1 &#39; cfaModel4C &lt;- &#39; #Factor loadings f1 =~ V1 + V2 + V3 f2 =~ V4 + V5 + V6 #Higher-order factor A1 = ~ fixloading*f1 + fixloading*f2 &#39; cfaModel4D &lt;- &#39; #Factor loadings f1 =~ V1 + V2 + V3 + V4 + V5 + V6 #Correlated residuals/errors V1 ~~ fixcor*V2 V2 ~~ fixcor*V3 V1 ~~ fixcor*V3 V4 ~~ fixcor*V5 V5 ~~ fixcor*V6 V4 ~~ fixcor*V6 &#39; cfaModel4E &lt;- &#39; #Factor loadings f1 =~ V1 + V2 + V3 + V4 + V5 + V6 #construct latent factor f2 =~ fixloading*V4 + fixloading*V5 + fixloading*V6 #method latent factor #Make construct and method latent factors orthogonal (uncorrelated) f1 ~~ 0*f2 &#39; 14.4.2.10.5.2 Model syntax in table form: Code lavaanify(cfaModel4A) Code lavaanify(cfaModel4B) Code lavaanify(cfaModel4C) Code lavaanify(cfaModel4D) Code lavaanify(cfaModel4E) 14.4.2.10.5.3 Fit the model Code cfaModel4AFit &lt;- cfa( cfaModel4A, sample.cov = covarianceMatrixModel4, sample.nobs = sampleSize, estimator = &quot;ML&quot;, std.lv = TRUE) cfaModel4BFit &lt;- cfa( cfaModel4B, sample.cov = covarianceMatrixModel4, sample.nobs = sampleSize, estimator = &quot;ML&quot;, std.lv = TRUE) cfaModel4CFit &lt;- cfa( cfaModel4C, sample.cov = covarianceMatrixModel4, sample.nobs = sampleSize, estimator = &quot;ML&quot;, std.lv = TRUE) cfaModel4DFit &lt;- cfa( cfaModel4D, sample.cov = covarianceMatrixModel4, sample.nobs = sampleSize, estimator = &quot;ML&quot;, std.lv = TRUE) cfaModel4EFit &lt;- cfa( cfaModel4E, sample.cov = covarianceMatrixModel4, sample.nobs = sampleSize, estimator = &quot;ML&quot;, std.lv = TRUE) 14.4.2.10.5.4 Display summary output Code summary( cfaModel4AFit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 9 iterations Estimator ML Optimization method NLMINB Number of model parameters 13 Number of observations 300 Model Test User Model: Test statistic 0.000 Degrees of freedom 8 P-value (Chi-square) 1.000 Model Test Baseline Model: Test statistic 681.419 Degrees of freedom 15 P-value 0.000 User Model versus Baseline Model: Comparative Fit Index (CFI) 1.000 Tucker-Lewis Index (TLI) 1.023 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -2210.375 Loglikelihood unrestricted model (H1) -2210.375 Akaike (AIC) 4446.750 Bayesian (BIC) 4494.899 Sample-size adjusted Bayesian (SABIC) 4453.671 Root Mean Square Error of Approximation: RMSEA 0.000 90 Percent confidence interval - lower 0.000 90 Percent confidence interval - upper 0.000 P-value H_0: RMSEA &lt;= 0.050 1.000 P-value H_0: RMSEA &gt;= 0.080 0.000 Standardized Root Mean Square Residual: SRMR 0.000 Parameter Estimates: Standard errors Standard Information Expected Information saturated (h1) model Structured Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 =~ V1 0.773 0.054 14.351 0.000 0.773 0.775 V2 0.773 0.054 14.351 0.000 0.773 0.775 V3 0.773 0.054 14.351 0.000 0.773 0.775 f2 =~ V4 0.773 0.054 14.351 0.000 0.773 0.775 V5 0.773 0.054 14.351 0.000 0.773 0.775 V6 0.773 0.054 14.351 0.000 0.773 0.775 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 ~~ f2 0.500 0.057 8.822 0.000 0.500 0.500 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .V1 0.399 0.049 8.067 0.000 0.399 0.400 .V2 0.399 0.049 8.067 0.000 0.399 0.400 .V3 0.399 0.049 8.067 0.000 0.399 0.400 .V4 0.399 0.049 8.067 0.000 0.399 0.400 .V5 0.399 0.049 8.067 0.000 0.399 0.400 .V6 0.399 0.049 8.067 0.000 0.399 0.400 f1 1.000 1.000 1.000 f2 1.000 1.000 1.000 R-Square: Estimate V1 0.600 V2 0.600 V3 0.600 V4 0.600 V5 0.600 V6 0.600 Code summary( cfaModel4BFit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 12 iterations Estimator ML Optimization method NLMINB Number of model parameters 13 Number of observations 300 Model Test User Model: Test statistic 0.000 Degrees of freedom 8 P-value (Chi-square) 1.000 Model Test Baseline Model: Test statistic 681.419 Degrees of freedom 15 P-value 0.000 User Model versus Baseline Model: Comparative Fit Index (CFI) 1.000 Tucker-Lewis Index (TLI) 1.023 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -2210.375 Loglikelihood unrestricted model (H1) -2210.375 Akaike (AIC) 4446.750 Bayesian (BIC) 4494.899 Sample-size adjusted Bayesian (SABIC) 4453.671 Root Mean Square Error of Approximation: RMSEA 0.000 90 Percent confidence interval - lower 0.000 90 Percent confidence interval - upper 0.000 P-value H_0: RMSEA &lt;= 0.050 1.000 P-value H_0: RMSEA &gt;= 0.080 0.000 Standardized Root Mean Square Residual: SRMR 0.000 Parameter Estimates: Standard errors Standard Information Expected Information saturated (h1) model Structured Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 =~ V1 0.773 0.054 14.351 0.000 0.773 0.775 V2 0.773 0.054 14.351 0.000 0.773 0.775 V3 0.773 0.054 14.351 0.000 0.773 0.775 f2 =~ V4 0.670 0.050 13.445 0.000 0.773 0.775 V5 0.670 0.050 13.445 0.000 0.773 0.775 V6 0.670 0.050 13.445 0.000 0.773 0.775 Regressions: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f2 ~ f1 0.577 0.087 6.616 0.000 0.500 0.500 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .V1 0.399 0.049 8.067 0.000 0.399 0.400 .V2 0.399 0.049 8.067 0.000 0.399 0.400 .V3 0.399 0.049 8.067 0.000 0.399 0.400 .V4 0.399 0.049 8.067 0.000 0.399 0.400 .V5 0.399 0.049 8.067 0.000 0.399 0.400 .V6 0.399 0.049 8.067 0.000 0.399 0.400 f1 1.000 1.000 1.000 .f2 1.000 0.750 0.750 R-Square: Estimate V1 0.600 V2 0.600 V3 0.600 V4 0.600 V5 0.600 V6 0.600 f2 0.250 Code summary( cfaModel4CFit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 15 iterations Estimator ML Optimization method NLMINB Number of model parameters 14 Number of equality constraints 1 Number of observations 300 Model Test User Model: Test statistic 0.000 Degrees of freedom 8 P-value (Chi-square) 1.000 Model Test Baseline Model: Test statistic 681.419 Degrees of freedom 15 P-value 0.000 User Model versus Baseline Model: Comparative Fit Index (CFI) 1.000 Tucker-Lewis Index (TLI) 1.023 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -2210.375 Loglikelihood unrestricted model (H1) -2210.375 Akaike (AIC) 4446.750 Bayesian (BIC) 4494.899 Sample-size adjusted Bayesian (SABIC) 4453.671 Root Mean Square Error of Approximation: RMSEA 0.000 90 Percent confidence interval - lower 0.000 90 Percent confidence interval - upper 0.000 P-value H_0: RMSEA &lt;= 0.050 1.000 P-value H_0: RMSEA &gt;= 0.080 0.000 Standardized Root Mean Square Residual: SRMR 0.000 Parameter Estimates: Standard errors Standard Information Expected Information saturated (h1) model Structured Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 =~ V1 0.547 0.046 12.003 0.000 0.773 0.775 V2 0.547 0.046 12.003 0.000 0.773 0.775 V3 0.547 0.046 12.003 0.000 0.773 0.775 f2 =~ V4 0.547 0.046 12.003 0.000 0.773 0.775 V5 0.547 0.046 12.003 0.000 0.773 0.775 V6 0.547 0.046 12.003 0.000 0.773 0.775 A1 =~ f1 (fxld) 1.000 0.113 8.822 0.000 0.707 0.707 f2 (fxld) 1.000 0.113 8.822 0.000 0.707 0.707 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .V1 0.399 0.049 8.067 0.000 0.399 0.400 .V2 0.399 0.049 8.067 0.000 0.399 0.400 .V3 0.399 0.049 8.067 0.000 0.399 0.400 .V4 0.399 0.049 8.067 0.000 0.399 0.400 .V5 0.399 0.049 8.067 0.000 0.399 0.400 .V6 0.399 0.049 8.067 0.000 0.399 0.400 .f1 1.000 0.500 0.500 .f2 1.000 0.500 0.500 A1 1.000 1.000 1.000 R-Square: Estimate V1 0.600 V2 0.600 V3 0.600 V4 0.600 V5 0.600 V6 0.600 f1 0.500 f2 0.500 Code summary( cfaModel4DFit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 17 iterations Estimator ML Optimization method NLMINB Number of model parameters 18 Number of equality constraints 5 Number of observations 300 Model Test User Model: Test statistic 0.000 Degrees of freedom 8 P-value (Chi-square) 1.000 Model Test Baseline Model: Test statistic 681.419 Degrees of freedom 15 P-value 0.000 User Model versus Baseline Model: Comparative Fit Index (CFI) 1.000 Tucker-Lewis Index (TLI) 1.023 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -2210.375 Loglikelihood unrestricted model (H1) -2210.375 Akaike (AIC) 4446.750 Bayesian (BIC) 4494.899 Sample-size adjusted Bayesian (SABIC) 4453.671 Root Mean Square Error of Approximation: RMSEA 0.000 90 Percent confidence interval - lower 0.000 90 Percent confidence interval - upper 0.000 P-value H_0: RMSEA &lt;= 0.050 1.000 P-value H_0: RMSEA &gt;= 0.080 0.000 Standardized Root Mean Square Residual: SRMR 0.000 Parameter Estimates: Standard errors Standard Information Expected Information saturated (h1) model Structured Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 =~ V1 0.547 0.068 7.994 0.000 0.547 0.548 V2 0.547 0.068 7.994 0.000 0.547 0.548 V3 0.547 0.068 7.994 0.000 0.547 0.548 V4 0.547 0.068 7.994 0.000 0.547 0.548 V5 0.547 0.068 7.994 0.000 0.547 0.548 V6 0.547 0.068 7.994 0.000 0.547 0.548 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .V1 ~~ .V2 (fxcr) 0.299 0.036 8.380 0.000 0.299 0.429 .V2 ~~ .V3 (fxcr) 0.299 0.036 8.380 0.000 0.299 0.429 .V1 ~~ .V3 (fxcr) 0.299 0.036 8.380 0.000 0.299 0.429 .V4 ~~ .V5 (fxcr) 0.299 0.036 8.380 0.000 0.299 0.429 .V5 ~~ .V6 (fxcr) 0.299 0.036 8.380 0.000 0.299 0.429 .V4 ~~ .V6 (fxcr) 0.299 0.036 8.380 0.000 0.299 0.429 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .V1 0.698 0.057 12.244 0.000 0.698 0.700 .V2 0.698 0.057 12.244 0.000 0.698 0.700 .V3 0.698 0.057 12.244 0.000 0.698 0.700 .V4 0.698 0.057 12.244 0.000 0.698 0.700 .V5 0.698 0.057 12.244 0.000 0.698 0.700 .V6 0.698 0.057 12.244 0.000 0.698 0.700 f1 1.000 1.000 1.000 R-Square: Estimate V1 0.300 V2 0.300 V3 0.300 V4 0.300 V5 0.300 V6 0.300 Code summary( cfaModel4EFit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 13 iterations Estimator ML Optimization method NLMINB Number of model parameters 15 Number of equality constraints 2 Number of observations 300 Model Test User Model: Test statistic 0.000 Degrees of freedom 8 P-value (Chi-square) 1.000 Model Test Baseline Model: Test statistic 681.419 Degrees of freedom 15 P-value 0.000 User Model versus Baseline Model: Comparative Fit Index (CFI) 1.000 Tucker-Lewis Index (TLI) 1.023 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -2210.375 Loglikelihood unrestricted model (H1) -2210.375 Akaike (AIC) 4446.750 Bayesian (BIC) 4494.899 Sample-size adjusted Bayesian (SABIC) 4453.671 Root Mean Square Error of Approximation: RMSEA 0.000 90 Percent confidence interval - lower 0.000 90 Percent confidence interval - upper 0.000 P-value H_0: RMSEA &lt;= 0.050 1.000 P-value H_0: RMSEA &gt;= 0.080 0.000 Standardized Root Mean Square Residual: SRMR 0.000 Parameter Estimates: Standard errors Standard Information Expected Information saturated (h1) model Structured Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 =~ V1 0.773 0.054 14.351 0.000 0.773 0.775 V2 0.773 0.054 14.351 0.000 0.773 0.775 V3 0.773 0.054 14.351 0.000 0.773 0.775 V4 0.387 0.061 6.347 0.000 0.387 0.387 V5 0.387 0.061 6.347 0.000 0.387 0.387 V6 0.387 0.061 6.347 0.000 0.387 0.387 f2 =~ V4 (fxld) 0.670 0.038 17.657 0.000 0.670 0.671 V5 (fxld) 0.670 0.038 17.657 0.000 0.670 0.671 V6 (fxld) 0.670 0.038 17.657 0.000 0.670 0.671 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 ~~ f2 0.000 0.000 0.000 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .V1 0.399 0.049 8.067 0.000 0.399 0.400 .V2 0.399 0.049 8.067 0.000 0.399 0.400 .V3 0.399 0.049 8.067 0.000 0.399 0.400 .V4 0.399 0.045 8.894 0.000 0.399 0.400 .V5 0.399 0.045 8.894 0.000 0.399 0.400 .V6 0.399 0.045 8.894 0.000 0.399 0.400 f1 1.000 1.000 1.000 f2 1.000 1.000 1.000 R-Square: Estimate V1 0.600 V2 0.600 V3 0.600 V4 0.600 V5 0.600 V6 0.600 14.4.2.10.5.5 Estimates of model fit Code fitMeasures( cfaModel4AFit, fit.measures = c( &quot;chisq&quot;, &quot;df&quot;, &quot;pvalue&quot;, &quot;rmsea&quot;, &quot;cfi&quot;, &quot;tli&quot;, &quot;srmr&quot;)) chisq df pvalue rmsea cfi tli srmr 0.000 8.000 1.000 0.000 1.000 1.023 0.000 Code fitMeasures( cfaModel4BFit, fit.measures = c( &quot;chisq&quot;, &quot;df&quot;, &quot;pvalue&quot;, &quot;rmsea&quot;, &quot;cfi&quot;, &quot;tli&quot;, &quot;srmr&quot;)) chisq df pvalue rmsea cfi tli srmr 0.000 8.000 1.000 0.000 1.000 1.023 0.000 Code fitMeasures( cfaModel4CFit, fit.measures = c( &quot;chisq&quot;, &quot;df&quot;, &quot;pvalue&quot;, &quot;rmsea&quot;, &quot;cfi&quot;, &quot;tli&quot;, &quot;srmr&quot;)) chisq df pvalue rmsea cfi tli srmr 0.000 8.000 1.000 0.000 1.000 1.023 0.000 Code fitMeasures( cfaModel4DFit, fit.measures = c( &quot;chisq&quot;, &quot;df&quot;, &quot;pvalue&quot;, &quot;rmsea&quot;, &quot;cfi&quot;, &quot;tli&quot;, &quot;srmr&quot;)) chisq df pvalue rmsea cfi tli srmr 0.000 8.000 1.000 0.000 1.000 1.023 0.000 Code fitMeasures( cfaModel4EFit, fit.measures = c( &quot;chisq&quot;, &quot;df&quot;, &quot;pvalue&quot;, &quot;rmsea&quot;, &quot;cfi&quot;, &quot;tli&quot;, &quot;srmr&quot;)) chisq df pvalue rmsea cfi tli srmr 0.000 8.000 1.000 0.000 1.000 1.023 0.000 14.4.2.10.5.6 Residuals Code residuals(cfaModel4AFit, type = &quot;cor&quot;) $type [1] &quot;cor.bollen&quot; $cov V1 V2 V3 V4 V5 V6 V1 0 V2 0 0 V3 0 0 0 V4 0 0 0 0 V5 0 0 0 0 0 V6 0 0 0 0 0 0 Code residuals(cfaModel4BFit, type = &quot;cor&quot;) $type [1] &quot;cor.bollen&quot; $cov V1 V2 V3 V4 V5 V6 V1 0 V2 0 0 V3 0 0 0 V4 0 0 0 0 V5 0 0 0 0 0 V6 0 0 0 0 0 0 Code residuals(cfaModel4CFit, type = &quot;cor&quot;) $type [1] &quot;cor.bollen&quot; $cov V1 V2 V3 V4 V5 V6 V1 0 V2 0 0 V3 0 0 0 V4 0 0 0 0 V5 0 0 0 0 0 V6 0 0 0 0 0 0 Code residuals(cfaModel4DFit, type = &quot;cor&quot;) $type [1] &quot;cor.bollen&quot; $cov V1 V2 V3 V4 V5 V6 V1 0 V2 0 0 V3 0 0 0 V4 0 0 0 0 V5 0 0 0 0 0 V6 0 0 0 0 0 0 Code residuals(cfaModel4EFit, type = &quot;cor&quot;) $type [1] &quot;cor.bollen&quot; $cov V1 V2 V3 V4 V5 V6 V1 0 V2 0 0 V3 0 0 0 V4 0 0 0 0 V5 0 0 0 0 0 V6 0 0 0 0 0 0 14.4.2.10.5.7 Modification indices Code modificationindices(cfaModel4AFit, sort. = TRUE) Code modificationindices(cfaModel4BFit, sort. = TRUE) Code modificationindices(cfaModel4CFit, sort. = TRUE) Code modificationindices(cfaModel4DFit, sort. = TRUE) Code modificationindices(cfaModel4EFit, sort. = TRUE) 14.4.2.10.5.8 Internal Consistency Reliability Internal consistency reliability of items composing the latent factors, as quantified by omega (\\(\\omega\\)) and average variance extracted (AVE), was estimated using the semTools package (Jorgensen et al., 2021). Code compRelSEM(cfaModel4AFit) f1 f2 0.818 0.818 Code compRelSEM(cfaModel4BFit) f1 f2 0.818 0.818 Code compRelSEM(cfaModel4CFit) f1 f2 0.818 0.818 Code compRelSEM(cfaModel4CFit, higher = &quot;A1&quot;) f1 f2 A1 0.818 0.818 0.581 Code compRelSEM(cfaModel4DFit) f1 0.581 Code compRelSEM(cfaModel4EFit) f1 f2 0.653 0.614 Code AVE(cfaModel4AFit) f1 f2 0.6 0.6 Code AVE(cfaModel4BFit) f1 f2 0.6 0.6 Code AVE(cfaModel4CFit) f1 f2 0.6 0.6 Code AVE(cfaModel4DFit) f1 0.3 Code AVE(cfaModel4EFit) f1 f2 NA NA 14.4.2.10.5.9 Path diagram Path diagrams of the models generated using the semPlot package (Epskamp, 2022) are in Figures 14.53, 14.54, 14.55, 14.56, and 14.57. Code semPaths( cfaModel4AFit, what = &quot;std&quot;, layout = &quot;tree2&quot;, edge.label.cex = 1.3) Figure 14.53: Confirmatory Factor Analysis Model 4a Diagram. Code semPaths( cfaModel4BFit, what = &quot;std&quot;, layout = &quot;tree2&quot;, edge.label.cex = 1.3) Figure 14.54: Confirmatory Factor Analysis Model 4b Diagram. Code semPaths( cfaModel4CFit, what = &quot;std&quot;, layout = &quot;tree2&quot;, edge.label.cex = 1.3) Figure 14.55: Confirmatory Factor Analysis Model 4c Diagram. Code semPaths( cfaModel4DFit, what = &quot;std&quot;, layout = &quot;tree2&quot;, edge.label.cex = 1.3) Figure 14.56: Confirmatory Factor Analysis Model 4d Diagram. Code semPaths( cfaModel4EFit, what = &quot;std&quot;, layout = &quot;tree2&quot;, edge.label.cex = 1.3) Figure 14.57: Confirmatory Factor Analysis Model 4e Diagram. 14.4.2.10.5.10 Equivalently fitting models Markov equivalent directed acyclic graphs (DAGs) were depicted using the dagitty package (Textor et al., 2021). Path diagrams of equivalent models are below. Code dagModel &lt;- lavaanToGraph(cfaModel4AFit) par(mfrow = c(4, 4)) Code lapply(equivalentDAGs(dagModel, n = 16), plot) Figure 14.58: Equivalently Fitting Models to Confirmatory Factor Analysis Model 4. Figure 14.59: Equivalently Fitting Models to Confirmatory Factor Analysis Model 4. Figure 14.60: Equivalently Fitting Models to Confirmatory Factor Analysis Model 4. Figure 14.61: Equivalently Fitting Models to Confirmatory Factor Analysis Model 4. Figure 14.62: Equivalently Fitting Models to Confirmatory Factor Analysis Model 4. Figure 14.63: Equivalently Fitting Models to Confirmatory Factor Analysis Model 4. Figure 14.64: Equivalently Fitting Models to Confirmatory Factor Analysis Model 4. Figure 14.65: Equivalently Fitting Models to Confirmatory Factor Analysis Model 4. Figure 14.66: Equivalently Fitting Models to Confirmatory Factor Analysis Model 4. Figure 14.67: Equivalently Fitting Models to Confirmatory Factor Analysis Model 4. Figure 14.68: Equivalently Fitting Models to Confirmatory Factor Analysis Model 4. Figure 14.69: Equivalently Fitting Models to Confirmatory Factor Analysis Model 4. Figure 14.70: Equivalently Fitting Models to Confirmatory Factor Analysis Model 4. Figure 14.71: Equivalently Fitting Models to Confirmatory Factor Analysis Model 4. Figure 14.72: Equivalently Fitting Models to Confirmatory Factor Analysis Model 4. Figure 14.73: Equivalently Fitting Models to Confirmatory Factor Analysis Model 4. [[1]] [[1]]$mar [1] 0 0 0 0 [[2]] [[2]]$mar [1] 0 0 0 0 [[3]] [[3]]$mar [1] 0 0 0 0 [[4]] [[4]]$mar [1] 0 0 0 0 [[5]] [[5]]$mar [1] 0 0 0 0 [[6]] [[6]]$mar [1] 0 0 0 0 [[7]] [[7]]$mar [1] 0 0 0 0 [[8]] [[8]]$mar [1] 0 0 0 0 [[9]] [[9]]$mar [1] 0 0 0 0 [[10]] [[10]]$mar [1] 0 0 0 0 [[11]] [[11]]$mar [1] 0 0 0 0 [[12]] [[12]]$mar [1] 0 0 0 0 [[13]] [[13]]$mar [1] 0 0 0 0 [[14]] [[14]]$mar [1] 0 0 0 0 [[15]] [[15]]$mar [1] 0 0 0 0 [[16]] [[16]]$mar [1] 0 0 0 0 14.4.2.11 Higher-Order Factor Model A higher-order (or hierarchical) factor model is a model in which a higher-order latent factor is thought to influence lower-order latent factors. Higher-order factor models are depicted in Figures 14.10 and 14.55. An analysis example of a higher-order factor model is provided in Section 14.4.2.10.4. 14.4.2.12 Bifactor Model A bifactor model is a model in which both a general latent factor and specific latent factors are extracted from items (Markon, 2019). The general factor represents the common variance among all items, whereas the specific factor represents the common variance among the specific items that load on the specific factor, after extracting the general factor variance. As estimated below, the general factor is orthogonal to the specific factors. Some modified versions of the bifactor allow the specific factors to inter-correlate (in addition to estimating the general factor). 14.4.2.12.1 Specify the model Code cfaModelBifactor_syntax &lt;- &#39; Verbal =~ VO1 + VO2 + VO3 + VW1 + VW2 + VW3 + VM1 + VM2 + VM3 Spatial =~ SO1 + SO2 + SO3 + SW1 + SW2 + SW3 + SM1 + SM2 + SM3 Quant =~ QO1 + QO2 + QO3 + QW1 + QW2 + QW3 + QM1 + QM2 + QM3 g =~ VO1 + VO2 + VO3 + VW1 + VW2 + VW3 + VM1 + VM2 + VM3 + SO1 + SO2 + SO3 + SW1 + SW2 + SW3 + SM1 + SM2 + SM3 + QO1 + QO2 + QO3 + QW1 + QW2 + QW3 + QM1 + QM2 + QM3 &#39; cfaModelBifactor_fullSyntax &lt;- &#39; #Factor loadings (free the factor loading of the first indicator) Verbal =~ NA*VO1 + VO2 + VO3 + VW1 + VW2 + VW3 + VM1 + VM2 + VM3 Spatial =~ NA*SO1 + SO2 + SO3 + SW1 + SW2 + SW3 + SM1 + SM2 + SM3 Quant =~ NA*QO1 + QO2 + QO3 + QW1 + QW2 + QW3 + QM1 + QM2 + QM3 g =~ NA*VO1 + VO2 + VO3 + VW1 + VW2 + VW3 + VM1 + VM2 + VM3 + SO1 + SO2 + SO3 + SW1 + SW2 + SW3 + SM1 + SM2 + SM3 + QO1 + QO2 + QO3 + QW1 + QW2 + QW3 + QM1 + QM2 + QM3 #Fix latent means to zero Verbal ~ 0 Spatial ~ 0 Quant ~ 0 g ~ 0 #Fix latent variances to one Verbal ~~ 1*Verbal Spatial ~~ 1*Spatial Quant ~~ 1*Quant g ~~ 1*g #Fix covariances among latent variables at zero Verbal ~~ 0*Spatial Verbal ~~ 0*Quant Spatial ~~ 0*Quant g ~~ 0*Verbal g ~~ 0*Spatial g ~~ 0*Quant #Estimate residual variances of manifest variables VO1 ~~ VO1 VO2 ~~ VO2 VO3 ~~ VO3 VW1 ~~ VW1 VW2 ~~ VW2 VW3 ~~ VW3 VM1 ~~ VM1 VM2 ~~ VM2 VM3 ~~ VM3 SO1 ~~ SO1 SO2 ~~ SO2 SO3 ~~ SO3 SW1 ~~ SW1 SW2 ~~ SW2 SW3 ~~ SW3 SM1 ~~ SM1 SM2 ~~ SM2 SM3 ~~ SM3 QO1 ~~ QO1 QO2 ~~ QO2 QO3 ~~ QO3 QW1 ~~ QW1 QW2 ~~ QW2 QW3 ~~ QW3 QM1 ~~ QM1 QM2 ~~ QM2 QM3 ~~ QM3 #Free intercepts of manifest variables VO1 ~ intVO1*1 VO2 ~ intVO2*1 VO3 ~ intVO3*1 VW1 ~ intVW1*1 VW2 ~ intVW2*1 VW3 ~ intVW3*1 VM1 ~ intVM1*1 VM2 ~ intVM2*1 VM3 ~ intVM3*1 SO1 ~ intSO1*1 SO2 ~ intSO2*1 SO3 ~ intSO3*1 SW1 ~ intSW1*1 SW2 ~ intSW2*1 SW3 ~ intSW3*1 SM1 ~ intSM1*1 SM2 ~ intSM2*1 SM3 ~ intSM3*1 QO1 ~ intQO1*1 QO2 ~ intQO2*1 QO3 ~ intQO3*1 QW1 ~ intQW1*1 QW2 ~ intQW2*1 QW3 ~ intQW3*1 QM1 ~ intQM1*1 QM2 ~ intQM2*1 QM3 ~ intQM3*1 &#39; 14.4.2.12.2 Model syntax in table form: Code lavaanify(cfaModelBifactor_syntax) Code lavaanify(cfaModelBifactor_fullSyntax) 14.4.2.12.3 Fit the model Code cfaModelBifactorFit &lt;- cfa( cfaModelBifactor_syntax, data = MTMM_data, orthogonal = TRUE, std.lv = TRUE, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) cfaModelBifactorFit_full &lt;- lavaan( cfaModelBifactor_fullSyntax, data = MTMM_data, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) 14.4.2.12.4 Display summary output Code summary( cfaModelBifactorFit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 45 iterations Estimator ML Optimization method NLMINB Number of model parameters 108 Number of observations 10000 Number of missing patterns 1 Model Test User Model: Standard Scaled Test Statistic 28930.049 29784.763 Degrees of freedom 297 297 P-value (Chi-square) 0.000 0.000 Scaling correction factor 0.971 Yuan-Bentler correction (Mplus variant) Model Test Baseline Model: Test statistic 146391.496 146201.022 Degrees of freedom 351 351 P-value 0.000 0.000 Scaling correction factor 1.001 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.804 0.798 Tucker-Lewis Index (TLI) 0.768 0.761 Robust Comparative Fit Index (CFI) 0.804 Robust Tucker-Lewis Index (TLI) 0.768 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -324368.770 -324368.770 Scaling correction factor 1.083 for the MLR correction Loglikelihood unrestricted model (H1) -309903.745 -309903.745 Scaling correction factor 1.001 for the MLR correction Akaike (AIC) 648953.539 648953.539 Bayesian (BIC) 649732.256 649732.256 Sample-size adjusted Bayesian (SABIC) 649389.048 649389.048 Root Mean Square Error of Approximation: RMSEA 0.098 0.100 90 Percent confidence interval - lower 0.097 0.099 90 Percent confidence interval - upper 0.099 0.101 P-value H_0: RMSEA &lt;= 0.050 0.000 0.000 P-value H_0: RMSEA &gt;= 0.080 1.000 1.000 Robust RMSEA 0.098 90 Percent confidence interval - lower 0.097 90 Percent confidence interval - upper 0.099 P-value H_0: Robust RMSEA &lt;= 0.050 0.000 P-value H_0: Robust RMSEA &gt;= 0.080 1.000 Standardized Root Mean Square Residual: SRMR 0.201 0.201 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all Verbal =~ VO1 0.484 0.011 42.953 0.000 0.484 0.484 VO2 0.572 0.012 49.110 0.000 0.572 0.571 VO3 0.648 0.011 59.594 0.000 0.648 0.649 VW1 0.602 0.021 28.399 0.000 0.602 0.597 VW2 0.522 0.017 31.037 0.000 0.522 0.516 VW3 0.457 0.015 31.401 0.000 0.457 0.452 VM1 0.764 0.009 81.260 0.000 0.764 0.758 VM2 0.837 0.009 91.851 0.000 0.837 0.835 VM3 0.574 0.010 58.967 0.000 0.574 0.576 Spatial =~ SO1 0.597 0.011 52.050 0.000 0.597 0.603 SO2 0.603 0.012 51.513 0.000 0.603 0.603 SO3 0.530 0.012 45.701 0.000 0.530 0.531 SW1 0.491 0.021 23.493 0.000 0.491 0.489 SW2 0.584 0.020 29.673 0.000 0.584 0.584 SW3 0.416 0.018 22.716 0.000 0.416 0.417 SM1 0.856 0.008 103.159 0.000 0.856 0.860 SM2 0.682 0.009 72.543 0.000 0.682 0.684 SM3 0.892 0.008 106.899 0.000 0.892 0.895 Quant =~ QO1 0.532 0.011 46.627 0.000 0.532 0.537 QO2 0.673 0.010 67.359 0.000 0.673 0.680 QO3 0.529 0.011 47.899 0.000 0.529 0.526 QW1 0.454 0.014 33.332 0.000 0.454 0.452 QW2 0.526 0.013 39.384 0.000 0.526 0.533 QW3 0.616 0.015 40.075 0.000 0.616 0.618 QM1 0.595 0.010 57.570 0.000 0.595 0.596 QM2 0.652 0.010 64.766 0.000 0.652 0.648 QM3 0.743 0.010 75.444 0.000 0.743 0.740 g =~ VO1 -0.116 0.020 -5.713 0.000 -0.116 -0.116 VO2 -0.155 0.023 -6.756 0.000 -0.155 -0.155 VO3 -0.181 0.024 -7.540 0.000 -0.181 -0.181 VW1 -0.699 0.021 -32.569 0.000 -0.699 -0.693 VW2 -0.514 0.020 -26.246 0.000 -0.514 -0.508 VW3 -0.397 0.018 -21.773 0.000 -0.397 -0.393 VM1 0.038 0.030 1.237 0.216 0.038 0.037 VM2 0.013 0.033 0.407 0.684 0.013 0.013 VM3 -0.013 0.023 -0.552 0.581 -0.013 -0.013 SO1 -0.202 0.023 -8.812 0.000 -0.202 -0.204 SO2 -0.208 0.023 -8.965 0.000 -0.208 -0.208 SO3 -0.153 0.023 -6.818 0.000 -0.153 -0.154 SW1 -0.684 0.018 -37.922 0.000 -0.684 -0.682 SW2 -0.633 0.021 -30.390 0.000 -0.633 -0.633 SW3 -0.563 0.016 -34.126 0.000 -0.563 -0.565 SM1 -0.010 0.031 -0.318 0.751 -0.010 -0.010 SM2 0.070 0.027 2.618 0.009 0.070 0.070 SM3 0.027 0.034 0.789 0.430 0.027 0.027 QO1 -0.088 0.019 -4.585 0.000 -0.088 -0.089 QO2 -0.133 0.020 -6.651 0.000 -0.133 -0.134 QO3 -0.090 0.018 -5.060 0.000 -0.090 -0.090 QW1 -0.452 0.015 -30.307 0.000 -0.452 -0.449 QW2 -0.461 0.016 -27.995 0.000 -0.461 -0.467 QW3 -0.574 0.018 -32.789 0.000 -0.574 -0.576 QM1 0.060 0.024 2.504 0.012 0.060 0.060 QM2 -0.000 0.023 -0.020 0.984 -0.000 -0.000 QM3 -0.040 0.026 -1.567 0.117 -0.040 -0.040 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all Verbal ~~ Spatial 0.000 0.000 0.000 Quant 0.000 0.000 0.000 g 0.000 0.000 0.000 Spatial ~~ Quant 0.000 0.000 0.000 g 0.000 0.000 0.000 Quant ~~ g 0.000 0.000 0.000 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .VO1 -0.007 0.010 -0.717 0.473 -0.007 -0.007 .VO2 -0.013 0.010 -1.254 0.210 -0.013 -0.013 .VO3 -0.008 0.010 -0.815 0.415 -0.008 -0.008 .VW1 0.015 0.010 1.464 0.143 0.015 0.015 .VW2 0.011 0.010 1.043 0.297 0.011 0.010 .VW3 -0.006 0.010 -0.572 0.567 -0.006 -0.006 .VM1 -0.011 0.010 -1.086 0.277 -0.011 -0.011 .VM2 -0.010 0.010 -0.992 0.321 -0.010 -0.010 .VM3 -0.012 0.010 -1.195 0.232 -0.012 -0.012 .SO1 -0.006 0.010 -0.588 0.557 -0.006 -0.006 .SO2 -0.014 0.010 -1.405 0.160 -0.014 -0.014 .SO3 -0.008 0.010 -0.827 0.408 -0.008 -0.008 .SW1 0.004 0.010 0.405 0.686 0.004 0.004 .SW2 0.007 0.010 0.673 0.501 0.007 0.007 .SW3 0.005 0.010 0.472 0.637 0.005 0.005 .SM1 -0.009 0.010 -0.913 0.361 -0.009 -0.009 .SM2 -0.008 0.010 -0.754 0.451 -0.008 -0.008 .SM3 -0.012 0.010 -1.185 0.236 -0.012 -0.012 .QO1 0.005 0.010 0.479 0.632 0.005 0.005 .QO2 0.005 0.010 0.458 0.647 0.005 0.005 .QO3 -0.001 0.010 -0.144 0.886 -0.001 -0.001 .QW1 0.010 0.010 0.950 0.342 0.010 0.009 .QW2 0.013 0.010 1.268 0.205 0.013 0.013 .QW3 0.025 0.010 2.555 0.011 0.025 0.026 .QM1 -0.000 0.010 -0.046 0.963 -0.000 -0.000 .QM2 0.009 0.010 0.876 0.381 0.009 0.009 .QM3 0.004 0.010 0.379 0.704 0.004 0.004 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .VO1 0.754 0.012 64.731 0.000 0.754 0.752 .VO2 0.652 0.010 62.803 0.000 0.652 0.650 .VO3 0.544 0.009 58.899 0.000 0.544 0.546 .VW1 0.166 0.005 34.098 0.000 0.166 0.163 .VW2 0.486 0.007 64.831 0.000 0.486 0.475 .VW3 0.656 0.010 68.200 0.000 0.656 0.642 .VM1 0.431 0.009 49.752 0.000 0.431 0.425 .VM2 0.303 0.008 37.128 0.000 0.303 0.302 .VM3 0.663 0.010 65.608 0.000 0.663 0.668 .SO1 0.581 0.009 66.646 0.000 0.581 0.594 .SO2 0.592 0.009 66.143 0.000 0.592 0.593 .SO3 0.692 0.010 67.404 0.000 0.692 0.695 .SW1 0.297 0.006 52.133 0.000 0.297 0.295 .SW2 0.259 0.005 52.718 0.000 0.259 0.259 .SW3 0.503 0.008 65.638 0.000 0.503 0.507 .SM1 0.258 0.005 49.904 0.000 0.258 0.260 .SM2 0.525 0.008 63.106 0.000 0.525 0.527 .SM3 0.198 0.006 34.081 0.000 0.198 0.199 .QO1 0.691 0.011 60.174 0.000 0.691 0.704 .QO2 0.510 0.009 54.337 0.000 0.510 0.520 .QO3 0.723 0.012 62.650 0.000 0.723 0.715 .QW1 0.600 0.009 64.160 0.000 0.600 0.594 .QW2 0.486 0.008 61.850 0.000 0.486 0.498 .QW3 0.285 0.006 46.027 0.000 0.285 0.286 .QM1 0.638 0.011 58.684 0.000 0.638 0.641 .QM2 0.586 0.010 59.577 0.000 0.586 0.580 .QM3 0.454 0.009 50.592 0.000 0.454 0.451 Verbal 1.000 1.000 1.000 Spatial 1.000 1.000 1.000 Quant 1.000 1.000 1.000 g 1.000 1.000 1.000 R-Square: Estimate VO1 0.248 VO2 0.350 VO3 0.454 VW1 0.837 VW2 0.525 VW3 0.358 VM1 0.575 VM2 0.698 VM3 0.332 SO1 0.406 SO2 0.407 SO3 0.305 SW1 0.705 SW2 0.741 SW3 0.493 SM1 0.740 SM2 0.473 SM3 0.801 QO1 0.296 QO2 0.480 QO3 0.285 QW1 0.406 QW2 0.502 QW3 0.714 QM1 0.359 QM2 0.420 QM3 0.549 Code summary( cfaModelBifactorFit_full, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 45 iterations Estimator ML Optimization method NLMINB Number of model parameters 108 Number of observations 10000 Number of missing patterns 1 Model Test User Model: Standard Scaled Test Statistic 28930.049 29784.763 Degrees of freedom 297 297 P-value (Chi-square) 0.000 0.000 Scaling correction factor 0.971 Yuan-Bentler correction (Mplus variant) Model Test Baseline Model: Test statistic 146391.496 146201.022 Degrees of freedom 351 351 P-value 0.000 0.000 Scaling correction factor 1.001 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.804 0.798 Tucker-Lewis Index (TLI) 0.768 0.761 Robust Comparative Fit Index (CFI) 0.804 Robust Tucker-Lewis Index (TLI) 0.768 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -324368.770 -324368.770 Scaling correction factor 1.083 for the MLR correction Loglikelihood unrestricted model (H1) -309903.745 -309903.745 Scaling correction factor 1.001 for the MLR correction Akaike (AIC) 648953.539 648953.539 Bayesian (BIC) 649732.256 649732.256 Sample-size adjusted Bayesian (SABIC) 649389.048 649389.048 Root Mean Square Error of Approximation: RMSEA 0.098 0.100 90 Percent confidence interval - lower 0.097 0.099 90 Percent confidence interval - upper 0.099 0.101 P-value H_0: RMSEA &lt;= 0.050 0.000 0.000 P-value H_0: RMSEA &gt;= 0.080 1.000 1.000 Robust RMSEA 0.098 90 Percent confidence interval - lower 0.097 90 Percent confidence interval - upper 0.099 P-value H_0: Robust RMSEA &lt;= 0.050 0.000 P-value H_0: Robust RMSEA &gt;= 0.080 1.000 Standardized Root Mean Square Residual: SRMR 0.201 0.201 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all Verbal =~ VO1 0.484 0.011 42.953 0.000 0.484 0.484 VO2 0.572 0.012 49.110 0.000 0.572 0.571 VO3 0.648 0.011 59.594 0.000 0.648 0.649 VW1 0.602 0.021 28.399 0.000 0.602 0.597 VW2 0.522 0.017 31.037 0.000 0.522 0.516 VW3 0.457 0.015 31.401 0.000 0.457 0.452 VM1 0.764 0.009 81.260 0.000 0.764 0.758 VM2 0.837 0.009 91.851 0.000 0.837 0.835 VM3 0.574 0.010 58.967 0.000 0.574 0.576 Spatial =~ SO1 0.597 0.011 52.050 0.000 0.597 0.603 SO2 0.603 0.012 51.513 0.000 0.603 0.603 SO3 0.530 0.012 45.701 0.000 0.530 0.531 SW1 0.491 0.021 23.493 0.000 0.491 0.489 SW2 0.584 0.020 29.673 0.000 0.584 0.584 SW3 0.416 0.018 22.716 0.000 0.416 0.417 SM1 0.856 0.008 103.159 0.000 0.856 0.860 SM2 0.682 0.009 72.543 0.000 0.682 0.684 SM3 0.892 0.008 106.899 0.000 0.892 0.895 Quant =~ QO1 0.532 0.011 46.627 0.000 0.532 0.537 QO2 0.673 0.010 67.359 0.000 0.673 0.680 QO3 0.529 0.011 47.899 0.000 0.529 0.526 QW1 0.454 0.014 33.332 0.000 0.454 0.452 QW2 0.526 0.013 39.384 0.000 0.526 0.533 QW3 0.616 0.015 40.075 0.000 0.616 0.618 QM1 0.595 0.010 57.570 0.000 0.595 0.596 QM2 0.652 0.010 64.766 0.000 0.652 0.648 QM3 0.743 0.010 75.444 0.000 0.743 0.740 g =~ VO1 -0.116 0.020 -5.713 0.000 -0.116 -0.116 VO2 -0.155 0.023 -6.756 0.000 -0.155 -0.155 VO3 -0.181 0.024 -7.540 0.000 -0.181 -0.181 VW1 -0.699 0.021 -32.569 0.000 -0.699 -0.693 VW2 -0.514 0.020 -26.246 0.000 -0.514 -0.508 VW3 -0.397 0.018 -21.773 0.000 -0.397 -0.393 VM1 0.038 0.030 1.237 0.216 0.038 0.037 VM2 0.013 0.033 0.407 0.684 0.013 0.013 VM3 -0.013 0.023 -0.552 0.581 -0.013 -0.013 SO1 -0.202 0.023 -8.812 0.000 -0.202 -0.204 SO2 -0.208 0.023 -8.965 0.000 -0.208 -0.208 SO3 -0.153 0.023 -6.818 0.000 -0.153 -0.154 SW1 -0.684 0.018 -37.922 0.000 -0.684 -0.682 SW2 -0.633 0.021 -30.390 0.000 -0.633 -0.633 SW3 -0.563 0.016 -34.126 0.000 -0.563 -0.565 SM1 -0.010 0.031 -0.318 0.751 -0.010 -0.010 SM2 0.070 0.027 2.618 0.009 0.070 0.070 SM3 0.027 0.034 0.789 0.430 0.027 0.027 QO1 -0.088 0.019 -4.585 0.000 -0.088 -0.089 QO2 -0.133 0.020 -6.651 0.000 -0.133 -0.134 QO3 -0.090 0.018 -5.060 0.000 -0.090 -0.090 QW1 -0.452 0.015 -30.307 0.000 -0.452 -0.449 QW2 -0.461 0.016 -27.995 0.000 -0.461 -0.467 QW3 -0.574 0.018 -32.789 0.000 -0.574 -0.576 QM1 0.060 0.024 2.504 0.012 0.060 0.060 QM2 -0.000 0.023 -0.020 0.984 -0.000 -0.000 QM3 -0.040 0.026 -1.567 0.117 -0.040 -0.040 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all Verbal ~~ Spatial 0.000 0.000 0.000 Quant 0.000 0.000 0.000 Spatial ~~ Quant 0.000 0.000 0.000 Verbal ~~ g 0.000 0.000 0.000 Spatial ~~ g 0.000 0.000 0.000 Quant ~~ g 0.000 0.000 0.000 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all Verbal 0.000 0.000 0.000 Spatial 0.000 0.000 0.000 Quant 0.000 0.000 0.000 g 0.000 0.000 0.000 .VO1 (iVO1) -0.007 0.010 -0.717 0.473 -0.007 -0.007 .VO2 (iVO2) -0.013 0.010 -1.254 0.210 -0.013 -0.013 .VO3 (iVO3) -0.008 0.010 -0.815 0.415 -0.008 -0.008 .VW1 (iVW1) 0.015 0.010 1.464 0.143 0.015 0.015 .VW2 (iVW2) 0.011 0.010 1.043 0.297 0.011 0.010 .VW3 (iVW3) -0.006 0.010 -0.572 0.567 -0.006 -0.006 .VM1 (iVM1) -0.011 0.010 -1.086 0.277 -0.011 -0.011 .VM2 (iVM2) -0.010 0.010 -0.992 0.321 -0.010 -0.010 .VM3 (iVM3) -0.012 0.010 -1.195 0.232 -0.012 -0.012 .SO1 (iSO1) -0.006 0.010 -0.588 0.557 -0.006 -0.006 .SO2 (iSO2) -0.014 0.010 -1.405 0.160 -0.014 -0.014 .SO3 (iSO3) -0.008 0.010 -0.827 0.408 -0.008 -0.008 .SW1 (iSW1) 0.004 0.010 0.405 0.686 0.004 0.004 .SW2 (iSW2) 0.007 0.010 0.673 0.501 0.007 0.007 .SW3 (iSW3) 0.005 0.010 0.472 0.637 0.005 0.005 .SM1 (iSM1) -0.009 0.010 -0.913 0.361 -0.009 -0.009 .SM2 (iSM2) -0.008 0.010 -0.754 0.451 -0.008 -0.008 .SM3 (iSM3) -0.012 0.010 -1.185 0.236 -0.012 -0.012 .QO1 (iQO1) 0.005 0.010 0.479 0.632 0.005 0.005 .QO2 (iQO2) 0.005 0.010 0.458 0.647 0.005 0.005 .QO3 (iQO3) -0.001 0.010 -0.144 0.886 -0.001 -0.001 .QW1 (iQW1) 0.010 0.010 0.950 0.342 0.010 0.009 .QW2 (iQW2) 0.013 0.010 1.268 0.205 0.013 0.013 .QW3 (iQW3) 0.025 0.010 2.555 0.011 0.025 0.026 .QM1 (iQM1) -0.000 0.010 -0.046 0.963 -0.000 -0.000 .QM2 (iQM2) 0.009 0.010 0.876 0.381 0.009 0.009 .QM3 (iQM3) 0.004 0.010 0.379 0.704 0.004 0.004 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all Verbal 1.000 1.000 1.000 Spatial 1.000 1.000 1.000 Quant 1.000 1.000 1.000 g 1.000 1.000 1.000 .VO1 0.754 0.012 64.731 0.000 0.754 0.752 .VO2 0.652 0.010 62.803 0.000 0.652 0.650 .VO3 0.544 0.009 58.899 0.000 0.544 0.546 .VW1 0.166 0.005 34.098 0.000 0.166 0.163 .VW2 0.486 0.007 64.831 0.000 0.486 0.475 .VW3 0.656 0.010 68.200 0.000 0.656 0.642 .VM1 0.431 0.009 49.752 0.000 0.431 0.425 .VM2 0.303 0.008 37.128 0.000 0.303 0.302 .VM3 0.663 0.010 65.608 0.000 0.663 0.668 .SO1 0.581 0.009 66.646 0.000 0.581 0.594 .SO2 0.592 0.009 66.143 0.000 0.592 0.593 .SO3 0.692 0.010 67.404 0.000 0.692 0.695 .SW1 0.297 0.006 52.133 0.000 0.297 0.295 .SW2 0.259 0.005 52.718 0.000 0.259 0.259 .SW3 0.503 0.008 65.638 0.000 0.503 0.507 .SM1 0.258 0.005 49.904 0.000 0.258 0.260 .SM2 0.525 0.008 63.106 0.000 0.525 0.527 .SM3 0.198 0.006 34.081 0.000 0.198 0.199 .QO1 0.691 0.011 60.174 0.000 0.691 0.704 .QO2 0.510 0.009 54.337 0.000 0.510 0.520 .QO3 0.723 0.012 62.650 0.000 0.723 0.715 .QW1 0.600 0.009 64.160 0.000 0.600 0.594 .QW2 0.486 0.008 61.850 0.000 0.486 0.498 .QW3 0.285 0.006 46.027 0.000 0.285 0.286 .QM1 0.638 0.011 58.684 0.000 0.638 0.641 .QM2 0.586 0.010 59.577 0.000 0.586 0.580 .QM3 0.454 0.009 50.592 0.000 0.454 0.451 R-Square: Estimate VO1 0.248 VO2 0.350 VO3 0.454 VW1 0.837 VW2 0.525 VW3 0.358 VM1 0.575 VM2 0.698 VM3 0.332 SO1 0.406 SO2 0.407 SO3 0.305 SW1 0.705 SW2 0.741 SW3 0.493 SM1 0.740 SM2 0.473 SM3 0.801 QO1 0.296 QO2 0.480 QO3 0.285 QW1 0.406 QW2 0.502 QW3 0.714 QM1 0.359 QM2 0.420 QM3 0.549 14.4.2.12.5 Estimates of model fit Code fitMeasures( cfaModelBifactorFit, fit.measures = c( &quot;chisq&quot;, &quot;df&quot;, &quot;pvalue&quot;, &quot;chisq.scaled&quot;, &quot;df.scaled&quot;, &quot;pvalue.scaled&quot;, &quot;chisq.scaling.factor&quot;, &quot;baseline.chisq&quot;,&quot;baseline.df&quot;,&quot;baseline.pvalue&quot;, &quot;rmsea&quot;, &quot;cfi&quot;, &quot;tli&quot;, &quot;srmr&quot;, &quot;rmsea.robust&quot;, &quot;cfi.robust&quot;, &quot;tli.robust&quot;)) chisq df pvalue 28930.049 297.000 0.000 chisq.scaled df.scaled pvalue.scaled 29784.763 297.000 0.000 chisq.scaling.factor baseline.chisq baseline.df 0.971 146391.496 351.000 baseline.pvalue rmsea cfi 0.000 0.098 0.804 tli srmr rmsea.robust 0.768 0.201 0.098 cfi.robust tli.robust 0.804 0.768 14.4.2.12.6 Residuals Code residuals(cfaModelBifactorFit, type = &quot;cor&quot;) $type [1] &quot;cor.bollen&quot; $cov VO1 VO2 VO3 VW1 VW2 VW3 VM1 VM2 VM3 SO1 VO1 0.000 VO2 0.202 0.000 VO3 0.139 0.175 0.000 VW1 -0.022 -0.015 -0.015 0.000 VW2 -0.011 -0.008 0.003 0.004 0.000 VW3 0.001 -0.006 0.006 0.003 0.000 0.000 VM1 -0.057 -0.058 -0.045 0.007 -0.010 0.000 0.000 VM2 -0.045 -0.052 -0.035 0.009 0.004 -0.001 0.039 0.000 VM3 -0.031 -0.026 -0.022 0.001 0.001 -0.011 0.021 0.013 0.000 SO1 0.336 0.408 0.398 0.215 0.197 0.181 0.312 0.345 0.239 0.000 SO2 0.331 0.406 0.393 0.206 0.196 0.183 0.304 0.342 0.237 0.170 SO3 0.401 0.480 0.419 0.187 0.180 0.157 0.257 0.285 0.213 0.214 SW1 0.119 0.146 0.167 0.180 0.153 0.117 0.282 0.299 0.200 -0.016 SW2 0.169 0.194 0.231 0.210 0.181 0.166 0.336 0.358 0.240 0.008 SW3 0.110 0.130 0.145 0.151 0.125 0.108 0.245 0.257 0.172 -0.013 SM1 0.250 0.300 0.356 0.350 0.309 0.253 0.544 0.588 0.389 -0.024 SM2 0.187 0.218 0.274 0.296 0.250 0.211 0.460 0.493 0.325 -0.055 SM3 0.261 0.312 0.366 0.380 0.324 0.280 0.598 0.644 0.422 -0.037 QO1 0.377 0.458 0.376 0.154 0.136 0.126 0.187 0.221 0.150 0.327 QO2 0.306 0.372 0.361 0.190 0.183 0.174 0.255 0.294 0.212 0.303 QO3 0.292 0.360 0.316 0.142 0.123 0.116 0.181 0.209 0.146 0.272 QW1 0.102 0.116 0.141 0.142 0.108 0.107 0.206 0.218 0.154 0.092 QW2 0.113 0.133 0.167 0.166 0.141 0.124 0.242 0.259 0.183 0.115 QW3 0.126 0.154 0.186 0.189 0.154 0.140 0.270 0.297 0.194 0.122 QM1 0.146 0.192 0.234 0.248 0.211 0.191 0.385 0.407 0.266 0.189 QM2 0.170 0.209 0.260 0.247 0.214 0.196 0.369 0.400 0.253 0.203 QM3 0.204 0.249 0.303 0.284 0.238 0.221 0.412 0.455 0.301 0.235 SO2 SO3 SW1 SW2 SW3 SM1 SM2 SM3 QO1 QO2 VO1 VO2 VO3 VW1 VW2 VW3 VM1 VM2 VM3 SO1 SO2 0.000 SO3 0.218 0.000 SW1 -0.010 -0.022 0.000 SW2 0.003 0.004 0.002 0.000 SW3 -0.010 -0.011 0.005 0.004 0.000 SM1 -0.023 -0.036 0.000 -0.003 -0.004 0.000 SM2 -0.058 -0.055 0.005 -0.008 0.011 0.003 0.000 SM3 -0.038 -0.047 0.006 0.000 0.003 0.018 0.032 0.000 QO1 0.328 0.427 0.077 0.112 0.075 0.172 0.124 0.182 0.000 QO2 0.301 0.345 0.099 0.156 0.091 0.240 0.179 0.251 0.155 0.000 QO3 0.267 0.329 0.073 0.111 0.067 0.169 0.122 0.180 0.196 0.104 QW1 0.091 0.081 0.082 0.097 0.069 0.178 0.147 0.194 -0.019 -0.022 QW2 0.115 0.096 0.094 0.125 0.087 0.224 0.182 0.234 -0.040 -0.019 QW3 0.129 0.110 0.111 0.134 0.104 0.241 0.203 0.261 -0.040 -0.021 QM1 0.185 0.167 0.185 0.219 0.170 0.382 0.313 0.416 -0.066 -0.056 QM2 0.206 0.171 0.180 0.203 0.154 0.361 0.296 0.394 -0.047 -0.025 QM3 0.232 0.201 0.182 0.232 0.163 0.392 0.327 0.428 -0.055 -0.020 QO3 QW1 QW2 QW3 QM1 QM2 QM3 VO1 VO2 VO3 VW1 VW2 VW3 VM1 VM2 VM3 SO1 SO2 SO3 SW1 SW2 SW3 SM1 SM2 SM3 QO1 QO2 QO3 0.000 QW1 -0.020 0.000 QW2 -0.029 0.006 0.000 QW3 -0.026 0.014 0.017 0.000 QM1 -0.064 0.016 0.009 0.019 0.000 QM2 -0.047 0.005 0.007 0.011 0.036 0.000 QM3 -0.043 -0.001 0.012 0.012 0.041 0.021 0.000 $mean VO1 VO2 VO3 VW1 VW2 VW3 VM1 VM2 VM3 SO1 SO2 SO3 SW1 SW2 SW3 SM1 SM2 SM3 QO1 QO2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 QO3 QW1 QW2 QW3 QM1 QM2 QM3 0 0 0 0 0 0 0 14.4.2.12.7 Modification indices Code modificationindices(cfaModelBifactorFit, sort. = TRUE) 14.4.2.12.8 Internal Consistency Reliability Internal consistency reliability of items composing the latent factors, as quantified by omega (\\(\\omega\\)) and average variance extracted (AVE), was estimated using the semTools package (Jorgensen et al., 2021). Code compRelSEM(cfaModelBifactorFit) Verbal Spatial Quant g 0.767 0.763 0.776 0.148 Code AVE(cfaModelBifactorFit) Verbal Spatial Quant g NA NA NA NA 14.4.2.12.9 Path diagram A path diagram of the model generated using the semPlot package (Epskamp, 2022) is in Figure 14.74. Code semPaths( cfaModelBifactorFit, what = &quot;std&quot;, layout = &quot;tree3&quot;, bifactor = c(&quot;g&quot;), edge.label.cex = 1.3) Figure 14.74: Bifactor Model. 14.4.2.12.10 Equivalently fitting models Markov equivalent directed acyclic graphs (DAGs) were depicted using the dagitty package (Textor et al., 2021). Path diagrams of equivalent models are below. Code dagModelBifactor &lt;- lavaanToGraph(cfaModelBifactorFit) par(mfrow = c(2, 2)) Code lapply(equivalentDAGs(dagModelBifactor, n = 4), plot) Figure 14.75: Equivalently Fitting Models to a Bifactor Model. Figure 14.76: Equivalently Fitting Models to a Bifactor Model. Figure 14.77: Equivalently Fitting Models to a Bifactor Model. Figure 14.78: Equivalently Fitting Models to a Bifactor Model. [[1]] [[1]]$mar [1] 0 0 0 0 [[2]] [[2]]$mar [1] 0 0 0 0 [[3]] [[3]]$mar [1] 0 0 0 0 [[4]] [[4]]$mar [1] 0 0 0 0 14.4.2.13 Multitrait-Multimethod Model (MTMM) 14.4.2.13.1 Simulate the data We simulated the data for the multitrait-multimethod model (MTMM) in Section 5.3.1.4.2.3. 14.4.2.13.2 Specify the model This example is courtesy of W. Joel Schneider. Code cfaModelMTMM_syntax &lt;- &#39; g =~ Verbal + Spatial + Quant Verbal =~ VO1 + VO2 + VO3 + VW1 + VW2 + VW3 + VM1 + VM2 + VM3 Spatial =~ SO1 + SO2 + SO3 + SW1 + SW2 + SW3 + SM1 + SM2 + SM3 Quant =~ QO1 + QO2 + QO3 + QW1 + QW2 + QW3 + QM1 + QM2 + QM3 Oral =~ VO1 + VO2 + VO3 + SO1 + SO2 + SO3 + QO1 + QO2 + QO3 Written =~ VW1 + VW2 + VW3 + SW1 + SW2 + SW3 + QW1 + QW2 + QW3 Manipulative =~ VM1 + VM2 + VM3 + SM1 + SM2 + SM3 + QM1 + QM2 + QM3 &#39; cfaModelMTMM_fullSyntax &lt;- &#39; #Factor loadings (free the factor loading of the first indicator) g =~ NA*Verbal + Spatial + Quant Verbal =~ NA*VO1 + VO2 + VO3 + VW1 + VW2 + VW3 + VM1 + VM2 + VM3 Spatial =~ NA*SO1 + SO2 + SO3 + SW1 + SW2 + SW3 + SM1 + SM2 + SM3 Quant =~ NA*QO1 + QO2 + QO3 + QW1 + QW2 + QW3 + QM1 + QM2 + QM3 Oral =~ NA*VO1 + VO2 + VO3 + SO1 + SO2 + SO3 + QO1 + QO2 + QO3 Written =~ NA*VW1 + VW2 + VW3 + SW1 + SW2 + SW3 + QW1 + QW2 + QW3 Manipulative =~ NA*VM1 + VM2 + VM3 + SM1 + SM2 + SM3 + QM1 + QM2 + QM3 #Fix latent means to zero Verbal ~ 0 Spatial ~ 0 Quant ~ 0 Oral ~ 0 Written ~ 0 Manipulative ~ 0 g ~ 0 #Fix latent variances to one Verbal ~~ 1*Verbal Spatial ~~ 1*Spatial Quant ~~ 1*Quant Oral ~~ 1*Oral Written ~~ 1*Written Manipulative ~~ 1*Manipulative g ~~ 1*g #Fix covariances among latent variables at zero Verbal ~~ 0*Spatial Verbal ~~ 0*Quant Verbal ~~ 0*Oral Verbal ~~ 0*Written Verbal ~~ 0*Manipulative Spatial ~~ 0*Quant Spatial ~~ 0*Oral Spatial ~~ 0*Written Spatial ~~ 0*Manipulative Quant ~~ 0*Oral Quant ~~ 0*Written Quant ~~ 0*Manipulative Oral ~~ 0*Written Oral ~~ 0*Manipulative Written ~~ 0*Manipulative g ~~ 0*Verbal g ~~ 0*Spatial g ~~ 0*Quant g ~~ 0*Oral g ~~ 0*Written g ~~ 0*Manipulative #Estimate residual variances of manifest variables VO1 ~~ VO1 VO2 ~~ VO2 VO3 ~~ VO3 VW1 ~~ VW1 VW2 ~~ VW2 VW3 ~~ VW3 VM1 ~~ VM1 VM2 ~~ VM2 VM3 ~~ VM3 SO1 ~~ SO1 SO2 ~~ SO2 SO3 ~~ SO3 SW1 ~~ SW1 SW2 ~~ SW2 SW3 ~~ SW3 SM1 ~~ SM1 SM2 ~~ SM2 SM3 ~~ SM3 QO1 ~~ QO1 QO2 ~~ QO2 QO3 ~~ QO3 QW1 ~~ QW1 QW2 ~~ QW2 QW3 ~~ QW3 QM1 ~~ QM1 QM2 ~~ QM2 QM3 ~~ QM3 #Free intercepts of manifest variables VO1 ~ intVO1*1 VO2 ~ intVO2*1 VO3 ~ intVO3*1 VW1 ~ intVW1*1 VW2 ~ intVW2*1 VW3 ~ intVW3*1 VM1 ~ intVM1*1 VM2 ~ intVM2*1 VM3 ~ intVM3*1 SO1 ~ intSO1*1 SO2 ~ intSO2*1 SO3 ~ intSO3*1 SW1 ~ intSW1*1 SW2 ~ intSW2*1 SW3 ~ intSW3*1 SM1 ~ intSM1*1 SM2 ~ intSM2*1 SM3 ~ intSM3*1 QO1 ~ intQO1*1 QO2 ~ intQO2*1 QO3 ~ intQO3*1 QW1 ~ intQW1*1 QW2 ~ intQW2*1 QW3 ~ intQW3*1 QM1 ~ intQM1*1 QM2 ~ intQM2*1 QM3 ~ intQM3*1 &#39; 14.4.2.13.3 Model syntax in table form: Code lavaanify(cfaModelMTMM_syntax) Code lavaanify(cfaModelMTMM_fullSyntax) 14.4.2.13.4 Fit the model Code cfaModelMTMMFit &lt;- cfa( cfaModelMTMM_syntax, data = MTMM_data, orthogonal = TRUE, std.lv = TRUE, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) cfaModelMTMMFit_full &lt;- lavaan( cfaModelMTMM_fullSyntax, data = MTMM_data, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) 14.4.2.13.5 Display summary output Code summary( cfaModelMTMMFit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 57 iterations Estimator ML Optimization method NLMINB Number of model parameters 111 Number of observations 10000 Number of missing patterns 1 Model Test User Model: Standard Scaled Test Statistic 254.449 254.155 Degrees of freedom 294 294 P-value (Chi-square) 0.954 0.955 Scaling correction factor 1.001 Yuan-Bentler correction (Mplus variant) Model Test Baseline Model: Test statistic 146391.496 146201.022 Degrees of freedom 351 351 P-value 0.000 0.000 Scaling correction factor 1.001 User Model versus Baseline Model: Comparative Fit Index (CFI) 1.000 1.000 Tucker-Lewis Index (TLI) 1.000 1.000 Robust Comparative Fit Index (CFI) 1.000 Robust Tucker-Lewis Index (TLI) 1.000 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -310030.970 -310030.970 Scaling correction factor 1.001 for the MLR correction Loglikelihood unrestricted model (H1) -309903.745 -309903.745 Scaling correction factor 1.001 for the MLR correction Akaike (AIC) 620283.939 620283.939 Bayesian (BIC) 621084.287 621084.287 Sample-size adjusted Bayesian (SABIC) 620731.545 620731.545 Root Mean Square Error of Approximation: RMSEA 0.000 0.000 90 Percent confidence interval - lower 0.000 0.000 90 Percent confidence interval - upper 0.000 0.000 P-value H_0: RMSEA &lt;= 0.050 1.000 1.000 P-value H_0: RMSEA &gt;= 0.080 0.000 0.000 Robust RMSEA 0.000 90 Percent confidence interval - lower 0.000 90 Percent confidence interval - upper 0.000 P-value H_0: Robust RMSEA &lt;= 0.050 1.000 P-value H_0: Robust RMSEA &gt;= 0.080 0.000 Standardized Root Mean Square Residual: SRMR 0.006 0.006 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all g =~ Verbal 2.331 0.120 19.490 0.000 0.919 0.919 Spatial 1.194 0.029 41.325 0.000 0.767 0.767 Quant 0.858 0.019 44.320 0.000 0.651 0.651 Verbal =~ VO1 0.197 0.009 21.502 0.000 0.499 0.498 VO2 0.239 0.011 22.383 0.000 0.607 0.605 VO3 0.280 0.012 22.655 0.000 0.710 0.710 VW1 0.282 0.012 23.078 0.000 0.715 0.709 VW2 0.242 0.011 22.365 0.000 0.613 0.607 VW3 0.211 0.010 21.774 0.000 0.535 0.529 VM1 0.242 0.011 22.563 0.000 0.614 0.610 VM2 0.277 0.012 22.926 0.000 0.702 0.703 VM3 0.195 0.009 21.218 0.000 0.494 0.496 Spatial =~ SO1 0.445 0.008 57.171 0.000 0.693 0.699 SO2 0.450 0.008 56.641 0.000 0.700 0.700 SO3 0.383 0.007 52.516 0.000 0.597 0.596 SW1 0.384 0.007 57.093 0.000 0.598 0.597 SW2 0.450 0.007 62.843 0.000 0.700 0.701 SW3 0.323 0.007 47.171 0.000 0.502 0.505 SM1 0.455 0.007 60.751 0.000 0.709 0.713 SM2 0.317 0.007 44.502 0.000 0.493 0.495 SM3 0.456 0.007 64.320 0.000 0.709 0.712 Quant =~ QO1 0.380 0.007 51.830 0.000 0.501 0.504 QO2 0.523 0.008 67.618 0.000 0.689 0.694 QO3 0.386 0.008 48.712 0.000 0.508 0.504 QW1 0.386 0.008 50.153 0.000 0.509 0.506 QW2 0.447 0.007 61.478 0.000 0.588 0.595 QW3 0.527 0.007 75.098 0.000 0.695 0.696 QM1 0.381 0.008 49.115 0.000 0.502 0.504 QM2 0.450 0.008 57.056 0.000 0.593 0.590 QM3 0.535 0.008 68.983 0.000 0.705 0.702 Oral =~ VO1 0.405 0.011 36.849 0.000 0.405 0.404 VO2 0.494 0.010 49.808 0.000 0.494 0.492 VO3 0.297 0.010 29.548 0.000 0.297 0.297 SO1 0.293 0.010 29.258 0.000 0.293 0.295 SO2 0.297 0.010 29.825 0.000 0.297 0.296 SO3 0.512 0.010 50.395 0.000 0.512 0.512 QO1 0.583 0.011 55.071 0.000 0.583 0.587 QO2 0.310 0.010 31.012 0.000 0.310 0.312 QO3 0.402 0.011 36.601 0.000 0.402 0.399 Written =~ VW1 0.596 0.008 74.289 0.000 0.596 0.591 VW2 0.402 0.010 41.570 0.000 0.402 0.397 VW3 0.289 0.011 26.689 0.000 0.289 0.286 SW1 0.599 0.009 70.011 0.000 0.599 0.598 SW2 0.503 0.008 59.564 0.000 0.503 0.503 SW3 0.489 0.010 49.691 0.000 0.489 0.491 QW1 0.400 0.010 39.373 0.000 0.400 0.397 QW2 0.394 0.009 41.734 0.000 0.394 0.398 QW3 0.503 0.008 59.491 0.000 0.503 0.504 Manipulative =~ VM1 0.495 0.009 52.937 0.000 0.495 0.493 VM2 0.495 0.009 57.185 0.000 0.495 0.495 VM3 0.291 0.011 26.756 0.000 0.291 0.293 SM1 0.475 0.009 55.234 0.000 0.475 0.478 SM2 0.498 0.010 48.305 0.000 0.498 0.499 SM3 0.583 0.008 73.226 0.000 0.583 0.585 QM1 0.402 0.010 38.302 0.000 0.402 0.403 QM2 0.311 0.010 30.685 0.000 0.311 0.309 QM3 0.300 0.010 31.426 0.000 0.300 0.299 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all g ~~ Oral 0.000 0.000 0.000 Written 0.000 0.000 0.000 Manipulative 0.000 0.000 0.000 Oral ~~ Written 0.000 0.000 0.000 Manipulative 0.000 0.000 0.000 Written ~~ Manipulative 0.000 0.000 0.000 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .VO1 -0.007 0.010 -0.717 0.473 -0.007 -0.007 .VO2 -0.013 0.010 -1.254 0.210 -0.013 -0.013 .VO3 -0.008 0.010 -0.815 0.415 -0.008 -0.008 .VW1 0.015 0.010 1.464 0.143 0.015 0.015 .VW2 0.011 0.010 1.043 0.297 0.011 0.010 .VW3 -0.006 0.010 -0.572 0.567 -0.006 -0.006 .VM1 -0.011 0.010 -1.086 0.277 -0.011 -0.011 .VM2 -0.010 0.010 -0.992 0.321 -0.010 -0.010 .VM3 -0.012 0.010 -1.195 0.232 -0.012 -0.012 .SO1 -0.006 0.010 -0.588 0.557 -0.006 -0.006 .SO2 -0.014 0.010 -1.405 0.160 -0.014 -0.014 .SO3 -0.008 0.010 -0.827 0.408 -0.008 -0.008 .SW1 0.004 0.010 0.405 0.686 0.004 0.004 .SW2 0.007 0.010 0.673 0.501 0.007 0.007 .SW3 0.005 0.010 0.472 0.637 0.005 0.005 .SM1 -0.009 0.010 -0.913 0.361 -0.009 -0.009 .SM2 -0.008 0.010 -0.754 0.451 -0.008 -0.008 .SM3 -0.012 0.010 -1.185 0.236 -0.012 -0.012 .QO1 0.005 0.010 0.479 0.632 0.005 0.005 .QO2 0.005 0.010 0.458 0.647 0.005 0.005 .QO3 -0.001 0.010 -0.144 0.886 -0.001 -0.001 .QW1 0.010 0.010 0.950 0.342 0.010 0.009 .QW2 0.013 0.010 1.268 0.205 0.013 0.013 .QW3 0.025 0.010 2.555 0.011 0.025 0.026 .QM1 -0.000 0.010 -0.046 0.963 -0.000 -0.000 .QM2 0.009 0.010 0.876 0.381 0.009 0.009 .QM3 0.004 0.010 0.379 0.704 0.004 0.004 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .VO1 0.591 0.009 63.350 0.000 0.591 0.589 .VO2 0.395 0.007 54.211 0.000 0.395 0.392 .VO3 0.407 0.007 56.302 0.000 0.407 0.408 .VW1 0.151 0.004 37.411 0.000 0.151 0.148 .VW2 0.485 0.007 64.821 0.000 0.485 0.474 .VW3 0.652 0.010 67.786 0.000 0.652 0.638 .VM1 0.389 0.007 57.784 0.000 0.389 0.385 .VM2 0.260 0.005 50.854 0.000 0.260 0.261 .VM3 0.662 0.010 66.688 0.000 0.662 0.668 .SO1 0.417 0.007 60.156 0.000 0.417 0.424 .SO2 0.423 0.007 59.184 0.000 0.423 0.423 .SO3 0.383 0.007 52.912 0.000 0.383 0.383 .SW1 0.287 0.005 52.962 0.000 0.287 0.286 .SW2 0.255 0.005 53.873 0.000 0.255 0.255 .SW3 0.499 0.008 65.345 0.000 0.499 0.504 .SM1 0.261 0.005 54.597 0.000 0.261 0.264 .SM2 0.503 0.008 63.845 0.000 0.503 0.506 .SM3 0.150 0.004 37.850 0.000 0.150 0.151 .QO1 0.396 0.008 47.010 0.000 0.396 0.402 .QO2 0.415 0.007 56.527 0.000 0.415 0.421 .QO3 0.595 0.009 63.503 0.000 0.595 0.586 .QW1 0.592 0.009 64.504 0.000 0.592 0.586 .QW2 0.475 0.008 61.934 0.000 0.475 0.487 .QW3 0.260 0.005 47.718 0.000 0.260 0.261 .QM1 0.581 0.009 64.008 0.000 0.581 0.584 .QM2 0.561 0.009 63.216 0.000 0.561 0.556 .QM3 0.420 0.008 55.556 0.000 0.420 0.417 g 1.000 1.000 1.000 .Verbal 1.000 0.155 0.155 .Spatial 1.000 0.412 0.412 .Quant 1.000 0.576 0.576 Oral 1.000 1.000 1.000 Written 1.000 1.000 1.000 Manipulative 1.000 1.000 1.000 R-Square: Estimate VO1 0.411 VO2 0.608 VO3 0.592 VW1 0.852 VW2 0.526 VW3 0.362 VM1 0.615 VM2 0.739 VM3 0.332 SO1 0.576 SO2 0.577 SO3 0.617 SW1 0.714 SW2 0.745 SW3 0.496 SM1 0.736 SM2 0.494 SM3 0.849 QO1 0.598 QO2 0.579 QO3 0.414 QW1 0.414 QW2 0.513 QW3 0.739 QM1 0.416 QM2 0.444 QM3 0.583 Verbal 0.845 Spatial 0.588 Quant 0.424 Code summary( cfaModelMTMMFit_full, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 57 iterations Estimator ML Optimization method NLMINB Number of model parameters 111 Number of observations 10000 Number of missing patterns 1 Model Test User Model: Standard Scaled Test Statistic 254.449 254.155 Degrees of freedom 294 294 P-value (Chi-square) 0.954 0.955 Scaling correction factor 1.001 Yuan-Bentler correction (Mplus variant) Model Test Baseline Model: Test statistic 146391.496 146201.022 Degrees of freedom 351 351 P-value 0.000 0.000 Scaling correction factor 1.001 User Model versus Baseline Model: Comparative Fit Index (CFI) 1.000 1.000 Tucker-Lewis Index (TLI) 1.000 1.000 Robust Comparative Fit Index (CFI) 1.000 Robust Tucker-Lewis Index (TLI) 1.000 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -310030.970 -310030.970 Scaling correction factor 1.001 for the MLR correction Loglikelihood unrestricted model (H1) -309903.745 -309903.745 Scaling correction factor 1.001 for the MLR correction Akaike (AIC) 620283.939 620283.939 Bayesian (BIC) 621084.287 621084.287 Sample-size adjusted Bayesian (SABIC) 620731.545 620731.545 Root Mean Square Error of Approximation: RMSEA 0.000 0.000 90 Percent confidence interval - lower 0.000 0.000 90 Percent confidence interval - upper 0.000 0.000 P-value H_0: RMSEA &lt;= 0.050 1.000 1.000 P-value H_0: RMSEA &gt;= 0.080 0.000 0.000 Robust RMSEA 0.000 90 Percent confidence interval - lower 0.000 90 Percent confidence interval - upper 0.000 P-value H_0: Robust RMSEA &lt;= 0.050 1.000 P-value H_0: Robust RMSEA &gt;= 0.080 0.000 Standardized Root Mean Square Residual: SRMR 0.006 0.006 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all g =~ Verbal 2.331 0.120 19.490 0.000 0.919 0.919 Spatial 1.194 0.029 41.325 0.000 0.767 0.767 Quant 0.858 0.019 44.320 0.000 0.651 0.651 Verbal =~ VO1 0.197 0.009 21.502 0.000 0.499 0.498 VO2 0.239 0.011 22.383 0.000 0.607 0.605 VO3 0.280 0.012 22.655 0.000 0.710 0.710 VW1 0.282 0.012 23.078 0.000 0.715 0.709 VW2 0.242 0.011 22.365 0.000 0.613 0.607 VW3 0.211 0.010 21.774 0.000 0.535 0.529 VM1 0.242 0.011 22.563 0.000 0.614 0.610 VM2 0.277 0.012 22.926 0.000 0.702 0.703 VM3 0.195 0.009 21.218 0.000 0.494 0.496 Spatial =~ SO1 0.445 0.008 57.171 0.000 0.693 0.699 SO2 0.450 0.008 56.641 0.000 0.700 0.700 SO3 0.383 0.007 52.516 0.000 0.597 0.596 SW1 0.384 0.007 57.093 0.000 0.598 0.597 SW2 0.450 0.007 62.843 0.000 0.700 0.701 SW3 0.323 0.007 47.171 0.000 0.502 0.505 SM1 0.455 0.007 60.751 0.000 0.709 0.713 SM2 0.317 0.007 44.502 0.000 0.493 0.495 SM3 0.456 0.007 64.320 0.000 0.709 0.712 Quant =~ QO1 0.380 0.007 51.830 0.000 0.501 0.504 QO2 0.523 0.008 67.618 0.000 0.689 0.694 QO3 0.386 0.008 48.712 0.000 0.508 0.504 QW1 0.386 0.008 50.153 0.000 0.509 0.506 QW2 0.447 0.007 61.478 0.000 0.588 0.595 QW3 0.527 0.007 75.098 0.000 0.695 0.696 QM1 0.381 0.008 49.115 0.000 0.502 0.504 QM2 0.450 0.008 57.056 0.000 0.593 0.590 QM3 0.535 0.008 68.983 0.000 0.705 0.702 Oral =~ VO1 0.405 0.011 36.849 0.000 0.405 0.404 VO2 0.494 0.010 49.808 0.000 0.494 0.492 VO3 0.297 0.010 29.548 0.000 0.297 0.297 SO1 0.293 0.010 29.258 0.000 0.293 0.295 SO2 0.297 0.010 29.825 0.000 0.297 0.296 SO3 0.512 0.010 50.395 0.000 0.512 0.512 QO1 0.583 0.011 55.071 0.000 0.583 0.587 QO2 0.310 0.010 31.012 0.000 0.310 0.312 QO3 0.402 0.011 36.601 0.000 0.402 0.399 Written =~ VW1 0.596 0.008 74.289 0.000 0.596 0.591 VW2 0.402 0.010 41.570 0.000 0.402 0.397 VW3 0.289 0.011 26.689 0.000 0.289 0.286 SW1 0.599 0.009 70.011 0.000 0.599 0.598 SW2 0.503 0.008 59.564 0.000 0.503 0.503 SW3 0.489 0.010 49.691 0.000 0.489 0.491 QW1 0.400 0.010 39.373 0.000 0.400 0.397 QW2 0.394 0.009 41.734 0.000 0.394 0.398 QW3 0.503 0.008 59.491 0.000 0.503 0.504 Manipulative =~ VM1 0.495 0.009 52.937 0.000 0.495 0.493 VM2 0.495 0.009 57.185 0.000 0.495 0.495 VM3 0.291 0.011 26.756 0.000 0.291 0.293 SM1 0.475 0.009 55.234 0.000 0.475 0.478 SM2 0.498 0.010 48.305 0.000 0.498 0.499 SM3 0.583 0.008 73.226 0.000 0.583 0.585 QM1 0.402 0.010 38.302 0.000 0.402 0.403 QM2 0.311 0.010 30.685 0.000 0.311 0.309 QM3 0.300 0.010 31.426 0.000 0.300 0.299 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .Verbal ~~ .Spatial 0.000 0.000 0.000 .Quant 0.000 0.000 0.000 Oral 0.000 0.000 0.000 Written 0.000 0.000 0.000 Manipulative 0.000 0.000 0.000 .Spatial ~~ .Quant 0.000 0.000 0.000 Oral 0.000 0.000 0.000 Written 0.000 0.000 0.000 Manipulative 0.000 0.000 0.000 .Quant ~~ Oral 0.000 0.000 0.000 Written 0.000 0.000 0.000 Manipulative 0.000 0.000 0.000 Oral ~~ Written 0.000 0.000 0.000 Manipulative 0.000 0.000 0.000 Written ~~ Manipulative 0.000 0.000 0.000 g ~~ .Verbal 0.000 0.000 0.000 .Spatial 0.000 0.000 0.000 .Quant 0.000 0.000 0.000 Oral 0.000 0.000 0.000 Written 0.000 0.000 0.000 Manipulative 0.000 0.000 0.000 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .Verbal 0.000 0.000 0.000 .Spatial 0.000 0.000 0.000 .Quant 0.000 0.000 0.000 Oral 0.000 0.000 0.000 Written 0.000 0.000 0.000 Manpltv 0.000 0.000 0.000 g 0.000 0.000 0.000 .VO1 (iVO1) -0.007 0.010 -0.717 0.473 -0.007 -0.007 .VO2 (iVO2) -0.013 0.010 -1.254 0.210 -0.013 -0.013 .VO3 (iVO3) -0.008 0.010 -0.815 0.415 -0.008 -0.008 .VW1 (iVW1) 0.015 0.010 1.464 0.143 0.015 0.015 .VW2 (iVW2) 0.011 0.010 1.043 0.297 0.011 0.010 .VW3 (iVW3) -0.006 0.010 -0.572 0.567 -0.006 -0.006 .VM1 (iVM1) -0.011 0.010 -1.086 0.277 -0.011 -0.011 .VM2 (iVM2) -0.010 0.010 -0.992 0.321 -0.010 -0.010 .VM3 (iVM3) -0.012 0.010 -1.195 0.232 -0.012 -0.012 .SO1 (iSO1) -0.006 0.010 -0.588 0.557 -0.006 -0.006 .SO2 (iSO2) -0.014 0.010 -1.405 0.160 -0.014 -0.014 .SO3 (iSO3) -0.008 0.010 -0.827 0.408 -0.008 -0.008 .SW1 (iSW1) 0.004 0.010 0.405 0.686 0.004 0.004 .SW2 (iSW2) 0.007 0.010 0.673 0.501 0.007 0.007 .SW3 (iSW3) 0.005 0.010 0.472 0.637 0.005 0.005 .SM1 (iSM1) -0.009 0.010 -0.913 0.361 -0.009 -0.009 .SM2 (iSM2) -0.008 0.010 -0.754 0.451 -0.008 -0.008 .SM3 (iSM3) -0.012 0.010 -1.185 0.236 -0.012 -0.012 .QO1 (iQO1) 0.005 0.010 0.479 0.632 0.005 0.005 .QO2 (iQO2) 0.005 0.010 0.458 0.647 0.005 0.005 .QO3 (iQO3) -0.001 0.010 -0.144 0.886 -0.001 -0.001 .QW1 (iQW1) 0.010 0.010 0.950 0.342 0.010 0.009 .QW2 (iQW2) 0.013 0.010 1.268 0.205 0.013 0.013 .QW3 (iQW3) 0.025 0.010 2.555 0.011 0.025 0.026 .QM1 (iQM1) -0.000 0.010 -0.046 0.963 -0.000 -0.000 .QM2 (iQM2) 0.009 0.010 0.876 0.381 0.009 0.009 .QM3 (iQM3) 0.004 0.010 0.379 0.704 0.004 0.004 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .Verbal 1.000 0.155 0.155 .Spatial 1.000 0.412 0.412 .Quant 1.000 0.576 0.576 Oral 1.000 1.000 1.000 Written 1.000 1.000 1.000 Manipulative 1.000 1.000 1.000 g 1.000 1.000 1.000 .VO1 0.591 0.009 63.350 0.000 0.591 0.589 .VO2 0.395 0.007 54.211 0.000 0.395 0.392 .VO3 0.407 0.007 56.302 0.000 0.407 0.408 .VW1 0.151 0.004 37.411 0.000 0.151 0.148 .VW2 0.485 0.007 64.821 0.000 0.485 0.474 .VW3 0.652 0.010 67.786 0.000 0.652 0.638 .VM1 0.389 0.007 57.784 0.000 0.389 0.385 .VM2 0.260 0.005 50.854 0.000 0.260 0.261 .VM3 0.662 0.010 66.688 0.000 0.662 0.668 .SO1 0.417 0.007 60.156 0.000 0.417 0.424 .SO2 0.423 0.007 59.184 0.000 0.423 0.423 .SO3 0.383 0.007 52.912 0.000 0.383 0.383 .SW1 0.287 0.005 52.962 0.000 0.287 0.286 .SW2 0.255 0.005 53.873 0.000 0.255 0.255 .SW3 0.499 0.008 65.345 0.000 0.499 0.504 .SM1 0.261 0.005 54.597 0.000 0.261 0.264 .SM2 0.503 0.008 63.845 0.000 0.503 0.506 .SM3 0.150 0.004 37.850 0.000 0.150 0.151 .QO1 0.396 0.008 47.010 0.000 0.396 0.402 .QO2 0.415 0.007 56.527 0.000 0.415 0.421 .QO3 0.595 0.009 63.503 0.000 0.595 0.586 .QW1 0.592 0.009 64.504 0.000 0.592 0.586 .QW2 0.475 0.008 61.934 0.000 0.475 0.487 .QW3 0.260 0.005 47.718 0.000 0.260 0.261 .QM1 0.581 0.009 64.008 0.000 0.581 0.584 .QM2 0.561 0.009 63.216 0.000 0.561 0.556 .QM3 0.420 0.008 55.556 0.000 0.420 0.417 R-Square: Estimate Verbal 0.845 Spatial 0.588 Quant 0.424 VO1 0.411 VO2 0.608 VO3 0.592 VW1 0.852 VW2 0.526 VW3 0.362 VM1 0.615 VM2 0.739 VM3 0.332 SO1 0.576 SO2 0.577 SO3 0.617 SW1 0.714 SW2 0.745 SW3 0.496 SM1 0.736 SM2 0.494 SM3 0.849 QO1 0.598 QO2 0.579 QO3 0.414 QW1 0.414 QW2 0.513 QW3 0.739 QM1 0.416 QM2 0.444 QM3 0.583 14.4.2.13.6 Estimates of model fit Code fitMeasures( cfaModelMTMMFit, fit.measures = c( &quot;chisq&quot;, &quot;df&quot;, &quot;pvalue&quot;, &quot;chisq.scaled&quot;, &quot;df.scaled&quot;, &quot;pvalue.scaled&quot;, &quot;chisq.scaling.factor&quot;, &quot;baseline.chisq&quot;,&quot;baseline.df&quot;,&quot;baseline.pvalue&quot;, &quot;rmsea&quot;, &quot;cfi&quot;, &quot;tli&quot;, &quot;srmr&quot;, &quot;rmsea.robust&quot;, &quot;cfi.robust&quot;, &quot;tli.robust&quot;)) chisq df pvalue 254.449 294.000 0.954 chisq.scaled df.scaled pvalue.scaled 254.155 294.000 0.955 chisq.scaling.factor baseline.chisq baseline.df 1.001 146391.496 351.000 baseline.pvalue rmsea cfi 0.000 0.000 1.000 tli srmr rmsea.robust 1.000 0.006 0.000 cfi.robust tli.robust 1.000 1.000 14.4.2.13.7 Residuals Code residuals(cfaModelMTMMFit, type = &quot;cor&quot;) $type [1] &quot;cor.bollen&quot; $cov VO1 VO2 VO3 VW1 VW2 VW3 VM1 VM2 VM3 SO1 VO1 0.000 VO2 -0.003 0.000 VO3 0.001 -0.002 0.000 VW1 -0.005 0.005 -0.005 0.000 VW2 -0.004 -0.001 0.000 0.000 0.000 VW3 0.002 -0.007 -0.005 0.001 -0.002 0.000 VM1 0.001 0.001 0.007 0.001 -0.008 0.004 0.000 VM2 0.008 -0.002 0.006 0.000 0.002 -0.001 -0.001 0.000 VM3 0.003 0.005 0.002 0.001 0.004 -0.009 0.010 0.000 0.000 SO1 -0.005 -0.003 -0.002 0.007 0.002 0.001 0.003 -0.004 -0.003 0.000 SO2 -0.010 -0.006 -0.007 0.001 0.002 0.004 -0.004 -0.007 -0.005 -0.001 SO3 0.003 -0.002 -0.003 -0.004 0.003 -0.005 -0.005 -0.012 0.007 -0.003 SW1 -0.012 -0.003 -0.008 0.002 0.007 -0.009 0.000 -0.006 0.000 0.002 SW2 -0.003 -0.007 -0.006 0.001 0.003 0.010 0.011 0.002 0.002 0.000 SW3 -0.002 0.003 -0.005 0.000 0.001 0.002 0.007 -0.001 0.003 0.001 SM1 0.001 -0.002 0.001 0.001 0.010 -0.009 0.002 -0.001 0.000 -0.001 SM2 0.005 -0.004 0.013 0.001 0.003 -0.001 0.004 0.002 0.005 -0.002 SM3 0.009 0.004 0.005 0.006 0.006 0.004 0.005 0.002 0.002 -0.001 QO1 0.000 0.001 0.004 0.002 -0.001 0.002 -0.001 0.008 0.001 -0.004 QO2 -0.011 -0.012 -0.002 -0.011 0.000 0.007 -0.003 0.000 0.007 -0.004 QO3 -0.010 -0.005 -0.001 -0.010 -0.015 -0.008 -0.007 -0.004 -0.003 -0.004 QW1 0.003 0.003 0.008 0.004 -0.005 0.010 0.004 -0.001 0.009 0.007 QW2 -0.010 -0.010 -0.002 0.002 0.003 0.005 0.007 0.002 0.012 0.002 QW3 -0.015 -0.009 -0.006 -0.006 -0.007 0.001 -0.005 -0.004 -0.005 -0.003 QM1 -0.011 0.000 0.009 -0.007 -0.002 0.008 0.005 -0.004 -0.002 0.001 QM2 -0.006 -0.005 0.009 -0.003 0.000 0.009 0.001 -0.001 -0.013 -0.003 QM3 -0.001 0.001 0.012 0.014 0.003 0.015 0.007 0.011 0.006 -0.002 SO2 SO3 SW1 SW2 SW3 SM1 SM2 SM3 QO1 QO2 VO1 VO2 VO3 VW1 VW2 VW3 VM1 VM2 VM3 SO1 SO2 0.000 SO3 0.001 0.000 SW1 0.009 -0.013 0.000 SW2 -0.004 -0.007 0.000 0.000 SW3 0.006 -0.004 -0.001 0.004 0.000 SM1 -0.001 -0.003 0.003 0.005 0.000 0.000 SM2 -0.006 0.002 -0.003 0.000 0.007 0.000 0.000 SM3 -0.002 -0.001 0.001 0.006 0.002 0.000 0.002 0.000 QO1 -0.003 -0.010 -0.013 -0.008 -0.002 -0.007 -0.006 0.000 0.000 QO2 -0.006 0.000 -0.016 -0.002 -0.008 -0.005 -0.002 0.000 0.000 0.000 QO3 -0.009 -0.012 -0.016 -0.008 -0.009 -0.009 -0.009 -0.002 -0.002 -0.001 QW1 0.007 0.000 0.000 0.004 0.000 0.002 -0.010 0.002 0.009 -0.005 QW2 0.004 -0.010 -0.004 0.012 0.005 0.017 0.003 0.010 -0.012 -0.008 QW3 0.005 -0.009 -0.005 0.001 0.006 -0.001 -0.009 -0.002 -0.008 -0.007 QM1 -0.003 0.008 -0.006 0.005 0.010 0.010 -0.008 0.003 -0.005 -0.008 QM2 0.000 -0.005 0.004 -0.003 0.005 0.003 -0.004 0.003 0.004 0.006 QM3 -0.005 -0.002 0.000 0.011 0.009 -0.001 0.001 0.002 -0.008 0.001 QO3 QW1 QW2 QW3 QM1 QM2 QM3 VO1 VO2 VO3 VW1 VW2 VW3 VM1 VM2 VM3 SO1 SO2 SO3 SW1 SW2 SW3 SM1 SM2 SM3 QO1 QO2 QO3 0.000 QW1 0.003 0.000 QW2 -0.007 -0.003 0.000 QW3 -0.001 0.000 -0.001 0.000 QM1 -0.010 0.004 -0.001 0.002 0.000 QM2 -0.004 0.000 0.001 0.001 0.000 0.000 QM3 -0.005 -0.004 0.007 0.003 0.006 -0.006 0.000 $mean VO1 VO2 VO3 VW1 VW2 VW3 VM1 VM2 VM3 SO1 SO2 SO3 SW1 SW2 SW3 SM1 SM2 SM3 QO1 QO2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 QO3 QW1 QW2 QW3 QM1 QM2 QM3 0 0 0 0 0 0 0 14.4.2.13.8 Modification indices Code modificationindices(cfaModelMTMMFit, sort. = TRUE) 14.4.2.13.9 Internal Consistency Reliability Internal consistency reliability of items composing the latent factors, as quantified by omega (\\(\\omega\\)) and average variance extracted (AVE), was estimated using the semTools package (Jorgensen et al., 2021). Code compRelSEM(cfaModelMTMMFit) Verbal Spatial Quant Oral Written Manipulative 0.775 0.777 0.767 0.332 0.404 0.360 Code compRelSEM(cfaModelMTMMFit, higher = &quot;g&quot;) Verbal Spatial Quant Oral Written Manipulative 0.775 0.777 0.767 0.332 0.404 0.360 g 0.646 Code AVE(cfaModelMTMMFit) Verbal Spatial Quant Oral Written Manipulative NA NA NA NA NA NA 14.4.2.13.10 Path diagram A path diagram of the model generated using the semPlot package (Epskamp, 2022) is in Figure 14.79. Code semPaths( cfaModelMTMMFit, what = &quot;std&quot;, layout = &quot;tree3&quot;, bifactor = c(&quot;Verbal&quot;,&quot;Spatial&quot;,&quot;Quant&quot;,&quot;g&quot;), edge.label.cex = 1.3) Figure 14.79: Multitrait-Multimethod Model in Confirmatory Factor Analysis. 14.4.2.13.11 Equivalently fitting models Markov equivalent directed acyclic graphs (DAGs) were depicted using the dagitty package (Textor et al., 2021). Path diagrams of equivalently fitting models are below. Code dagModelMTMM &lt;- lavaanToGraph(cfaModelMTMMFit) par(mfrow = c(2, 2)) Code lapply(equivalentDAGs( dagModelMTMM, n = 4), plot) Figure 14.80: Equivalently Fitting Models to a Multitrait-Multimethod Model in Confirmatory Factor Analysis. Figure 14.81: Equivalently Fitting Models to a Multitrait-Multimethod Model in Confirmatory Factor Analysis. Figure 14.82: Equivalently Fitting Models to a Multitrait-Multimethod Model in Confirmatory Factor Analysis. Figure 14.83: Equivalently Fitting Models to a Multitrait-Multimethod Model in Confirmatory Factor Analysis. [[1]] [[1]]$mar [1] 0 0 0 0 [[2]] [[2]]$mar [1] 0 0 0 0 [[3]] [[3]]$mar [1] 0 0 0 0 [[4]] [[4]]$mar [1] 0 0 0 0 14.4.3 Exploratory Structural Equation Model (ESEM) 14.4.3.1 Example 1 14.4.3.1.1 Specify the model To specify an exploratory structural equation model (ESEM), you use the factor loadings from an EFA model as the starting values in a structural equation model. In this example, I use the factor loadings from the three-factor EFA model, as specified in Section 14.4.1.2.2.2. The factor loadings are in Table ??. Code esem_loadings &lt;- parameterEstimates( efa3factorObliqueLavaan_fit, standardized = TRUE ) %&gt;% dplyr::filter(efa == &quot;efa1&quot;) %&gt;% dplyr::select(lhs, rhs, est) %&gt;% dplyr::rename(item = rhs, latent = lhs, loading = est) esem_loadings In ESEM, you specify one anchor item for each latent factor. You want the anchor item to be an item that has a strong loading on one latent factor and to have weak loadings on every other latent factor. The cross-loadings for the anchor item are fixed to the respective loadings from the original EFA model. I specify an anchor item for each latent factor below: Code anchors &lt;- c(f1 = &quot;x3&quot;, f2 = &quot;x5&quot;, f3 = &quot;x7&quot;) The petersenlab package (Petersen, 2024b) includes the make_esem_model() function that creates lavaan syntax for an ESEM model from the factor loadings of an EFA model, as adapted from Mateus Silvestrin: https://msilvestrin.me/post/esem (archived at https://perma.cc/FA8D-4NB9) Code esemModel_syntax &lt;- make_esem_model( esem_loadings, anchors) writeLines(esemModel_syntax) f1 =~ start(0.731519508972419)*x1+start(0.583748282093231)*x2+start(0.847715844516999)*x3+start(0.00612247333365327)*x4+-0.00498244648290284*x5+start(0.0966229100325138)*x6+-0.00106781946960533*x7+start(0.283600589925253)*x8+start(0.496276958522452)*x9 f2 =~ start(0.139474774414072)*x1+start(-0.00800942699525699)*x2+-0.145067356090553*x3+start(0.990877218971296)*x4+start(1.12736459930128)*x5+start(0.804382015220083)*x6+0.0602437609661441*x7+start(-0.00753295801747841)*x8+start(0.00782352417177395)*x9 f3 =~ start(-0.0429741005345117)*x1+start(-0.179604528416622)*x2+0.00636249155609383*x3+start(0.00736980272118693)*x4+0.0341008173386781*x5+start(-0.00133134520538106)*x6+start(0.75570572411579)*x7+start(0.638194651454281)*x8+start(0.432390907984239)*x9 14.4.3.1.2 Model syntax in table form: Code lavaanify(esemModel_syntax) 14.4.3.1.3 Fit the model Code esemModelFit &lt;- sem( esemModel_syntax, data = HolzingerSwineford1939, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) 14.4.3.1.4 Display summary output Code summary( esemModelFit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 92 iterations Estimator ML Optimization method NLMINB Number of model parameters 42 Number of observations 301 Number of missing patterns 75 Model Test User Model: Standard Scaled Test Statistic 23.051 23.724 Degrees of freedom 12 12 P-value (Chi-square) 0.027 0.022 Scaling correction factor 0.972 Yuan-Bentler correction (Mplus variant) Model Test Baseline Model: Test statistic 693.305 646.574 Degrees of freedom 36 36 P-value 0.000 0.000 Scaling correction factor 1.072 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.983 0.981 Tucker-Lewis Index (TLI) 0.950 0.942 Robust Comparative Fit Index (CFI) 0.980 Robust Tucker-Lewis Index (TLI) 0.939 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -3192.304 -3192.304 Scaling correction factor 1.090 for the MLR correction Loglikelihood unrestricted model (H1) -3180.778 -3180.778 Scaling correction factor 1.064 for the MLR correction Akaike (AIC) 6468.608 6468.608 Bayesian (BIC) 6624.307 6624.307 Sample-size adjusted Bayesian (SABIC) 6491.107 6491.107 Root Mean Square Error of Approximation: RMSEA 0.055 0.057 90 Percent confidence interval - lower 0.018 0.020 90 Percent confidence interval - upper 0.089 0.091 P-value H_0: RMSEA &lt;= 0.050 0.358 0.329 P-value H_0: RMSEA &gt;= 0.080 0.124 0.144 Robust RMSEA 0.070 90 Percent confidence interval - lower 0.017 90 Percent confidence interval - upper 0.115 P-value H_0: Robust RMSEA &lt;= 0.050 0.215 P-value H_0: Robust RMSEA &gt;= 0.080 0.393 Standardized Root Mean Square Residual: SRMR 0.020 0.020 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 =~ x1 0.732 0.732 0.632 x2 0.584 0.131 4.442 0.000 0.584 0.505 x3 0.848 0.182 4.661 0.000 0.848 0.753 x4 0.006 0.105 0.058 0.954 0.006 0.005 x5 -0.005 -0.005 -0.004 x6 0.097 0.087 1.108 0.268 0.097 0.093 x7 -0.001 -0.001 -0.001 x8 0.284 0.112 2.539 0.011 0.284 0.280 x9 0.496 0.123 4.023 0.000 0.496 0.487 f2 =~ x1 0.139 0.139 0.120 x2 -0.008 0.098 -0.082 0.935 -0.008 -0.007 x3 -0.145 -0.145 -0.129 x4 0.991 0.401 2.472 0.013 0.991 0.843 x5 1.127 0.442 2.549 0.011 1.127 0.869 x6 0.804 0.333 2.414 0.016 0.804 0.774 x7 0.060 0.060 0.056 x8 -0.008 0.090 -0.083 0.934 -0.008 -0.007 x9 0.008 0.075 0.104 0.917 0.008 0.008 f3 =~ x1 -0.043 -0.043 -0.037 x2 -0.180 0.325 -0.553 0.580 -0.180 -0.155 x3 0.006 0.006 0.006 x4 0.007 0.117 0.063 0.950 0.007 0.006 x5 0.034 0.034 0.026 x6 -0.001 0.090 -0.015 0.988 -0.001 -0.001 x7 0.756 1.429 0.529 0.597 0.756 0.708 x8 0.638 1.337 0.477 0.633 0.638 0.630 x9 0.432 0.884 0.489 0.625 0.432 0.424 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 ~~ f2 0.396 0.178 2.228 0.026 0.396 0.396 f3 0.112 0.261 0.432 0.666 0.112 0.112 f2 ~~ f3 0.107 0.164 0.654 0.513 0.107 0.107 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 4.974 0.071 69.866 0.000 4.974 4.297 .x2 6.047 0.070 86.533 0.000 6.047 5.230 .x3 2.221 0.069 32.224 0.000 2.221 1.971 .x4 3.095 0.071 43.794 0.000 3.095 2.634 .x5 4.341 0.078 55.604 0.000 4.341 3.347 .x6 2.188 0.064 34.270 0.000 2.188 2.104 .x7 4.177 0.064 64.895 0.000 4.177 3.914 .x8 5.520 0.062 89.727 0.000 5.520 5.449 .x9 5.401 0.063 86.038 0.000 5.401 5.297 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 0.711 0.121 5.855 0.000 0.711 0.531 .x2 0.991 0.119 8.325 0.000 0.991 0.741 .x3 0.626 0.124 5.040 0.000 0.626 0.493 .x4 0.393 0.073 5.413 0.000 0.393 0.284 .x5 0.406 0.084 4.836 0.000 0.406 0.241 .x6 0.364 0.054 6.681 0.000 0.364 0.336 .x7 0.555 0.174 3.187 0.001 0.555 0.487 .x8 0.501 0.117 4.291 0.000 0.501 0.488 .x9 0.554 0.073 7.638 0.000 0.554 0.533 f1 1.000 0.294 3.402 0.001 1.000 1.000 f2 1.000 0.796 1.256 0.209 1.000 1.000 f3 1.000 3.947 0.253 0.800 1.000 1.000 R-Square: Estimate x1 0.469 x2 0.259 x3 0.507 x4 0.716 x5 0.759 x6 0.664 x7 0.513 x8 0.512 x9 0.467 14.4.3.1.5 Estimates of model fit Code fitMeasures( esemModelFit, fit.measures = c( &quot;chisq&quot;, &quot;df&quot;, &quot;pvalue&quot;, &quot;chisq.scaled&quot;, &quot;df.scaled&quot;, &quot;pvalue.scaled&quot;, &quot;chisq.scaling.factor&quot;, &quot;baseline.chisq&quot;,&quot;baseline.df&quot;,&quot;baseline.pvalue&quot;, &quot;rmsea&quot;, &quot;cfi&quot;, &quot;tli&quot;, &quot;srmr&quot;, &quot;rmsea.robust&quot;, &quot;cfi.robust&quot;, &quot;tli.robust&quot;)) chisq df pvalue 23.051 12.000 0.027 chisq.scaled df.scaled pvalue.scaled 23.724 12.000 0.022 chisq.scaling.factor baseline.chisq baseline.df 0.972 693.305 36.000 baseline.pvalue rmsea cfi 0.000 0.055 0.983 tli srmr rmsea.robust 0.950 0.020 0.070 cfi.robust tli.robust 0.980 0.939 14.4.3.1.6 Nested model comparison A nested model comparison indicates that the ESEM model fits significantly better than the CFA model. Code anova(cfaModelFit, esemModelFit) 14.4.3.1.7 Residuals Code residuals(esemModelFit, type = &quot;cor&quot;) $type [1] &quot;cor.bollen&quot; $cov x1 x2 x3 x4 x5 x6 x7 x8 x9 x1 0.000 x2 -0.008 0.000 x3 0.001 -0.004 0.000 x4 0.040 -0.022 -0.001 0.000 x5 -0.019 0.013 -0.031 -0.004 0.000 x6 -0.034 0.015 0.024 -0.002 0.005 0.000 x7 -0.034 -0.037 0.027 0.035 -0.043 0.001 0.000 x8 0.027 0.029 -0.041 -0.044 0.016 0.016 -0.007 0.000 x9 -0.010 0.013 0.014 -0.033 0.042 -0.024 -0.009 -0.002 0.000 $mean x1 x2 x3 x4 x5 x6 x7 x8 x9 0.003 -0.001 0.003 -0.003 -0.001 0.005 0.003 -0.001 0.003 14.4.3.1.8 Modification indices Code modificationindices(esemModelFit, sort. = TRUE) 14.4.3.1.9 Internal Consistency Reliability Internal consistency reliability of items composing the latent factors, as quantified by omega (\\(\\omega\\)) and average variance extracted (AVE), was estimated using the semTools package (Jorgensen et al., 2021). Code compRelSEM(esemModelFit) f1 f2 f3 0.264 0.252 0.078 Code AVE(esemModelFit) f1 f2 f3 NA NA NA 14.4.3.1.10 Path diagram A path diagram of the model generated using the semPlot package (Epskamp, 2022) is in Figure 14.84. Code semPaths( esemModelFit, what = &quot;std&quot;, layout = &quot;tree3&quot;, edge.label.cex = 1.3) Figure 14.84: Exploratory Structural Equation Model With All Cross-loadings. 14.4.3.1.11 Equivalently fitting models Markov equivalent directed acyclic graphs (DAGs) were depicted using the dagitty package (Textor et al., 2021). Path diagrams of equivalent models are below. Code dagModelESEM &lt;- lavaanToGraph(esemModelFit) par(mfrow = c(2, 2)) Code lapply(equivalentDAGs( dagModelESEM, n = 4), plot) Figure 14.85: Equivalently Fitting Models to an Exploratory Structural Equation Model. Figure 14.86: Equivalently Fitting Models to an Exploratory Structural Equation Model. Figure 14.87: Equivalently Fitting Models to an Exploratory Structural Equation Model. Figure 14.88: Equivalently Fitting Models to an Exploratory Structural Equation Model. [[1]] [[1]]$mar [1] 0 0 0 0 [[2]] [[2]]$mar [1] 0 0 0 0 [[3]] [[3]]$mar [1] 0 0 0 0 [[4]] [[4]]$mar [1] 0 0 0 0 14.4.3.2 Example 2 14.4.3.2.1 Specify the model Another way to fit a ESEM model in lavaan is to fit exploratory factors, confirmatory factors, and structural components in the same model. For instance, below is the syntax for a model with exploratory factors, confirmatory factors, and structural components: Code esem_syntax &lt;- &#39; # exploratory factors: EFA Block 1 efa(&quot;efa1&quot;)*f1 + efa(&quot;efa1&quot;)*f2 =~ x1 + x2 + x3 + x4 + x5 + x6 # confirmatory factors speed =~ x7 + x8 + x9 # regression paths speed ~ f1 + f2 &#39; 14.4.3.2.2 Model syntax in table form: Code lavaanify(esem_syntax) 14.4.3.2.3 Fit the model Code esem_fit &lt;- sem( esem_syntax, data = HolzingerSwineford1939, rotation = &quot;geomin&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) 14.4.3.2.4 Display summary output Code summary( esem_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 50 iterations Estimator ML Optimization method NLMINB Number of model parameters 34 Rotation method GEOMIN OBLIQUE Geomin epsilon 0.001 Rotation algorithm (rstarts) GPA (30) Standardized metric TRUE Row weights None Number of observations 301 Number of missing patterns 75 Model Test User Model: Standard Scaled Test Statistic 55.969 57.147 Degrees of freedom 20 20 P-value (Chi-square) 0.000 0.000 Scaling correction factor 0.979 Yuan-Bentler correction (Mplus variant) Model Test Baseline Model: Test statistic 693.305 646.574 Degrees of freedom 36 36 P-value 0.000 0.000 Scaling correction factor 1.072 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.945 0.939 Tucker-Lewis Index (TLI) 0.902 0.890 Robust Comparative Fit Index (CFI) 0.940 Robust Tucker-Lewis Index (TLI) 0.892 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -3208.763 -3208.763 Scaling correction factor 1.114 for the MLR correction Loglikelihood unrestricted model (H1) -3180.778 -3180.778 Scaling correction factor 1.064 for the MLR correction Akaike (AIC) 6485.526 6485.526 Bayesian (BIC) 6611.567 6611.567 Sample-size adjusted Bayesian (SABIC) 6503.739 6503.739 Root Mean Square Error of Approximation: RMSEA 0.077 0.079 90 Percent confidence interval - lower 0.054 0.055 90 Percent confidence interval - upper 0.102 0.103 P-value H_0: RMSEA &lt;= 0.050 0.030 0.025 P-value H_0: RMSEA &gt;= 0.080 0.453 0.488 Robust RMSEA 0.093 90 Percent confidence interval - lower 0.064 90 Percent confidence interval - upper 0.124 P-value H_0: Robust RMSEA &lt;= 0.050 0.011 P-value H_0: Robust RMSEA &gt;= 0.080 0.785 Standardized Root Mean Square Residual: SRMR 0.055 0.055 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 =~ efa1 x1 0.643 0.142 4.524 0.000 0.643 0.557 x2 0.488 0.115 4.258 0.000 0.488 0.423 x3 0.878 0.108 8.121 0.000 0.878 0.780 x4 0.004 0.020 0.186 0.853 0.004 0.003 x5 -0.053 0.078 -0.688 0.491 -0.053 -0.041 x6 0.088 0.072 1.217 0.224 0.088 0.084 f2 =~ efa1 x1 0.264 0.118 2.237 0.025 0.264 0.229 x2 0.081 0.108 0.753 0.451 0.081 0.070 x3 -0.031 0.031 -0.991 0.322 -0.031 -0.028 x4 0.985 0.070 13.992 0.000 0.985 0.839 x5 1.153 0.074 15.501 0.000 1.153 0.889 x6 0.820 0.063 12.997 0.000 0.820 0.788 speed =~ x7 1.000 0.592 0.553 x8 1.224 0.152 8.044 0.000 0.724 0.715 x9 1.191 0.303 3.931 0.000 0.705 0.691 Regressions: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all speed ~ f1 0.235 0.055 4.240 0.000 0.396 0.396 f2 0.125 0.057 2.194 0.028 0.211 0.211 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all f1 ~~ f2 0.268 0.105 2.557 0.011 0.268 0.268 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 4.974 0.071 70.040 0.000 4.974 4.306 .x2 6.050 0.070 86.003 0.000 6.050 5.234 .x3 2.225 0.069 32.271 0.000 2.225 1.976 .x4 3.095 0.071 43.809 0.000 3.095 2.634 .x5 4.338 0.078 55.560 0.000 4.338 3.343 .x6 2.188 0.064 34.263 0.000 2.188 2.104 .x7 4.180 0.065 64.460 0.000 4.180 3.905 .x8 5.517 0.062 89.572 0.000 5.517 5.449 .x9 5.405 0.063 86.055 0.000 5.405 5.297 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 0.760 0.143 5.312 0.000 0.760 0.569 .x2 1.069 0.122 8.760 0.000 1.069 0.801 .x3 0.511 0.190 2.697 0.007 0.511 0.403 .x4 0.408 0.068 5.987 0.000 0.408 0.295 .x5 0.384 0.081 4.740 0.000 0.384 0.228 .x6 0.363 0.054 6.687 0.000 0.363 0.336 .x7 0.796 0.100 7.972 0.000 0.796 0.695 .x8 0.501 0.136 3.670 0.000 0.501 0.488 .x9 0.545 0.131 4.161 0.000 0.545 0.523 f1 1.000 1.000 1.000 f2 1.000 1.000 1.000 .speed 0.264 0.105 2.513 0.012 0.754 0.754 R-Square: Estimate x1 0.431 x2 0.199 x3 0.597 x4 0.705 x5 0.772 x6 0.664 x7 0.305 x8 0.512 x9 0.477 speed 0.246 14.4.3.2.5 Estimates of model fit Code fitMeasures( esem_fit, fit.measures = c( &quot;chisq&quot;, &quot;df&quot;, &quot;pvalue&quot;, &quot;chisq.scaled&quot;, &quot;df.scaled&quot;, &quot;pvalue.scaled&quot;, &quot;chisq.scaling.factor&quot;, &quot;baseline.chisq&quot;,&quot;baseline.df&quot;,&quot;baseline.pvalue&quot;, &quot;rmsea&quot;, &quot;cfi&quot;, &quot;tli&quot;, &quot;srmr&quot;, &quot;rmsea.robust&quot;, &quot;cfi.robust&quot;, &quot;tli.robust&quot;)) chisq df pvalue 55.969 20.000 0.000 chisq.scaled df.scaled pvalue.scaled 57.147 20.000 0.000 chisq.scaling.factor baseline.chisq baseline.df 0.979 693.305 36.000 baseline.pvalue rmsea cfi 0.000 0.077 0.945 tli srmr rmsea.robust 0.902 0.055 0.093 cfi.robust tli.robust 0.940 0.892 14.4.3.2.6 Residuals Code residuals(esem_fit, type = &quot;cor&quot;) $type [1] &quot;cor.bollen&quot; $cov x1 x2 x3 x4 x5 x6 x7 x8 x9 x1 0.000 x2 0.036 0.000 x3 -0.009 -0.001 0.000 x4 0.034 -0.027 -0.009 0.000 x5 -0.012 0.017 -0.013 -0.004 0.000 x6 -0.038 0.015 0.013 0.003 0.002 0.000 x7 -0.160 -0.216 -0.100 0.004 -0.055 -0.048 0.000 x8 0.011 -0.052 -0.044 -0.085 -0.007 -0.026 0.075 0.000 x9 0.118 0.060 0.152 -0.004 0.085 0.017 -0.037 -0.045 0.000 $mean x1 x2 x3 x4 x5 x6 x7 x8 x9 0.003 -0.003 -0.001 -0.002 0.001 0.004 0.000 0.002 0.000 14.4.3.2.7 Modification indices Code modificationindices(esem_fit, sort. = TRUE) 14.4.3.2.8 Internal Consistency Reliability Internal consistency reliability of items composing the latent factors, as quantified by omega (\\(\\omega\\)) and average variance extracted (AVE), was estimated using the semTools package (Jorgensen et al., 2021). Code compRelSEM(esem_fit) f1 f2 speed 0.191 0.487 0.692 Code AVE(esem_fit) f1 f2 speed NA NA 0.428 14.4.3.2.9 Path diagram A path diagram of the model generated using the semPlot package (Epskamp, 2022) is in Figure 14.89. Code semPaths( esem_fit, what = &quot;std&quot;, layout = &quot;tree3&quot;, edge.label.cex = 1.3) Figure 14.89: Exploratory Structural Equation Model With Exploratory Factors, Confirmatory Factors, and Regression Paths. 14.4.3.2.10 Equivalently fitting models Markov equivalent directed acyclic graphs (DAGs) were depicted using the dagitty package (Textor et al., 2021). Path diagrams of equivalent models are below. Code dagModelESEM2 &lt;- lavaanToGraph(esem_fit) par(mfrow = c(2, 2)) Code lapply(equivalentDAGs( dagModelESEM2, n = 4), plot) Figure 14.90: Equivalently Fitting Models to an Exploratory Structural Equation Model. Figure 14.91: Equivalently Fitting Models to an Exploratory Structural Equation Model. Figure 14.92: Equivalently Fitting Models to an Exploratory Structural Equation Model. Figure 14.93: Equivalently Fitting Models to an Exploratory Structural Equation Model. [[1]] [[1]]$mar [1] 0 0 0 0 [[2]] [[2]]$mar [1] 0 0 0 0 [[3]] [[3]]$mar [1] 0 0 0 0 [[4]] [[4]]$mar [1] 0 0 0 0 14.5 Principal Component Analysis (PCA) Principal component analysis (PCA) is a technique for data reduction. PCA seeks to identify the principal components of the data, so that data can be reduced to that smaller set of components. That is, PCA seeks to identify the smallest set of components that explain the most variance in the variables. Although PCA is sometimes referred to as factor analysis, PCA is not factor analysis (Lilienfeld et al., 2015). PCA uses the total variance of all variables, whereas factor analysis uses the common variance among the variables. 14.5.1 Determine number of components Determine the number of components to retain using the Scree plot and Very Simple Structure (VSS) plot. 14.5.1.1 Scree Plot Scree plots were generated using the psych (Revelle, 2022) and nFactors (Raiche &amp; Magis, 2020) packages. Note that eigenvalues of factors in PCA models are higher than factors from EFA models, because PCA components include common variance among the indicators in addition to error variance, whereas EFA factors include only the common variance among the indicators. A scree plot based on parallel analysis is in Figure 14.94. The number of components to keep would depend on which criteria one uses. Based on the rule to keep components whose eigenvalues are greater than 1 and based on the parallel test, we would keep three factors. However, based on the Cattell scree test (as operationalized by the optimal coordinates and acceleration factor), we would keep one or three factors. Therefore, interpretability of the factors would be important for deciding between whether to keep one, two, or three factors. Code fa.parallel( x = HolzingerSwineford1939[,vars], fm = &quot;ml&quot;, fa = &quot;pc&quot;) Figure 14.94: Scree Plot Based on Parallel Analysis in Principal Component Analysis. Parallel analysis suggests that the number of factors = NA and the number of components = 3 A scree plot is in Figure 14.95. Code plot(nScree( x = cor( HolzingerSwineford1939[,vars], use = &quot;pairwise.complete.obs&quot;), model = &quot;components&quot;)) Figure 14.95: Scree Plot in Principal Component Analysis. 14.5.1.2 Very Simple Structure (VSS) Plot Very simple structure (VSS) plots were generated using the psych package (Revelle, 2022). 14.5.1.2.1 Orthogonal (Varimax) rotation In the example with orthogonal rotation below, the very simple structure (VSS) criterion is highest with three or four components. A VSS plot of PCA with orthogonal rotation is in Figure 14.96. Code vss( HolzingerSwineford1939[,vars], rotate = &quot;varimax&quot;, fm = &quot;pc&quot;) Figure 14.96: Very Simple Structure Plot With Orthogonal Rotation in Principal Component Analysis. Very Simple Structure Call: vss(x = HolzingerSwineford1939[, vars], rotate = &quot;varimax&quot;, fm = &quot;pc&quot;) VSS complexity 1 achieves a maximimum of 0.75 with 4 factors VSS complexity 2 achieves a maximimum of 0.91 with 4 factors The Velicer MAP achieves a minimum of 0.07 with 2 factors BIC achieves a minimum of Inf with factors Sample Size adjusted BIC achieves a minimum of Inf with factors Statistics by number of factors vss1 vss2 map dof chisq prob sqresid fit RMSEA BIC SABIC complex eChisq 1 0.64 0.00 0.078 0 NA NA 5.853 0.64 NA NA NA NA NA 2 0.69 0.80 0.067 0 NA NA 3.266 0.80 NA NA NA NA NA 3 0.74 0.88 0.071 0 NA NA 1.434 0.91 NA NA NA NA NA 4 0.75 0.91 0.125 0 NA NA 0.954 0.94 NA NA NA NA NA 5 0.71 0.87 0.199 0 NA NA 0.643 0.96 NA NA NA NA NA 6 0.71 0.85 0.403 0 NA NA 0.356 0.98 NA NA NA NA NA 7 0.71 0.84 0.447 0 NA NA 0.147 0.99 NA NA NA NA NA 8 0.59 0.82 1.000 0 NA NA 0.052 1.00 NA NA NA NA NA SRMR eCRMS eBIC 1 NA NA NA 2 NA NA NA 3 NA NA NA 4 NA NA NA 5 NA NA NA 6 NA NA NA 7 NA NA NA 8 NA NA NA 14.5.1.2.2 Oblique (Oblimin) rotation In the example with orthogonal rotation below, the VSS criterion is highest with 2 components. A VSS plot of PCA with oblique rotation is in Figure 14.97. Code vss( HolzingerSwineford1939[,vars], rotate = &quot;oblimin&quot;, fm = &quot;pc&quot;) Figure 14.97: Very Simple Structure Plot With Oblique Rotation in Principal Component Analysis. Very Simple Structure Call: vss(x = HolzingerSwineford1939[, vars], rotate = &quot;oblimin&quot;, fm = &quot;pc&quot;) VSS complexity 1 achieves a maximimum of 0.98 with 5 factors VSS complexity 2 achieves a maximimum of 0.99 with 5 factors The Velicer MAP achieves a minimum of 0.07 with 2 factors BIC achieves a minimum of Inf with factors Sample Size adjusted BIC achieves a minimum of Inf with factors Statistics by number of factors vss1 vss2 map dof chisq prob sqresid fit RMSEA BIC SABIC complex eChisq 1 0.64 0.00 0.078 0 NA NA 5.853 0.64 NA NA NA NA NA 2 0.75 0.80 0.067 0 NA NA 3.266 0.80 NA NA NA NA NA 3 0.84 0.91 0.071 0 NA NA 1.434 0.91 NA NA NA NA NA 4 0.88 0.94 0.125 0 NA NA 0.954 0.94 NA NA NA NA NA 5 0.88 0.95 0.199 0 NA NA 0.643 0.96 NA NA NA NA NA 6 0.87 0.88 0.403 0 NA NA 0.356 0.98 NA NA NA NA NA 7 0.97 0.98 0.447 0 NA NA 0.147 0.99 NA NA NA NA NA 8 0.98 0.99 1.000 0 NA NA 0.052 1.00 NA NA NA NA NA SRMR eCRMS eBIC 1 NA NA NA 2 NA NA NA 3 NA NA NA 4 NA NA NA 5 NA NA NA 6 NA NA NA 7 NA NA NA 8 NA NA NA 14.5.1.2.3 No rotation In the example with no rotation below, the VSS criterion is highest with 3 or 4 components. A VSS plot of PCA with no rotation is in Figure 14.98. Code vss( HolzingerSwineford1939[,vars], rotate = &quot;none&quot;, fm = &quot;pc&quot;) Figure 14.98: Very Simple Structure Plot With No Rotation in Principal Component Analysis. Very Simple Structure Call: vss(x = HolzingerSwineford1939[, vars], rotate = &quot;none&quot;, fm = &quot;pc&quot;) VSS complexity 1 achieves a maximimum of 0.75 with 4 factors VSS complexity 2 achieves a maximimum of 0.91 with 4 factors The Velicer MAP achieves a minimum of 0.07 with 2 factors BIC achieves a minimum of Inf with factors Sample Size adjusted BIC achieves a minimum of Inf with factors Statistics by number of factors vss1 vss2 map dof chisq prob sqresid fit RMSEA BIC SABIC complex eChisq 1 0.64 0.00 0.078 0 NA NA 5.853 0.64 NA NA NA NA NA 2 0.69 0.80 0.067 0 NA NA 3.266 0.80 NA NA NA NA NA 3 0.74 0.88 0.071 0 NA NA 1.434 0.91 NA NA NA NA NA 4 0.75 0.91 0.125 0 NA NA 0.954 0.94 NA NA NA NA NA 5 0.71 0.87 0.199 0 NA NA 0.643 0.96 NA NA NA NA NA 6 0.71 0.85 0.403 0 NA NA 0.356 0.98 NA NA NA NA NA 7 0.71 0.84 0.447 0 NA NA 0.147 0.99 NA NA NA NA NA 8 0.59 0.82 1.000 0 NA NA 0.052 1.00 NA NA NA NA NA SRMR eCRMS eBIC 1 NA NA NA 2 NA NA NA 3 NA NA NA 4 NA NA NA 5 NA NA NA 6 NA NA NA 7 NA NA NA 8 NA NA NA 14.5.2 Run the Principal Component Analysis Principal component analysis (PCA) models were fit using the psych package (Revelle, 2022). 14.5.2.1 Orthogonal (Varimax) rotation Code pca1factorOrthogonal &lt;- principal( HolzingerSwineford1939[,vars], nfactors = 1, rotate = &quot;varimax&quot;) pca2factorOrthogonal &lt;- principal( HolzingerSwineford1939[,vars], nfactors = 2, rotate = &quot;varimax&quot;) pca3factorOrthogonal &lt;- principal( HolzingerSwineford1939[,vars], nfactors = 3, rotate = &quot;varimax&quot;) pca4factorOrthogonal &lt;- principal( HolzingerSwineford1939[,vars], nfactors = 4, rotate = &quot;varimax&quot;) pca5factorOrthogonal &lt;- principal( HolzingerSwineford1939[,vars], nfactors = 5, rotate = &quot;varimax&quot;) pca6factorOrthogonal &lt;- principal( HolzingerSwineford1939[,vars], nfactors = 6, rotate = &quot;varimax&quot;) pca7factorOrthogonal &lt;- principal( HolzingerSwineford1939[,vars], nfactors = 7, rotate = &quot;varimax&quot;) pca8factorOrthogonal &lt;- principal( HolzingerSwineford1939[,vars], nfactors = 8, rotate = &quot;varimax&quot;) pca9factorOrthogonal &lt;- principal( HolzingerSwineford1939[,vars], nfactors = 9, rotate = &quot;varimax&quot;) 14.5.2.2 Oblique (Oblimin) rotation Code pca1factorOblique &lt;- principal( HolzingerSwineford1939[,vars], nfactors = 1, rotate = &quot;oblimin&quot;) pca2factorOblique &lt;- principal( HolzingerSwineford1939[,vars], nfactors = 2, rotate = &quot;oblimin&quot;) pca3factorOblique &lt;- principal( HolzingerSwineford1939[,vars], nfactors = 3, rotate = &quot;oblimin&quot;) pca4factorOblique &lt;- principal( HolzingerSwineford1939[,vars], nfactors = 4, rotate = &quot;oblimin&quot;) pca5factorOblique &lt;- principal( HolzingerSwineford1939[,vars], nfactors = 5, rotate = &quot;oblimin&quot;) pca6factorOblique &lt;- principal( HolzingerSwineford1939[,vars], nfactors = 6, rotate = &quot;oblimin&quot;) pca7factorOblique &lt;- principal( HolzingerSwineford1939[,vars], nfactors = 7, rotate = &quot;oblimin&quot;) pca8factorOblique &lt;- principal( HolzingerSwineford1939[,vars], nfactors = 8, rotate = &quot;oblimin&quot;) pca9factorOblique &lt;- principal( HolzingerSwineford1939[,vars], nfactors = 9, rotate = &quot;oblimin&quot;) 14.5.3 Component Loadings 14.5.3.1 Orthogonal (Varimax) rotation Code pca1factorOrthogonal Principal Components Analysis Call: principal(r = HolzingerSwineford1939[, vars], nfactors = 1, rotate = &quot;varimax&quot;) Standardized loadings (pattern matrix) based upon correlation matrix PC1 h2 u2 com x1 0.66 0.43 0.57 1 x2 0.40 0.16 0.84 1 x3 0.53 0.28 0.72 1 x4 0.75 0.56 0.44 1 x5 0.76 0.57 0.43 1 x6 0.73 0.53 0.47 1 x7 0.33 0.11 0.89 1 x8 0.51 0.26 0.74 1 x9 0.59 0.35 0.65 1 PC1 SS loadings 3.26 Proportion Var 0.36 Mean item complexity = 1 Test of the hypothesis that 1 component is sufficient. The root mean square of the residuals (RMSR) is 0.16 with the empirical chi square 586.74 with prob &lt; 1.7e-106 Fit based upon off diagonal values = 0.74 Code pca2factorOrthogonal Principal Components Analysis Call: principal(r = HolzingerSwineford1939[, vars], nfactors = 2, rotate = &quot;varimax&quot;) Standardized loadings (pattern matrix) based upon correlation matrix RC1 RC2 h2 u2 com x1 0.51 0.42 0.43 0.57 1.9 x2 0.32 0.23 0.16 0.84 1.8 x3 0.27 0.54 0.36 0.64 1.5 x4 0.88 0.06 0.77 0.23 1.0 x5 0.86 0.09 0.75 0.25 1.0 x6 0.85 0.06 0.73 0.27 1.0 x7 -0.05 0.65 0.42 0.58 1.0 x8 0.08 0.77 0.60 0.40 1.0 x9 0.18 0.77 0.63 0.37 1.1 RC1 RC2 SS loadings 2.71 2.15 Proportion Var 0.30 0.24 Cumulative Var 0.30 0.54 Proportion Explained 0.56 0.44 Cumulative Proportion 0.56 1.00 Mean item complexity = 1.3 Test of the hypothesis that 2 components are sufficient. The root mean square of the residuals (RMSR) is 0.12 with the empirical chi square 306.64 with prob &lt; 8.7e-54 Fit based upon off diagonal values = 0.86 Code pca3factorOrthogonal Principal Components Analysis Call: principal(r = HolzingerSwineford1939[, vars], nfactors = 3, rotate = &quot;varimax&quot;) Standardized loadings (pattern matrix) based upon correlation matrix RC1 RC3 RC2 h2 u2 com x1 0.32 0.68 0.15 0.59 0.41 1.5 x2 0.09 0.73 -0.12 0.55 0.45 1.1 x3 0.04 0.78 0.20 0.64 0.36 1.1 x4 0.89 0.13 0.08 0.82 0.18 1.1 x5 0.88 0.12 0.11 0.81 0.19 1.1 x6 0.86 0.13 0.07 0.77 0.23 1.1 x7 0.08 -0.16 0.83 0.73 0.27 1.1 x8 0.10 0.15 0.81 0.68 0.32 1.1 x9 0.10 0.45 0.65 0.63 0.37 1.8 RC1 RC3 RC2 SS loadings 2.46 1.90 1.87 Proportion Var 0.27 0.21 0.21 Cumulative Var 0.27 0.48 0.69 Proportion Explained 0.39 0.31 0.30 Cumulative Proportion 0.39 0.70 1.00 Mean item complexity = 1.2 Test of the hypothesis that 3 components are sufficient. The root mean square of the residuals (RMSR) is 0.08 with the empirical chi square 151.71 with prob &lt; 2.5e-26 Fit based upon off diagonal values = 0.93 Code pca4factorOrthogonal Principal Components Analysis Call: principal(r = HolzingerSwineford1939[, vars], nfactors = 4, rotate = &quot;varimax&quot;) Standardized loadings (pattern matrix) based upon correlation matrix RC1 RC2 RC3 RC4 h2 u2 com x1 0.31 0.06 0.77 0.09 0.70 0.296 1.4 x2 0.11 0.00 0.22 0.96 0.97 0.028 1.1 x3 0.03 0.11 0.84 0.16 0.75 0.255 1.1 x4 0.89 0.06 0.18 -0.02 0.82 0.178 1.1 x5 0.88 0.13 0.07 0.12 0.82 0.181 1.1 x6 0.86 0.07 0.12 0.06 0.77 0.229 1.1 x7 0.07 0.83 -0.06 -0.17 0.73 0.267 1.1 x8 0.10 0.82 0.14 0.11 0.72 0.284 1.1 x9 0.09 0.62 0.47 0.14 0.63 0.368 2.0 RC1 RC2 RC3 RC4 SS loadings 2.45 1.80 1.64 1.02 Proportion Var 0.27 0.20 0.18 0.11 Cumulative Var 0.27 0.47 0.65 0.77 Proportion Explained 0.35 0.26 0.24 0.15 Cumulative Proportion 0.35 0.61 0.85 1.00 Mean item complexity = 1.2 Test of the hypothesis that 4 components are sufficient. The root mean square of the residuals (RMSR) is 0.07 with the empirical chi square 119.21 with prob &lt; 2.4e-23 Fit based upon off diagonal values = 0.95 Code pca5factorOrthogonal Principal Components Analysis Call: principal(r = HolzingerSwineford1939[, vars], nfactors = 5, rotate = &quot;varimax&quot;) Standardized loadings (pattern matrix) based upon correlation matrix RC1 RC2 RC3 RC4 RC5 h2 u2 com x1 0.26 0.08 0.29 0.13 0.87 0.94 0.063 1.5 x2 0.11 -0.02 0.18 0.96 0.11 0.97 0.027 1.1 x3 0.08 0.01 0.87 0.15 0.26 0.86 0.144 1.3 x4 0.88 0.05 0.08 -0.02 0.18 0.82 0.178 1.1 x5 0.89 0.12 0.06 0.12 0.04 0.83 0.173 1.1 x6 0.87 0.06 0.07 0.06 0.09 0.77 0.225 1.1 x7 0.09 0.82 0.11 -0.17 -0.15 0.74 0.259 1.2 x8 0.07 0.85 0.01 0.13 0.29 0.83 0.171 1.3 x9 0.14 0.54 0.62 0.12 0.03 0.71 0.288 2.2 RC1 RC2 RC3 RC4 RC5 SS loadings 2.44 1.72 1.29 1.03 0.99 Proportion Var 0.27 0.19 0.14 0.11 0.11 Cumulative Var 0.27 0.46 0.61 0.72 0.83 Proportion Explained 0.33 0.23 0.17 0.14 0.13 Cumulative Proportion 0.33 0.56 0.73 0.87 1.00 Mean item complexity = 1.3 Test of the hypothesis that 5 components are sufficient. The root mean square of the residuals (RMSR) is 0.07 with the empirical chi square 98 with prob &lt; 4.2e-23 Fit based upon off diagonal values = 0.96 Code pca6factorOrthogonal Principal Components Analysis Call: principal(r = HolzingerSwineford1939[, vars], nfactors = 6, rotate = &quot;varimax&quot;) Standardized loadings (pattern matrix) based upon correlation matrix RC1 RC2 RC6 RC5 RC4 RC3 h2 u2 com x1 0.26 0.00 0.12 0.87 0.13 0.28 0.94 0.062 1.5 x2 0.11 -0.05 0.07 0.11 0.97 0.16 0.99 0.010 1.1 x3 0.08 0.06 0.21 0.24 0.19 0.88 0.91 0.086 1.4 x4 0.88 0.08 -0.02 0.18 0.00 0.10 0.83 0.169 1.1 x5 0.89 0.02 0.20 0.05 0.10 -0.03 0.85 0.153 1.1 x6 0.87 0.06 0.04 0.09 0.07 0.07 0.78 0.224 1.1 x7 0.09 0.94 0.09 -0.09 -0.09 0.13 0.93 0.066 1.1 x8 0.07 0.64 0.49 0.37 0.12 -0.19 0.84 0.158 2.9 x9 0.13 0.19 0.90 0.08 0.06 0.26 0.94 0.064 1.3 RC1 RC2 RC6 RC5 RC4 RC3 SS loadings 2.44 1.35 1.15 1.03 1.02 1.01 Proportion Var 0.27 0.15 0.13 0.11 0.11 0.11 Cumulative Var 0.27 0.42 0.55 0.66 0.78 0.89 Proportion Explained 0.31 0.17 0.14 0.13 0.13 0.13 Cumulative Proportion 0.31 0.47 0.62 0.75 0.87 1.00 Mean item complexity = 1.4 Test of the hypothesis that 6 components are sufficient. The root mean square of the residuals (RMSR) is 0.05 with the empirical chi square 62.84 with prob &lt; NA Fit based upon off diagonal values = 0.97 Code pca7factorOrthogonal Principal Components Analysis Call: principal(r = HolzingerSwineford1939[, vars], nfactors = 7, rotate = &quot;varimax&quot;) Standardized loadings (pattern matrix) based upon correlation matrix RC1 RC2 RC3 RC4 RC7 RC6 RC5 h2 u2 com x1 0.23 -0.02 0.22 0.14 0.12 0.13 0.91 0.98 0.01620 1.4 x2 0.10 -0.06 0.15 0.97 0.03 0.07 0.12 1.00 0.00079 1.1 x3 0.09 0.03 0.94 0.16 0.06 0.18 0.20 0.99 0.01059 1.3 x4 0.86 0.16 0.01 0.03 -0.10 0.05 0.27 0.86 0.14081 1.3 x5 0.89 -0.01 -0.02 0.10 0.09 0.18 0.05 0.85 0.15266 1.1 x6 0.88 -0.01 0.15 0.04 0.15 -0.03 0.04 0.83 0.17414 1.1 x7 0.07 0.95 0.03 -0.06 0.24 0.15 -0.01 0.99 0.01101 1.2 x8 0.10 0.26 0.06 0.03 0.92 0.21 0.11 0.98 0.01810 1.3 x9 0.12 0.16 0.19 0.08 0.21 0.92 0.13 0.99 0.01167 1.4 RC1 RC2 RC3 RC4 RC7 RC6 RC5 SS loadings 2.42 1.03 1.01 1.01 1.00 1.00 0.99 Proportion Var 0.27 0.11 0.11 0.11 0.11 0.11 0.11 Cumulative Var 0.27 0.38 0.50 0.61 0.72 0.83 0.94 Proportion Explained 0.29 0.12 0.12 0.12 0.12 0.12 0.12 Cumulative Proportion 0.29 0.41 0.53 0.65 0.76 0.88 1.00 Mean item complexity = 1.3 Test of the hypothesis that 7 components are sufficient. The root mean square of the residuals (RMSR) is 0.03 with the empirical chi square 21.82 with prob &lt; NA Fit based upon off diagonal values = 0.99 Code pca8factorOrthogonal Principal Components Analysis Call: principal(r = HolzingerSwineford1939[, vars], nfactors = 8, rotate = &quot;varimax&quot;) Standardized loadings (pattern matrix) based upon correlation matrix RC1 RC2 RC4 RC3 RC7 RC5 RC6 RC8 h2 u2 com x1 0.21 -0.01 0.14 0.22 0.11 0.92 0.14 0.07 0.99 1.1e-02 1.4 x2 0.09 -0.06 0.97 0.15 0.03 0.12 0.07 0.04 1.00 5.2e-04 1.1 x3 0.07 0.03 0.16 0.95 0.07 0.20 0.17 0.05 1.00 1.0e-05 1.2 x4 0.89 0.14 0.03 0.06 -0.05 0.25 0.01 0.13 0.90 1.0e-01 1.3 x5 0.91 -0.04 0.09 0.03 0.15 0.02 0.14 0.15 0.91 9.3e-02 1.2 x6 0.61 0.03 0.06 0.08 0.07 0.11 0.04 0.77 1.00 5.4e-05 2.0 x7 0.07 0.95 -0.06 0.03 0.23 -0.01 0.15 0.02 0.99 8.2e-03 1.2 x8 0.08 0.26 0.03 0.07 0.93 0.11 0.20 0.05 0.99 7.7e-03 1.3 x9 0.12 0.17 0.08 0.18 0.21 0.13 0.92 0.03 1.00 2.6e-03 1.4 RC1 RC2 RC4 RC3 RC7 RC5 RC6 RC8 SS loadings 2.08 1.02 1.01 1.01 1.00 1.00 0.99 0.65 Proportion Var 0.23 0.11 0.11 0.11 0.11 0.11 0.11 0.07 Cumulative Var 0.23 0.35 0.46 0.57 0.68 0.79 0.90 0.97 Proportion Explained 0.24 0.12 0.12 0.12 0.11 0.11 0.11 0.07 Cumulative Proportion 0.24 0.35 0.47 0.58 0.70 0.81 0.93 1.00 Mean item complexity = 1.3 Test of the hypothesis that 8 components are sufficient. The root mean square of the residuals (RMSR) is 0.02 with the empirical chi square 9.61 with prob &lt; NA Fit based upon off diagonal values = 1 Code pca9factorOrthogonal Principal Components Analysis Call: principal(r = HolzingerSwineford1939[, vars], nfactors = 9, rotate = &quot;varimax&quot;) Standardized loadings (pattern matrix) based upon correlation matrix RC8 RC2 RC4 RC3 RC5 RC7 RC6 RC1 RC9 h2 u2 com x1 0.11 -0.01 0.14 0.22 0.93 0.11 0.13 0.09 0.15 1 -2.2e-16 1.3 x2 0.05 -0.06 0.97 0.15 0.12 0.03 0.07 0.06 0.04 1 1.8e-15 1.1 x3 0.06 0.03 0.16 0.95 0.20 0.07 0.17 0.03 0.04 1 7.8e-16 1.2 x4 0.34 0.08 0.05 0.05 0.19 0.01 0.05 0.36 0.84 1 1.7e-15 1.9 x5 0.35 0.03 0.08 0.03 0.10 0.09 0.11 0.86 0.32 1 4.4e-16 1.8 x6 0.90 0.03 0.06 0.07 0.12 0.07 0.05 0.30 0.27 1 6.7e-16 1.5 x7 0.03 0.96 -0.06 0.03 -0.01 0.23 0.15 0.02 0.06 1 1.1e-16 1.2 x8 0.06 0.25 0.03 0.07 0.10 0.93 0.20 0.07 0.02 1 1.3e-15 1.3 x9 0.05 0.16 0.08 0.18 0.13 0.21 0.93 0.09 0.05 1 2.2e-16 1.3 RC8 RC2 RC4 RC3 RC5 RC7 RC6 RC1 RC9 SS loadings 1.07 1.02 1.02 1.01 1.01 1.00 0.99 0.98 0.91 Proportion Var 0.12 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.10 Cumulative Var 0.12 0.23 0.34 0.46 0.57 0.68 0.79 0.90 1.00 Proportion Explained 0.12 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.10 Cumulative Proportion 0.12 0.23 0.34 0.46 0.57 0.68 0.79 0.90 1.00 Mean item complexity = 1.4 Test of the hypothesis that 9 components are sufficient. The root mean square of the residuals (RMSR) is 0 with the empirical chi square 0 with prob &lt; NA Fit based upon off diagonal values = 1 14.5.3.2 Oblique (Oblimin) rotation Code pca1factorOblique Principal Components Analysis Call: principal(r = HolzingerSwineford1939[, vars], nfactors = 1, rotate = &quot;oblimin&quot;) Standardized loadings (pattern matrix) based upon correlation matrix PC1 h2 u2 com x1 0.66 0.43 0.57 1 x2 0.40 0.16 0.84 1 x3 0.53 0.28 0.72 1 x4 0.75 0.56 0.44 1 x5 0.76 0.57 0.43 1 x6 0.73 0.53 0.47 1 x7 0.33 0.11 0.89 1 x8 0.51 0.26 0.74 1 x9 0.59 0.35 0.65 1 PC1 SS loadings 3.26 Proportion Var 0.36 Mean item complexity = 1 Test of the hypothesis that 1 component is sufficient. The root mean square of the residuals (RMSR) is 0.16 with the empirical chi square 586.74 with prob &lt; 1.7e-106 Fit based upon off diagonal values = 0.74 Code pca2factorOblique Principal Components Analysis Call: principal(r = HolzingerSwineford1939[, vars], nfactors = 2, rotate = &quot;oblimin&quot;) Standardized loadings (pattern matrix) based upon correlation matrix TC1 TC2 h2 u2 com x1 0.45 0.39 0.43 0.57 2.0 x2 0.29 0.21 0.16 0.84 1.8 x3 0.19 0.53 0.36 0.64 1.2 x4 0.88 -0.02 0.77 0.23 1.0 x5 0.87 0.01 0.75 0.25 1.0 x6 0.86 -0.02 0.73 0.27 1.0 x7 -0.15 0.67 0.42 0.58 1.1 x8 -0.05 0.79 0.60 0.40 1.0 x9 0.06 0.78 0.63 0.37 1.0 TC1 TC2 SS loadings 2.67 2.20 Proportion Var 0.30 0.24 Cumulative Var 0.30 0.54 Proportion Explained 0.55 0.45 Cumulative Proportion 0.55 1.00 With component correlations of TC1 TC2 TC1 1.00 0.25 TC2 0.25 1.00 Mean item complexity = 1.2 Test of the hypothesis that 2 components are sufficient. The root mean square of the residuals (RMSR) is 0.12 with the empirical chi square 306.64 with prob &lt; 8.7e-54 Fit based upon off diagonal values = 0.86 Code pca3factorOblique Principal Components Analysis Call: principal(r = HolzingerSwineford1939[, vars], nfactors = 3, rotate = &quot;oblimin&quot;) Standardized loadings (pattern matrix) based upon correlation matrix TC1 TC3 TC2 h2 u2 com x1 0.24 0.65 0.05 0.59 0.41 1.3 x2 0.02 0.74 -0.21 0.55 0.45 1.2 x3 -0.06 0.79 0.11 0.64 0.36 1.1 x4 0.90 0.00 -0.01 0.82 0.18 1.0 x5 0.89 -0.01 0.03 0.81 0.19 1.0 x6 0.88 0.01 -0.01 0.77 0.23 1.0 x7 0.03 -0.20 0.86 0.73 0.27 1.1 x8 0.02 0.12 0.79 0.68 0.32 1.0 x9 0.00 0.43 0.60 0.63 0.37 1.8 TC1 TC3 TC2 SS loadings 2.49 1.90 1.82 Proportion Var 0.28 0.21 0.20 Cumulative Var 0.28 0.49 0.69 Proportion Explained 0.40 0.31 0.29 Cumulative Proportion 0.40 0.71 1.00 With component correlations of TC1 TC3 TC2 TC1 1.00 0.27 0.18 TC3 0.27 1.00 0.16 TC2 0.18 0.16 1.00 Mean item complexity = 1.2 Test of the hypothesis that 3 components are sufficient. The root mean square of the residuals (RMSR) is 0.08 with the empirical chi square 151.71 with prob &lt; 2.5e-26 Fit based upon off diagonal values = 0.93 Code pca4factorOblique Principal Components Analysis Call: principal(r = HolzingerSwineford1939[, vars], nfactors = 4, rotate = &quot;oblimin&quot;) Standardized loadings (pattern matrix) based upon correlation matrix TC1 TC2 TC3 TC4 h2 u2 com x1 0.21 -0.05 0.78 -0.03 0.70 0.296 1.2 x2 0.02 -0.01 0.00 0.98 0.97 0.028 1.0 x3 -0.10 0.02 0.86 0.05 0.75 0.255 1.0 x4 0.90 -0.03 0.09 -0.08 0.82 0.178 1.0 x5 0.89 0.06 -0.07 0.09 0.82 0.181 1.0 x6 0.87 0.00 0.00 0.02 0.77 0.229 1.0 x7 0.03 0.85 -0.11 -0.15 0.73 0.267 1.1 x8 0.02 0.82 0.03 0.11 0.72 0.284 1.0 x9 -0.02 0.58 0.41 0.09 0.63 0.368 1.9 TC1 TC2 TC3 TC4 SS loadings 2.46 1.79 1.61 1.06 Proportion Var 0.27 0.20 0.18 0.12 Cumulative Var 0.27 0.47 0.65 0.77 Proportion Explained 0.36 0.26 0.23 0.15 Cumulative Proportion 0.36 0.61 0.85 1.00 With component correlations of TC1 TC2 TC3 TC4 TC1 1.00 0.18 0.28 0.17 TC2 0.18 1.00 0.21 0.03 TC3 0.28 0.21 1.00 0.36 TC4 0.17 0.03 0.36 1.00 Mean item complexity = 1.1 Test of the hypothesis that 4 components are sufficient. The root mean square of the residuals (RMSR) is 0.07 with the empirical chi square 119.21 with prob &lt; 2.4e-23 Fit based upon off diagonal values = 0.95 Code pca5factorOblique Principal Components Analysis Call: principal(r = HolzingerSwineford1939[, vars], nfactors = 5, rotate = &quot;oblimin&quot;) Standardized loadings (pattern matrix) based upon correlation matrix TC1 TC2 TC3 TC4 TC5 h2 u2 com x1 0.09 0.06 0.13 0.00 0.88 0.94 0.063 1.1 x2 0.02 -0.01 0.03 0.98 -0.02 0.97 0.027 1.0 x3 -0.02 -0.11 0.87 0.05 0.17 0.86 0.144 1.1 x4 0.89 -0.03 0.00 -0.08 0.11 0.82 0.178 1.1 x5 0.91 0.04 -0.02 0.08 -0.07 0.83 0.173 1.0 x6 0.88 -0.02 0.00 0.02 0.01 0.77 0.225 1.0 x7 0.08 0.79 0.12 -0.17 -0.21 0.74 0.259 1.3 x8 -0.04 0.88 -0.11 0.10 0.24 0.83 0.171 1.2 x9 0.06 0.45 0.60 0.08 -0.09 0.71 0.288 2.0 TC1 TC2 TC3 TC4 TC5 SS loadings 2.45 1.68 1.27 1.06 1.01 Proportion Var 0.27 0.19 0.14 0.12 0.11 Cumulative Var 0.27 0.46 0.60 0.72 0.83 Proportion Explained 0.33 0.22 0.17 0.14 0.14 Cumulative Proportion 0.33 0.55 0.72 0.86 1.00 With component correlations of TC1 TC2 TC3 TC4 TC5 TC1 1.00 0.18 0.19 0.17 0.28 TC2 0.18 1.00 0.23 0.01 0.08 TC3 0.19 0.23 1.00 0.26 0.31 TC4 0.17 0.01 0.26 1.00 0.29 TC5 0.28 0.08 0.31 0.29 1.00 Mean item complexity = 1.2 Test of the hypothesis that 5 components are sufficient. The root mean square of the residuals (RMSR) is 0.07 with the empirical chi square 98 with prob &lt; 4.2e-23 Fit based upon off diagonal values = 0.96 Code pca6factorOblique Principal Components Analysis Call: principal(r = HolzingerSwineford1939[, vars], nfactors = 6, rotate = &quot;oblimin&quot;) Standardized loadings (pattern matrix) based upon correlation matrix TC1 TC6 TC2 TC5 TC4 TC3 h2 u2 com x1 0.08 -0.09 -0.03 0.93 0.01 0.10 0.94 0.062 1.1 x2 0.01 -0.02 -0.04 -0.02 1.00 0.02 0.99 0.010 1.0 x3 -0.03 0.11 0.21 0.20 0.12 0.78 0.91 0.086 1.4 x4 0.88 0.07 -0.10 0.12 -0.05 0.06 0.83 0.169 1.1 x5 0.90 -0.06 0.17 -0.06 0.05 -0.10 0.85 0.153 1.1 x6 0.87 0.04 -0.02 0.01 0.03 0.02 0.78 0.224 1.0 x7 0.05 0.99 -0.04 -0.09 -0.04 0.07 0.93 0.066 1.0 x8 -0.07 0.48 0.34 0.36 0.10 -0.39 0.84 0.158 3.9 x9 0.05 -0.02 0.97 -0.03 -0.02 0.09 0.94 0.064 1.0 TC1 TC6 TC2 TC5 TC4 TC3 SS loadings 2.39 1.29 1.23 1.17 1.07 0.85 Proportion Var 0.27 0.14 0.14 0.13 0.12 0.09 Cumulative Var 0.27 0.41 0.55 0.68 0.80 0.89 Proportion Explained 0.30 0.16 0.15 0.15 0.13 0.11 Cumulative Proportion 0.30 0.46 0.61 0.76 0.89 1.00 With component correlations of TC1 TC6 TC2 TC5 TC4 TC3 TC1 1.00 0.11 0.18 0.31 0.18 0.08 TC6 0.11 1.00 0.39 0.16 -0.02 -0.07 TC2 0.18 0.39 1.00 0.36 0.23 0.10 TC5 0.31 0.16 0.36 1.00 0.30 0.22 TC4 0.18 -0.02 0.23 0.30 1.00 0.20 TC3 0.08 -0.07 0.10 0.22 0.20 1.00 Mean item complexity = 1.4 Test of the hypothesis that 6 components are sufficient. The root mean square of the residuals (RMSR) is 0.05 with the empirical chi square 62.84 with prob &lt; NA Fit based upon off diagonal values = 0.97 Code pca7factorOblique Principal Components Analysis Call: principal(r = HolzingerSwineford1939[, vars], nfactors = 7, rotate = &quot;oblimin&quot;) Standardized loadings (pattern matrix) based upon correlation matrix TC1 TC4 TC6 TC5 TC7 TC3 TC2 h2 u2 com x1 0.00 0.02 -0.04 0.95 0.02 0.04 0.06 0.98 0.01620 1.0 x2 0.00 1.00 0.00 0.00 -0.01 0.00 0.00 1.00 0.00079 1.0 x3 0.01 0.02 0.02 0.03 0.04 0.97 -0.02 0.99 0.01059 1.0 x4 0.82 -0.01 0.18 0.23 0.00 -0.05 -0.19 0.86 0.14081 1.4 x5 0.90 0.06 -0.06 -0.05 0.16 -0.08 0.04 0.85 0.15266 1.1 x6 0.90 -0.01 -0.05 -0.06 -0.11 0.15 0.14 0.83 0.17414 1.2 x7 -0.01 0.00 0.97 -0.04 0.01 0.02 0.06 0.99 0.01101 1.0 x8 0.02 0.00 0.07 0.06 0.04 -0.02 0.93 0.98 0.01810 1.0 x9 0.01 0.00 0.01 0.01 0.96 0.04 0.02 0.99 0.01167 1.0 TC1 TC4 TC6 TC5 TC7 TC3 TC2 SS loadings 2.34 1.02 1.03 1.05 1.03 1.01 0.99 Proportion Var 0.26 0.11 0.11 0.12 0.11 0.11 0.11 Cumulative Var 0.26 0.37 0.49 0.60 0.72 0.83 0.94 Proportion Explained 0.28 0.12 0.12 0.12 0.12 0.12 0.12 Cumulative Proportion 0.28 0.40 0.52 0.64 0.76 0.88 1.00 With component correlations of TC1 TC4 TC6 TC5 TC7 TC3 TC2 TC1 1.00 0.16 0.12 0.34 0.19 0.14 0.12 TC4 0.16 1.00 -0.09 0.28 0.20 0.32 0.08 TC6 0.12 -0.09 1.00 0.07 0.31 0.07 0.40 TC5 0.34 0.28 0.07 1.00 0.29 0.39 0.16 TC7 0.19 0.20 0.31 0.29 1.00 0.33 0.39 TC3 0.14 0.32 0.07 0.39 0.33 1.00 0.17 TC2 0.12 0.08 0.40 0.16 0.39 0.17 1.00 Mean item complexity = 1.1 Test of the hypothesis that 7 components are sufficient. The root mean square of the residuals (RMSR) is 0.03 with the empirical chi square 21.82 with prob &lt; NA Fit based upon off diagonal values = 0.99 Code pca8factorOblique Principal Components Analysis Call: principal(r = HolzingerSwineford1939[, vars], nfactors = 8, rotate = &quot;oblimin&quot;) Standardized loadings (pattern matrix) based upon correlation matrix TC1 TC8 TC3 TC4 TC2 TC6 TC7 TC5 h2 u2 com x1 0.01 0.02 0.02 0.02 0.03 -0.03 0.04 0.95 0.99 1.1e-02 1.0 x2 0.00 0.00 0.00 1.00 0.00 0.01 -0.01 0.01 1.00 5.2e-04 1.0 x3 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 1.0e-05 1.0 x4 0.86 0.03 0.02 -0.02 -0.06 0.14 -0.12 0.18 0.90 1.0e-01 1.2 x5 0.90 0.05 0.00 0.04 0.08 -0.11 0.12 -0.11 0.91 9.3e-02 1.1 x6 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 5.4e-05 1.0 x7 0.00 0.01 0.00 0.00 0.03 0.97 0.04 -0.03 0.99 8.2e-03 1.0 x8 0.01 0.00 0.01 -0.01 0.00 0.04 0.97 0.04 0.99 7.7e-03 1.0 x9 0.00 0.00 0.00 0.00 0.99 0.02 -0.01 0.02 1.00 2.6e-03 1.0 TC1 TC8 TC3 TC4 TC2 TC6 TC7 TC5 SS loadings 1.64 1.07 1.02 1.01 1.02 1.01 1.01 1.01 Proportion Var 0.18 0.12 0.11 0.11 0.11 0.11 0.11 0.11 Cumulative Var 0.18 0.30 0.41 0.53 0.64 0.75 0.86 0.97 Proportion Explained 0.19 0.12 0.12 0.12 0.12 0.11 0.11 0.12 Cumulative Proportion 0.19 0.31 0.42 0.54 0.66 0.77 0.88 1.00 With component correlations of TC1 TC8 TC3 TC4 TC2 TC6 TC7 TC5 TC1 1.00 0.71 0.14 0.16 0.21 0.11 0.14 0.32 TC8 0.71 1.00 0.20 0.17 0.18 0.09 0.17 0.30 TC3 0.14 0.20 1.00 0.34 0.39 0.08 0.18 0.43 TC4 0.16 0.17 0.34 1.00 0.20 -0.10 0.09 0.28 TC2 0.21 0.18 0.39 0.20 1.00 0.31 0.45 0.28 TC6 0.11 0.09 0.08 -0.10 0.31 1.00 0.43 0.05 TC7 0.14 0.17 0.18 0.09 0.45 0.43 1.00 0.19 TC5 0.32 0.30 0.43 0.28 0.28 0.05 0.19 1.00 Mean item complexity = 1 Test of the hypothesis that 8 components are sufficient. The root mean square of the residuals (RMSR) is 0.02 with the empirical chi square 9.61 with prob &lt; NA Fit based upon off diagonal values = 1 Code pca9factorOblique Principal Components Analysis Call: principal(r = HolzingerSwineford1939[, vars], nfactors = 9, rotate = &quot;oblimin&quot;) Standardized loadings (pattern matrix) based upon correlation matrix TC4 TC6 TC2 TC3 TC7 TC5 TC8 TC1 TC9 h2 u2 com x1 0 0 0 0 0 1 0 0 0 1 -2.2e-16 1 x2 1 0 0 0 0 0 0 0 0 1 1.8e-15 1 x3 0 0 0 1 0 0 0 0 0 1 7.8e-16 1 x4 0 0 0 0 0 0 0 0 1 1 1.7e-15 1 x5 0 0 0 0 0 0 0 1 0 1 4.4e-16 1 x6 0 0 0 0 0 0 1 0 0 1 6.7e-16 1 x7 0 1 0 0 0 0 0 0 0 1 1.1e-16 1 x8 0 0 1 0 0 0 0 0 0 1 1.3e-15 1 x9 0 0 0 0 1 0 0 0 0 1 2.2e-16 1 TC4 TC6 TC2 TC3 TC7 TC5 TC8 TC1 TC9 SS loadings 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 Proportion Var 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 Cumulative Var 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.00 Proportion Explained 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 Cumulative Proportion 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.00 With component correlations of TC4 TC6 TC2 TC3 TC7 TC5 TC8 TC1 TC9 TC4 1.00 -0.09 0.09 0.34 0.20 0.32 0.17 0.19 0.15 TC6 -0.09 1.00 0.49 0.09 0.35 0.04 0.10 0.11 0.15 TC2 0.09 0.49 1.00 0.20 0.46 0.26 0.19 0.21 0.13 TC3 0.34 0.09 0.20 1.00 0.40 0.46 0.19 0.15 0.18 TC7 0.20 0.35 0.46 0.40 1.00 0.34 0.18 0.26 0.19 TC5 0.32 0.04 0.26 0.46 0.34 1.00 0.32 0.31 0.40 TC8 0.17 0.10 0.19 0.19 0.18 0.32 1.00 0.69 0.68 TC1 0.19 0.11 0.21 0.15 0.26 0.31 0.69 1.00 0.73 TC9 0.15 0.15 0.13 0.18 0.19 0.40 0.68 0.73 1.00 Mean item complexity = 1 Test of the hypothesis that 9 components are sufficient. The root mean square of the residuals (RMSR) is 0 with the empirical chi square 0 with prob &lt; NA Fit based upon off diagonal values = 1 14.5.4 Component Scores 14.5.4.1 Orthogonal (Varimax) rotation Code pca3Orthogonal &lt;- pca3factorOrthogonal$scores 14.5.4.2 Oblique (Oblimin) rotation Code pca3Oblique &lt;- pca3factorOblique$scores 14.5.5 Plots Biplots were generated using the psych package (Revelle, 2022). Pairs panel plots were generated using the psych package (Revelle, 2022). Correlation plots were generated using the corrplot package (Wei &amp; Simko, 2021). 14.5.5.1 Biplot A biplot is in Figure 14.99. Code biplot(pca2factorOrthogonal) abline(h = 0, v = 0, lty = 2) Figure 14.99: Biplot Using Orthogonal Rotation in Principal Component Analysis. 14.5.5.2 Orthogonal (Varimax) rotation A pairs panel plot is in Figure 14.100. Code pairs.panels(pca3Orthogonal) Figure 14.100: Pairs Panel Plot Using Orthogonal Rotation in Principal Component Analysis. A correlation plot is in Figure 14.101. Code corrplot(cor( pca3Orthogonal, use = &quot;pairwise.complete.obs&quot;)) Figure 14.101: Correlation Plot Using Orthogonal Rotation in Principal Component Analysis. 14.5.5.3 Oblique (Oblimin) rotation A pairs panel plot is in Figure 14.102. Code pairs.panels(pca3Oblique) Figure 14.102: Pairs Panel Plot Using Oblique Rotation in Principal Component Analysis. A correlation plot is in Figure 14.103. Code corrplot(cor( pca3Oblique, use = &quot;pairwise.complete.obs&quot;)) Figure 14.103: Correlation Plot Using Oblique Rotation in Principal Component Analysis. 14.6 Conclusion PCA is not factor analysis. PCA is a technique for data reduction, whereas factor analysis uses latent variables and can be used to identify the structure of measures/constructs or for data reduction. Many people use PCA when they should use factor analysis instead, such as when they are assessing latent constructs. Nevertheless, factor analysis has weaknesses including indeterminacy—i.e., a given data matrix can produce many different factor models, and you cannot determine which one is correct based on the data matrix alone. There are many decisions to make in factor analysis. These decisions can have important impacts on the resulting solution. Thus, it can be helpful for theory and interpretability to help guide decision-making when conducting factor analysis. 14.7 Suggested Readings Floyd &amp; Widaman (1995) 14.8 Exercises 14.8.1 Questions Note: Several of the following questions use data from the Children of the National Longitudinal Survey of Youth Survey (CNLSY). The CNLSY is a publicly available longitudinal data set provided by the Bureau of Labor Statistics (https://www.bls.gov/nls/nlsy79-children.htm#topical-guide; archived at https://perma.cc/EH38-HDRN). The CNLSY data file for these exercises is located on the book’s page of the Open Science Framework (https://osf.io/3pwza). Children’s behavior problems were rated in 1988 (time 1: T1) and then again in 1990 (time 2: T2) on the Behavior Problems Index (BPI). Below are the items corresponding to the Antisocial subscale of the BPI: cheats or tells lies bullies or is cruel/mean to others does not seem to feel sorry after misbehaving breaks things deliberately is disobedient at school has trouble getting along with teachers has sudden changes in mood or feeling Fit an exploratory factor analysis (EFA) model using maximum likelihood to the seven items of the Antisocial subscale of the Behavior Problems Index at T1. Create a scree plot using the raw data (not the correlation or covariance matrix). How many factors should be retained according to the Kaiser–Guttman rule? How many factors should be retained according to a Parallel Test? Based on the available information, how many factors would you retain? Extract two factors with an oblique (oblimin) rotation. Which items load most strongly on Factor 1? Which items load most strongly on Factor 2? Why do you think these items load onto Factor 2? Which item has the highest cross-loading (on both Factors 1 and 2)? What is the association between Factors 1 and 2 when using an oblique rotation? What does this indicate about whether you should use an orthogonal or oblique rotation? For exercises about confirmatory factor analysis, see the exercises in Section #ref(semExercises) of the chapter on Structural Equation Modeling. Fit a principal component analysis (PCA) model to the seven items of the Antisocial subscale of the Behavior Problems Index at T1. How many components should be kept? Fit a one-factor confirmatory factor analysis (CFA) model to the seven items of the Antisocial subscale of the Behavior Problems Index at T1. Set the scale of the latent factor by standardizing the latent factor—set the mean of the factor to one and the variance of the factor to zero. Freely estimate the factor loadings of all indicators. Allow the residuals of indicators 5 and 6 to be correlated. Use full information maximum likelihood (FIML) to account for missing data. Use robust standard errors to account for non-normally distributed data. How strongly are the factor scores from the one-factor CFA model associated with the factors from a one-factor EFA model and a one-component PCA model? If your model fits well, why is it important to consider alternative models? What are some possible alternative models? 14.8.2 Answers A scree plot is in Figure 14.104. Figure 14.104: Scree Plot From Exploratory Factor Analysis. The Kaiser-Guttman rule states that all factors should be retained whose eigenvalues are greater than 1, because it is not conceptually elegant to include factors that explain less than one variable’s worth of variance (i.e., one eigenvalue). According to the Kaiser-Guttman rule, one factor should be retained in this case. However, for factor analysis (rather than PCA), we modify the guidelines to retain factors whose eigenvalues are greater than zero, so we would keep two factors. According to the Parallel Test, factors should be retained that explain more variance than randomly generated data. According to the Parallel Test, two factors should be retained. Items 1 (loading = .52), 2 (loading = .66), 3 (loading = .48), 4 (loading = .52), and 7 (loading = .39) load most strongly on Factor 1. Items 5 (“disobedient at school”; loading = 1.00) and 6 (“trouble getting along with teachers”; loading = .42) load most strongly on Factor 2. It is likely that these two items load onto a separate factor from the other antisocial items because they reflect school-related antisocial behavior. Item 6 has the highest cross-loading—in addition to loading onto Factor 2 (.42), it has a non-trivial loading on Factor 1 (.22). The association between Factors 1 and 2 is .48. This indicates that the factors are moderately correlated and that using an orthogonal rotation would likely be improper. An oblique rotation would be important to capture the association between the two factors. However, this could come at the expense of having simple structure and, therefore, straightforward interpretation of the factors. See the exercises on “Structural Equation Modeling” in Section 7.17. According to the Kaiser-Guttman rule and the Parallel Test, one component should be retained. The factor scores from the CFA model are correlated \\(r = .97\\) and \\(r = .98\\) with the factor scores of the EFA model and the component scores of the PCA model, respectively. The factor scores of the EFA model are correlated \\(r = .997\\) with the component scores of the PCA model. Even if a given model fits well, there are many other models that fit just as well. Any given data matrix can produce an infinite number of factor models that accurately represent the data structure (indeterminacy). For any model, there always exist an infinite number of models that fit exactly the same. There is no way to distinguish which factor model is correct from the data matrix. Although the fit of the structural model may be consistent with the hypothesized model, fit does not demonstrate that a given model is uniquely valid. Thus, it is important to consider alternative models that may better capture the causal processes, even if their fit is mathematically equivalent. For instance, you could consider models with multiple factors, correlated factors, correlated residuals, cross-loading indicators, regression paths, hierarchical (higher-order) factors, bifactors, etc. Remember, an important part of science is specifying and testing plausible alternative explanations! Some alternative, equivalently fitting models are in Figure 14.105. Figure 14.105: Equivalently Fitting Models in Confirmatory Factor Analysis. References Bollen, K. A. (2002). Latent variables in psychology and the social sciences. Annual Review of Psychology, 53(1), 605–634. https://doi.org/10.1146/annurev.psych.53.100901.135239 Dien, J. (2012). Applying principal components analysis to event-related potentials: A tutorial. Developmental Neuropsychology, 37(6), 497–517. https://doi.org/10.1080/87565641.2012.697503 Dinno, A. (2014). Gently clarifying the application of Horn’s parallel analysis to principal component analysis versus factor analysis. http://archives.pdx.edu/ds/psu/10527 Epskamp, S. (2022). semPlot: Path diagrams and visual analysis of various SEM packages’ output. https://github.com/SachaEpskamp/semPlot Floyd, F. J., &amp; Widaman, K. F. (1995). Factor analysis in the development and refinement of clinical assessment instruments. Psychological Assessment, 7, 286–299. https://doi.org/10.1037/1040-3590.7.3.286 Jorgensen, T. D., Pornprasertmanit, S., Schoemann, A. M., &amp; Rosseel, Y. (2021). semTools: Useful tools for structural equation modeling. https://github.com/simsem/semTools/wiki Lilienfeld, S. O., Sauvigne, K., Lynn, S. J., Latzman, R. D., Cautin, R., &amp; Waldman, I. D. (2015). Fifty psychological and psychiatric terms to avoid: A list of inaccurate, misleading, misused, ambiguous, and logically confused words and phrases. Frontiers in Psychology, 6. https://doi.org/10.3389/fpsyg.2015.01100 Markon, K. E. (2019). Bifactor and hierarchical models: Specification, inference, and interpretation. Annual Review of Clinical Psychology, 15(1), 51–69. https://doi.org/10.1146/annurev-clinpsy-050718-095522 Marsh, H. W., Morin, A. J. S., Parker, P. D., &amp; Kaur, G. (2014). Exploratory structural equation modeling: An integration of the best features of exploratory and confirmatory factor analysis. Annual Review of Clinical Psychology, 10(1), 85–110. https://doi.org/10.1146/annurev-clinpsy-032813-153700 Petersen, I. T. (2024b). petersenlab: Package of R functions for the Petersen Lab. https://doi.org/10.5281/zenodo.7602890 Raiche, G., &amp; Magis, D. (2020). nFactors: Parallel analysis and other non graphical solutions to the Cattell scree test. https://CRAN.R-project.org/package=nFactors Raykov, T., &amp; Marcoulides, G. A. (2001). Can there be infinitely many models equivalent to a given covariance structure model? Structural Equation Modeling: A Multidisciplinary Journal, 8(1), 142–149. https://doi.org/10.1207/S15328007SEM0801_8 Revelle, W. (2022). psych: Procedures for psychological, psychometric, and personality research. https://personality-project.org/r/psych/ Revelle, W., &amp; Rocklin, T. (1979). Very simple structure: An alternative procedure for estimating the optimal number of interpretable factors. Multivariate Behavioral Research, 14(4), 403–414. https://doi.org/10.1207/s15327906mbr1404_2 Rosseel, Y., Jorgensen, T. D., &amp; Rockwood, N. (2022). lavaan: Latent variable analysis. https://lavaan.ugent.be Ruscio, J., &amp; Roche, B. (2012). Determining the number of factors to retain in an exploratory factor analysis using comparison data of known factorial structure. Psychological Assessment, 24(2), 282–292. https://doi.org/10.1037/a0025697 Sellbom, M., &amp; Tellegen, A. (2019). Factor analysis in psychological assessment research: Common pitfalls and recommendations. Psychological Assessment, 31(12), 1428–1441. https://doi.org/10.1037/pas0000623 Strauss, M. E., &amp; Smith, G. T. (2009). Construct validity: Advances in theory and methodology. Annual Review of Clinical Psychology, 5(1), 1–25. https://doi.org/10.1146/annurev.clinpsy.032408.153639 Tackett, J. L., Lang, J. W. B., Markon, K. E., &amp; Herzhoff, K. (2019). A correlated traits, correlated methods model for thin-slice child personality assessment. Psychological Assessment, 31(4), 545–556. https://doi.org/10.1037/pas0000635 Textor, J., van der Zander, B., &amp; Ankan, A. (2021). dagitty: Graphical analysis of structural causal models. https://CRAN.R-project.org/package=dagitty Wei, T., &amp; Simko, V. (2021). R package “corrplot\": Visualization of a correlation matrix. https://github.com/taiyun/corrplot "],["intellectual.html", "Chapter 15 Intellectual Assessment 15.1 Defining Intelligence 15.2 History of Intelligence Research 15.3 Alternative Conceptualizations of Intelligence 15.4 Purposes of Intelligence Tests 15.5 Intelligence Versus Achievement Versus Aptitude 15.6 Theory Influences Intepretation of Scores 15.7 Time-Related Influences 15.8 Concerns with Intelligence Tests 15.9 Aptitude Testing 15.10 Scales 15.11 Conclusion 15.12 Suggested Readings", " Chapter 15 Intellectual Assessment 15.1 Defining Intelligence Definitions for intelligence are many. Some define intelligence as problem-solving abilities and the capacity for learning, where general intelligence is called “g” and reflects the ability to use this capacity. We can also consider emotional aspects of intelligence. For instance, social cognition reflects adaptive functioning. We can consider intelligence to be what reflects adaptive functioning. We can also consider whatever we value to be intelligence. For example, you might hear about a player’s “basketball IQ”. Some conceptualizations of intelligence emphasize it as a brain thing that involves speed of processing, processing capacity, and connectedness. From this perspective, intelligence includes components such as working memory, processing speed, visual and auditory perception, etc. We could also consider the products of these skills and components in interaction with the world. This is the goal of behavioral measures that are thought to reflect the underlying neural mechanism of general intelligence (“g”). 15.2 History of Intelligence Research A history of intelligence research is described in Sechrest et al. (1998). The assessment of intelligence is considered one of the key successes in psychology. Intelligence is one of the most widely assessed constructs in research and practice. Information from intelligence tests has important uses. Many people have contributed in important ways to the assessment of intelligence. Here, we outline just a few key figures as a way to introduce different models of intelligence. Please note that our discussion of the history of intelligence and key figures is no endorsement of their views. For instance, historically, some of the researchers on intelligence were proponents of eugenics. 15.2.1 Sir Francis Galton (1822–1911) Sir Francis Galton was known for his work on lots of topics, including correlation, regression, twin studies, heredity, individual differences, the fax machine, and the telegraph. His work discovered the importance of reaction time, and conceptualized it as reflecting an underlying construct. He examined individual differences in many domains: vision, hearing, reaction time, other sensorimotor functions, and physical characteristics. Many of these individual differences would fall into the construct of “intelligence” now. 15.2.2 James Cattell (1860–1944) James Cattell was Wihelm Wundt’s first American student. Cattell measured sensory acuity and psychophysical characteristics. Cattell’s work followed Galton’s, and he examined whether reaction time might shed light on consciousness. Cattell used factor analysis to formally approach Galton’s questions about why scores correlated—why people who tended to be good at one thing also tended to be good at lots of other things. He was credited with initiating the individual differences movement and with the first published reference to a mental test in the psychological literature. 15.2.3 Alfred Binet (1857–1911) Alfred Binet was a French psychologist, and his work was more applied than the work by Galton and Cattell. He tried to apply principles of psychology to ameliorate mental problems. In France, he was forward-thinking at the time: he sought to identify children with an intellectual disability so that they could receive training and live a productive life. He and his colleague, Théodore Simon, asked who is trainable and when. And with their work, so begins clinical assessment. In 1905, Binet and Simon developed the Binet–Simon Intelligence Scale for children. It was developed from a government commission to study the education of children with an intellectual disability (called mentally retarded or “arriéré” at the time). Items on the scales were administered from easiest to hardest, until the participant could not do three-quarters of the items. This approach is similar to modern-day item response theory approaches to adaptive testing. Items were selected for inclusion in the scales if they showed criterion-related validity in relation to two criteria: age—discriminative validity (i.e., age differentiation); items were identified for which older children performed better than younger children, and performance in school; items were selected that differentiated children who showed success versus failure in school. This approach of using ceiling criteria to establish stop points in testing is still widely used today. The Binet–Simon Intelligence Scale included subtests that are still widely used today involving: Naming objects in pictures (similar to the Peabody Picture Vocabulary Test): phone, squash, etc. Digit span and reverse digit span: having a respondent repeat numbers of a progressively longer and longer length, then having respondents repeat numbers in the reverse number; examines how much information a person can store in working memory and their underlying capacity. A vocabulary test: asking children to define words to show they know the meaning of words; a concrete measure reflective of ability to learn words. Similarities: examines abstraction and understanding. Block design: having the respondent manipulate blocks to make different designs; examines visuospatial reasoning. Comprehension questions: how much the person has learned information about life and culture because of their capacity. Binet and Simon assessed performance of typically developing children and children with intellectual disability. They quantified children’s performance on the tests with what they called “mental age” (as opposed to chronological age). The child’s mental age was estimated by correcting for the expected (median) score at that age. For instance, if a child has a mental age of a 7-year-old, that means that the child’s performance is the same as the average (50th percentile) 7-year-old. The Binet–Simon Intelligence Scale was validated with the opinions of teachers of children’s academic functioning. Teachers’ reports used to show stronger criterion-related validity compared to intelligence tests. Now, intelligence tests are the gold standard; they show stronger criterion-related validity compared to teacher reports. 15.2.4 Lewis Terman (1877–1956) Lewis Terman at Stanford University revised the Binet–Simon Intelligence Scale, and the revised test became known as the Stanford–Binet Intelligence Scales. After the release of the Stanford–Binet, academics became excited about intelligence testing. 15.2.5 William Stern (1871–1938) A key challenge that intelligence researchers had was how to quantify children’s performance on the tests. Binet had introduced the concept of the mental age—the typical age for participants with that score. Mental age had a generalizable notion that was informative. It provided meaning compared to saying that someone received a score of 8 on the measure. Mental age conceptualizes intelligence as a measure of current functioning to inform how to improve their functioning to efficiently use resources. However, performance does not increase across all domains with age through adulthood. That is, the same level of performance could have multiple mental ages. Therefore, William Stern changed the scoring to a trait measure by developing the Intelligence Quotient (IQ) that Terman used in the Stanford–Binet. IQ was calculated using Equation (15.1): \\[\\begin{equation} \\text{Intelligence Quotient (IQ)} = \\frac{\\text{mental age}}{\\text{chronological age}} \\times 100 \\tag{15.1} \\end{equation}\\] This helped keep IQ to be a more stable as a characteristic of the person. IQ was more trait-like and stable than mental age, and it does not change as much with age because it accounts for age. Today, this index is known as the “Ratio IQ”. It involves taking a ratio of two numbers, but it is not a ratio measure because it does not have a meaningful zero, and the gaps between the scores might not be equidistant. Stern was a German psychologist who moved to America and promoted Binet’s ideas. Compared to England where you became what your parents were, in America, your future was thought to be determined by your underlying ability. This was based on the idea that “everyone who is able could become successful”. Therefore, intelligence took hold with the idea that individual abilities determine success. Intelligence took hold, then World War I occurred. 15.2.6 Robert Yerkes (1876–1956) World War I involved using lots of technology. In 1917, under the lead of Robert Yerkes and others, the United States Army developed the Army Alpha and Beta Test, a precursor to the eventual Armed Services Vocational Aptitude Battery (ASVAB). The U.S. Army sought to improve the army by classifying people in rank and tasks according to their capabilities—it is important to have the best army possible or you will lose the war. The content was similar to the Binet scales, but it was changed from a clinical test to a multiple-choice, paper-and-pencil test that a clerk could give to large groups. The Alpha Test included words and letters for literate recruits. The Beta Test was a picture-based intelligence test without words for illiterate recruits or recent immigrants. For instance, the Beta Test involved having recruits code digits with symbols, which is still part of the Wechsler Adult Intelligence Scales today. The Army Alpha and Beta Tests are among the first examples of affirmative action because anybody could get picked based on their test performance compared to only doing what one’s parents did. However, the Army Alpha and Beta Tests turned out to be biased against rural, non-whites because they were less experienced with standardized setting. The Army Alpha and Beta Tests were the precursors to much of the modern psychometric movement in psychology, and to the development of more advanced intelligence tests. 15.2.7 Charles Spearman (1863–1945) Charles Spearman found that performance on many tasks was highly correlated. Using factor analysis, he proposed that an underlying “force” (“g”) drove performance on all tests. However, he found that “g” could not explain all variance in tasks. He proposed a two factor-theory. According to Spearman two-factor theory, the variation between people on all cognitive tasks is a function of two factors: general intelligence (“g”) and test-specific variance (“s”), as depicted in Figure 15.1. Figure 15.1: Depiction of Spearman’s Two-Factor Theory of Intelligence. 15.2.8 Louis Leon Thurstone (1887–1955) Louis Leon Thurstone had restricted range in his samples and observed lower correlations among the skills than Charles Spearman observed. Thurstone hypothesized that the tests tapped specific abilities or faculties, not just a general factor. Thus, he argued that intelligence was not one thing—it was many things. He argued that the intelligences consisted of independent, lower-order processes, such as memory, verbal skills, and spatial skills, with a clearer psychological meaning than “g” or test-specific skills, and that these lower-order processes might better explain the pattern of correlations among cognitive tests than a general factor. Examples of independent abilities are depicted in Figure 15.2. Figure 15.2: Depiction of Thurstone’s Theory of Intelligence (With Only Two Factors Depicted for Simplicity). 15.2.9 Raymond Cattell (1905–1998) Raymond Cattell proposed splitting “g” into two factors: fluid intelligence (\\(G_f\\)) and crystallized intelligence (\\(G_c\\)), as depicted in Figure 15.3. Fluid intelligence (\\(G_f\\)) was considered to be the capacity to respond adaptively in novel situations. It was thought to be based on neural integrity (versus noise), neural connectedness, processing speed, memory size, spatial processing, abstract reasoning, and one’s capacity for novel learning. Crystallized intelligence (\\(G_c\\)) was considered to be preserved with age and was based on life experience (e.g., quality of schools, number of words spoken to child, etc.), history, learned skills, cultural knowledge, verbal skills, and vocabulary. Crystallized intelligence could be further sub-divided into historical \\(G_c\\) and current \\(G_c\\). Historical \\(G_c\\) was considered what a person had learned prior to receiving specialized training or practice. Current \\(G_c\\) was considered the domains of expert knowledge and skills. Most assessment tests assess historical \\(G_c\\) rather than current \\(G_c\\). Tests include both \\(G_f\\) and \\(G_c\\), but we do not know how much. Figure 15.3: Depiction of Cattell’s \\(G_f\\)-\\(G_c\\) Theory of Intelligence. \\(G_f\\) = fluid intelligence; \\(G_c\\) = crystallized intelligence. Fluid intelligence was hypothesized by Cattell to be one’s inborn and physiologically based cognitive processing capacity which develops rapidly in childhood and is then invested in developing crystallized skills. Fluid intelligence was thought to be necessary for acquiring knowledge that becomes crystallized intelligence. Fluid intelligence increases with age from childhood to adulthood, peaks in the early 20s, and decreases in middle and older age. Fluid intelligence is more likely to be impaired (compared to crystallized intelligence) following brain trauma. Crystallized intelligence is theoretically expected to continue expanding with age, but historical \\(G_c\\) does seem to peak around late 30s. Age-related changes in crystallized and fluid intelligence are depicted in Figure 15.4. Given extensive age-related changes in intelligence (including fluid and crystallized intelligence), performance needs to be compared to same-aged peers. Figure 15.4: Cross-sectional aging data showing behavioral performance on measures of speed of processing (ie, Digit Symbol, Letter Comparison, Pattern Comparison), working memory (ie, Letter rotation, Line span, Computation Span, Reading Span), Long-Term Memory (ie, Benton, Rey, Cued Recall, Free Recall), and world knowledge (ie, Shipley Vocabulary, Antonym Vocabulary, Synonym Vocabulary). Almost all measures of cognitive function (fluid intelligence) show decline with age, except world knowledge (crystallized intelligence), which may even show some improvement. (Figure reprinted from Park &amp; Bischof (2013), Figure 1, p. 111. Park, D. C., &amp; Bischof, G. N. (2013). The aging mind: neuroplasticity in response to cognitive training. Dialogues in Clinical Neuroscience, 15(1), 109–119. https://doi.org/10.31887/DCNS.2013.15.1/dpark Copyright (c) Journal World Federation of Societies of Biological Psychiatry, reprinted by permission of Taylor &amp; Francis Ltd, https://www.tandfonline.com on behalf of (c) Journal World Federation of Societies of Biological Psychiatry. No license is provided to any third party permission to reproduce this copyrighted material.) 15.2.10 John Horn (1926–2006) John Horn extended Cattell’s \\(G_f\\)-\\(G_c\\) model to include 10 broad abilities, including: \\(G_f\\): fluid reasoning (fluid intelligence) \\(G_c\\): comprehension-knowledge (crystallized intelligence) \\(G_a\\): auditory processing \\(G_{lr}\\): long-term storage and retrieval \\(G_q\\): quantitative knowledge \\(G_{rw}\\): reading and writing ability \\(G_s\\): processing speed \\(G_{sm}\\): short-term memory \\(G_t\\): reaction time/decision speed \\(G_v\\): visual processing 15.2.11 John Carroll (1916–2003) John Carroll proposed a three-stratum hierarchical model that included narrow abilities (stratum 1), broad abilities (stratum 2), and general abilities (stratum 3), as depicted in Figure 15.5. Figure 15.5: Depiction of the Cattell-Horn-Carroll Hierarhical Theory of Intelligence. This model of intelligence became known as the Cattell-Horn-Carroll (CHC) model, which is now the most prominent contemporary theory of intelligence. 15.2.12 David Wechsler (1896–1981) David Wechsler developed the Wechsler Adult Intelligence Scale (WAIS) and Wechsler Intelligence Scale for Children (WISC), which are among the most widely used intelligence tests. These tests sub-divide intelligence into verbal IQ and performance IQ, along with more narrowly defined abilities. 15.3 Alternative Conceptualizations of Intelligence As noted earlier, the CHC model (Figure 15.5) is the most prominent contemporary theory of intelligence. However, there are alternative conceptualizations of intelligence. One alternative conceptualization of intelligence is the bifactor model, which is depicted in Figure 15.6. In a bifactor model, “g” is modeled in a non-hierarchical fashion where it is extracted first, and the primary factors are extracted separately from the remaining common variance among the tests. In this model, the resulting factors are independent of variance due to “g”. Variation in performance on subtests can be due to g and/or to primary factors, which are independent. Evidence has provided support for the bifactor model of intelligence when fit to several intelligence measures, though the structure often depends on the measure used (Dombrowski et al., 2021). Figure 15.6: Depiction of Bifactor Model of Intelligence. 15.4 Purposes of Intelligence Tests Intelligence tests have multiple purposes, though these uses are limited. Scores on intelligence tests have been shown to predict future academic achievement. They can also be helpful for characterizing functional impairment. For instance, they can be helpful for academic planning—i.e., whether a person is gifted or in need of remediation. Intelligence tests can also be helpful for identifying change in functioning over time, for example that may be related to dementia or brain injury. Scores on intelligence tests can also influence approaches to clinical assessment. Many assessment strategies assume average intelligence for question comprehension. A clinician might use assessments with more basic instructions or nonverbal assessments for those with poorer intelligence. Moreover, intelligence scores can influence the approaches to treatment, including the vocabulary used by the clinician, the extent of use of written material, and the clinician’s expectations that the client engages in abstract reasoning. For example, a clinician might not focus on the cognitive side of therapy in cognitive-behavior therapy; instead, they might focus more on the behavioral side of cognitive-behavior therapy: exposure therapy for anxiety and behavioral activation for depression. 15.5 Intelligence Versus Achievement Versus Aptitude In theory, intelligence is what you “have” (in terms of ability), whereas achievement is “what you do with it”. Aptitude is what you will be able to do with maximum training and opportunity. Tests that are intended to assess intelligence include the Wechsler Adult Intelligence Scale (WAIS), Wechsler Intelligence Scale for Children (WISC), Wechsler Abbreviated Scale of Intelligence (WASI), Wechsler Preschool and Primary Scale of Intelligence (WPPSI), Woodcock-Johnson Tests of Cognitive Abilities, Stanford–Binet Intelligence Scale, Differential Ability Scales (DAS), and Kaufman Brief Intelligence Test (KBIT). Tests that are intended to assess achievement include the Wechsler Individual Achievement Test (WIAT), Wide Range Achievement Test (WRAT), and Woodcock-Johnson Tests of Achievement. Tests that are intended to assess aptitude include the Graduate Record Examination (GRE), Scholastic Aptitude Test (SAT), American College Testing (ACT), Graduate Management Admission Test (GMAT), and Law School Admission Test (LSAT). Achievement presumably depends on both intelligence and aptitude, as well as motivation and effort. In reality, these constructs are difficult to differentiate at a measurement level. Assessment instruments that are intended to assess one of these likely assess elements of all of these. 15.6 Theory Influences Intepretation of Scores Despite the considerable research on intelligence, it is still difficult to define intelligence. What is intelligence? According to the 1994 academic workgroup, “Mainstream Science on Intelligence”, intelligence “involves the ability to reason, plan, solve problems, think abstractly, comprehend complex ideas, learn quickly, and learn from experience” (Arvey et al., 1994; L. S. Gottfredson, 1997). Some of the most widely used assessments of intelligence lack a clear theoretical foundation. They are more based on psychometrics than theory. Thus, it has been argued that “intelligence is what intelligence tests measure” (Boring, 1923). Theory plays an important role in interpretation of scores from tests. With the Iowa Tests of Basic Skills (ITBS) and other tests, they have divergent goals from traditional intelligence tests: evaluating schools as opposed to evaluating children. For example, some stakeholders may be more interested in how much learning occurs in particular schools rather than merely a person’s level of ability. Thus, the content that comprises “intelligence” differs depending on the purpose of assessment. You would also prioritize different predictive criteria depending on the goal, with items and subtests selected accordingly. For example, predictive criteria could focus on school performance versus job performance versus life skills. 15.7 Time-Related Influences Individual differences in intelligence scores tend to be relatively stable as people age, despite changes in absolute performance. Nevertheless, there are time-related influences on intelligence scores. One time-related influence on intelligence scores is called the Flynn effect. The Flynn effect is the phenomenon that there are age cohort differences in IQ scores such that IQ scores rise around 3 points every decade; younger generations perform better on intelligence tests than older generations, when they were at the same age. So your children’s generation is likely to be more intelligent than your generation, at least as assessed with the currently available instruments. 15.8 Concerns with Intelligence Tests Despite the utility of intelligence tests, there are numerous concerns with intelligence tests. For instance, there are concerns about the content validity of intelligence tests. That is, there are concerns that intelligence tests do not assess all important aspects of the construct of intelligence. Intelligence tests typically do not assess creativity, emotional intelligence, empathy, etc. Another concern is that intelligence tests encourage participants to put forth maximum mental effort. However, the typical mental effort needed to solve problems may be more important for many assessment questions than what someone can answer with maximum mental effort. Fluid intelligence is most likely to be affected by effort and attention. Another concern is that scores can be influenced by factors other than intelligence. For instance, IQ scores may be affected, at least indirectly, by socioeconomic status and educational background. IQ scores are moderately associated with socioeconomic status and level of education. IQ scores might also be influenced by cultural background (due to the potential for intelligence tests to have cultural biases), motivation, intimidation or anxiety, and the physical testing conditions. There are also concerns about the predictive utility of intelligence tests. Scores from intelligence tests have been shown to predict some aspects of functioning, but only in limited ways. For instance, IQ scores have been shown to predict grades in K–12 school, number of school years completed, academic performance through freshman year of college, and whether someone obtains a job. However, there are limits to what IQ scores can predict and the (weak) magnitude in which they predict these phenomena. Moreover, IQ scores do not appear to predict how successfully one performs in their job, the quality of their interpersonal relationships, or their happiness. 15.9 Aptitude Testing The primary origin of considerations of the predictive utility of aptitude testing was World War I. Robert Ladd Thorndike (1910–1990) developed a test to assess aptitude for becoming a pilot. The tests examined individual differences in keeping track of all things to predict who will be a good pilot—including skills involving stress, vision, attention, reaction time, working memory, navigation, being upside down, etc. Intelligence and aptitude testing took off from there. McClelland (1973; 1994) examined the data on aptitude predicting outcomes. He examined the question of whether grades and intelligence tests predict life and occupation success. He says no—or at least, intelligence does not influence life and occupation success, but rather intelligence opens up access to education, which serves as an important credential for access to good jobs. That is, McClelland argued that there is criterion contamination in the evaluation of criterion-related validity of intelligence tests. He argued that intelligence and aptitude tests are culturally and socioeconomically biased. That is, the associations between intelligence test scores and job scores that have been observed in prior research may be an artifact given that socioeconomic status (SES) influences both. Others argue that IQ scores do predict later life and occupation success, and that they have shown validity across groups and SES levels—i.e., that the association is not an artifact of SES. Moreover, others have argued that the observed associations are weaker than true associations because of restriction of range. For example, students with lowest SAT/GRE scores are typically not admitted, so one cannot examine their outcomes in association with the full range of SAT/GRE scores because we do not have access to outcomes for the people who are not admitted. McClelland argues that, instead of performing aptitude tests, we should look at life characteristics and job-related competencies. To understand McClelland’s position, it can be helpful to distinguish between signs and samples. Signs are instances where human behavior is viewed as a sign of some underlying characteristic. For instance, the Rorschach Inkblot Test is all sign and no sample. Psychodynamic theorists view responses as signs of underlying personality dynamics. By contrast, samples focus on operations (behaviors). Samples come from a behavioral perspective in which behaviors are just what they are; no underlying causes are inferred—i.e., there is less inference for samples than for signs. Examples of a sample for an occupation as a pilot could involve evaluating how well the person flies an airplane. A sample from a driving test would be to literally sample driving behavior. These are examples of criterion sampling. Criterion sampling involves sampling of the behavior in the area of interest to use as the criterion. For example, criterion sampling could involve assessing a sample of abilities relevant for a position (i.e., competency testing) based on a job analysis of the duties and responsibilities of a particular job and the skills and abilities necessary to perform the job well. McClelland’s goal is to use the criterion sample to reduce bias for minorities. However, using a criterion sample actually increased bias. McClelland was interested in shifting the field from using signs to using samples. McClelland argued that use of intelligence tests for job selection involves too much inference about an underlying ability—i.e., the test is used as a sign rather than as a sample. McClelland would argue that, instead, employers should do testing for competencies such as communication skills, goal setting, ego development, and patience. However, he does not say how to assess these skills. He would argue that it is important to sample multiple aspects of the construct of interest; a wider array of talents—not just academic performance. For a relevant criterion sample of competencies for a particular job or position, you need strong theory. However, criterion sampling is very difficult. Consider tests for selection of firefighters. Evaluative tests for selecting firefighters include fitness tests, among other tests. Fitness tests may include activities like running with heavy objects, climbing ladders, and willingness to go into burning buildings—skills which translate to the activities that many firefighters perform on the job. However, these fitness tests have been shown to lead to adverse impact toward women—i.e., they result in lower selection rates of women than men. The fitness tests are also biased in favor of people who had practiced, and who had previous training. Criterion sampling without pre-training makes things worse. You could lead bootcamps of pre-training to level the playing field for job applicants and then conduct testing. However, pre-training takes time and money, and many organizations may not be willing to do that. Range restriction leads to artifactually lower validity estimates of intelligence/aptitude tests with school and job performance. After accounting for range restriction, predictive validity coefficients are around .5 with school and job performance. Thus, many intelligence/aptitude tests tend to show some predictive validity for academic/job performance. However, it is also important to consider whether test bias exists. We discuss test bias in the next chapter. 15.10 Scales IQ has had various scales. 15.10.1 Ratio IQ Ratio IQ is mental age divided by chronological age, known as the intelligence quotient (see Equation (15.1)). Ratio IQ changes with age, and it is not widely used today. Deviation IQ is more widely used today. 15.10.2 Deviation IQ Deviation IQ (also called Standard Scores) is a standardized norm-referenced score with a mean of 100 and a standard deviation of 15. Deviation IQ was developed by David Wechsler. Scores are norm-referenced to same-aged peers. It has no reference to mental age. Using Deviation IQ scores, intellectual disability is defined as a score of 70 or below (along with functional impairment) because the score represents two standard deviations below the mean. However, this cutoff is based on an arbitrary statistical decision and tradition. Giftedness is defined as a score of 130 or above, which is two standard deviations above the mean. 15.10.3 Nominal, Ordinal, Interval, or Ratio? Intelligence scores are ordinal or interval, not ratio. You cannot talk about someone as being twice as intelligent as someone else because we do not know what zero intelligence is or what the maximum intelligence is. 15.11 Conclusion Intelligence has been defined in many ways, and intelligence assessment has a long history. The assessment of intelligence is considered one of the key successes in psychology. The most prominent contemporary theory of intelligence is the CHC model, which is a three-stratum hierarchical model that includes narrow abilities (stratum 1), broad abilities (stratum 2), and general abilities (stratum 3). Assessments of intelligence have utility for various purposes, including predicting future academic achievement, characterizing functional impairment, and identifying change in functioning over time. However, there are many things that intelligence does not predict well, such as how successfully one will perform in their job, the quality of their interpersonal relationships, or their happiness. Moreover, there are potential weaknesses of intelligence tests, including content validity because they do not assess creativity or emotional aspects of intelligence such as empathy. 15.12 Suggested Readings Ackerman (2013) References Ackerman, P. L. (2013). Assessment of intellectual functioning in adults. In K. F. Geisinger, J. F. Carlson, J.-I. C. Hansen, N. R. Kuncel, S. P. Reise, &amp; M. C. Rodriguez (Eds.), APA handbook of testing and assessment in psychology, Vol 2: Testing and assessment in clinical and counseling psychology (pp. 119–132). American Psychological Association. Arvey, R. D., Bouchard, T. J., Carroll, J. B., Cattell, R. B., Cohen, D. B., Dawis, R. V., Detterman, D. K., Dunnette, M., Eysenck, H., Feldman, J. M., Fleishman, E. A., Gilmore, G. C., Gordon, R. A., Gottfredson, L. S., Greene, R. L., Haier, R. J., Hardin, G., Hogan, R., Horn, J. M., … Willerman, L. (1994). Mainstream science on intelligence. Wall Street Journal, 13(1), 18–25. Boring, E. G. (1923). Intelligence as the tests test it. New Republic, 36, 35–37. Dombrowski, S. C., McGill, R. J., &amp; Morgan, G. B. (2021). Monte Carlo modeling of contemporary intelligence test (IQ) factor structure: Implications for IQ assessment, interpretation, and theory. Assessment, 28(3), 977–993. https://doi.org/10.1177/1073191119869828 Gottfredson, L. S. (1997). Mainstream science on intelligence: An editorial with 52 signatories, history, and bibliography. Intelligence, 24(1), 13–23. McClelland, D. C. (1973). Testing for competence rather than for “intelligence.” American Psychologist, 28, 1–14. https://doi.org/10.1037/h0034092 McClelland, D. C. (1994). The knowledge-testing-educational complex strikes back. American Psychologist, 49(1), 66–69. https://doi.org/10.1037/0003-066X.49.1.66 Park, D. C., &amp; Bischof, G. N. (2013). The aging mind: Neuroplasticity in response to cognitive training. Dialogues in Clinical Neuroscience, 15(1), 109–119. https://doi.org/10.31887/DCNS.2013.15.1/dpark Sechrest, L., Stickle, T. R., &amp; Stewart, M. (1998). The role of assessment in clinical psychology. In A. Bellack, M. Hersen, &amp; C. R. Reynolds (Eds.), Comprehensive clinical psychology, Vol. 4: Assessment. Pergamon. "],["bias.html", "Chapter 16 Test Bias 16.1 Overview of Bias 16.2 Ways to Investigate/Detect Test Bias 16.3 Examples of Bias 16.4 Test Fairness 16.5 Correcting For Bias 16.6 Getting Started 16.7 Examples of Unbiased Tests (in Terms of Predictive Bias) 16.8 Predictive Bias: Different Regression Lines 16.9 Differential Item Functioning (DIF) 16.10 Measurement/Factorial Invariance 16.11 Conclusion 16.12 Suggested Readings 16.13 Exercises", " Chapter 16 Test Bias 16.1 Overview of Bias There are multiple definitions of the term “bias” depending on the context. In general, bias is a systematic error (Reynolds &amp; Suzuki, 2012). Mean error is an example of systematic error, and is sometimes called bias. Cognitive biases are systematic errors in thinking, including confirmation bias and hindsight bias. Method biases are a form of systematic error that involve the influence of measurement on a person’s score that is not due to the person’s level on the construct. Method biases include response biases or response styles, including acquiescence and social desirability bias. Attentional bias refers to the tendency to process some types of stimuli more than others. Sometimes bias is used to refer in particular to systematic error (in measurement, prediction, etc.) as a function of group membership, where test bias refers to the same score having different meaning for different groups. Under this meaning, a test is unbiased if a given test score has the same meaning regardless of group membership. For example, a test is biased if there is differential validity of test scores for groups (e.g., age, education, culture, race, sex). Test bias would exist, for instance, if a test is a less valid predictor for racial minorities or linguistic minorities. Test bias would also exist if scores on the Scholastic Aptitude Test (SAT) under-estimate women’s grades in college, for instance. There are some known instances of test bias, as described in Section 16.3. Research has not produced much empirical evidence of test bias (Brown et al., 1999; Hall et al., 1999; Jensen, 1980; Kuncel &amp; Hezlett, 2010; Reynolds et al., 2021; Reynolds &amp; Suzuki, 2012; Sackett et al., 2008; Sackett &amp; Wilk, 1994), though some item-level bias is not uncommon. Moreover, where test bias has been observed, it is often small, unclear, and does not always generalize (N. S. Cole, 1981). However, just because there is not much empirical evidence of test bias does not mean that test bias does not exist. Moreover, just because a test does not show bias does not mean that it should be used. Furthermore, just because a test does not show bias does not mean that there are not race-, social class-, and gender-related biases in clinical judgment during the assessment process. It is also worth pointing out that group differences in scores do not necessarily indicate bias. Group differences in scores could reflect true group differences in the construct. For instance, women have better verbal abilities, on average, compared to men. So, if women’s scores on a verbal ability test are higher on average than men’s scores, this would not be sufficient evidence for bias. There are two broad categories of test bias: predictive bias test structure bias Predictive bias refers to differences between groups in the relation between the test and criterion. As with all criterion-related validity tests, the findings depend on the strength and quality of the criterion. Test structure bias refers to differences in the internal test characteristics across groups. 16.2 Ways to Investigate/Detect Test Bias 16.2.1 Predictive Bias Predictive bias exists when differences emerge between groups in terms of predictive validity to a criterion. It is assessed by using a regression line looking at the association between test score and job performance. For instance, consider a 2x2 confusion matrix used for the standard prediction problem. A confusion matrix for whom to select for a job is depicted in Figure 16.1. Figure 16.1: 2x2 Confusion Matrix for Job Selection. TP = true positive; TN = true negative; FP = false positive; FN = false negative. We can also visualize the confusion matrix in terms of a scatterplot of the test scores (i.e., predicted job performance) and the “truth” scores (i.e., actual job performance), as depicted in Figure 16.2. The predictor (test score) is on the x-axis. The criterion (job performance) is on the y-axis. The quadrants reflect the cutoffs (i.e., thresholds) imposed from the 2x2 confusion matrix. The vertical line reflects the cutoff for selecting someone for a job. The horizontal line reflects the cutoff for good job performance (i.e., people who should have been selected for the job). Figure 16.2: 2x2 Confusion Matrix for Job Selection in the Form of a Graph With Predicted Performance on the x-Axis and Actual Job Performance on the y-Axis. TP = true positive; TN = true negative; FP = false positive; FN = false negative. The data points in the top right quadrant are true positives: people who the test predicted would do a good job and who did a good job. The data points in the bottom left quadrant are true negatives: people who the test predicted would do a poor job and who would have done a poor job. The data points in the bottom right quadrant are false positives: people who the test predicted would do a good job and who did a poor job. The data points in the top left quadrant are false negatives: people who the test predicted would do a poor job and who would have done a good job. Figure 16.3 depicts a strong predictor. The best-fit regression line has a steep slope where there are lots of data points that are true positives and true negatives, with relatively few false positives and false negatives. Figure 16.3: Example of a Strong Predictor. TP = true positive; TN = true negative; FP = false positive; FN = false negative. Figure 16.4 depicts a poor predictor. The best-fit regression line has a shallow slope where there are just as many data points that are in the false cells (false positives and false negatives) as there are in the true cells (true positives and true negatives). In general, the steeper the slope, the better the predictor. Figure 16.4: Example of a Poor Predictor. TP = true positive; TN = true negative; FP = false positive; FN = false negative. We can evaluate predictive bias using a best-fit regression line between the predictor and criterion for each group. 16.2.1.1 Types of Predictive Bias There are three types of predictive bias: Different slopes Different intercepts Different intercepts and slopes The slope of the regression line is the steepness of the line. The intercept of the regression line is the y-value of the point where the line crosses the y-axis (i.e., when \\(x = 0\\)). If a measure shows predictive test bias, when looking at the regression line for each group, the groups’ regression lines differ in either slopes and/or intercepts. 16.2.1.1.1 Different Slopes Predictive bias in terms of different slopes exists when there are differences in the slope of the regression line between minority and majority groups. The slope describes the direction and steepness of the regression line. The slope of a regression line is the amount of change in \\(y\\) for every unit change in \\(x\\) (i.e., rise over run). Differing slopes indicate differential predictive validity, in which the test is a more effective predictor of performance in one group over the other. Different slopes predictive bias is depicted in Figure 16.5. In the figure, the predictor performs well in the majority group. However, the slope is close to zero in the minority group, indicating that there is no association between the predictor and the criterion for the minority group. Figure 16.5: Test Bias: Different Slopes. TP = true positive; TN = true negative; FP = false positive; FN = false negative. Different slopes can especially occur if we develop our measure and criterion based on the normative majority group. Not much evidence has found empirical evidence of different slopes across groups. However, samples often do not have the power to detect differing slopes (Aguinis et al., 2010). Theoretically, to fix biases related to different slopes, you should find another measure that is more predictive for the minority group. If the predictor is a strong predictor in both groups but shows slight differences in the slope, within-group norming could be used. 16.2.1.1.2 Different Intercepts Predictive bias in terms of different intercepts exists when there are differences in the intercept of the regression line between minority and majority groups. The \\(y\\)-intercept describes the point on the \\(y\\)-axis that the line intersects with the \\(y\\)-axis (when \\(x = 0\\)). When the distributions have similar slopes, intercept differences suggest that the measure systematically under- or over-estimates group performance relative to the person’s ability. The same test score leads to systematically different predictions for the majority and minority groups. In other words, minority group members get different tests scores than majority group members with the same ability. Different intercepts predictive bias is depicted in Figure 16.6. Figure 16.6: Test Bias: Different Intercepts. TP = true positive; TN = true negative; FP = false positive; FN = false negative. A higher intercept (relative to zero) indicates that the measure under-estimates a person’s ability (at that test score)—i.e., the person’s job performance is better than what the test score would suggest. A lower intercept (relative to zero) indicates that the measure over-estimates a person’s ability (at that test score)—i.e., the person’s job performance is worse than what the test score would suggest. Figure 16.6 indicates that the measure systematically under-estimates the job performance of the minority group. Performance among members of a minority group could be under- or over-estimated. For example, historically, women’s grades in math and engineering classes tended to be under-estimated by the Scholastic Aptitude Test [SAT; M. J. Clark &amp; Grandy (1984)]. However, where intercept differences have been observed, measures often show small over-estimation of school and job performance among minority groups (Reynolds &amp; Suzuki, 2012). For example, women’s physical strength and endurance is over-estimated based on physical ability tests (Sackett &amp; Wilk, 1994). In addition, over-estimation of African Americans’ and Hispanics’ school and job performance has been observed based on cognitive ability tests (N. S. Cole, 1981; Reynolds &amp; Suzuki, 2012; Sackett et al., 2008; Sackett &amp; Wilk, 1994). At the same time, the Black–White difference in job performance is less than the Black–White difference in test performance. The over-prediction of lower-scoring groups is likely mostly an artifact of measurement error (L. S. Gottfredson, 1994). The over-estimation of African Americans’ and Hispanics’ school and job performance may be due to measurement error in the tests. Moreover, test scores explain only a portion of the variation in job performance. Black people are far less disadvantaged on the noncognitive determinants of job performance than on the cognitive ones. Nevertheless, the over-estimation that has been often observed is on average—the performance is not over-estimated for all individuals of the groups even if there is an average over-estimation effect. In addition, simulation findings indicate that lower intercepts (i.e., over-estimation) among minority groups compared to majority groups could be observed if there are different slopes but not different intercepts in the population, because different slopes are likely to go undetected due to low power (Aguinis et al., 2010). That is, if a test shows weaker validity for a minority group than the majority group, it could appear as different intercepts that favor the minority group when, in fact, it reflects shallower slopes of the minority group that go undetected. Predictive biases in intercepts could especially occur if we develop tests that are based on the majority group, and the items assess constructs other than the construct of interest which are systematically biased in favor of the majority group or against the minority group. Arguments about reduced power to detect differences are less relevant for intercepts and means than for slopes. To correct for a bias in intercepts, we could add bonus points to the scores for the minority group to correct for the amount of the systematic error, and to result in the same regression line. But if the minority group is over-predicted (as has often been the case where intercept differences have been observed), we would not want to use score adjustment to lower the minority group’s scores. 16.2.1.1.3 Different Intercepts and Slopes Predictive bias in terms of different intercepts and slopes exists when there are differences in the intercept and slope of the regression line between minority and majority groups. In cases of different intercepts and slopes, there is both differential validity (because the regression lines have different slopes), as well as varying under- and over-estimation of groups’ performance at particular scores. Different intercepts and slopes predictive bias is depicted in Figure 16.7. Figure 16.7: Test Bias: Different Intercepts and Slopes. TP = true positive; TN = true negative; FP = false positive; FN = false negative. In instances of different intercepts and slopes predictive bias, a measure can simultaneously over-estimate and under-estimate a person’s ability at different test scores. For instance, a measure can under-estimate a person’s ability at higher test scores and can over-estimate a person’s ability at lower test scores. Different intercepts and slopes across groups is possibly more realistic than just different intercepts or just different slopes. However, different intercepts and slopes predictive bias is more complicated to study, represent, and resolve. It is difficult to examine because of complexity, and it is not easy to fix. Currently, we have nothing to address different intercepts and slopes predictive bias. We would need to use a different measure or measures for each group. 16.2.2 Test Structure Bias In addition to predictive bias, another type of test bias is test structure bias. Test structure bias involves differences in internal test characteristics across groups. Examining test structure bias is different from examining the total score, as is used when examining predictive bias. Test structure bias can be identified empirically or based on theory/judgment. Empirically, test structure bias can be examined in multiple ways. 16.2.2.1 Empirical Approaches to Identification 16.2.2.1.1 Item \\(\\times\\) Group tests (ANOVA) Item \\(\\times\\) Group tests in analysis of variance (ANOVA) examine whether the difference between groups on the overall score match comparisons among smaller items sets between groups. Item \\(\\times\\) Group tests are used to rule out that items are operating in different ways in different groups. If the items operate in different ways in different groups, they do not have the same meaning across groups. For example, if we are going to use a measure for multiple groups, we would expect its items to operate similarly across groups. So, if women show higher scores on a depression measure compared to men, would also expect them to show similar elevations on each item (e.g., sleep loss). 16.2.2.1.2 Item Response Theory Using item response theory, we can examine differential item functioning (DIF). Evidence of DIF, indicates that there are differences between group in terms of discrimination and/or difficulty/severity of items. Differences between groups in terms of the item characteristic curve (which combines the item’s discrimination and severity) would be evidence against construct validity invariance between the groups and would provide evidence of bias. DIF examines stretching and compression of different groups. As an example, consider the item “bites others” in relation to externalizing problems. The item would be expected to show a weaker discrimination and higher severity in adults compared to children. DIF is discussed in Section 16.9. 16.2.2.1.3 Confirmatory Factor Analysis Confirmatory factor analysis allows tests of measurement invariance (also called factorial invariance). Measurement invariance examines whether the factor structure of the underlying latent variables in the test is consistent across groups. It also examines whether the manifestation of the construct differs between groups. Measurement invariance is discussed in Section 16.10. Even if you find the same slope and intercepts across groups in a prediction model, the measure would still be assessing different constructs across groups if the measure has a different factor structure between the groups. A different factor structure across groups is depicted in Figure 16.8. Figure 16.8: Different Factor Structure Across Groups. An example of a different factor structure across groups is the differentiation of executive functions from two factors to three factors (inhibition, working memory, cognitive flexibility) across childhood (Lee et al., 2013). There are different degrees of measurement invariance (for a review, see Putnick &amp; Bornstein, 2016): Configural invariance: same number of factors in each group, and which indicators load on which factors are the same in each group (i.e., the same pattern of significant loadings in each group). Metric (“weak factorial”) invariance: items have the same factor loadings (discrimination) in each group. Scalar (“strong factorial”) invariance: items have the same intercepts (difficulty/severity) in each group. Residual (“strict factorial”) invariance: items have the same residual/unique variances in each group. 16.2.2.1.4 Structural Equation Modeling Structural equation modeling is a confirmatory factor analysis (CFA) model that incorporates prediction. Structural equation modeling allows examining differences in the underlying structure with differences in prediction in the same model. 16.2.2.1.5 Signal Detection Theory Signal detection theory is a dynamic measure of bias. It allows examining the overall bias in selection systems, including both accuracy and errors at various cutoffs (sensitivity, specificity, positive predictive value, and negative predictive value), as well as accuracy across all possible cutoffs (the area under the receiver operating characteristic curve). While there may be similar predictive validity between groups, the type of errors we are making across groups might differ. It is important to decide which types of error to emphasize depending on the fairness goals and examining sensitivity/specificity to adjust cutoffs. 16.2.2.1.6 Empirical Evidence of Test Structure Bias It is not uncommon to find items that show differences across groups in severity (intercepts) and/or discrimination (factor loadings). However, cross-group differences in item functioning tend to be small and not consistent across studies, suggesting that some of the differences may reflect Type I errors that result from sampling error and multiple testing. That said, some instances of cross-group differences in item parameters could reflect test structure bias that is real and important to address. 16.2.2.2 Theoretical/Judgmental Approaches to Identification 16.2.2.2.1 Facial Validity Bias Facial validity bias considers the extent to which an average person thinks that an item is biased—i.e., the item has differing validity between minority and majority groups. If so, the item should be reconsidered. Does an item disfavor certain groups? Is the language specific to a particular group? Is it offensive to some people? This type of judgment moves into the realm of whether or not an item should be used. 16.2.2.2.2 Content Validity Bias Content validity bias is determined by judgments of construct experts who look for items that do not do an adequate job assessing the construct between groups. A construct may include some content facets in one group, but may include different content facets in another group, as depicted in Figure 16.9. Figure 16.9: Different Content Facets in a Given Construct for Two Groups. Examples include information questions and vocabulary questions on the Wechsler Adult Intelligence Scale. If an item is linguistically complicated, grammatically complex or convoluted, or a double negative, it may be less valid or predictive for rural populations and those with less education. Also, stereotype threat may contribute to content validity bias. Stereotype threat occurs when people are or feel at risk of conforming themselves to stereotypes about their social group, thus leading them to show poorer performance in ways that are consistent with the stereotype. Stereotype threat may partially explain why some women may perform more poorly on some math items than some men. Another example of content validity bias is when the same measure is used to assess a construct across ages even though the construct shows heterotypic continuity. Heterotypic continuity occurs when a construct changes in its behavioral manifestation with development (Petersen et al., 2020). That is, the same construct may look different at different points in development. An example of a construct that shows heterotypic continuity is externalizing problems. In early childhood, externalizing problems often manifest in overt forms, including physical aggression (e.g., biting) and temper tantrums. By contrast, in adolescence and adulthood, externalizing problems more often manifest in covert ways, including relational aggression and substance use. Content validity and facial validity bias judgments are often related, but not always. 16.3 Examples of Bias As described in the overview in Section 16.1, there is not much empirical evidence of test bias (Brown et al., 1999; Hall et al., 1999; Jensen, 1980; Kuncel &amp; Hezlett, 2010; Reynolds et al., 2021; Reynolds &amp; Suzuki, 2012; Sackett et al., 2008; Sackett &amp; Wilk, 1994). That said, some item-level bias is not uncommon. One instance of test bias is that, historically, women’s grades in math and engineering classes tended to be under-estimated by the Scholastic Aptitude Test [SAT; M. J. Clark &amp; Grandy (1984)]. Fernández &amp; Abe (2018) review the evidence on other instances of test and item bias. For instance, test bias can occur if a subgroup is less familiar with the language, the stimulus material, or the response procedures, or if they have different response styles. In addition to test bias, there are known patterns of bias in clinical judgment, as described in Section 25.3.11. 16.4 Test Fairness There is interest in examining more than just the accuracy of measures. It is also important to examine the errors being made and differentiate the weight or value of different kinds of errors (and correct decisions). Consider an example of an unbiased test, as depicted in Figure 16.10, adapted from L. S. Gottfredson (1994). Although the example is of a White group and a Black group, we could substitute any two groups into the example (e.g., males versus females). Figure 16.10: Potential Unfairness in Testing. The ovals represent the distributions of individuals’ performance both on a test and a job performance criterion. TP = true positive; TN = true negative; FP = false positive; FN = false negative. (Adapted from L. S. Gottfredson (1994), Figure 1, p. 958. Gottfredson, L. S. (1994). The science and politics of race-norming. American Psychologist, 49(11), 955–963. https://doi.org/10.1037/0003-066X.49.11.955) The example is of an unbiased test between White and Black job applicants. There are no differences between the two groups in terms of slope. If we drew a regression line, the line would go through the centroid of both ovals. Thus, the measure is equally predictive in both groups even though that the Black group failed the test at a higher rate than the White group. Moreover, there is no difference between the groups in terms of intercept. Thus, the performance of one group is not over-estimated relative to the performance of the other group. To demonstrate what a different intercept would look like, Group X shows a different intercept. In sum, there is no predictive validity bias between the two groups. But just because the test predicts just as well in both groups does not mean that the selection procedures are fair. Although the test is unbiased, there are differences in the quality of prediction: there are more false negatives in the Black group compared to the White group. This gives the White group an advantage and the Black group additional disadvantages. If the measure showed the same quality of prediction, we would say the test is fair. The point of the example is that just because a test is unbiased does not mean that the test is fair. There are two kinds of errors: false negatives and false positives. Each error type has very different implications. False negatives would be when the test predicts that an applicant would perform poorly and we do not give them the job even though they would have performed well. False negatives have a negative effect on the applicant. And, in this example, there are more false negatives in the Black group. By contrast, false positives would be when we predict that an applicant would do well, and we give them the job but they perform poorly. False positives are a benefit to the applicant but have a negative effect on the employer. In this example, there are more false positives in the White group, which is an undeserved benefit based on the selection ratio; therefore, the White group benefits. In sum, equal accuracy of prediction (i.e., equal total number of errors) does not necessarily mean the test is fair; we must examine the types of errors. Merely ensuring accuracy does not ensure fairness! 16.4.1 Adverse Impact Adverse impact is defined as rejecting members of one group at a higher rate than another group. Adverse impact is different from test validity. According to federal guidelines, adverse impact is present if the selection rate of one group is less than four-fifths (80%) the selection rate of the group with the highest selection rate. There is much more evidence of adverse impact than test bias. Indeed, disparate impact of tests on personnel selection across groups is the norm rather than the exception, even when using valid tests that are unbiased, which in part reflect group-related differences in job-related skills (L. S. Gottfredson, 1994). Examples of adverse impact include: physical ability tests, which produce substantial adverse impact against women (despite over-estimation of women’s performance), cognitive ability tests, which produce substantial impact against some ethnic minority groups, especially Black and Hispanic people (despite over-estimation of Black and Hispanic people’s performance), even though cognitive ability tests tend to be among the strongest predictors of job performance (Sackett et al., 2008; Schmidt &amp; Hunter, 1981), and personality tests, which produce higher estimates of dominance among men than women; it is unclear whether this has predictive bias. 16.4.2 Bias Versus Fairness Whether a measure is accurate or shows test bias is a scientific question. By contrast, whether a test is fair and thus should be used for a given purpose is not just a scientific question; it is also an ethical question. It involves the consideration of the potential consequences of testing in terms of social values and consequential validity. 16.4.3 Operationalizing Fairness There are many perspectives to what should be considered when evaluating test fairness (American Educational Research Association et al., 2014; Camilli, 2013; Committee on the General Aptitude Test Battery et al., 1989; Dorans, 2017; Fletcher et al., 2021; Gipps &amp; Stobart, 2009; Helms, 2006; Jonson &amp; Geisinger, 2022; Melikyan et al., 2019; Sackett et al., 2008; Thorndike, 1971; Zieky, 2006, 2013). As described in Fletcher et al. (2021), there are three primary ways of operationalizing fairness: Equal outcomes: the selection rate is the same across groups. Equal opportunity: the sensitivity (true positive rate; 1 \\(-\\) false negative rate) is the same across groups. Equal odds: the sensitivity is the same across groups and the specificity (true negative rate; 1 \\(-\\) false positive rate) is the same across groups. For example, the job selection procedure shows equal outcomes if the proportion of men selected is equal to the proportion of women selected. The job selection procedure shows equal opportunity if, among those who show strong job performance, the proportion of classification errors (false negatives) is the same for men and women. Receiver operating characteristic (ROC) curves are depicted for two groups in Figure 16.11. A cutoff that represents equal opportunity is depicted with a horizontal line (i.e., the same sensitivity) in Figure 16.11. The job selection procedure shows equal odds if (a), among those who show strong job performance, the proportion of classification errors (false negatives) is the same for men and women, and (b), among those who show poor job performance, the proportion of classification errors (false positives) is the same for men and women. A cutoff that represents equal odds is depicted where the ROC curve for Group A intersects with the ROC curve from Group B in Figure 16.11. The equal odds approach to fairness is consistent with a National Academy of Sciences committee on fairness (Committee on the General Aptitude Test Battery et al., 1989; L. S. Gottfredson, 1994). Approaches to operationalizing fairness in the context of prediction models are described by Paulus &amp; Kent (2020). Figure 16.11: Receiver Operating Characteristic (ROC) Curves for Two Groups. (Figure reprinted from Fletcher et al. (2021), Figure 2, p. 3. Fletcher, R. R., Nakeshimana, A., &amp; Olubeko, O. (2021). Addressing fairness, bias, and appropriate use of artificial intelligence and machine learning in global health. Frontiers in Artificial Intelligence, 3(116). https://doi.org/10.3389/frai.2020.561802) It is not possible to meet all three types of fairness simultaneously (i.e., equal selection rates, sensitivity, and specificity across groups) unless the base rates are the same across groups or the selection is perfectly accurate (Fletcher et al., 2021). In the medical context, equal odds is the most common approach to fairness. However, using the cutoff associated with equal odds typically reduces overall classification accuracy. And, changing the cutoff for specific groups can lead to negative consequences. In the case that equal odds results in a classification accuracy that is too low, it may be worth considering using separate assessment procedures/tests for each group. In general, it is best to follow one of these approaches to fairness. It is difficult to get right, so try to minimize negative impact. Many fairness supporters argue for simpler rules. In the 1991 Civil Rights Act, score adjustments based on race, gender, and ethnicity (e.g., within-race norming or race-conscious score adjustments) were made illegal in personnel selection (L. S. Gottfredson, 1994). Another perspective to fairness is that selection procedures should predict job performance and if they are correlated with any group membership (e.g., race, socioeconomic status, or gender), the test should not be used (Helms, 2006). That is, according to Helms, we should not use any test that assesses anything other than the construct of interest (job performance). Unfortunately, however, no measures like this exist. Every measure assesses multiple things, and factors such as poverty can have long-lasting impacts across many domains. Another perspective to fairness is to make the selection procedures equal the number of successes within each group (Thorndike, 1971). According to this perspective, if you want to do selection, you should hire all people, then look at job performance. If among successful employees, 60% are White and 40% are Black, then set this selection rate for each group (i.e., hiring 80% White individuals and 20% Black individuals is not okay). According to this perspective, a selection system is only fair if the majority–minority differences on the selection device used are equal in magnitude to majority–minority differences in job performance. Selection criteria should be made based on prior distributions of success rates. However, you likely will not ever really know the true base rate in these situations. No one uses this approach because you would have a period where you have to accept everyone to find the percent that works. Also, this would only work in a narrow window of time because the selection pool changes over time. There are lots of groups and subgroups. Ensuring fairness is very complex, and there is no way to accomplish the goal of being equally fair to all people. Therefore, do the best you can and try to minimize negative impact. 16.5 Correcting For Bias 16.5.1 What to Do When Detecting Bias When examining item bias (using differential item functioning/DIF or measurement non-invariance) with many items (or measures) across many groups, there can be many tests, which will make it likely that DIF/non-invariance will be detected, especially with a large sample. Some detected DIF may be artificial or trivial, but other DIF may be real and important to address. It is important to consider how you will proceed when detecting DIF/non-invariance. Considerations of effect size and theory can be important for evaluating the DIF/non-invariance and whether it is negligible or important to address. When detecting bias, there are several steps to take. First, consider what the bias indicates. Does the bias present adverse impact for a minority group? For what reasons might the bias exist? Second, examine the effect size of the bias. If the effects are small, if the bias does not present adverse impact for a minority group, and if there is no compelling theoretical reason for the bias, the bias might not be sufficient to scrap the instrument for the population. Some detected bias may be artificial, but other bias may be real. Gender and cultural differences have shown a number of statistically significant effects for a number of different assessment purposes, but many of the observed effects are quite small and likely trivial, and they do not present compelling reasons to change the assessment (Youngstrom &amp; Van Meter, 2016). However, if you find bias, correct for it! There are a number of score adjustment and non-score adjustment approaches to correct for bias, as described in Sections 16.5.2 and 16.5.3. If the bias occurs at the item level (e.g., test structure bias), it is generally recommended to remove or resolve items that show non-negligible bias. There are three primary options: (1) drop the item for both groups, (2) drop the item for one group but keep it for the other group, or (3) freely estimate the parameters for the item across groups. Addressing items that show larger bias can also reduce artificial bias in other items (Hagquist &amp; Andrich, 2017). Thus, researchers are encouraged to handle item bias sequentially from high to low in magnitude. If the bias occurs at the test score level (e.g., predictive bias), score adjustments may be considered. If you do not correct for bias, consider the impact of the test, procedure, and selection procedure when interpreting scores. Interpret scores with caution and provide necessary caveats in resulting papers or reports regarding the interpretations in question. In sum, it is important to examine the possibility of bias—it is important to consider how much “erroneous junk” you are introducing into your research. 16.5.2 Score Adjustment to Correct for Bias Score adjustment involves adjusting scores for a particular group or groups. 16.5.2.1 Why Adjust Scores? There may be several reasons to adjust scores for various groups in a given situation. First, there may be social goals to adjust scores. For example, we may want our selection device to yield personnel that better represent the nation or region, including diversity of genders, races, majors, social classes, etc. Score adjustments are typically discussed with respect to racial minority differences due to historical and systemic inequities. Our society aims to provide equal opportunity, including the opportunity to gain a fair share (i.e., proportional representation) of jobs. A diversity of perspectives in a job is a strength; a diversity of perspectives can lead to greater creativity and improved problem-solving. A second potential reason that we may want to apply score adjustment is to correct for bias. A third potential reason that we may want to apply score adjustment is to improve the fairness of a test. 16.5.2.2 Types of Score Adjustment There are a number of potential techniques that have been used in attempts to correct for bias, i.e., to reduce negative impact of the test on an under-represented group. What is considered an under-represented group may depend on the context. For instance, men are under-represented compared to women as nurses, preschool teachers, and college students. However, men may not face the same systemic challenges compared to women, so even though men may show under-representation in some domains, it is arguable whether scores should be adjusted to increase their representation. Techniques for score adjustment include: Bonus points Within-group norming Separate cutoffs Top-down selection from different lists Banding Banding with bonus points Sliding band Separate tests Item elimination based on group differences 16.5.2.2.1 Bonus Points Providing bonus points involves adding a constant number of points to the scores of all individuals who are members of a particular group with the goal of eliminating or reducing group differences. Bonus points is used to correct for predictive bias differences in intercepts between groups. An example of bonus points is military veterans in placement for civil service jobs—points are added to the initial score for all veterans (e.g., add 5 points to test scores of all veterans). An example of using bonus points as a score adjustment is depicted in Figure 16.12. Figure 16.12: Using Bonus Points as a Scoring Adjustment. There are several pros of bonus points. If the distribution of each group is the same, this will effectively reduce group differences. Moreover, it is a simple way of impacting test selection and procedure without changing the test, which is therefore a great advantage. There are several cons of bonus points. If there are differences in group standard deviations, adding bonus points may not actually correct for bias. The use of bonus points also obscures what is actually being done to scores, so other methods like using separate cutoffs may be more explicit. In addition, the simplicity of bonus points is also a great disadvantage because it is easily understood and often not viewed as “fair” because some people are getting extra points that others do not. 16.5.2.2.2 Within-Group Norming A norm is the standard of performance that a person’s performance can be compared to. Within-group norming treats the person’s group in the sample as the norm. Within-group norming converts an individual’s score to standardized scores (e.g., T scores) or percentiles within one’s own group. Then, the people are selected based on the highest standard scores across groups. Withing-group norming is used to correct for predictive bias differences in slopes between groups. An example of using within-group norming as a score adjustment is depicted in Figure 16.13. Figure 16.13: Using Within-Group Norming as a Scoring Adjustment. There are several pros of within-group norming. First, it accounts for differences in group standard deviations and means, so it does not have the same problem as bonus points and is generally more effective at eliminating adverse impact compared to bonus points. Second, some general (non-group-specific) norms are clearly irrelevant for characterizing a person’s functioning. Group-specific norms aim to describe a person’s performance relative to people with a similar background, thus potentially reducing cultural bias. Third, group-specific norms may better reflect cultural, educational, socioeconomic, and other factors that may influence a person’s score (Burlew et al., 2019). Fourth, group-specific norms may increase specificity, and reduce over-pathologizing by preventing giving a diagnosis to people who might not show a condition (Manly &amp; Echemendia, 2007). There are several cons of within-group norming. First, group differences could be maintained if one decides to norm based on a reference sample or, when scores are skewed, a local sample, especially when using standardized scores. However, percentile scores will consistently eliminate adverse impact. Second, using group-specific norms may obscure background variables that explain underlying reasons for group-related differences in test performance (Manly, 2005; Manly &amp; Echemendia, 2007). Third, group-specific norms do not address the problem if the measure shows test bias (Burlew et al., 2019). Fourth, group-specific norms may reduce sensitivity to detect conditions (Manly &amp; Echemendia, 2007). For instance, they may prevent people from getting treatment who would benefit. It is worth noting that within-group norming on the basis of sex, gender, and ethnicity is illegal for the basis of personnel selection according to the 1991 Civil Rights Act. As an example of within-group norming, the National Football League used to use race-norming for identification of concussions. The effect of race-norming, however, was that it lowered Black players’ concussion risk scores, which prevented many Black players from being identified as having sustained a concussion and from receiving needed treatment. Race-norming compared the Black football players cognitive test scores to group-specific norms: the cognitive test scores of Black people in the general population (not to common norms). Using Black-specific norms assumed that Black football players showed lower cognitive ability than other groups, so a low cognitive ability score for a Black player was less likely to be flagged as concerning. Thus, the race-specific norms led to lower identified rates of concussions among Black football players compared to White football players. Due to the adverse impact, Black players sued the National Football League, and the league stopped the controversial practice of race-norming for identification of concussion (https://www.washingtonpost.com/sports/2021/06/03/nfl-concussion-settlement-race-norming/; archived at https://perma.cc/KN3L-5Z7R). A common question is whether to use group-specific norms or common norms. Group-specific norms are a controversial practice, and the answer depends. If you are interested in a person’s absolute functioning (e.g., for determining whether someone is concussed or whether they are suitable to drive), recommendations are to use common norms, not group-specific norms (Barrash et al., 2010; Silverberg &amp; Millis, 2009). If, by contrast, you are interested in a person’s relative functioning compared to a specific group, within-group norming could make sense if there is an appropriate reference group. The question about which norms to use are complex, and psychologists should evaluate the cost and benefit of each norm, and use the norm with the greatest benefit and the least cost for the client (Manly &amp; Echemendia, 2007). 16.5.2.2.3 Separate Cutoffs Using separate cutoffs involves using a separate cutoff score per group and selecting the top number from each group. That is, using separate cutoffs involves using different criteria for each group. Using separate cutoffs functions the same as adding bonus points, but it has greater transparency—i.e., you are lowering the standard for one group compared to another group. An example of using separate cutoffs as a score adjustment is depicted in Figure 16.14. Figure 16.14: Using Separate Cutoffs as a Scoring Adjustment. In this example, the cutoff for the majority group is 128; the cutoff for the minority group is 123. 16.5.2.2.4 Top-Down Selection from Different Lists Top-down selection from different lists involves taking the best from two different lists according to a preset rule as to how many to select from each group. Top-down selection from different lists functions the same as within-group norming. An example of using top-down selection from different lists as a score adjustment is depicted in Figure 16.15. Figure 16.15: Using Top-Down Selection From Different Lists as a Scoring Adjustment. In this example, the top three candidates are selected from each group. 16.5.2.2.5 Banding Banding uses a tier system that is based on the assumption that individuals within a specific score range are regarded as having equivalent scores. So that we do not over-estimate small score differences, scores within the same band are seen as equivalent—and the order of selection within the band can be modified depending on selection goals. The standard error of measurement (SEM) is used to estimate the precision (reliability) of the test scores, and it is used as the width of the band. Consider an example: if a person received a score with confidence interval of 18–22, then scores between 18 to 22 are not necessarily different due to random fluctuation (measurement error). Therefore, scores in that range are considered the same, and we take a band of scores. However, banding by itself may not result in increased selection of lower scoring groups. The band provides a subsample of applicants so that we can use other criteria (other than the test) to select a candidate. Giving “minority preference” involves selecting members of minority group in a given band before selecting members of the majority group. An example of using banding as a score adjustment is depicted in Figure 16.16. Figure 16.16: Using Banding as a Scoring Adjustment. The problem with banding is that bands are set by the standard error of measurement: you can select the first group from the first band, but then whom do you select after the first band? There is no rationale where to “stop” the band because there are indistinguishable scores on the edges of each band to the next band. That is, 17 is indistinguishable from 18 (in terms of its confidence interval), 16 is indistinguishable from 17, and so on. Therefore, banding works okay for the top scores, but if you are going to hire a lot of candidates, it is a problem. A solution to this problem with banding is to use a sliding band, as described later. 16.5.2.2.6 Banding with Bonus Points Banding is often used with bonus points to reduce the negative impact for minority groups. An example of using banding with bonus points as a score adjustment is depicted in Figure 16.17. Figure 16.17: Using Banding With Bonus Points as a Scoring Adjustment. 16.5.2.2.7 Sliding Band Using a sliding band is a solution to the problem of which bands to use when using banding. Using a sliding band can help increase the number of minorities selected. Using the top band, you select all members of a minority group in the top band, then select members of the majority group with the top score of the band, then slide the band down (based on SEM), and repeat. You work your way down with bands though groups that are indistinguishable based on SEM, until getting a cell needed to select a relevant candidate. For instance, if the top score is 22 and the SEM is 4 points, the first band would be: [18, 22]. Here is how you would proceed: Select the minority group members who have a score between 18 to 22. Select the majority group members who have a score of 22. Slide the band down based on the SEM to the next highest score: [17, 21]. Select the minority group members who have a score between 17 to 21. Select the majority group members who have a score of 21. Slide the band down based on the SEM to the next highest score: [16, 20]. … And so on An example of using a sliding band as a score adjustment is depicted in Figure 16.18. Figure 16.18: Using a Sliding Band as a Scoring Adjustment. In sum, using a sliding band, scores that are not significantly lower than the highest remaining score should not be treated as different. Using a sliding band has the same effects on decisions as bonus points that are the width of the band. For example, if the SEM is 3, it has the same decisions as bonus points of 3; therefore, any scores within 3 of the highest score are now considered equal. A sliding band is popular because of its scientific and statistical rationale. Also, it is more confusing and, therefore, preferred by some because it may be less likely to be sued. However, a sliding band may not always eliminate adverse impact. A sliding band has never been overturned in court (or at least, not yet). 16.5.2.2.8 Separate Tests Using separate tests for each group is another option to reduce bias. For instance, you might use one test for the majority group and a different test for the minority group, making sure that each test is valid for the relevant group. Using separate tests is an extreme version of top-down selection and within-group norming. Using separate tests would be an option if a measure shows different slopes predictive bias. One way of developing separate tests is to use empirical keying by group: different items for each group are selected based on each item’s association with the criterion in each group. Empirical keying is an example of dustbowl empiricism (i.e., relying on empiricism rather than theory). However, theory can also inform the item selection. 16.5.2.2.9 Item Elimination based on Group Differences Items that show large group differences in scores can be eliminated from the test. If you remove enough items showing differences between groups, you can get similar scores between groups and can get equal group selection. A problem of item elimination based on group differences is that if you get rid of predictive items, then two goals, equal selection and predictive power, are not met. If you use this method, you often have to be willing for the measure to show decreases in predictive power. 16.5.2.3 Use of Score Adjustment Score adjustment can be used in a number of different domains, including tests of aptitude and intelligence. Score adjustment also comes up in other areas. For example, the number of drinks it takes to be considered binge drinking differs between men (five) and women (four). Although the list of score adjustment options is long, they all really reduce to two ways: Bonus points Within-group norming Bonus points and within-group norming are the techniques that are most often used in the real world. These techniques differ in their degree of obscurity—i.e., confusion that is caused not for scientific reasons, but for social, political, and dissemination and implementation reasons. Often procedures that are hard to understand are preferred because it is hard to argue against, critique, or game the system. Basically, you have two options for score adjustment. One option is to adjust scores by raising scores in one group or lowering the criterion in one group. The second primary option is to renorm or change the scores. In sum, you can change the scores, or you can change the decisions you make based on the scores. 16.5.3 Other Ways to Correct for Bias Because score adjustment is controversial, it is also important to consider other potential ways to correct for bias that do not involve score adjustment. Strategies other than score adjustment to correct for bias are described by Sackett et al. (2001). 16.5.3.1 Use Multiple Predictors In general, high-stakes decisions should not be made based on the results from one test. So, for instance, do not make hiring decisions based just on aptitude assessments. For example, college admissions decisions are not made just based on SAT scores, but also one’s grades, personal statement, extracurricular activities, letters of recommendation, etc. Using multiple predictors works best when the predictors are not correlated with the assessment that has adverse impact, which is difficult to achieve. There are larger majority–minority subgroup differences in verbal and cognitive ability tests than in noncognitive skills (e.g., motivation, personality, and interpersonal skills). So, it is important to include assessment of relevant noncognitive skills. Include as many relevant aspects of the construct as possible for content validity. For a job, consider as many factors as possible that are relevant for success, e.g., cognitive and noncognitive abilities. 16.5.3.2 Change the Criterion Another option is to change the criterion so that the predictive validity of tests is less skewed. It may be that the selection instrument is not biased but the way in which we are thinking about selection procedures is biased. For example, for judging the quality of universities, there are many different criteria we could use. It could be valuable to examine the various criteria, and you might find what is driving adverse effects. 16.5.3.3 Remove Biased Items Using item response theory or confirmatory factor analysis, you can identify items that function differently across groups (i.e., differential item functioning/DIF or measurement non-invariance). For instance, you can identify items that show different discrimination/factor loadings or difficulty/intercepts by group. You do not just want to remove items that show mean-level differences in scores (or different rates of endorsement) for one group than another, because there may be true group differences in their level on particular items. If an item is clearly invalid in one group but valid in another group, another option is to keep the item in one group, and to remove it in another group. Be careful when removing items because removing items can lead to poorer content validity—i.e., items may no longer be a representative set of the content of the construct. Removing items also reduces a measure’s reliability and ability to detect individual differences (Hagquist, 2019; Hagquist &amp; Andrich, 2017). DIF effects tend to be small and inconsistent; removing items showing DIF may not have a big impact. 16.5.3.4 Resolve Biased Items Another option, for items identified that show differential item functioning using IRT or measurement non-invariance using CFA, is to resolve instead of remove items. Resolving items involves allowing an item to have a different discrimination/factor loading and/or difficulty/intercept parameter for each group. Allowing item parameters to differ across groups has a very small effect on reliability and person separation, so it can be preferable to removing items (Hagquist, 2019; Hagquist &amp; Andrich, 2017). 16.5.3.5 Use Alternative Modes of Testing Another option is to use alternative modes of testing. For example, you could use audio or video to present test items, rather than requiring a person to read the items, or write answers. Typical testing and computerized exams are oriented toward the upper-middle class, which is therefore a procedure problem! McClelland’s (1973) argument is that we need more real-life testing. Real-life testing could help address stereotype threats and the effects of learning disabilities. However, testing in different modalities could change the construct(s) being assessed. 16.5.3.6 Use Work Records Using work records is based on McClelland’s (1973) argument to use more realistic and authentic assessments of job-relevant abilities. Evidence on the value of work records for personnel selection is mixed. In some cases, use of work records can actually increase adverse impact on under-represented groups because the primary group typically already has an idea of how to get into the relevant job or is already in the relevant job; therefore, they have a leg up. It would be acceptable to use work records if you trained people first and then tested, but no one spends the time to do this. 16.5.3.7 Increase Time Limit Another option is to allot people more testing time, as long as doing so does not change the construct. Time limits often lead to greater measurement error because scores conflate pace and quality of work. Increasing time limits requires convincing stakeholders that job performance is typically not “how fast you do things” but “how well you do them”—i.e., that time does not correlate with outcome of interest. The utility of increasing time limits depends on the domain. In some domains, efficiency is crucial (e.g., medicine, pilot). Increasing time limits is not that effective in reducing group differences, and it may actually increase group differences. 16.5.3.8 Use Motivation Sets Using motivation sets involves finding ways to increase testing motivation for minority groups. It is probably an error to think that a test assesses just aptitude; therefore, we should also consider an individual’s motivation to test. Thus, part of the score has to do with ability and some of the score has to do with motivation. You should try to maximize each examinee’s motivation, so that the person’s score on the measure better captures their true ability score. Motivation sets could include, for example, using more realistic test stimuli that are clearly applicable to the school or job requirements (i.e., that have face validity) to motivate all test takers. 16.5.3.9 Use Instructional Sets Using instructional sets involves coaching and training. For instance, you could inform examinees about the test content, provide study materials, and recommend test-taking strategies. This could narrow the gap between groups because there is an implicit assumption that the primary group already has “light” training. Using instructional sets aims to reduce error variance due to test anxiety, unfamiliar test format, and poor test-taking skills. Giving minority groups better access to test preparation is based on the assumption that group differences emerge because of different access to test preparation materials. This could theoretically help to systematically reduce test score differences across groups. Standardized tests like the SAT/GRE/LSAT/GMAT/MCAT, etc. embrace coaching/training. For instance, the organization ETS gives training materials for free. After training, scores on standardized tests show some but minimal improvement. In general, training yields some improvement on quantitative subscales but minimal change on verbal subscales. However, the improvements tend to apply across groups, and they do not seem to lessen group differences in scores. 16.6 Getting Started 16.6.1 Load Libraries Code library(&quot;petersenlab&quot;) #to install: install.packages(&quot;remotes&quot;); remotes::install_github(&quot;DevPsyLab/petersenlab&quot;) library(&quot;lavaan&quot;) library(&quot;semTools&quot;) library(&quot;semPlot&quot;) library(&quot;mirt&quot;) library(&quot;dmacs&quot;) #to install: install.packages(&quot;remotes&quot;); remotes::install_github(&quot;ddueber/dmacs&quot;) library(&quot;strucchange&quot;) library(&quot;MOTE&quot;) library(&quot;tidyverse&quot;) library(&quot;here&quot;) library(&quot;tinytex&quot;) 16.6.2 Prepare Data 16.6.2.1 Load Data cnlsy is a subset of a data set from the Children of the National Longitudinal Survey of Youth Survey (CNLSY). The CNLSY is a publicly available longitudinal data set provided by the Bureau of Labor Statistics (https://perma.cc/EH38-HDRN). The CNLSY data file for these examples is located on the book’s page of the Open Science Framework (https://osf.io/3pwza). Code cnlsy &lt;- read_csv(here(&quot;Data&quot;, &quot;cnlsy.csv&quot;)) 16.6.2.2 Simulate Data For reproducibility, I set the seed below. Using the same seed will yield the same answer every time. There is nothing special about this particular seed. Code sampleSize &lt;- 4000 set.seed(52242) mydataBias &lt;- data.frame( ID = 1:sampleSize, group = factor(c(&quot;male&quot;,&quot;female&quot;), levels = c(&quot;male&quot;,&quot;female&quot;)), unbiasedPredictor1 = NA, unbiasedPredictor2 = NA, unbiasedPredictor3 = NA, unbiasedCriterion1 = NA, unbiasedCriterion2 = NA, unbiasedCriterion3 = NA, predictor = rnorm(sampleSize, mean = 100, sd = 15), criterion1 = NA, criterion2 = NA, criterion3 = NA, criterion4 = NA, criterion5 = NA) mydataBias$unbiasedPredictor1 &lt;- rnorm(sampleSize, mean = 100, sd = 15) mydataBias$unbiasedPredictor2[which(mydataBias$group == &quot;male&quot;)] &lt;- rnorm(length(which(mydataBias$group == &quot;male&quot;)), mean = 70, sd = 15) mydataBias$unbiasedPredictor2[which(mydataBias$group == &quot;female&quot;)] &lt;- rnorm(length(which(mydataBias$group == &quot;female&quot;)), mean = 130, sd = 15) mydataBias$unbiasedPredictor3[which(mydataBias$group == &quot;male&quot;)] &lt;- rnorm(length(which(mydataBias$group == &quot;male&quot;)), mean = 130, sd = 15) mydataBias$unbiasedPredictor3[which(mydataBias$group == &quot;female&quot;)] &lt;- rnorm(length(which(mydataBias$group == &quot;female&quot;)), mean = 70, sd = 15) mydataBias$unbiasedCriterion1 &lt;- 1 * mydataBias$unbiasedPredictor1 + rnorm(sampleSize, mean = 0, sd = 15) mydataBias$unbiasedCriterion2 &lt;- 1 * mydataBias$unbiasedPredictor2 + rnorm(sampleSize, mean = 0, sd = 15) mydataBias$unbiasedCriterion3 &lt;- 1 * mydataBias$unbiasedPredictor3 + rnorm(sampleSize, mean = 0, sd = 15) mydataBias$criterion1[which(mydataBias$group == &quot;male&quot;)] &lt;- .7 * mydataBias$predictor[which(mydataBias$group == &quot;male&quot;)] + rnorm(length(which(mydataBias$group == &quot;male&quot;)), mean = 0, sd = 5) mydataBias$criterion1[which(mydataBias$group == &quot;female&quot;)] &lt;- .7 * mydataBias$predictor[which(mydataBias$group == &quot;female&quot;)] + rnorm(length(which(mydataBias$group == &quot;female&quot;)), mean = 0, sd = 5) mydataBias$criterion2[which(mydataBias$group == &quot;male&quot;)] &lt;- .7 * mydataBias$predictor[which(mydataBias$group == &quot;male&quot;)] + rnorm(length(which(mydataBias$group == &quot;male&quot;)), mean = 10, sd = 5) mydataBias$criterion2[which(mydataBias$group == &quot;female&quot;)] &lt;- .7 * mydataBias$predictor[which(mydataBias$group == &quot;female&quot;)] + rnorm(length(which(mydataBias$group == &quot;female&quot;)), mean = 0, sd = 5) mydataBias$criterion3[which(mydataBias$group == &quot;male&quot;)] &lt;- .7 * mydataBias$predictor[which(mydataBias$group == &quot;male&quot;)] + rnorm(length(which(mydataBias$group == &quot;male&quot;)), mean = 0, sd = 5) mydataBias$criterion3[which(mydataBias$group == &quot;female&quot;)] &lt;- .3 * mydataBias$predictor[which(mydataBias$group == &quot;female&quot;)] + rnorm(length(which(mydataBias$group == &quot;female&quot;)), mean = 0, sd = 5) mydataBias$criterion4[which(mydataBias$group == &quot;male&quot;)] &lt;- .7 * mydataBias$predictor[which(mydataBias$group == &quot;male&quot;)] + rnorm(length(which(mydataBias$group == &quot;male&quot;)), mean = 0, sd = 5) mydataBias$criterion4[which(mydataBias$group == &quot;female&quot;)] &lt;- .3 * mydataBias$predictor[which(mydataBias$group == &quot;female&quot;)] + rnorm(length(which(mydataBias$group == &quot;female&quot;)), mean = 30, sd = 5) mydataBias$criterion5[which(mydataBias$group == &quot;male&quot;)] &lt;- .7 * mydataBias$predictor[which(mydataBias$group == &quot;male&quot;)] + rnorm(length(which(mydataBias$group == &quot;male&quot;)), mean = 0, sd = 30) mydataBias$criterion5[which(mydataBias$group == &quot;female&quot;)] &lt;- .7 * mydataBias$predictor[which(mydataBias$group == &quot;female&quot;)] + rnorm(length(which(mydataBias$group == &quot;female&quot;)), mean = 0, sd = 5) 16.6.2.3 Add Missing Data Adding missing data to dataframes helps make examples more realistic to real-life data and helps you get in the habit of programming to account for missing data. HolzingerSwineford1939 is a data set from the lavaan package (Rosseel et al., 2022) that contains mental ability test scores (x1–x9) for seventh- and eighth-grade children. Code varNames &lt;- names(mydataBias) dimensionsDf &lt;- dim(mydataBias[,-c(1,2)]) unlistedDf &lt;- unlist(mydataBias[,-c(1,2)]) unlistedDf[sample( 1:length(unlistedDf), size = .01 * length(unlistedDf))] &lt;- NA mydataBias &lt;- cbind( mydataBias[,c(&quot;ID&quot;,&quot;group&quot;)], as.data.frame( matrix( unlistedDf, ncol = dimensionsDf[2]))) names(mydataBias) &lt;- varNames varNames &lt;- names(HolzingerSwineford1939) dimensionsDf &lt;- dim(HolzingerSwineford1939[,paste(&quot;x&quot;, 1:9, sep = &quot;&quot;)]) unlistedDf &lt;- unlist(HolzingerSwineford1939[,paste(&quot;x&quot;, 1:9, sep = &quot;&quot;)]) unlistedDf[sample( 1:length(unlistedDf), size = .01 * length(unlistedDf))] &lt;- NA HolzingerSwineford1939 &lt;- cbind( HolzingerSwineford1939[,1:6], as.data.frame(matrix( unlistedDf, ncol = dimensionsDf[2]))) names(HolzingerSwineford1939) &lt;- varNames 16.7 Examples of Unbiased Tests (in Terms of Predictive Bias) 16.7.1 Unbiased test where males and females have equal means on predictor and criterion Figure 16.19 depicts an example of an unbiased test where males and females have equal means on the predictor and criterion. The test is unbiased because there are no significant differences in the regression lines (of predictor predicting criterion) between males and females. Code summary(lm( unbiasedCriterion1 ~ unbiasedPredictor1 + group + unbiasedPredictor1:group, data = mydataBias)) Call: lm(formula = unbiasedCriterion1 ~ unbiasedPredictor1 + group + unbiasedPredictor1:group, data = mydataBias) Residuals: Min 1Q Median 3Q Max -54.623 -10.050 -0.025 10.373 66.811 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.930697 2.295058 0.406 0.685 unbiasedPredictor1 0.996976 0.022624 44.066 &lt;2e-16 *** groupfemale -1.165200 3.250481 -0.358 0.720 unbiasedPredictor1:groupfemale -0.004397 0.032115 -0.137 0.891 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 15.13 on 3913 degrees of freedom (83 observations deleted due to missingness) Multiple R-squared: 0.4963, Adjusted R-squared: 0.496 F-statistic: 1285 on 3 and 3913 DF, p-value: &lt; 2.2e-16 Code plot( unbiasedCriterion1 ~ unbiasedPredictor1, data = mydataBias, xlim = c( 0, max(c( mydataBias$unbiasedCriterion1, mydataBias$unbiasedPredictor1), na.rm = TRUE)), ylim = c( 0, max(c( mydataBias$criterion1, mydataBias$predictor), na.rm = TRUE)), type = &quot;n&quot;, xlab = &quot;predictor&quot;, ylab = &quot;criterion&quot;) points( mydataBias$unbiasedPredictor1[which(mydataBias$group == &quot;male&quot;)], mydataBias$unbiasedCriterion1[which(mydataBias$group == &quot;male&quot;)], pch = 20, col = &quot;blue&quot;) points(mydataBias$unbiasedPredictor1[which(mydataBias$group == &quot;female&quot;)], mydataBias$unbiasedCriterion1[which(mydataBias$group == &quot;female&quot;)], pch = 1, col = &quot;red&quot;) abline(lm( unbiasedCriterion1 ~ unbiasedPredictor1, data = mydataBias[which(mydataBias$group == &quot;male&quot;),]), lty = 1, col = &quot;blue&quot;) abline(lm( unbiasedCriterion1 ~ unbiasedPredictor1, data = mydataBias[which(mydataBias$group == &quot;female&quot;),]), lty = 2, col = &quot;red&quot;) legend( &quot;bottomright&quot;, c(&quot;Male&quot;,&quot;Female&quot;), lty = c(1,2), pch = c(20,1), col = c(&quot;blue&quot;,&quot;red&quot;)) Figure 16.19: Unbiased Test Where Males and Females Have Equal Means on Predictor and Criterion. 16.7.2 Unbiased test where females have higher means than males on predictor and criterion Figure 16.20 depicts an example of an unbiased test where females have higher means than males on the predictor and criterion. The test is unbiased because there are no differences in the regression lines (of predictor predicting criterion) between males and females. Code summary(lm( unbiasedCriterion2 ~ unbiasedPredictor2 + group + unbiasedPredictor2:group, data = mydataBias)) Call: lm(formula = unbiasedCriterion2 ~ unbiasedPredictor2 + group + unbiasedPredictor2:group, data = mydataBias) Residuals: Min 1Q Median 3Q Max -47.302 -9.989 0.010 9.860 53.791 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.29332 1.57773 -0.820 0.412 unbiasedPredictor2 1.02006 0.02218 46.000 &lt;2e-16 *** groupfemale 1.21294 3.28319 0.369 0.712 unbiasedPredictor2:groupfemale -0.01501 0.03125 -0.480 0.631 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 14.81 on 3919 degrees of freedom (77 observations deleted due to missingness) Multiple R-squared: 0.8412, Adjusted R-squared: 0.8411 F-statistic: 6921 on 3 and 3919 DF, p-value: &lt; 2.2e-16 Code plot( unbiasedCriterion2 ~ unbiasedPredictor2, data = mydataBias, xlim = c( 0, max(c( mydataBias$unbiasedCriterion2, mydataBias$unbiasedPredictor2), na.rm = TRUE)), ylim = c( 0, max(c( mydataBias$criterion1, mydataBias$predictor), na.rm = TRUE)), type = &quot;n&quot;, xlab = &quot;predictor&quot;, ylab = &quot;criterion&quot;) points( mydataBias$unbiasedPredictor2[which(mydataBias$group == &quot;male&quot;)], mydataBias$unbiasedCriterion2[which(mydataBias$group == &quot;male&quot;)], pch = 20, col = &quot;blue&quot;) points( mydataBias$unbiasedPredictor2[which(mydataBias$group == &quot;female&quot;)], mydataBias$unbiasedCriterion2[which(mydataBias$group == &quot;female&quot;)], pch = 1, col = &quot;red&quot;) abline(lm( unbiasedCriterion2 ~ unbiasedPredictor2, data = mydataBias[which(mydataBias$group == &quot;male&quot;),]), lty = 1, col = &quot;blue&quot;) abline(lm( unbiasedCriterion2 ~ unbiasedPredictor2, data = mydataBias[which(mydataBias$group == &quot;female&quot;),]), lty = 2, col = &quot;red&quot;) legend( &quot;bottomright&quot;, c(&quot;Male&quot;,&quot;Female&quot;), lty = c(1,2), pch = c(20,1), col = c(&quot;blue&quot;,&quot;red&quot;)) Figure 16.20: Unbiased Test Where Females Have Higher Means Than Males on Predictor and Criterion. 16.7.3 Unbiased test where males have higher means than females on predictor and criterion Figure 16.21 depicts an example of an unbiased test where males have higher means than females on the predictor and criterion. The test is unbiased because there are no differences in the regression lines (of predictor predicting criterion) between males and females. Code summary(lm( unbiasedCriterion3 ~ unbiasedPredictor3 + group + unbiasedPredictor3:group, data = mydataBias)) Call: lm(formula = unbiasedCriterion3 ~ unbiasedPredictor3 + group + unbiasedPredictor3:group, data = mydataBias) Residuals: Min 1Q Median 3Q Max -48.613 -10.115 -0.068 9.598 57.126 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.68352 2.84985 -0.591 0.555 unbiasedPredictor3 1.01072 0.02179 46.375 &lt;2e-16 *** groupfemale 1.42842 3.26227 0.438 0.662 unbiasedPredictor3:groupfemale -0.01187 0.03109 -0.382 0.703 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 14.82 on 3916 degrees of freedom (80 observations deleted due to missingness) Multiple R-squared: 0.8376, Adjusted R-squared: 0.8375 F-statistic: 6732 on 3 and 3916 DF, p-value: &lt; 2.2e-16 Code plot( unbiasedCriterion3 ~ unbiasedPredictor3, data = mydataBias, xlim = c( 0, max(c( mydataBias$unbiasedCriterion3, mydataBias$unbiasedPredictor3), na.rm = TRUE)), ylim = c(0, max(c( mydataBias$criterion1, mydataBias$predictor), na.rm = TRUE)), type = &quot;n&quot;, xlab = &quot;predictor&quot;, ylab = &quot;criterion&quot;) points( mydataBias$unbiasedPredictor3[which(mydataBias$group == &quot;male&quot;)], mydataBias$unbiasedCriterion3[which(mydataBias$group == &quot;male&quot;)], pch = 20, col = &quot;blue&quot;) points( mydataBias$unbiasedPredictor3[which(mydataBias$group == &quot;female&quot;)], mydataBias$unbiasedCriterion3[which(mydataBias$group == &quot;female&quot;)], pch = 1, col = &quot;red&quot;) abline(lm( unbiasedCriterion3 ~ unbiasedPredictor3, data = mydataBias[which(mydataBias$group == &quot;male&quot;),]), lty = 1, col = &quot;blue&quot;) abline(lm( unbiasedCriterion3 ~ unbiasedPredictor3, data = mydataBias[which(mydataBias$group == &quot;female&quot;),]), lty = 2, col = &quot;red&quot;) legend( &quot;bottomright&quot;, c(&quot;Male&quot;,&quot;Female&quot;), lty = c(1,2), pch = c(20,1), col = c(&quot;blue&quot;,&quot;red&quot;)) Figure 16.21: Unbiased Test Where Males Have Higher Means Than Females on Predictor and Criterion. 16.8 Predictive Bias: Different Regression Lines 16.8.1 Example of unbiased prediction (no differences in intercepts or slopes) Figure 16.22 depicts an example of an unbiased test where males and females have equal means on the predictor and criterion. The test is unbiased because there are no differences in the regression lines (of predictor predicting criterion1) between males and females. Code summary(lm( criterion1 ~ predictor + group + predictor:group, data = mydataBias)) Call: lm(formula = criterion1 ~ predictor + group + predictor:group, data = mydataBias) Residuals: Min 1Q Median 3Q Max -18.831 -3.338 0.004 3.330 19.335 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -0.264357 0.747269 -0.354 0.724 predictor 0.701726 0.007370 95.207 &lt;2e-16 *** groupfemale -0.515860 1.059267 -0.487 0.626 predictor:groupfemale 0.006002 0.010476 0.573 0.567 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 4.952 on 3908 degrees of freedom (88 observations deleted due to missingness) Multiple R-squared: 0.8225, Adjusted R-squared: 0.8223 F-statistic: 6035 on 3 and 3908 DF, p-value: &lt; 2.2e-16 Code plot( criterion1 ~ predictor, data = mydataBias, xlim = c( 0, max(c( mydataBias$criterion1, mydataBias$predictor), na.rm = TRUE)), ylim = c( 0, max(c( mydataBias$criterion1, mydataBias$predictor), na.rm = TRUE)), type = &quot;n&quot;, xlab = &quot;predictor&quot;, ylab = &quot;criterion&quot;) points( mydataBias$predictor[which(mydataBias$group == &quot;male&quot;)], mydataBias$criterion1[which(mydataBias$group == &quot;male&quot;)], pch = 20, col = &quot;blue&quot;) points( mydataBias$predictor[which(mydataBias$group == &quot;female&quot;)], mydataBias$criterion1[which(mydataBias$group == &quot;female&quot;)], pch = 1, col = &quot;red&quot;) abline(lm( criterion1 ~ predictor, data = mydataBias[which(mydataBias$group == &quot;male&quot;),]), lty = 1, col = &quot;blue&quot;) abline(lm( criterion1 ~ predictor, data = mydataBias[which(mydataBias$group == &quot;female&quot;),]), lty = 2, col = &quot;red&quot;) legend( &quot;bottomright&quot;, c(&quot;Male&quot;,&quot;Female&quot;), lty = c(1,2), pch = c(20,1), col = c(&quot;blue&quot;,&quot;red&quot;)) Figure 16.22: Example of Unbiased Prediction (No Differences in Intercepts or Slopes Between Males and Females). 16.8.2 Example of intercept bias Figure 16.23 depicts an example of a biased test due to intercept bias. There are differences in the intercepts of the regression lines (of predictor predicting criterion2) between males and females: males have a higher intercept than females. That is, the same score on the predictor results in higher predictions for males than females. Code summary(lm( criterion2 ~ predictor + group + predictor:group, data = mydataBias)) Call: lm(formula = criterion2 ~ predictor + group + predictor:group, data = mydataBias) Residuals: Min 1Q Median 3Q Max -18.6853 -3.4153 -0.0385 3.3979 18.0864 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 10.054048 0.760084 13.228 &lt;2e-16 *** predictor 0.697987 0.007499 93.075 &lt;2e-16 *** groupfemale -10.288893 1.074168 -9.578 &lt;2e-16 *** predictor:groupfemale 0.004748 0.010625 0.447 0.655 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 5.037 on 3913 degrees of freedom (83 observations deleted due to missingness) Multiple R-squared: 0.8452, Adjusted R-squared: 0.8451 F-statistic: 7124 on 3 and 3913 DF, p-value: &lt; 2.2e-16 Code plot( criterion2 ~ predictor, data = mydataBias, xlim = c( 0, max(c( mydataBias$criterion2, mydataBias$predictor), na.rm = TRUE)), ylim = c( 0, max(c( mydataBias$criterion2, mydataBias$predictor), na.rm = TRUE)), type = &quot;n&quot;, xlab = &quot;predictor&quot;, ylab = &quot;criterion&quot;) points( mydataBias$predictor[which(mydataBias$group == &quot;male&quot;)], mydataBias$criterion2[which(mydataBias$group == &quot;male&quot;)], pch = 20, col = &quot;blue&quot;) points( mydataBias$predictor[which(mydataBias$group == &quot;female&quot;)], mydataBias$criterion2[which(mydataBias$group == &quot;female&quot;)], pch = 1, col = &quot;red&quot;) abline(lm( criterion2 ~ predictor, data = mydataBias[which(mydataBias$group == &quot;male&quot;),]), lty = 1, col = &quot;blue&quot;) abline(lm( criterion2 ~ predictor, data = mydataBias[which(mydataBias$group == &quot;female&quot;),]), lty = 2, col = &quot;red&quot;) legend( &quot;bottomright&quot;, c(&quot;Male&quot;,&quot;Female&quot;), lty = c(1,2), pch = c(20,1), col = c(&quot;blue&quot;,&quot;red&quot;)) Figure 16.23: Example of Intercept Bias in Prediction (Different Intercepts Between Males and Females). 16.8.3 Example of slope bias Figure 16.24 depicts an example of a biased test due to slope bias. There are differences in the slopes of the regression lines (of predictor predicting criterion3) between males and females: males have a higher slope than females. That is, scores have stronger predictive validity for males than females. Code summary(lm( criterion3 ~ predictor + group + predictor:group, data = mydataBias)) Call: lm(formula = criterion3 ~ predictor + group + predictor:group, data = mydataBias) Residuals: Min 1Q Median 3Q Max -18.3183 -3.3715 0.0256 3.3844 19.3484 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.041940 0.757914 -1.375 0.169 predictor 0.708670 0.007475 94.804 &lt;2e-16 *** groupfemale 1.478534 1.074064 1.377 0.169 predictor:groupfemale -0.413233 0.010621 -38.907 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 5.031 on 3927 degrees of freedom (69 observations deleted due to missingness) Multiple R-squared: 0.9489, Adjusted R-squared: 0.9489 F-statistic: 2.432e+04 on 3 and 3927 DF, p-value: &lt; 2.2e-16 Code plot( criterion3 ~ predictor, data = mydataBias, xlim = c( 0, max(c( mydataBias$criterion3, mydataBias$predictor), na.rm = TRUE)), ylim = c( 0, max(c( mydataBias$criterion3, mydataBias$predictor), na.rm = TRUE)), type = &quot;n&quot;, xlab = &quot;predictor&quot;, ylab = &quot;criterion&quot;) points( mydataBias$predictor[which(mydataBias$group == &quot;male&quot;)], mydataBias$criterion3[which(mydataBias$group == &quot;male&quot;)], pch = 20, col = &quot;blue&quot;) points( mydataBias$predictor[which(mydataBias$group == &quot;female&quot;)], mydataBias$criterion3[which(mydataBias$group == &quot;female&quot;)], pch = 1, col = &quot;red&quot;) abline(lm( criterion3 ~ predictor, data = mydataBias[which(mydataBias$group == &quot;male&quot;),]), lty = 1, col = &quot;blue&quot;) abline(lm( criterion3 ~ predictor, data = mydataBias[which(mydataBias$group == &quot;female&quot;),]), lty = 2, col = &quot;red&quot;) legend( &quot;bottomright&quot;, c(&quot;Male&quot;,&quot;Female&quot;), lty = c(1,2), pch = c(20,1), col = c(&quot;blue&quot;,&quot;red&quot;)) Figure 16.24: Example of Slope Bias in Prediction (Different Slopes Between Males And Females). 16.8.4 Example of intercept and slope bias Figure 16.25 depicts an example of a biased test due to intercept and slope bias. There are differences in the intercepts and slopes of the regression lines (of predictor predicting criterion4) between males and females: males have a higher slope than females. That is, scores have stronger predictive validity for males than females. Females have a higher intercept than males. That is, at lower scores, the same score on the predictor results in higher predictions for females than males; at higher scores on the predictor, the same score results in higher predictions for males than females. Code summary(lm( criterion4 ~ predictor + group + predictor:group, data = mydataBias)) Call: lm(formula = criterion4 ~ predictor + group + predictor:group, data = mydataBias) Residuals: Min 1Q Median 3Q Max -18.2805 -3.4400 0.0269 3.3199 17.8824 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -0.276228 0.767080 -0.36 0.719 predictor 0.702178 0.007565 92.81 &lt;2e-16 *** groupfemale 29.156167 1.086254 26.84 &lt;2e-16 *** predictor:groupfemale -0.392016 0.010743 -36.49 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 5.099 on 3930 degrees of freedom (66 observations deleted due to missingness) Multiple R-squared: 0.7843, Adjusted R-squared: 0.7842 F-statistic: 4764 on 3 and 3930 DF, p-value: &lt; 2.2e-16 Code plot( criterion4 ~ predictor, data = mydataBias, xlim = c( 0, max(c( mydataBias$criterion4, mydataBias$predictor), na.rm = TRUE)), ylim = c( 0, max(c( mydataBias$criterion4, mydataBias$predictor), na.rm = TRUE)), type = &quot;n&quot;, xlab = &quot;predictor&quot;, ylab = &quot;criterion&quot;) points( mydataBias$predictor[which(mydataBias$group == &quot;male&quot;)], mydataBias$criterion4[which(mydataBias$group == &quot;male&quot;)], pch = 20, col = &quot;blue&quot;) points( mydataBias$predictor[which(mydataBias$group == &quot;female&quot;)], mydataBias$criterion4[which(mydataBias$group == &quot;female&quot;)], pch = 1, col = &quot;red&quot;) abline(lm( criterion4 ~ predictor, data = mydataBias[which(mydataBias$group == &quot;male&quot;),]), lty = 1, col = &quot;blue&quot;) abline(lm( criterion4 ~ predictor, data = mydataBias[which(mydataBias$group == &quot;female&quot;),]), lty = 2, col = &quot;red&quot;) legend( &quot;bottomright&quot;, c(&quot;Male&quot;,&quot;Female&quot;), lty = c(1,2), pch = c(20,1), col = c(&quot;blue&quot;,&quot;red&quot;)) Figure 16.25: Example of Intercept And Slope Bias in Prediction (Different Intercepts and Slopes Between Males and Females). 16.8.5 Example of different measurement reliability/error across groups In the example depicted in Figure 16.26, there are differences in the measurement reliability/error on the criterion between males and females: males’ scores have a lower reliability (higher measurement error) on the criterion than females’ scores. That is, we are more confident about a female’s level on the criterion given a particular score on the predictor than we are about a male’s level on the criterion given a particular score on the predictor. Code summary(lm( criterion5 ~ predictor + group + predictor:group, data = mydataBias)) Call: lm(formula = criterion5 ~ predictor + group + predictor:group, data = mydataBias) Residuals: Min 1Q Median 3Q Max -109.295 -7.065 0.077 6.836 108.428 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -9.28386 3.32617 -2.791 0.00528 ** predictor 0.79172 0.03280 24.136 &lt; 2e-16 *** groupfemale 9.12452 4.70470 1.939 0.05252 . predictor:groupfemale -0.09083 0.04654 -1.952 0.05102 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 22.03 on 3920 degrees of freedom (76 observations deleted due to missingness) Multiple R-squared: 0.2087, Adjusted R-squared: 0.2081 F-statistic: 344.6 on 3 and 3920 DF, p-value: &lt; 2.2e-16 Code plot( criterion5 ~ predictor, data = mydataBias, xlim = c( 0, max(c(mydataBias$criterion5, mydataBias$predictor), na.rm = TRUE)), ylim = c( 0, max(c( mydataBias$criterion5, mydataBias$predictor), na.rm = TRUE)), type = &quot;n&quot;, xlab = &quot;predictor&quot;, ylab = &quot;criterion&quot;) points( mydataBias$predictor[which(mydataBias$group == &quot;male&quot;)], mydataBias$criterion5[which(mydataBias$group == &quot;male&quot;)], pch = 20, col = &quot;blue&quot;) points(mydataBias$predictor[which(mydataBias$group == &quot;female&quot;)], mydataBias$criterion5[which(mydataBias$group == &quot;female&quot;)], pch = 1, col = &quot;red&quot;) abline(lm(criterion5 ~ predictor, data = mydataBias[which(mydataBias$group == &quot;male&quot;),]), lty = 1, col = &quot;blue&quot;) abline(lm(criterion5 ~ predictor, data = mydataBias[which(mydataBias$group == &quot;female&quot;),]), lty = 2, col = &quot;red&quot;) legend( &quot;bottomright&quot;, c(&quot;Male&quot;,&quot;Female&quot;), lty = c(1,2), pch = c(20,1), col = c(&quot;blue&quot;,&quot;red&quot;)) Figure 16.26: Example of Different Measurement Reliability/Error Across Groups. 16.9 Differential Item Functioning (DIF) Differential item functioning (DIF) indicates that one or more items functions differently across groups. That is, one or more of the item parameters for an item differs across groups. For instance, the severity or discrimination of an item could be higher in one group compared to another group. If an item functions differently across groups, it can lead to biased scores for particular groups. Thus, when observing group differences in level on the measure or group differences in the measure’s association with other measures, it is unclear whether the observed group differences reflect true group differences or differences in the functioning of the measure across groups. For instance, consider an item such as “disobedience to authority”, which is thought to reflect externalizing behavior in childhood. However, in adulthood, disobedience to authority could reflect prosocial functions, such as protesting against societally unjust actions, and may show weaker construct validity with respect to externalizing problems, as operationalized by a weaker discrimination coefficient in adulthood than in childhood. Tests of differential item functioning are equivalent to tests of measurement invariance. Approaches to addressing DIF are described in Section 16.5.1. If we identify that an item shows non-negligible DIF, we have three primary options (described above): (1) drop the item for both groups, (2) drop the item for one group but keep it for the other group, or (3) freely estimate the parameters (discrimination and difficulty) for the item across groups. Tests of DIF were conducted using the mirt package (Chalmers, 2020). 16.9.1 Item Descriptive Statistics Code itemstats( data = cnlsy[,c( &quot;bpi_antisocialT1_1&quot;,&quot;bpi_antisocialT1_2&quot;,&quot;bpi_antisocialT1_3&quot;, &quot;bpi_antisocialT1_4&quot;,&quot;bpi_antisocialT1_5&quot;,&quot;bpi_antisocialT1_6&quot;, &quot;bpi_antisocialT1_7&quot;)], group = cnlsy$sex, ts.tables = TRUE) $female $female$overall N.complete N mean_total.score sd_total.score ave.r sd.r alpha 1620 5641 2.554 2.132 0.255 0.083 0.681 $female$itemstats N mean sd total.r total.r_if_rm alpha_if_rm bpi_antisocialT1_1 1928 0.538 0.587 0.662 0.456 0.626 bpi_antisocialT1_2 1925 0.278 0.510 0.648 0.471 0.624 bpi_antisocialT1_3 1925 0.456 0.654 0.625 0.376 0.657 bpi_antisocialT1_4 1933 0.113 0.355 0.523 0.384 0.654 bpi_antisocialT1_5 1649 0.186 0.431 0.594 0.438 0.637 bpi_antisocialT1_6 1658 0.103 0.345 0.572 0.446 0.643 bpi_antisocialT1_7 1944 0.834 0.639 0.553 0.292 0.683 $female$proportions 0 1 2 NA bpi_antisocialT1_1 0.174 0.151 0.016 0.658 bpi_antisocialT1_2 0.257 0.075 0.010 0.659 bpi_antisocialT1_3 0.216 0.094 0.031 0.659 bpi_antisocialT1_4 0.308 0.030 0.004 0.657 bpi_antisocialT1_5 0.243 0.044 0.005 0.708 bpi_antisocialT1_6 0.268 0.023 0.004 0.706 bpi_antisocialT1_7 0.104 0.194 0.046 0.655 $female$total.score_frequency 0 1 2 3 4 5 6 7 8 9 10 11 12 13 Freq 217 364 349 283 166 91 67 38 12 10 11 4 4 4 $female$total.score_means 0 1 2 bpi_antisocialT1_1 1.354919 3.339237 7.216867 bpi_antisocialT1_2 1.846343 4.173789 8.192308 bpi_antisocialT1_3 1.593417 3.846682 5.406667 bpi_antisocialT1_4 2.204952 5.090278 9.045455 bpi_antisocialT1_5 2.010386 4.946721 7.892857 bpi_antisocialT1_6 2.189959 5.701613 9.227273 bpi_antisocialT1_7 1.004357 2.778970 4.746725 $female$total.score_sds 0 1 2 bpi_antisocialT1_1 1.262270 1.668474 2.763209 bpi_antisocialT1_2 1.480206 1.809814 2.664498 bpi_antisocialT1_3 1.329902 1.837884 2.783211 bpi_antisocialT1_4 1.743521 2.280698 2.802673 bpi_antisocialT1_5 1.566126 2.196431 3.258485 bpi_antisocialT1_6 1.684322 2.215906 2.844072 bpi_antisocialT1_7 1.232733 1.798814 2.475591 $male $male$overall N.complete N mean_total.score sd_total.score ave.r sd.r alpha 1645 5891 3.16 2.385 0.266 0.081 0.703 $male$itemstats N mean sd total.r total.r_if_rm alpha_if_rm bpi_antisocialT1_1 1948 0.615 0.585 0.634 0.449 0.660 bpi_antisocialT1_2 1947 0.372 0.559 0.655 0.485 0.650 bpi_antisocialT1_3 1949 0.520 0.668 0.582 0.346 0.692 bpi_antisocialT1_4 1948 0.233 0.486 0.596 0.441 0.666 bpi_antisocialT1_5 1676 0.367 0.547 0.627 0.454 0.662 bpi_antisocialT1_6 1688 0.177 0.446 0.556 0.406 0.675 bpi_antisocialT1_7 1962 0.834 0.646 0.582 0.357 0.687 $male$proportions 0 1 2 NA bpi_antisocialT1_1 0.145 0.168 0.017 0.669 bpi_antisocialT1_2 0.220 0.097 0.013 0.669 bpi_antisocialT1_3 0.191 0.107 0.033 0.669 bpi_antisocialT1_4 0.263 0.058 0.010 0.669 bpi_antisocialT1_5 0.190 0.085 0.010 0.715 bpi_antisocialT1_6 0.243 0.035 0.008 0.713 bpi_antisocialT1_7 0.102 0.185 0.046 0.667 $male$total.score_frequency 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Freq 163 294 310 259 208 144 104 74 40 25 9 7 5 1 2 $male$total.score_means 0 1 2 bpi_antisocialT1_1 1.622478 3.944056 7.397849 bpi_antisocialT1_2 2.134191 4.767821 8.106061 bpi_antisocialT1_3 2.016754 4.400763 5.819277 bpi_antisocialT1_4 2.484091 5.539286 8.177778 bpi_antisocialT1_5 2.174111 4.817073 7.910714 bpi_antisocialT1_6 2.616762 5.810680 8.093023 bpi_antisocialT1_7 1.412500 3.375679 5.782787 $male$total.score_sds 0 1 2 bpi_antisocialT1_1 1.515237 1.982218 2.454542 bpi_antisocialT1_2 1.678482 1.955386 2.437709 bpi_antisocialT1_3 1.654589 2.069463 2.774973 bpi_antisocialT1_4 1.845772 2.119687 2.534210 bpi_antisocialT1_5 1.709715 2.047072 2.725481 bpi_antisocialT1_6 1.911288 2.181537 2.982600 bpi_antisocialT1_7 1.522500 1.906515 2.652344 16.9.2 Unconstrained Model 16.9.2.1 Fit Model Code unconstrainedModel &lt;- multipleGroup( data = cnlsy[,c( &quot;bpi_antisocialT1_1&quot;,&quot;bpi_antisocialT1_2&quot;,&quot;bpi_antisocialT1_3&quot;, &quot;bpi_antisocialT1_4&quot;,&quot;bpi_antisocialT1_5&quot;,&quot;bpi_antisocialT1_6&quot;, &quot;bpi_antisocialT1_7&quot;)], model = 1, group = cnlsy$sex, SE = TRUE) 16.9.2.2 Model Summary Items that appear to differ: items 5 and 6 are more discriminating (\\(a\\) parameter) among females than males item 4 has higher difficulty/severity (\\(b_1\\) and \\(b_2\\) parameters) among males than females Code summary(unconstrainedModel) ---------- GROUP: female F1 h2 bpi_antisocialT1_1 0.698 0.487 bpi_antisocialT1_2 0.706 0.499 bpi_antisocialT1_3 0.532 0.283 bpi_antisocialT1_4 0.683 0.466 bpi_antisocialT1_5 0.763 0.582 bpi_antisocialT1_6 0.838 0.702 bpi_antisocialT1_7 0.429 0.184 SS loadings: 3.202 Proportion Var: 0.457 Factor correlations: F1 F1 1 ---------- GROUP: male F1 h2 bpi_antisocialT1_1 0.663 0.440 bpi_antisocialT1_2 0.736 0.542 bpi_antisocialT1_3 0.531 0.282 bpi_antisocialT1_4 0.713 0.508 bpi_antisocialT1_5 0.691 0.477 bpi_antisocialT1_6 0.702 0.492 bpi_antisocialT1_7 0.507 0.257 SS loadings: 2.999 Proportion Var: 0.428 Factor correlations: F1 F1 1 Code coef(unconstrainedModel, simplify = TRUE, IRTpars = TRUE) $female $items a b1 b2 bpi_antisocialT1_1 1.657 0.029 2.451 bpi_antisocialT1_2 1.697 0.953 2.763 bpi_antisocialT1_3 1.069 0.650 2.585 bpi_antisocialT1_4 1.590 1.895 3.465 bpi_antisocialT1_5 2.008 1.281 2.936 bpi_antisocialT1_6 2.614 1.625 2.770 bpi_antisocialT1_7 0.807 -1.184 2.583 $means F1 0 $cov F1 F1 1 $male $items a b1 b2 bpi_antisocialT1_1 1.507 -0.238 2.505 bpi_antisocialT1_2 1.853 0.582 2.455 bpi_antisocialT1_3 1.067 0.384 2.480 bpi_antisocialT1_4 1.729 1.165 2.765 bpi_antisocialT1_5 1.625 0.635 2.757 bpi_antisocialT1_6 1.677 1.494 2.873 bpi_antisocialT1_7 1.002 -0.986 2.143 $means F1 0 $cov F1 F1 1 16.9.3 Constrained Model Constrain item parameters to be equal across groups (to use as baseline model for identifying DIF). 16.9.3.1 Fit Model Code constrainedModel &lt;- multipleGroup( data = cnlsy[,c( &quot;bpi_antisocialT1_1&quot;,&quot;bpi_antisocialT1_2&quot;,&quot;bpi_antisocialT1_3&quot;, &quot;bpi_antisocialT1_4&quot;,&quot;bpi_antisocialT1_5&quot;,&quot;bpi_antisocialT1_6&quot;, &quot;bpi_antisocialT1_7&quot;)], model = 1, group = cnlsy$sex, invariance = c(c( &quot;bpi_antisocialT1_1&quot;,&quot;bpi_antisocialT1_2&quot;,&quot;bpi_antisocialT1_3&quot;, &quot;bpi_antisocialT1_4&quot;,&quot;bpi_antisocialT1_5&quot;,&quot;bpi_antisocialT1_6&quot;, &quot;bpi_antisocialT1_7&quot;), &quot;free_means&quot;, &quot;free_var&quot;), SE = TRUE) 16.9.3.2 Model Summary Code summary(constrainedModel) ---------- GROUP: female F1 h2 bpi_antisocialT1_1 0.659 0.434 bpi_antisocialT1_2 0.709 0.503 bpi_antisocialT1_3 0.514 0.264 bpi_antisocialT1_4 0.704 0.495 bpi_antisocialT1_5 0.737 0.543 bpi_antisocialT1_6 0.768 0.590 bpi_antisocialT1_7 0.441 0.195 SS loadings: 3.024 Proportion Var: 0.432 Factor correlations: F1 F1 1 ---------- GROUP: male F1 h2 bpi_antisocialT1_1 0.669 0.447 bpi_antisocialT1_2 0.719 0.516 bpi_antisocialT1_3 0.524 0.275 bpi_antisocialT1_4 0.713 0.508 bpi_antisocialT1_5 0.745 0.556 bpi_antisocialT1_6 0.776 0.602 bpi_antisocialT1_7 0.451 0.203 SS loadings: 3.107 Proportion Var: 0.444 Factor correlations: F1 F1 1 Code coef( constrainedModel, simplify = TRUE, IRTpars = TRUE) $female $items a b1 b2 bpi_antisocialT1_1 1.492 0.089 2.791 bpi_antisocialT1_2 1.713 0.978 2.883 bpi_antisocialT1_3 1.020 0.733 2.836 bpi_antisocialT1_4 1.686 1.687 3.265 bpi_antisocialT1_5 1.854 1.138 3.014 bpi_antisocialT1_6 2.040 1.779 3.034 bpi_antisocialT1_7 0.837 -0.952 2.697 $means F1 0 $cov F1 F1 1 $male $items a b1 b2 bpi_antisocialT1_1 1.492 0.089 2.791 bpi_antisocialT1_2 1.713 0.978 2.883 bpi_antisocialT1_3 1.020 0.733 2.836 bpi_antisocialT1_4 1.686 1.687 3.265 bpi_antisocialT1_5 1.854 1.138 3.014 bpi_antisocialT1_6 2.040 1.779 3.034 bpi_antisocialT1_7 0.837 -0.952 2.697 $means F1 0.39 $cov F1 F1 1.054 16.9.4 Compare model fit of constrained model to unconstrained model The constrained model and the unconstrained model are considered “nested” models. The constrained model is nested within the unconstrained model because the unconstrained model includes all of the terms of the constrained model along with additional terms. Model fit of nested models can be compared with a chi-square difference test. Code anova(constrainedModel, unconstrainedModel) The constrained model fits significantly worse than the unconstrained model, which suggests that item parameters differ between males and females. 16.9.5 Identify DIF by iteratively removing constraints from fully constrained model One way to identify DIF is to iteratively remove constraints from a model in which all parameters are constrained to be the same across groups. Removing constraints allows individual items to have different item parameters across groups, to identify which items yield a significant improvement in model fit when allowing their discrimination and/or severity to differ across groups. Items 1, 3, 4, 5, 6, and 7 showed DIF in discrimination and/or severity, based on a significant chi-square difference test. 16.9.5.1 Items that differ in discrimination and/or severity Code difDropItems &lt;- DIF( constrainedModel, c(&quot;a1&quot;,&quot;d1&quot;,&quot;d2&quot;), scheme = &quot;drop&quot;, simplify = TRUE) difDropItems 16.9.5.2 Items that differ in discrimination Items 1, 3, 4, and 5 showed DIF in discrimination, based on a significant chi-square difference test. Code difDropItemsDiscrimination &lt;- DIF( constrainedModel, c(&quot;a1&quot;), scheme = &quot;drop&quot;, simplify = TRUE) difDropItemsDiscrimination 16.9.5.3 Items that differ in severity Items 1, 3, 4, 5, and 7 showed DIF in difficulty/severity, based on a significant chi-square difference test. Code difDropItemsSeverity &lt;- DIF( constrainedModel, c(&quot;d1&quot;,&quot;d2&quot;), scheme = &quot;drop&quot;, simplify = TRUE) difDropItemsSeverity 16.9.6 Identify DIF by iteratively adding constraints to unconstrained model 16.9.6.1 Items that differ in discrimination and/or severity Another way to identify DIF is to iteratively add constraints to a model in which all parameters are allowed to differ across groups. Adding constraints forces individual items to have the same item parameters across groups, to identify which items yield a significant worsening in model fit when constraining their discrimination and/or severity to be the same across groups. Items 1, 2, 3, 4, 5, and 6 showed DIF in discrimination and/or severity, based on a significant chi-square difference test. The DIF in discrimination and/or severity is depicted in Figure 16.27. Code difAddItems &lt;- DIF( unconstrainedModel, c(&quot;a1&quot;,&quot;d1&quot;,&quot;d2&quot;), scheme = &quot;add&quot;, simplify = TRUE, plotdif = TRUE) Figure 16.27: Item Probability Functions for Examining Differential Item Functioning in Terms of Discrimination and/or Severity. Code difAddItems 16.9.6.2 Items that differ in discrimination Items 6 and 7 showed DIF in discrimination, based on a significant chi-square difference test. The DIF in discrimination is depicted in Figure 16.28. Code difAddItemsDiscrimination &lt;- DIF( unconstrainedModel, c(&quot;a1&quot;), scheme = &quot;add&quot;, simplify = TRUE, plotdif = TRUE) Figure 16.28: Item Probability Functions for Examining Differential Item Functioning in Terms of Discrimination. Code difAddItemsDiscrimination 16.9.6.3 Items that differ in severity Items 1, 2, 3, 4, 5, and 6 showed DIF in difficulty/severity, based on a significant chi-square difference test. The DIF in difficulty/severity is depicted in Figure 16.29 Code difAddItemsSeverity &lt;- DIF( unconstrainedModel, c(&quot;d1&quot;,&quot;d2&quot;), scheme = &quot;add&quot;, simplify = TRUE, plotdif = TRUE) Figure 16.29: Item Probability Functions for Examining Differential Item Functioning in Terms of Severity. Code difAddItemsSeverity 16.9.7 Compute effect size of DIF Effect size measures of DIF were computed based on expected scores (Meade, 2010) using the mirt package (Chalmers, 2020). Some researchers recommend using a simulation-based procedure to examine the impact of DIF on screening accuracy (Gonzalez &amp; Pelham, 2021). 16.9.7.1 Test-level DIF In addition to consideration of differential test functioning, we can also consider whether the test as a whole (i.e., the collection of items) differs in its functioning by group—called differential test functioning. Differential test functioning is depicted in Figure 16.30. Estimates of differential test functioning are in Table ??. Code empirical_ES( unconstrainedModel, DIF = FALSE) In general, the measure showed greater difficulty for females than for males. That is, at a given construct level, males were more likely than females to endorse the items. Otherwise said, it takes a higher construct level for females to obtain the same score on the measure as males. Code empirical_ES( unconstrainedModel, DIF = FALSE, plot = TRUE) Figure 16.30: Differential Test Functioning by Sex. 16.9.7.2 Item-level DIF Differential item functioning is depicted in Figure 16.31. Estimates of differential item functioning are in Table ??. Code empirical_ES( unconstrainedModel, DIF = TRUE) %&gt;% round(., 2) The measure-level differences in functioning appear to be largely driven by greater difficulty of items 4 and 5 for females than males. That is, at a given construct level, males were more likely than females to endorse items 4 and 5, in particular. Otherwise said, it takes a higher construct level for females than males to endorse items 4 and 5. Code empirical_ES( unconstrainedModel, DIF = TRUE, plot = TRUE) Figure 16.31: Differential Item Functioning by Sex. 16.9.8 Item plots Plots of item response category characteristic curves by sex are in Figures 16.32–16.38 below. Code itemplot( unconstrainedModel, &quot;bpi_antisocialT1_1&quot;, type = &quot;trace&quot;, par.settings = standard.theme(&quot;pdf&quot;, color = FALSE)) Figure 16.32: Item Response Category Characteristic Curves by Sex: Item 1. Code itemplot( unconstrainedModel, &quot;bpi_antisocialT1_2&quot;, type = &quot;trace&quot;, par.settings = standard.theme(&quot;pdf&quot;, color = FALSE)) Figure 16.33: Item Response Category Characteristic Curves by Sex: Item 2. Code itemplot( unconstrainedModel, &quot;bpi_antisocialT1_3&quot;, type = &quot;trace&quot;, par.settings = standard.theme(&quot;pdf&quot;, color = FALSE)) Figure 16.34: Item Response Category Characteristic Curves by Sex: Item 3. Code itemplot( unconstrainedModel, &quot;bpi_antisocialT1_4&quot;, type = &quot;trace&quot;, par.settings = standard.theme(&quot;pdf&quot;, color = FALSE)) Figure 16.35: Item Response Category Characteristic Curves by Sex: Item 4. Code itemplot( unconstrainedModel, &quot;bpi_antisocialT1_5&quot;, type = &quot;trace&quot;, par.settings = standard.theme(&quot;pdf&quot;, color = FALSE)) Figure 16.36: Item Response Category Characteristic Curves by Sex: Item 5. Code itemplot( unconstrainedModel, &quot;bpi_antisocialT1_6&quot;, type = &quot;trace&quot;, par.settings = standard.theme(&quot;pdf&quot;, color = FALSE)) Figure 16.37: Item Response Category Characteristic Curves by Sex: Item 6. Code itemplot( unconstrainedModel, &quot;bpi_antisocialT1_7&quot;, type = &quot;trace&quot;, par.settings = standard.theme(&quot;pdf&quot;, color = FALSE)) Figure 16.38: Item Response Category Characteristic Curves by Sex: Item 7. Plots of item information functions by sex are in Figures 16.39–16.45 below. Items 1, 5, and 6 showed greater information for females than for males. Items 2, 4, and 7 showed greater information for females than for males. Code itemplot( unconstrainedModel, &quot;bpi_antisocialT1_1&quot;, type = &quot;info&quot;, par.settings = standard.theme(&quot;pdf&quot;, color = FALSE)) Figure 16.39: Item Information Curves by Sex: Item 1. Code itemplot( unconstrainedModel, &quot;bpi_antisocialT1_2&quot;, type = &quot;info&quot;, par.settings = standard.theme(&quot;pdf&quot;, color = FALSE)) Figure 16.40: Item Information Curves by Sex: Item 2. Code itemplot( unconstrainedModel, &quot;bpi_antisocialT1_3&quot;, type = &quot;info&quot;, par.settings = standard.theme(&quot;pdf&quot;, color = FALSE)) Figure 16.41: Item Information Curves by Sex: Item 3. Code itemplot( unconstrainedModel, &quot;bpi_antisocialT1_4&quot;, type = &quot;info&quot;, par.settings = standard.theme(&quot;pdf&quot;, color = FALSE)) Figure 16.42: Item Information Curves by Sex: Item 4. Code itemplot( unconstrainedModel, &quot;bpi_antisocialT1_5&quot;, type = &quot;info&quot;, par.settings = standard.theme(&quot;pdf&quot;, color = FALSE)) Figure 16.43: Item Information Curves by Sex: Item 5. Code itemplot( unconstrainedModel, &quot;bpi_antisocialT1_6&quot;, type = &quot;info&quot;, par.settings = standard.theme(&quot;pdf&quot;, color = FALSE)) Figure 16.44: Item Information Curves by Sex: Item 6. Code itemplot( unconstrainedModel, &quot;bpi_antisocialT1_7&quot;, type = &quot;info&quot;, par.settings = standard.theme(&quot;pdf&quot;, color = FALSE)) Figure 16.45: Item Information Curves by Sex: Item 7. Plots of expected item scores by sex are are in Figures 16.46–16.52 below. Code itemplot( unconstrainedModel, &quot;bpi_antisocialT1_1&quot;, type = &quot;score&quot;, par.settings = standard.theme(&quot;pdf&quot;, color = FALSE)) Figure 16.46: Expected Item Score by Sex: Item 1. Code itemplot( unconstrainedModel, &quot;bpi_antisocialT1_2&quot;, type = &quot;score&quot;, par.settings = standard.theme(&quot;pdf&quot;, color = FALSE)) Figure 16.47: Expected Item Score by Sex: Item 2. Code itemplot( unconstrainedModel, &quot;bpi_antisocialT1_3&quot;, type = &quot;score&quot;, par.settings = standard.theme(&quot;pdf&quot;, color = FALSE)) Figure 16.48: Expected Item Score by Sex: Item 3. Code itemplot( unconstrainedModel, &quot;bpi_antisocialT1_4&quot;, type = &quot;score&quot;, par.settings = standard.theme(&quot;pdf&quot;, color = FALSE)) Figure 16.49: Expected Item Score by Sex: Item 4. Code itemplot( unconstrainedModel, &quot;bpi_antisocialT1_5&quot;, type = &quot;score&quot;, par.settings = standard.theme(&quot;pdf&quot;, color = FALSE)) Figure 16.50: Expected Item Score by Sex: Item 5. Code itemplot( unconstrainedModel, &quot;bpi_antisocialT1_6&quot;, type = &quot;score&quot;, par.settings = standard.theme(&quot;pdf&quot;, color = FALSE)) Figure 16.51: Expected Item Score by Sex: Item 6. Code itemplot( unconstrainedModel, &quot;bpi_antisocialT1_7&quot;, type = &quot;score&quot;, par.settings = standard.theme(&quot;pdf&quot;, color = FALSE)) Figure 16.52: Expected Item Score by Sex: Item 7. 16.9.9 Addressing DIF Based on the analyses above, item 5 shows the largest magnitude of DIF. Specifically, item 5 has a stronger discrimination parameter for women than men. So, we should handle item 5 first. If we deem the DIF for item 5 to be non-negligible, we have three primary options: (1) drop item 5 for both men and women, (2) drop item 5 for men but keep it for women, or (3) freely estimate the parameters (discrimination and difficulty) for item 5 across groups. 16.9.9.1 Drop item for both groups The first option is to drop item 5 for both men and women groups. You might do this if you want to use a measure that has only those items that function equivalently across groups. However, this can lead to lower reliability and weaker ability to detect individual differences. Code constrainedModelDropItem5 &lt;- multipleGroup( data = cnlsy[,c( &quot;bpi_antisocialT1_1&quot;,&quot;bpi_antisocialT1_2&quot;,&quot;bpi_antisocialT1_3&quot;, &quot;bpi_antisocialT1_4&quot;,&quot;bpi_antisocialT1_6&quot;,&quot;bpi_antisocialT1_7&quot;)], model = 1, group = cnlsy$sex, invariance = c(c( &quot;bpi_antisocialT1_1&quot;,&quot;bpi_antisocialT1_2&quot;,&quot;bpi_antisocialT1_3&quot;, &quot;bpi_antisocialT1_4&quot;,&quot;bpi_antisocialT1_6&quot;,&quot;bpi_antisocialT1_7&quot;), &quot;free_means&quot;, &quot;free_var&quot;), SE = TRUE) coef(constrainedModelDropItem5, simplify = TRUE) 16.9.9.2 Drop item for one group but not another group A second option is to drop item 5 for men but to keep it for women. You might do this if the item is invalid for men, but still valid for women. The coefficients show an item parameter for item 5 for men, but it was constrained to be equal to the parameter for women, and data were removed from the estimation of item 5 for men. Code dropItem5ForMen &lt;- cnlsy dropItem5ForMen[which( dropItem5ForMen$sex == &quot;male&quot;), &quot;bpi_antisocialT1_5&quot;] &lt;- NA constrainedModelDropItem5ForMen &lt;- multipleGroup( data = dropItem5ForMen[,c( &quot;bpi_antisocialT1_1&quot;,&quot;bpi_antisocialT1_2&quot;,&quot;bpi_antisocialT1_3&quot;, &quot;bpi_antisocialT1_4&quot;,&quot;bpi_antisocialT1_5&quot;,&quot;bpi_antisocialT1_6&quot;, &quot;bpi_antisocialT1_7&quot;)], model = 1, group = dropItem5ForMen$sex, invariance = c(c( &quot;bpi_antisocialT1_1&quot;,&quot;bpi_antisocialT1_2&quot;,&quot;bpi_antisocialT1_3&quot;, &quot;bpi_antisocialT1_4&quot;,&quot;bpi_antisocialT1_5&quot;,&quot;bpi_antisocialT1_6&quot;, &quot;bpi_antisocialT1_7&quot;), &quot;free_means&quot;, &quot;free_var&quot;), SE = TRUE) coef(constrainedModelDropItem5ForMen, simplify = TRUE) 16.9.9.3 Freely estimate item to have different parameters across groups Alternatively, we can resolve DIF by allowing the item to have different parameters (discrimination and difficulty) across both groups. You might do this if the item is valid for both men and women, and it has importantly different item parameters nonetheless. Code constrainedModelResolveItem5 &lt;- multipleGroup( data = cnlsy[,c( &quot;bpi_antisocialT1_1&quot;,&quot;bpi_antisocialT1_2&quot;,&quot;bpi_antisocialT1_3&quot;, &quot;bpi_antisocialT1_4&quot;,&quot;bpi_antisocialT1_5&quot;,&quot;bpi_antisocialT1_6&quot;, &quot;bpi_antisocialT1_7&quot;)], model = 1, group = cnlsy$sex, invariance = c(c( &quot;bpi_antisocialT1_1&quot;,&quot;bpi_antisocialT1_2&quot;,&quot;bpi_antisocialT1_3&quot;, &quot;bpi_antisocialT1_4&quot;,&quot;bpi_antisocialT1_6&quot;,&quot;bpi_antisocialT1_7&quot;), &quot;free_means&quot;, &quot;free_var&quot;), SE = TRUE) coef(constrainedModelResolveItem5, simplify = TRUE) 16.9.9.4 Compare model fit We can compare the model fit against the fully constrained model. In this case, all three of these approaches for handling DIF resulted in improvement in model fit based on AIC and, for the two nested models, the chi-square difference test. Code anova(constrainedModel, constrainedModelDropItem5) Code anova(constrainedModel, constrainedModelDropItem5ForMen) Code anova(constrainedModel, constrainedModelResolveItem5) 16.9.9.5 Next steps Whichever of these approaches we select, we would then identify and handle the remaining non-negligent DIF sequentially by magnitude. For instance, if we chose to resolve DIF in item 5 by allowing the item to have different parameters across both groups, we would then iteratively drop constraints to see which items continue to show non-negligible DIF. Code difDropItemsNew &lt;- DIF( constrainedModelResolveItem5, c(&quot;a1&quot;,&quot;d1&quot;,&quot;d2&quot;), scheme = &quot;drop&quot;, simplify = TRUE) Table 16.1: Differential Item Functioning After Resolving DIF in Item 5. groups converged AIC SABIC HQ BIC X2 df p bpi_antisocialT1_1 female,male TRUE -5.3749227 3.9116912 1.3031195 13.4443213 11.3749227 3 0.0098620 bpi_antisocialT1_2 female,male TRUE 3.9053742 13.1919881 10.5834164 22.7246182 2.0946258 3 0.5530008 bpi_antisocialT1_3 female,male TRUE 1.5024730 10.7890869 8.1805152 20.3217170 4.4975270 3 0.2125110 bpi_antisocialT1_4 female,male TRUE -26.4516356 -17.1650217 -19.7735934 -7.6323916 32.4516356 3 0.0000004 bpi_antisocialT1_5 female,male TRUE 0.0008518 0.0008518 0.0008518 0.0008518 -0.0008518 0 NaN bpi_antisocialT1_6 female,male TRUE -9.9119308 -0.6253169 -3.2338886 8.9073132 15.9119308 3 0.0011821 bpi_antisocialT1_7 female,male TRUE -16.5525142 -7.2659003 -9.8744720 2.2667298 22.5525142 3 0.0000501 Estimates of DIF after resolving item 5 are in Table 16.1. Of the remaining DIF, item 4 appears to have the largest DIF. If we deem item 4 to show non-negligible DIF, we would address it using one of the three approaches above and then see which items continue to show the largest DIF, address it if necessary, then identify the remaining DIF, etc. 16.10 Measurement/Factorial Invariance Before making comparisons across groups in terms of associations between constructs or their level on the construct, it is important to establish measurement invariance (also called factorial invariance) across the groups (Millsap, 2011). Tests of measurement invariance are equivalent to tests of differential item functioning. Measurement invariance can help provide greater confidence that the measure functions equivalently across groups, that is, the items have the same strength of association with the latent factor, and the latent factor is on the same metric (i.e., the items have the same level when accounting for the latent factor). For instance, If you observe differences between groups in level of depression without establishing measurement invariance across the groups, you do not know whether the differences observed reflect true group-related differences in depression or differences in the functioning of the measure across the groups. Measurement invariance can be tested using many methods, including confirmatory factor analysis and IRT. CFA approaches to testing measurement invariance include multi-group CFA (MGCFA), multiple-indicator, multiple-causes (MIMIC) models, moderated nonlinear factor analysis (MNLFA), score-based tests (T. Wang et al., 2014), and the alignment method (Lai, 2021). The IRT approach to testing measurement (non-)invariance is to examine whether items show differential item functioning. The tests of measurement invariance in confirmatory factor analysis (CFA) models were fit in the lavaan package (Rosseel et al., 2022). The examples were adapted from lavaan documentation: https://lavaan.ugent.be/tutorial/groups.html (archived at https://perma.cc/2FBK-RSAH). Procedures for testing measurement invariance are outlined in Putnick &amp; Bornstein (2016). In addition to nested chi-square difference tests (\\(\\chi^2_{\\text{diff}}\\)), I also demonstrate permutation procedures for testing measurement invariance, as described by Jorgensen et al. (2018). Also, you are encouraged to read about MIMIC models (Cheng et al., 2016; W.-C. Wang et al., 2009), score-based tests (T. Wang et al., 2014), and MNLFA that allows for testing measurement invariance across continuous moderators (Bauer et al., 2020; Curran et al., 2014; N. C. Gottfredson et al., 2019). MNLFA is implemented in the mnlfa package (Robitzsch, 2019). You can also generate syntax for conducting MNLFA in Mplus software (Muthén &amp; Muthén, 2019) using the aMNLFA package (V. Cole et al., 2018). The alignment method allows many groups to be compared (Han et al., 2019). Model fit of nested models can be compared with a chi-square difference test, as a way of testing measurement invariance. In this approach to testing measurement invariance, first model fit is evaluated in a configural invariance model, in which the same number of factors is specified in each group, and which indicators load on which factors are the same in each group. Then, successive constraints are made across groups, including constraining factor loadings, intercepts, and residuals across groups. The metric (“weak factorial”) invariance model is similar to the configural invariance model, but it constrains the factor loadings to be the same across groups. The scalar (“strong factorial”) invariance model keeps the constraints of the metric invariance model, but it also constrains the intercepts to be the same across groups. The residual (“strict factorial”) invariance model keeps the constraints of the scalar invariance model, but it also constrains the residuals to be the same across groups. The fit of each constrained model is compared to the previous model without such constraints. That is, the fit of the metric invariance model is compared to the fit of the configural invariance model, the fit of the scalar invariance model is compared to the fit of the metric invariance model, and the fit of the residual invariance model is compared to the fit of the scalar invariance model. The chi-square difference test is sensitive to sample size, and trivial differences in fit can be detected with large samples (Cheung &amp; Rensvold, 2002). As a result, researchers also recommend examining change in additional criteria, including CFI, RMSEA, and SRMR (F. F. Chen, 2007). Cheung &amp; Rensvold (2002) recommend a cutoff of \\(\\Delta \\text{CFI} \\geq -.01\\) for identifying measurement non-invariance. F. F. Chen (2007) recommends the following cutoffs for identifying measurement non-invariance: with a small sample size (total \\(N \\leq 300\\)): testing invariance of factor loadings: \\(\\Delta \\text{CFI} \\geq -.005\\) supplemented by \\(\\Delta \\text{RMSEA} \\geq .010\\) or \\(\\Delta \\text{SRMR} \\geq .025\\) testing invariance of intercepts or residuals: \\(\\Delta \\text{CFI} \\geq -.005\\) supplemented by \\(\\Delta \\text{RMSEA} \\geq .010\\) or \\(\\Delta \\text{SRMR} \\geq .005\\) with an adequate sample size (total \\(N &gt; 300\\)): testing invariance of factor loadings: \\(\\Delta \\text{CFI} \\geq -.010\\) supplemented by \\(\\Delta \\text{RMSEA} \\geq .015\\) or \\(\\Delta \\text{SRMR} \\geq .030\\) testing invariance of intercepts or residuals: \\(\\Delta \\text{CFI} \\geq -.005\\) supplemented by \\(\\Delta \\text{RMSEA} \\geq .015\\) or \\(\\Delta \\text{SRMR} \\geq .010\\) Little et al. (2007) suggested that researchers establish at least partial invariance of factor loadings (metric invariance) to compare covariances across groups, and at least partial invariance of intercepts (scalar invariance) to compare mean levels across groups. Partial invariance refers to invariance with some but not all indicators. So, to examine associations with other variables, invariance of at least some factor loadings would be preferable. To examine differences in level or growth, invariance of at least some factor loadings and intercepts would be preferable. Residual invariance is considered overly restrictive, and it is not generally expected that one establish residual invariance (Little, 2013). Although measurement invariance is important to test, it is also worth noting that tests of measurement invariance and DIF rest on various fundamentally untestable assumptions related to scale setting (Raykov et al., 2020). For instance, to give the latent factor units, oftentimes researchers set one item’s factor loading to be equal across time (i.e., the marker variable). Results of factorial invariance tests can depend highly on which item is used as the anchor item for setting the scale of the latent factor (Belzak &amp; Bauer, 2020). And subsequent tests of measurement invariance then rest on the fundamentally untestable assumption that changes in the level of the latent factor precipitates the same amount of change in the item across groups. If this scaling is not proper, however, it could lead to improper conclusions. Researchers are, therefore, not conclusively able to establish measurement invariance. Several possible approaches may help address this. First, it can be helpful to place greater focus on degree of measurement invariance and confidence (rather than presence versus absence of measurement invariance). To the extent that non-invariance is trivial in effect size, it provides the researcher with greater confidence that they can use the measure to assess a construct in a comparable way over time. A number of studies describe how to test the effect size of measurement invariance (Gunn et al., 2020; e.g., Liu et al., 2017) or differential item functioning (Gonzalez &amp; Pelham, 2021; e.g., Meade, 2010). Second, there may be important robustness checks. I describe each in greater detail below. One important sensitivity analysis could be to see if measurement invariance holds when using different marker variables. This would provide greater evidence that their apparent measurement invariance was not specific to one marker variable (i.e., one particular set of assumptions). A second robustness check would be to use effects coding, in which the average of items’ factor loadings is equal to 1, so the metric of the latent variable is on the metric of all of the items rather than just one item (Little et al., 2006). A third robustness check would be to use regularization, which is an alternative method to select the anchor items and to identify differential item functioning based on a machine learning technique that applies penalization to remove parameters that have little impact on model fit (Bauer et al., 2020; Belzak &amp; Bauer, 2020). Another issue with measurement invariance is that the null hypothesis that adding constraints across groups will not worsen fit is likely always false, because there are often at least slight differences across groups in factor loadings, intercepts, or residuals that reflect sampling variability. However, we are not interested in trivial differences across groups that reflect sampling variability. Instead, we are interested in the extent to which there are substantive, meaningful differences across the groups of large enough magnitude to be practically meaningful. Thus, it can also be helpful to consider whether the measures show approximate measurement invariance (Van De Schoot et al., 2015). For details on how to test approximate measurement invariance, see Van De Schoot et al. (2013). When detecting measurement non-invariance in a given parameter (e.g., factor loadings, intercepts, or residuals), one can identify the specific items that show measurement non-invariance in one of two primary ways: (1) starting with a model that allows the given parameter to differ across groups, a researcher can iteratively add constraints to identify the item(s) for which measurement invariance fails, or (2) starting with a model that constrains the given parameter to be the same across groups, a researcher can iteratively remove constraints to identify the item(s) for which measurement invariance becomes established (and by process of elimination, the items for which measurement invariance does not become established). Approaches to addressing measurement non-invariance are described in Section 16.5.1. If we identify that an item shows non-negligible non-invariance, we have three primary options (described above): (1) drop the item for both groups, (2) drop the item for one group but keep it for the other group, or (3) freely estimate the parameters (factor loadings and/or intercepts) for the item across groups. Using the traditional chi-square difference test, tests of measurement invariance compare the model fit to a model with perfect fit. As the sample size grows larger, smaller differences in model fit will be detected as significant, thus rejecting measurement invariance even if the model fits well. So, it can also be useful to compare the model fit to a null hypothesis that the fit is poor, instead of the null hypothesis that the model is a perfect fit. Chi-square equivalence tests use the poor model fit as the null hypothesis. Thus, a significant chi-square equivalence test suggests that the equality constraints are plausible, providing support for the alternative hypothesis that the model fit is acceptable (Counsell et al., 2020). 16.10.1 Specify Models 16.10.1.1 Null Model Fix residual variances and intercepts of manifest variables to be equal across groups. Code nullModel &lt;- &#39; #Fix residual variances of manifest variables to be equal across groups x1 ~~ c(psi1, psi1)*x1 x2 ~~ c(psi2, psi2)*x2 x3 ~~ c(psi3, psi3)*x3 x4 ~~ c(psi4, psi4)*x4 x5 ~~ c(psi5, psi5)*x5 x6 ~~ c(psi6, psi6)*x6 x7 ~~ c(psi7, psi7)*x7 x8 ~~ c(psi8, psi8)*x8 x9 ~~ c(psi9, psi9)*x9 #Fix intercepts of manifest variables to be equal across groups x1 ~ c(tau1, tau1)*1 x2 ~ c(tau2, tau2)*1 x3 ~ c(tau3, tau3)*1 x4 ~ c(tau4, tau4)*1 x5 ~ c(tau5, tau5)*1 x6 ~ c(tau6, tau6)*1 x7 ~ c(tau7, tau7)*1 x8 ~ c(tau8, tau8)*1 x9 ~ c(tau9, tau9)*1 &#39; 16.10.1.2 CFA Model Code cfaModel &lt;- &#39; #Factor loadings visual =~ x1 + x2 + x3 textual =~ x4 + x5 + x6 speed =~ x7 + x8 + x9 &#39; 16.10.1.3 Configural Invariance Specify the same number of factors in each group, and which indicators load on which factors are the same in each group. Code cfaModel_configuralInvariance &lt;- &#39; #Factor loadings (free the factor loading of the first indicator) visual =~ NA*x1 + x2 + x3 textual =~ NA*x4 + x5 + x6 speed =~ NA*x7 + x8 + x9 #Fix latent means to zero visual ~ 0 textual ~ 0 speed ~ 0 #Fix latent variances to one visual ~~ 1*visual textual ~~ 1*textual speed ~~ 1*speed #Estimate covariances among latent variables visual ~~ textual visual ~~ speed textual ~~ speed #Estimate residual variances of manifest variables x1 ~~ x1 x2 ~~ x2 x3 ~~ x3 x4 ~~ x4 x5 ~~ x5 x6 ~~ x6 x7 ~~ x7 x8 ~~ x8 x9 ~~ x9 #Free intercepts of manifest variables x1 ~ NA*1 x2 ~ NA*1 x3 ~ NA*1 x4 ~ NA*1 x5 ~ NA*1 x6 ~ NA*1 x7 ~ NA*1 x8 ~ NA*1 x9 ~ NA*1 &#39; 16.10.1.4 Metric (“Weak Factorial”) Invariance Specify invariance of factor loadings across groups. Code cfaModel_metricInvariance &lt;- &#39; #Fix factor loadings to be the same across groups visual =~ c(lambdax1,lambdax1)*x1 + c(lambdax2,lambdax2)*x2 + c(lambdax3,lambdax3)*x3 textual =~ c(lambdax4,lambdax4)*x4 + c(lambdax5,lambdax5)*x5 + c(lambdax6,lambdax6)*x6 speed =~ c(lambdax7,lambdax7)*x7 + c(lambdax8,lambdax8)*x8 + c(lambdax9,lambdax9)*x9 #Fix latent means to zero visual ~ 0 textual ~ 0 speed ~ 0 #Fix latent variances to one in group 1; free latent variances in group 2 visual ~~ c(1, NA)*visual textual ~~ c(1, NA)*textual speed ~~ c(1, NA)*speed #Estimate covariances among latent variables visual ~~ textual visual ~~ speed textual ~~ speed #Estimate residual variances of manifest variables x1 ~~ x1 x2 ~~ x2 x3 ~~ x3 x4 ~~ x4 x5 ~~ x5 x6 ~~ x6 x7 ~~ x7 x8 ~~ x8 x9 ~~ x9 #Free intercepts of manifest variables x1 ~ NA*1 x2 ~ NA*1 x3 ~ NA*1 x4 ~ NA*1 x5 ~ NA*1 x6 ~ NA*1 x7 ~ NA*1 x8 ~ NA*1 x9 ~ NA*1 &#39; 16.10.1.5 Scalar (“Strong Factorial”) Invariance Specify invariance of factor loadings and intercepts across groups. Code cfaModel_scalarInvariance &lt;- &#39; #Fix factor loadings to be the same across groups visual =~ c(lambdax1,lambdax1)*x1 + c(lambdax2,lambdax2)*x2 + c(lambdax3,lambdax3)*x3 textual =~ c(lambdax4,lambdax4)*x4 + c(lambdax5,lambdax5)*x5 + c(lambdax6,lambdax6)*x6 speed =~ c(lambdax7,lambdax7)*x7 + c(lambdax8,lambdax8)*x8 + c(lambdax9,lambdax9)*x9 #Fix latent means to zero in group 1; free latent means in group 2 visual ~ c(0, NA)*1 textual ~ c(0, NA)*1 speed ~ c(0, NA)*1 #Fix latent variances to one in group 1; free latent variances in group 2 visual ~~ c(1, NA)*visual textual ~~ c(1, NA)*textual speed ~~ c(1, NA)*speed #Estimate covariances among latent variables visual ~~ textual visual ~~ speed textual ~~ speed #Estimate residual variances of manifest variables x1 ~~ x1 x2 ~~ x2 x3 ~~ x3 x4 ~~ x4 x5 ~~ x5 x6 ~~ x6 x7 ~~ x7 x8 ~~ x8 x9 ~~ x9 #Fix intercepts of manifest variables across groups x1 ~ c(intx1, intx1)*1 x2 ~ c(intx2, intx2)*1 x3 ~ c(intx3, intx3)*1 x4 ~ c(intx4, intx4)*1 x5 ~ c(intx5, intx5)*1 x6 ~ c(intx6, intx6)*1 x7 ~ c(intx7, intx7)*1 x8 ~ c(intx8, intx8)*1 x9 ~ c(intx9, intx9)*1 &#39; 16.10.1.6 Residual (“Strict Factorial”) Invariance Specify invariance of factor loadings, intercepts, and residuals across groups. Code cfaModel_residualInvariance &lt;- &#39; #Fix factor loadings to be the same across groups visual =~ c(lambdax1,lambdax1)*x1 + c(lambdax2,lambdax2)*x2 + c(lambdax3,lambdax3)*x3 textual =~ c(lambdax4,lambdax4)*x4 + c(lambdax5,lambdax5)*x5 + c(lambdax6,lambdax6)*x6 speed =~ c(lambdax7,lambdax7)*x7 + c(lambdax8,lambdax8)*x8 + c(lambdax9,lambdax9)*x9 #Fix latent means to zero visual ~ c(0, NA)*1 textual ~ c(0, NA)*1 speed ~ c(0, NA)*1 #Fix latent variances to one in group 1; free latent variances in group 2 visual ~~ c(1, NA)*visual textual ~~ c(1, NA)*textual speed ~~ c(1, NA)*speed #Estimate covariances among latent variables visual ~~ textual visual ~~ speed textual ~~ speed #Fix residual variances of manifest variables across groups x1 ~~ c(residx1, residx1)*x1 x2 ~~ c(residx2, residx2)*x2 x3 ~~ c(residx3, residx3)*x3 x4 ~~ c(residx4, residx4)*x4 x5 ~~ c(residx5, residx5)*x5 x6 ~~ c(residx6, residx6)*x6 x7 ~~ c(residx7, residx7)*x7 x8 ~~ c(residx8, residx8)*x8 x9 ~~ c(residx9, residx9)*x9 #Fix intercepts of manifest variables across groups x1 ~ c(intx1, intx1)*1 x2 ~ c(intx2, intx2)*1 x3 ~ c(intx3, intx3)*1 x4 ~ c(intx4, intx4)*1 x5 ~ c(intx5, intx5)*1 x6 ~ c(intx6, intx6)*1 x7 ~ c(intx7, intx7)*1 x8 ~ c(intx8, intx8)*1 x9 ~ c(intx9, intx9)*1 &#39; 16.10.2 Specify the Fit Indices of Interest Code myAFIs &lt;- c(&quot;chisq&quot;, &quot;chisq.scaled&quot;, &quot;rmsea&quot;, &quot;cfi&quot;, &quot;tli&quot;, &quot;srmr&quot;, &quot;rmsea.robust&quot;, &quot;cfi.robust&quot;, &quot;tli.robust&quot;) moreAFIs &lt;- NULL # c(&quot;gammaHat&quot;,&quot;gammaHat.scaled&quot;) 16.10.3 Null Model 16.10.3.1 Fit the Model Code nullModelFit &lt;- lavaan( nullModel, data = HolzingerSwineford1939, group = &quot;school&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) 16.10.3.2 Model Summary Code summary( nullModelFit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 42 iterations Estimator ML Optimization method NLMINB Number of model parameters 36 Number of equality constraints 18 Number of observations per group: 2 132 1 127 Number of missing patterns per group: 2 48 1 52 Model Test User Model: Standard Scaled Test Statistic 657.507 618.302 Degrees of freedom 90 90 P-value (Chi-square) 0.000 0.000 Scaling correction factor 1.063 Yuan-Bentler correction (Mplus variant) Test statistic for each group: 2 314.801 296.031 1 342.706 322.272 Model Test Baseline Model: Test statistic 604.129 571.412 Degrees of freedom 72 72 P-value 0.000 0.000 Scaling correction factor 1.057 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.000 0.000 Tucker-Lewis Index (TLI) 0.147 0.154 Robust Comparative Fit Index (CFI) 0.000 Robust Tucker-Lewis Index (TLI) 0.150 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -2979.251 -2979.251 Scaling correction factor 0.541 for the MLR correction Loglikelihood unrestricted model (H1) -2650.498 -2650.498 Scaling correction factor 1.066 for the MLR correction Akaike (AIC) 5994.503 5994.503 Bayesian (BIC) 6058.526 6058.526 Sample-size adjusted Bayesian (SABIC) 6001.459 6001.459 Root Mean Square Error of Approximation: RMSEA 0.221 0.213 90 Percent confidence interval - lower 0.205 0.198 90 Percent confidence interval - upper 0.237 0.228 P-value H_0: RMSEA &lt;= 0.050 0.000 0.000 P-value H_0: RMSEA &gt;= 0.080 1.000 1.000 Robust RMSEA 0.257 90 Percent confidence interval - lower 0.238 90 Percent confidence interval - upper 0.277 P-value H_0: Robust RMSEA &lt;= 0.050 0.000 P-value H_0: Robust RMSEA &gt;= 0.080 1.000 Standardized Root Mean Square Residual: SRMR 0.281 0.281 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Group 1 [2]: Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all x1 (tau1) 4.923 0.079 62.591 0.000 4.923 4.249 x2 (tau2) 6.060 0.076 79.251 0.000 6.060 5.249 x3 (tau3) 2.191 0.076 28.807 0.000 2.191 1.974 x4 (tau4) 3.050 0.080 37.996 0.000 3.050 2.603 x5 (tau5) 4.317 0.084 51.540 0.000 4.317 3.475 x6 (tau6) 2.179 0.070 31.159 0.000 2.179 2.140 x7 (tau7) 4.175 0.069 60.464 0.000 4.175 4.013 x8 (tau8) 5.509 0.068 80.899 0.000 5.509 5.467 x9 (tau9) 5.371 0.069 78.103 0.000 5.371 5.278 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all x1 (psi1) 1.343 0.134 9.992 0.000 1.343 1.000 x2 (psi2) 1.333 0.139 9.570 0.000 1.333 1.000 x3 (psi3) 1.232 0.090 13.697 0.000 1.232 1.000 x4 (psi4) 1.373 0.143 9.607 0.000 1.373 1.000 x5 (psi5) 1.544 0.130 11.887 0.000 1.544 1.000 x6 (psi6) 1.037 0.132 7.842 0.000 1.037 1.000 x7 (psi7) 1.083 0.092 11.749 0.000 1.083 1.000 x8 (psi8) 1.015 0.132 7.719 0.000 1.015 1.000 x9 (psi9) 1.036 0.111 9.332 0.000 1.036 1.000 Group 2 [1]: Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all x1 (tau1) 4.923 0.079 62.591 0.000 4.923 4.249 x2 (tau2) 6.060 0.076 79.251 0.000 6.060 5.249 x3 (tau3) 2.191 0.076 28.807 0.000 2.191 1.974 x4 (tau4) 3.050 0.080 37.996 0.000 3.050 2.603 x5 (tau5) 4.317 0.084 51.540 0.000 4.317 3.475 x6 (tau6) 2.179 0.070 31.159 0.000 2.179 2.140 x7 (tau7) 4.175 0.069 60.464 0.000 4.175 4.013 x8 (tau8) 5.509 0.068 80.899 0.000 5.509 5.467 x9 (tau9) 5.371 0.069 78.103 0.000 5.371 5.278 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all x1 (psi1) 1.343 0.134 9.992 0.000 1.343 1.000 x2 (psi2) 1.333 0.139 9.570 0.000 1.333 1.000 x3 (psi3) 1.232 0.090 13.697 0.000 1.232 1.000 x4 (psi4) 1.373 0.143 9.607 0.000 1.373 1.000 x5 (psi5) 1.544 0.130 11.887 0.000 1.544 1.000 x6 (psi6) 1.037 0.132 7.842 0.000 1.037 1.000 x7 (psi7) 1.083 0.092 11.749 0.000 1.083 1.000 x8 (psi8) 1.015 0.132 7.719 0.000 1.015 1.000 x9 (psi9) 1.036 0.111 9.332 0.000 1.036 1.000 16.10.4 Configural Invariance Specify the same number of factors in each group, and which indicators load on which factors are the same in each group. 16.10.4.1 Model Syntax Code configuralInvarianceModel &lt;- measEq.syntax( configural.model = cfaModel, data = HolzingerSwineford1939, ID.fac = &quot;std.lv&quot;, group = &quot;school&quot;) Code cat(as.character(configuralInvarianceModel)) ## LOADINGS: visual =~ c(NA, NA)*x1 + c(lambda.1_1.g1, lambda.1_1.g2)*x1 visual =~ c(NA, NA)*x2 + c(lambda.2_1.g1, lambda.2_1.g2)*x2 visual =~ c(NA, NA)*x3 + c(lambda.3_1.g1, lambda.3_1.g2)*x3 textual =~ c(NA, NA)*x4 + c(lambda.4_2.g1, lambda.4_2.g2)*x4 textual =~ c(NA, NA)*x5 + c(lambda.5_2.g1, lambda.5_2.g2)*x5 textual =~ c(NA, NA)*x6 + c(lambda.6_2.g1, lambda.6_2.g2)*x6 speed =~ c(NA, NA)*x7 + c(lambda.7_3.g1, lambda.7_3.g2)*x7 speed =~ c(NA, NA)*x8 + c(lambda.8_3.g1, lambda.8_3.g2)*x8 speed =~ c(NA, NA)*x9 + c(lambda.9_3.g1, lambda.9_3.g2)*x9 ## INTERCEPTS: x1 ~ c(NA, NA)*1 + c(nu.1.g1, nu.1.g2)*1 x2 ~ c(NA, NA)*1 + c(nu.2.g1, nu.2.g2)*1 x3 ~ c(NA, NA)*1 + c(nu.3.g1, nu.3.g2)*1 x4 ~ c(NA, NA)*1 + c(nu.4.g1, nu.4.g2)*1 x5 ~ c(NA, NA)*1 + c(nu.5.g1, nu.5.g2)*1 x6 ~ c(NA, NA)*1 + c(nu.6.g1, nu.6.g2)*1 x7 ~ c(NA, NA)*1 + c(nu.7.g1, nu.7.g2)*1 x8 ~ c(NA, NA)*1 + c(nu.8.g1, nu.8.g2)*1 x9 ~ c(NA, NA)*1 + c(nu.9.g1, nu.9.g2)*1 ## UNIQUE-FACTOR VARIANCES: x1 ~~ c(NA, NA)*x1 + c(theta.1_1.g1, theta.1_1.g2)*x1 x2 ~~ c(NA, NA)*x2 + c(theta.2_2.g1, theta.2_2.g2)*x2 x3 ~~ c(NA, NA)*x3 + c(theta.3_3.g1, theta.3_3.g2)*x3 x4 ~~ c(NA, NA)*x4 + c(theta.4_4.g1, theta.4_4.g2)*x4 x5 ~~ c(NA, NA)*x5 + c(theta.5_5.g1, theta.5_5.g2)*x5 x6 ~~ c(NA, NA)*x6 + c(theta.6_6.g1, theta.6_6.g2)*x6 x7 ~~ c(NA, NA)*x7 + c(theta.7_7.g1, theta.7_7.g2)*x7 x8 ~~ c(NA, NA)*x8 + c(theta.8_8.g1, theta.8_8.g2)*x8 x9 ~~ c(NA, NA)*x9 + c(theta.9_9.g1, theta.9_9.g2)*x9 ## LATENT MEANS/INTERCEPTS: visual ~ c(0, 0)*1 + c(alpha.1.g1, alpha.1.g2)*1 textual ~ c(0, 0)*1 + c(alpha.2.g1, alpha.2.g2)*1 speed ~ c(0, 0)*1 + c(alpha.3.g1, alpha.3.g2)*1 ## COMMON-FACTOR VARIANCES: visual ~~ c(1, 1)*visual + c(psi.1_1.g1, psi.1_1.g2)*visual textual ~~ c(1, 1)*textual + c(psi.2_2.g1, psi.2_2.g2)*textual speed ~~ c(1, 1)*speed + c(psi.3_3.g1, psi.3_3.g2)*speed ## COMMON-FACTOR COVARIANCES: visual ~~ c(NA, NA)*textual + c(psi.2_1.g1, psi.2_1.g2)*textual visual ~~ c(NA, NA)*speed + c(psi.3_1.g1, psi.3_1.g2)*speed textual ~~ c(NA, NA)*speed + c(psi.3_2.g1, psi.3_2.g2)*speed Code configuralInvarianceSyntax &lt;- as.character(configuralInvarianceModel) 16.10.4.1.1 Summary of Model Features Code summary(configuralInvarianceModel) This lavaan model syntax specifies a CFA with 9 manifest indicators of 3 common factor(s). To identify the location and scale of each common factor, the factor means and variances were fixed to 0 and 1, respectively, unless equality constraints on measurement parameters allow them to be freed. Pattern matrix indicating num(eric), ord(ered), and lat(ent) indicators per factor: visual textual speed x1 num x2 num x3 num x4 num x5 num x6 num x7 num x8 num x9 num This model hypothesizes only configural invariance. 16.10.4.1.2 Model Syntax in Table Form Code lavaanify(cfaModel, ngroups = 2) 16.10.4.2 Fit the Model Code configuralInvarianceModel_fit &lt;- cfa( configuralInvarianceSyntax, data = HolzingerSwineford1939, group = &quot;school&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) Code configuralInvarianceModelFullSyntax_fit &lt;- lavaan( cfaModel_configuralInvariance, data = HolzingerSwineford1939, group = &quot;school&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) 16.10.4.3 Model Summary Code summary( configuralInvarianceModel_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 62 iterations Estimator ML Optimization method NLMINB Number of model parameters 60 Number of observations per group: 2 132 1 127 Number of missing patterns per group: 2 48 1 52 Model Test User Model: Standard Scaled Test Statistic 71.443 70.904 Degrees of freedom 48 48 P-value (Chi-square) 0.016 0.017 Scaling correction factor 1.008 Yuan-Bentler correction (Mplus variant) Test statistic for each group: 2 43.974 43.643 1 27.469 27.262 Model Test Baseline Model: Test statistic 604.129 571.412 Degrees of freedom 72 72 P-value 0.000 0.000 Scaling correction factor 1.057 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.956 0.954 Tucker-Lewis Index (TLI) 0.934 0.931 Robust Comparative Fit Index (CFI) 0.955 Robust Tucker-Lewis Index (TLI) 0.933 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -2686.219 -2686.219 Scaling correction factor 1.114 for the MLR correction Loglikelihood unrestricted model (H1) -2650.498 -2650.498 Scaling correction factor 1.066 for the MLR correction Akaike (AIC) 5492.439 5492.439 Bayesian (BIC) 5705.848 5705.848 Sample-size adjusted Bayesian (SABIC) 5515.627 5515.627 Root Mean Square Error of Approximation: RMSEA 0.061 0.061 90 Percent confidence interval - lower 0.027 0.026 90 Percent confidence interval - upper 0.090 0.089 P-value H_0: RMSEA &lt;= 0.050 0.252 0.264 P-value H_0: RMSEA &gt;= 0.080 0.151 0.142 Robust RMSEA 0.072 90 Percent confidence interval - lower 0.018 90 Percent confidence interval - upper 0.111 P-value H_0: Robust RMSEA &lt;= 0.050 0.192 P-value H_0: Robust RMSEA &gt;= 0.080 0.397 Standardized Root Mean Square Residual: SRMR 0.067 0.067 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Group 1 [2]: Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual =~ x1 (l.1_) 0.986 0.165 5.980 0.000 0.986 0.818 x2 (l.2_) 0.540 0.150 3.598 0.000 0.540 0.449 x3 (l.3_) 0.748 0.122 6.123 0.000 0.748 0.664 textual =~ x4 (l.4_) 0.910 0.105 8.698 0.000 0.910 0.794 x5 (l.5_) 1.186 0.115 10.345 0.000 1.186 0.902 x6 (l.6_) 0.783 0.107 7.298 0.000 0.783 0.807 speed =~ x7 (l.7_) 0.442 0.201 2.199 0.028 0.442 0.437 x8 (l.8_) 0.594 0.200 2.969 0.003 0.594 0.637 x9 (l.9_) 0.610 0.211 2.892 0.004 0.610 0.612 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual ~~ textul (p.2_) 0.453 0.125 3.614 0.000 0.453 0.453 speed (p.3_1) 0.423 0.225 1.882 0.060 0.423 0.423 textual ~~ speed (p.3_2) 0.311 0.115 2.707 0.007 0.311 0.311 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 (n.1.) 4.973 0.114 43.612 0.000 4.973 4.122 .x2 (n.2.) 5.978 0.110 54.281 0.000 5.978 4.965 .x3 (n.3.) 2.375 0.106 22.330 0.000 2.375 2.108 .x4 (n.4.) 2.818 0.105 26.854 0.000 2.818 2.462 .x5 (n.5.) 4.025 0.120 33.603 0.000 4.025 3.058 .x6 (n.6.) 1.975 0.087 22.640 0.000 1.975 2.036 .x7 (n.7.) 4.363 0.093 46.747 0.000 4.363 4.314 .x8 (n.8.) 5.464 0.088 61.957 0.000 5.464 5.856 .x9 (n.9.) 5.404 0.093 58.016 0.000 5.404 5.426 visual (a.1.) 0.000 0.000 0.000 textual (a.2.) 0.000 0.000 0.000 speed (a.3.) 0.000 0.000 0.000 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 (t.1_) 0.482 0.265 1.822 0.068 0.482 0.331 .x2 (t.2_) 1.158 0.199 5.817 0.000 1.158 0.798 .x3 (t.3_) 0.711 0.163 4.372 0.000 0.711 0.560 .x4 (t.4_) 0.484 0.093 5.198 0.000 0.484 0.369 .x5 (t.5_) 0.324 0.128 2.542 0.011 0.324 0.187 .x6 (t.6_) 0.328 0.075 4.364 0.000 0.328 0.348 .x7 (t.7_) 0.827 0.172 4.810 0.000 0.827 0.809 .x8 (t.8_) 0.518 0.204 2.535 0.011 0.518 0.595 .x9 (t.9_) 0.620 0.261 2.375 0.018 0.620 0.625 visual (p.1_) 1.000 1.000 1.000 textual (p.2_) 1.000 1.000 1.000 speed (p.3_) 1.000 1.000 1.000 R-Square: Estimate x1 0.669 x2 0.202 x3 0.440 x4 0.631 x5 0.813 x6 0.652 x7 0.191 x8 0.405 x9 0.375 Group 2 [1]: Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual =~ x1 (l.1_) 0.741 0.127 5.830 0.000 0.741 0.661 x2 (l.2_) 0.459 0.107 4.309 0.000 0.459 0.424 x3 (l.3_) 0.783 0.103 7.615 0.000 0.783 0.745 textual =~ x4 (l.4_) 0.971 0.098 9.898 0.000 0.971 0.867 x5 (l.5_) 0.931 0.106 8.806 0.000 0.931 0.815 x6 (l.6_) 0.879 0.113 7.764 0.000 0.879 0.823 speed =~ x7 (l.7_) 0.629 0.105 5.982 0.000 0.629 0.605 x8 (l.8_) 0.852 0.146 5.815 0.000 0.852 0.797 x9 (l.9_) 0.712 0.150 4.755 0.000 0.712 0.687 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual ~~ textul (p.2_) 0.551 0.104 5.299 0.000 0.551 0.551 speed (p.3_1) 0.627 0.142 4.410 0.000 0.627 0.627 textual ~~ speed (p.3_2) 0.350 0.174 2.015 0.044 0.350 0.350 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 (n.1.) 4.882 0.105 46.528 0.000 4.882 4.354 .x2 (n.2.) 6.160 0.103 59.887 0.000 6.160 5.692 .x3 (n.3.) 1.990 0.098 20.392 0.000 1.990 1.894 .x4 (n.4.) 3.305 0.104 31.664 0.000 3.305 2.951 .x5 (n.5.) 4.669 0.104 44.694 0.000 4.669 4.089 .x6 (n.6.) 2.444 0.103 23.796 0.000 2.444 2.286 .x7 (n.7.) 4.004 0.097 41.179 0.000 4.004 3.850 .x8 (n.8.) 5.570 0.099 56.357 0.000 5.570 5.211 .x9 (n.9.) 5.374 0.099 54.445 0.000 5.374 5.189 visual (a.1.) 0.000 0.000 0.000 textual (a.2.) 0.000 0.000 0.000 speed (a.3.) 0.000 0.000 0.000 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 (t.1_) 0.708 0.178 3.977 0.000 0.708 0.563 .x2 (t.2_) 0.961 0.189 5.083 0.000 0.961 0.820 .x3 (t.3_) 0.492 0.123 3.984 0.000 0.492 0.445 .x4 (t.4_) 0.311 0.083 3.745 0.000 0.311 0.248 .x5 (t.5_) 0.438 0.096 4.575 0.000 0.438 0.336 .x6 (t.6_) 0.369 0.095 3.867 0.000 0.369 0.323 .x7 (t.7_) 0.686 0.118 5.795 0.000 0.686 0.634 .x8 (t.8_) 0.417 0.220 1.899 0.058 0.417 0.365 .x9 (t.9_) 0.566 0.156 3.639 0.000 0.566 0.528 visual (p.1_) 1.000 1.000 1.000 textual (p.2_) 1.000 1.000 1.000 speed (p.3_) 1.000 1.000 1.000 R-Square: Estimate x1 0.437 x2 0.180 x3 0.555 x4 0.752 x5 0.664 x6 0.677 x7 0.366 x8 0.635 x9 0.472 Code summary( configuralInvarianceModelFullSyntax_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 62 iterations Estimator ML Optimization method NLMINB Number of model parameters 60 Number of observations per group: 2 132 1 127 Number of missing patterns per group: 2 48 1 52 Model Test User Model: Standard Scaled Test Statistic 71.443 70.904 Degrees of freedom 48 48 P-value (Chi-square) 0.016 0.017 Scaling correction factor 1.008 Yuan-Bentler correction (Mplus variant) Test statistic for each group: 2 43.974 43.643 1 27.469 27.262 Model Test Baseline Model: Test statistic 604.129 571.412 Degrees of freedom 72 72 P-value 0.000 0.000 Scaling correction factor 1.057 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.956 0.954 Tucker-Lewis Index (TLI) 0.934 0.931 Robust Comparative Fit Index (CFI) 0.955 Robust Tucker-Lewis Index (TLI) 0.933 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -2686.219 -2686.219 Scaling correction factor 1.114 for the MLR correction Loglikelihood unrestricted model (H1) -2650.498 -2650.498 Scaling correction factor 1.066 for the MLR correction Akaike (AIC) 5492.439 5492.439 Bayesian (BIC) 5705.848 5705.848 Sample-size adjusted Bayesian (SABIC) 5515.627 5515.627 Root Mean Square Error of Approximation: RMSEA 0.061 0.061 90 Percent confidence interval - lower 0.027 0.026 90 Percent confidence interval - upper 0.090 0.089 P-value H_0: RMSEA &lt;= 0.050 0.252 0.264 P-value H_0: RMSEA &gt;= 0.080 0.151 0.142 Robust RMSEA 0.072 90 Percent confidence interval - lower 0.018 90 Percent confidence interval - upper 0.111 P-value H_0: Robust RMSEA &lt;= 0.050 0.192 P-value H_0: Robust RMSEA &gt;= 0.080 0.397 Standardized Root Mean Square Residual: SRMR 0.067 0.067 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Group 1 [2]: Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual =~ x1 0.986 0.165 5.980 0.000 0.986 0.818 x2 0.540 0.150 3.598 0.000 0.540 0.449 x3 0.748 0.122 6.123 0.000 0.748 0.664 textual =~ x4 0.910 0.105 8.698 0.000 0.910 0.794 x5 1.186 0.115 10.345 0.000 1.186 0.902 x6 0.783 0.107 7.298 0.000 0.783 0.807 speed =~ x7 0.442 0.201 2.199 0.028 0.442 0.437 x8 0.594 0.200 2.969 0.003 0.594 0.637 x9 0.610 0.211 2.892 0.004 0.610 0.612 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual ~~ textual 0.453 0.125 3.614 0.000 0.453 0.453 speed 0.423 0.225 1.882 0.060 0.423 0.423 textual ~~ speed 0.311 0.115 2.707 0.007 0.311 0.311 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual 0.000 0.000 0.000 textual 0.000 0.000 0.000 speed 0.000 0.000 0.000 .x1 4.973 0.114 43.612 0.000 4.973 4.122 .x2 5.978 0.110 54.281 0.000 5.978 4.965 .x3 2.375 0.106 22.330 0.000 2.375 2.108 .x4 2.818 0.105 26.854 0.000 2.818 2.462 .x5 4.025 0.120 33.603 0.000 4.025 3.058 .x6 1.975 0.087 22.640 0.000 1.975 2.036 .x7 4.363 0.093 46.747 0.000 4.363 4.314 .x8 5.464 0.088 61.957 0.000 5.464 5.856 .x9 5.404 0.093 58.016 0.000 5.404 5.426 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual 1.000 1.000 1.000 textual 1.000 1.000 1.000 speed 1.000 1.000 1.000 .x1 0.482 0.265 1.822 0.068 0.482 0.331 .x2 1.158 0.199 5.817 0.000 1.158 0.798 .x3 0.711 0.163 4.372 0.000 0.711 0.560 .x4 0.484 0.093 5.198 0.000 0.484 0.369 .x5 0.324 0.128 2.542 0.011 0.324 0.187 .x6 0.328 0.075 4.364 0.000 0.328 0.348 .x7 0.827 0.172 4.810 0.000 0.827 0.809 .x8 0.518 0.204 2.535 0.011 0.518 0.595 .x9 0.620 0.261 2.375 0.018 0.620 0.625 R-Square: Estimate x1 0.669 x2 0.202 x3 0.440 x4 0.631 x5 0.813 x6 0.652 x7 0.191 x8 0.405 x9 0.375 Group 2 [1]: Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual =~ x1 0.741 0.127 5.830 0.000 0.741 0.661 x2 0.459 0.107 4.309 0.000 0.459 0.424 x3 0.783 0.103 7.615 0.000 0.783 0.745 textual =~ x4 0.971 0.098 9.898 0.000 0.971 0.867 x5 0.931 0.106 8.806 0.000 0.931 0.815 x6 0.879 0.113 7.764 0.000 0.879 0.823 speed =~ x7 0.629 0.105 5.982 0.000 0.629 0.605 x8 0.852 0.146 5.815 0.000 0.852 0.797 x9 0.712 0.150 4.755 0.000 0.712 0.687 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual ~~ textual 0.551 0.104 5.299 0.000 0.551 0.551 speed 0.627 0.142 4.410 0.000 0.627 0.627 textual ~~ speed 0.350 0.174 2.015 0.044 0.350 0.350 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual 0.000 0.000 0.000 textual 0.000 0.000 0.000 speed 0.000 0.000 0.000 .x1 4.882 0.105 46.528 0.000 4.882 4.354 .x2 6.160 0.103 59.887 0.000 6.160 5.692 .x3 1.990 0.098 20.392 0.000 1.990 1.894 .x4 3.305 0.104 31.664 0.000 3.305 2.951 .x5 4.669 0.104 44.694 0.000 4.669 4.089 .x6 2.444 0.103 23.796 0.000 2.444 2.286 .x7 4.004 0.097 41.179 0.000 4.004 3.850 .x8 5.570 0.099 56.357 0.000 5.570 5.211 .x9 5.374 0.099 54.445 0.000 5.374 5.189 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual 1.000 1.000 1.000 textual 1.000 1.000 1.000 speed 1.000 1.000 1.000 .x1 0.708 0.178 3.977 0.000 0.708 0.563 .x2 0.961 0.189 5.083 0.000 0.961 0.820 .x3 0.492 0.123 3.984 0.000 0.492 0.445 .x4 0.311 0.083 3.745 0.000 0.311 0.248 .x5 0.438 0.096 4.575 0.000 0.438 0.336 .x6 0.369 0.095 3.867 0.000 0.369 0.323 .x7 0.686 0.118 5.795 0.000 0.686 0.634 .x8 0.417 0.220 1.899 0.058 0.417 0.365 .x9 0.566 0.156 3.639 0.000 0.566 0.528 R-Square: Estimate x1 0.437 x2 0.180 x3 0.555 x4 0.752 x5 0.664 x6 0.677 x7 0.366 x8 0.635 x9 0.472 16.10.4.4 Model Fit You can specify the null model as the baseline model using: baseline.model = nullModelFit Code fitMeasures(configuralInvarianceModel_fit) npar fmin 60.000 0.138 chisq df 71.443 48.000 pvalue chisq.scaled 0.016 70.904 df.scaled pvalue.scaled 48.000 0.017 chisq.scaling.factor baseline.chisq 1.008 604.129 baseline.df baseline.pvalue 72.000 0.000 baseline.chisq.scaled baseline.df.scaled 571.412 72.000 baseline.pvalue.scaled baseline.chisq.scaling.factor 0.000 1.057 cfi tli 0.956 0.934 cfi.scaled tli.scaled 0.954 0.931 cfi.robust tli.robust 0.955 0.933 nnfi rfi 0.934 0.823 nfi pnfi 0.882 0.588 ifi rni 0.958 0.956 nnfi.scaled rfi.scaled 0.931 0.814 nfi.scaled pnfi.scaled 0.876 0.584 ifi.scaled rni.scaled 0.956 0.954 nnfi.robust rni.robust 0.933 0.955 logl unrestricted.logl -2686.219 -2650.498 aic bic 5492.439 5705.848 ntotal bic2 259.000 5515.627 scaling.factor.h1 scaling.factor.h0 1.066 1.114 rmsea rmsea.ci.lower 0.061 0.027 rmsea.ci.upper rmsea.ci.level 0.090 0.900 rmsea.pvalue rmsea.close.h0 0.252 0.050 rmsea.notclose.pvalue rmsea.notclose.h0 0.151 0.080 rmsea.scaled rmsea.ci.lower.scaled 0.061 0.026 rmsea.ci.upper.scaled rmsea.pvalue.scaled 0.089 0.264 rmsea.notclose.pvalue.scaled rmsea.robust 0.142 0.072 rmsea.ci.lower.robust rmsea.ci.upper.robust 0.018 0.111 rmsea.pvalue.robust rmsea.notclose.pvalue.robust 0.192 0.397 rmr rmr_nomean 0.080 0.088 srmr srmr_bentler 0.067 0.067 srmr_bentler_nomean crmr 0.073 0.073 crmr_nomean srmr_mplus 0.081 0.067 srmr_mplus_nomean cn_05 0.073 237.261 cn_01 gfi 268.119 0.994 agfi pgfi 0.987 0.442 mfi ecvi 0.956 0.739 Code configuralInvarianceModelFitIndices &lt;- fitMeasures( configuralInvarianceModel_fit)[c( &quot;cfi.robust&quot;, &quot;rmsea.robust&quot;, &quot;srmr&quot;)] configuralInvarianceModel_chisquare &lt;- fitMeasures( configuralInvarianceModel_fit)[c(&quot;chisq.scaled&quot;)] configuralInvarianceModel_chisquareScaling &lt;- fitMeasures( configuralInvarianceModel_fit)[c(&quot;chisq.scaling.factor&quot;)] configuralInvarianceModel_df &lt;- fitMeasures( configuralInvarianceModel_fit)[c(&quot;df.scaled&quot;)] configuralInvarianceModel_N &lt;- lavInspect( configuralInvarianceModel_fit, what = &quot;ntotal&quot;) 16.10.4.5 Effect size of non-invariance 16.10.4.5.1 dMACS The effect size of measurement non-invariance, as described by Nye et al. (2019), was calculated using the dmacs package (Dueber, 2019). Code lavaan_dmacs(configuralInvarianceModel_fit) $DMACS visual textual speed x1 0.2250656 NA NA x2 0.1731621 NA NA x3 0.3523180 NA NA x4 NA 0.4264504 NA x5 NA 0.5748120 NA x6 NA 0.4769889 NA x7 NA NA 0.3927474 x8 NA NA 0.2762461 x9 NA NA 0.1037844 $ItemDeltaMean visual textual speed x1 -0.09091876 NA NA x2 0.18252780 NA NA x3 -0.38511799 NA NA x4 NA 0.4868954 NA x5 NA 0.6445379 NA x6 NA 0.4690029 NA x7 NA NA -0.35907161 x8 NA NA 0.10636300 x9 NA NA -0.02947139 $MeanDiff visual textual speed -0.293509 1.600436 -0.282180 16.10.4.6 Equivalence Test The petersenlab package (Petersen, 2024b) contains the equiv_chi() function from Counsell et al. (2020) that performs an equivalence test: https://osf.io/cqu8v. An equivalence test evaluates the null hypothesis that a model is equivalent to another model. In this case, the equivalence test evaluates the null hypothesis that a model is equivalent to another model in terms of (poor) fit. Here, we operationalize a mediocre-fitting model as a model whose RMSEA is .08 or greater. So, the equivalence test evaluates whether our invariance model fits significantly better than a mediocre-fitting model. Thus, a statistically significant p-value indicates that our invariance model fits significantly better than the mediocre-fitting model (i.e., that invariance is established), whereas a non-significant p-value indicates that our invariance model does not fit significantly better than the mediocre-fitting model (i.e., that invariance is failed). The chi-square equivalence test is non-significant, suggesting that the model fit of the configural invariance model is not acceptable (i.e., it is not better than the mediocre-fitting model). In other words, configural invariance failed. Code equiv_chi( alpha = .05, chi = configuralInvarianceModel_chisquare, df = configuralInvarianceModel_df, m = 2, N_sample = configuralInvarianceModel_N, popRMSEA = .08) 16.10.4.7 Permutation Test Permutation procedures for testing measurement invariance are described in Jorgensen et al. (2018). The permutation test evaluates the null hypothesis that the model fit is not worse than the best-possible fitting model. A significant p-value indicates that the model fits significanlty worse than the best-possible fitting model. A non-significant p-value indicates that the model does not fit significantly worse than the best-possible fitting model. Code numPermutations &lt;- 100 For reproducibility, I set the seed below. Using the same seed will yield the same answer every time. There is nothing special about this particular seed. You can specify the null model as the baseline model using: baseline.model = nullModelFit. Warning: this code takes a while to run based on \\(100\\) iterations. You can reduce the number of iterations to be faster. Code set.seed(52242) configuralInvarianceTest &lt;- permuteMeasEq( nPermute = numPermutations, modelType = &quot;mgcfa&quot;, con = configuralInvarianceModel_fit, uncon = NULL, AFIs = myAFIs, moreAFIs = moreAFIs, parallelType = &quot;multicore&quot;, #only &#39;snow&#39; works on Windows, but right now, it is throwing an error iseed = 52242) Code RNGkind(&quot;default&quot;, &quot;default&quot;, &quot;default&quot;) set.seed(52242) configuralInvarianceTest Omnibus p value based on parametric chi-squared difference test: Chisq diff Df diff Pr(&gt;Chisq) 70.904 48.000 0.017 Omnibus p values based on nonparametric permutation method: AFI.Difference p.value chisq 71.443 0.59 chisq.scaled 70.904 0.57 rmsea 0.061 0.59 cfi 0.956 0.58 tli 0.934 0.58 srmr 0.067 0.53 rmsea.robust 0.072 0.56 cfi.robust 0.955 0.56 tli.robust 0.933 0.56 The p-values are non-significant, indicating that the model does not fit significantly worse than the best-possible fitting model. In other words, configural invariance held. 16.10.4.8 Internal Consistency Reliability Internal consistency reliability of items composing the latent factors, as quantified by omega (\\(\\omega\\)) and average variance extracted (AVE), was estimated using the semTools package (Jorgensen et al., 2021). Code compRelSEM(configuralInvarianceModel_fit) Code AVE(configuralInvarianceModel_fit) 16.10.4.9 Path Diagram A path diagram of the model generated using the semPlot package (Epskamp, 2022) is in the figures below (“1” = group 1; “2” = group 2). Code semPaths( configuralInvarianceModel_fit, what = &quot;est&quot;, layout = &quot;tree2&quot;, ask = FALSE, edge.label.cex = 1.2) Figure 16.53: Configural Invariance Model in Confirmatory Factor Analysis (1 = Group 1; 2 = Group 2). Figure 16.54: Configural Invariance Model in Confirmatory Factor Analysis (1 = Group 1; 2 = Group 2). 16.10.5 Metric (“Weak Factorial”) Invariance Model Specify invariance of factor loadings across groups. 16.10.5.1 Model Syntax Code metricInvarianceModel &lt;- measEq.syntax( configural.model = cfaModel, data = HolzingerSwineford1939, ID.fac = &quot;std.lv&quot;, group = &quot;school&quot;, group.equal = &quot;loadings&quot;) Code cat(as.character(metricInvarianceModel)) ## LOADINGS: visual =~ c(NA, NA)*x1 + c(lambda.1_1, lambda.1_1)*x1 visual =~ c(NA, NA)*x2 + c(lambda.2_1, lambda.2_1)*x2 visual =~ c(NA, NA)*x3 + c(lambda.3_1, lambda.3_1)*x3 textual =~ c(NA, NA)*x4 + c(lambda.4_2, lambda.4_2)*x4 textual =~ c(NA, NA)*x5 + c(lambda.5_2, lambda.5_2)*x5 textual =~ c(NA, NA)*x6 + c(lambda.6_2, lambda.6_2)*x6 speed =~ c(NA, NA)*x7 + c(lambda.7_3, lambda.7_3)*x7 speed =~ c(NA, NA)*x8 + c(lambda.8_3, lambda.8_3)*x8 speed =~ c(NA, NA)*x9 + c(lambda.9_3, lambda.9_3)*x9 ## INTERCEPTS: x1 ~ c(NA, NA)*1 + c(nu.1.g1, nu.1.g2)*1 x2 ~ c(NA, NA)*1 + c(nu.2.g1, nu.2.g2)*1 x3 ~ c(NA, NA)*1 + c(nu.3.g1, nu.3.g2)*1 x4 ~ c(NA, NA)*1 + c(nu.4.g1, nu.4.g2)*1 x5 ~ c(NA, NA)*1 + c(nu.5.g1, nu.5.g2)*1 x6 ~ c(NA, NA)*1 + c(nu.6.g1, nu.6.g2)*1 x7 ~ c(NA, NA)*1 + c(nu.7.g1, nu.7.g2)*1 x8 ~ c(NA, NA)*1 + c(nu.8.g1, nu.8.g2)*1 x9 ~ c(NA, NA)*1 + c(nu.9.g1, nu.9.g2)*1 ## UNIQUE-FACTOR VARIANCES: x1 ~~ c(NA, NA)*x1 + c(theta.1_1.g1, theta.1_1.g2)*x1 x2 ~~ c(NA, NA)*x2 + c(theta.2_2.g1, theta.2_2.g2)*x2 x3 ~~ c(NA, NA)*x3 + c(theta.3_3.g1, theta.3_3.g2)*x3 x4 ~~ c(NA, NA)*x4 + c(theta.4_4.g1, theta.4_4.g2)*x4 x5 ~~ c(NA, NA)*x5 + c(theta.5_5.g1, theta.5_5.g2)*x5 x6 ~~ c(NA, NA)*x6 + c(theta.6_6.g1, theta.6_6.g2)*x6 x7 ~~ c(NA, NA)*x7 + c(theta.7_7.g1, theta.7_7.g2)*x7 x8 ~~ c(NA, NA)*x8 + c(theta.8_8.g1, theta.8_8.g2)*x8 x9 ~~ c(NA, NA)*x9 + c(theta.9_9.g1, theta.9_9.g2)*x9 ## LATENT MEANS/INTERCEPTS: visual ~ c(0, 0)*1 + c(alpha.1.g1, alpha.1.g2)*1 textual ~ c(0, 0)*1 + c(alpha.2.g1, alpha.2.g2)*1 speed ~ c(0, 0)*1 + c(alpha.3.g1, alpha.3.g2)*1 ## COMMON-FACTOR VARIANCES: visual ~~ c(1, NA)*visual + c(psi.1_1.g1, psi.1_1.g2)*visual textual ~~ c(1, NA)*textual + c(psi.2_2.g1, psi.2_2.g2)*textual speed ~~ c(1, NA)*speed + c(psi.3_3.g1, psi.3_3.g2)*speed ## COMMON-FACTOR COVARIANCES: visual ~~ c(NA, NA)*textual + c(psi.2_1.g1, psi.2_1.g2)*textual visual ~~ c(NA, NA)*speed + c(psi.3_1.g1, psi.3_1.g2)*speed textual ~~ c(NA, NA)*speed + c(psi.3_2.g1, psi.3_2.g2)*speed Code metricInvarianceSyntax &lt;- as.character(metricInvarianceModel) 16.10.5.1.1 Summary of Model Features Code summary(metricInvarianceModel) This lavaan model syntax specifies a CFA with 9 manifest indicators of 3 common factor(s). To identify the location and scale of each common factor, the factor means and variances were fixed to 0 and 1, respectively, unless equality constraints on measurement parameters allow them to be freed. Pattern matrix indicating num(eric), ord(ered), and lat(ent) indicators per factor: visual textual speed x1 num x2 num x3 num x4 num x5 num x6 num x7 num x8 num x9 num The following types of parameter were constrained to equality across groups: loadings 16.10.5.2 Model Syntax in Table Form Code lavaanify(metricInvarianceModel, ngroups = 2) Code lavaanify(cfaModel_metricInvariance, ngroups = 2) 16.10.5.3 Fit the Model Code metricInvarianceModel_fit &lt;- cfa( metricInvarianceSyntax, data = HolzingerSwineford1939, group = &quot;school&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) Code metricInvarianceModelFullSyntax_fit &lt;- lavaan( cfaModel_metricInvariance, data = HolzingerSwineford1939, group = &quot;school&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) 16.10.5.4 Model Summary Code summary( metricInvarianceModel_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 67 iterations Estimator ML Optimization method NLMINB Number of model parameters 63 Number of equality constraints 9 Number of observations per group: 2 132 1 127 Number of missing patterns per group: 2 48 1 52 Model Test User Model: Standard Scaled Test Statistic 78.011 75.195 Degrees of freedom 54 54 P-value (Chi-square) 0.018 0.030 Scaling correction factor 1.037 Yuan-Bentler correction (Mplus variant) Test statistic for each group: 2 47.087 45.387 1 30.924 29.808 Model Test Baseline Model: Test statistic 604.129 571.412 Degrees of freedom 72 72 P-value 0.000 0.000 Scaling correction factor 1.057 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.955 0.958 Tucker-Lewis Index (TLI) 0.940 0.943 Robust Comparative Fit Index (CFI) 0.946 Robust Tucker-Lewis Index (TLI) 0.928 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -2689.503 -2689.503 Scaling correction factor 0.939 for the MLR correction Loglikelihood unrestricted model (H1) -2650.498 -2650.498 Scaling correction factor 1.066 for the MLR correction Akaike (AIC) 5487.007 5487.007 Bayesian (BIC) 5679.075 5679.075 Sample-size adjusted Bayesian (SABIC) 5507.876 5507.876 Root Mean Square Error of Approximation: RMSEA 0.059 0.055 90 Percent confidence interval - lower 0.025 0.019 90 Percent confidence interval - upper 0.086 0.082 P-value H_0: RMSEA &lt;= 0.050 0.296 0.368 P-value H_0: RMSEA &gt;= 0.080 0.104 0.069 Robust RMSEA 0.075 90 Percent confidence interval - lower 0.030 90 Percent confidence interval - upper 0.110 P-value H_0: Robust RMSEA &lt;= 0.050 0.147 P-value H_0: Robust RMSEA &gt;= 0.080 0.431 Standardized Root Mean Square Residual: SRMR 0.073 0.073 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Group 1 [2]: Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual =~ x1 (l.1_) 0.913 0.130 7.016 0.000 0.913 0.769 x2 (l.2_) 0.539 0.097 5.557 0.000 0.539 0.450 x3 (l.3_) 0.819 0.088 9.333 0.000 0.819 0.714 textual =~ x4 (l.4_) 0.950 0.090 10.553 0.000 0.950 0.811 x5 (l.5_) 1.069 0.117 9.122 0.000 1.069 0.852 x6 (l.6_) 0.824 0.092 8.926 0.000 0.824 0.832 speed =~ x7 (l.7_) 0.471 0.085 5.523 0.000 0.471 0.465 x8 (l.8_) 0.636 0.108 5.897 0.000 0.636 0.679 x9 (l.9_) 0.557 0.112 4.971 0.000 0.557 0.563 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual ~~ textul (p.2_) 0.455 0.127 3.578 0.000 0.455 0.455 speed (p.3_1) 0.396 0.139 2.837 0.005 0.396 0.396 textual ~~ speed (p.3_2) 0.318 0.115 2.773 0.006 0.318 0.318 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 (n.1.) 4.974 0.113 44.143 0.000 4.974 4.189 .x2 (n.2.) 5.977 0.110 54.446 0.000 5.977 4.983 .x3 (n.3.) 2.375 0.106 22.343 0.000 2.375 2.069 .x4 (n.4.) 2.817 0.105 26.778 0.000 2.817 2.404 .x5 (n.5.) 4.022 0.119 33.914 0.000 4.022 3.206 .x6 (n.6.) 1.971 0.087 22.590 0.000 1.971 1.990 .x7 (n.7.) 4.362 0.093 46.812 0.000 4.362 4.303 .x8 (n.8.) 5.464 0.088 61.938 0.000 5.464 5.834 .x9 (n.9.) 5.402 0.094 57.768 0.000 5.402 5.456 visual (a.1.) 0.000 0.000 0.000 textual (a.2.) 0.000 0.000 0.000 speed (a.3.) 0.000 0.000 0.000 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 (t.1_) 0.576 0.179 3.221 0.001 0.576 0.409 .x2 (t.2_) 1.147 0.185 6.212 0.000 1.147 0.798 .x3 (t.3_) 0.646 0.134 4.811 0.000 0.646 0.491 .x4 (t.4_) 0.470 0.090 5.216 0.000 0.470 0.343 .x5 (t.5_) 0.431 0.109 3.941 0.000 0.431 0.274 .x6 (t.6_) 0.302 0.075 4.033 0.000 0.302 0.308 .x7 (t.7_) 0.806 0.122 6.590 0.000 0.806 0.784 .x8 (t.8_) 0.473 0.120 3.928 0.000 0.473 0.539 .x9 (t.9_) 0.670 0.149 4.495 0.000 0.670 0.683 visual (p.1_) 1.000 1.000 1.000 textual (p.2_) 1.000 1.000 1.000 speed (p.3_) 1.000 1.000 1.000 R-Square: Estimate x1 0.591 x2 0.202 x3 0.509 x4 0.657 x5 0.726 x6 0.692 x7 0.216 x8 0.461 x9 0.317 Group 2 [1]: Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual =~ x1 (l.1_) 0.913 0.130 7.016 0.000 0.801 0.704 x2 (l.2_) 0.539 0.097 5.557 0.000 0.473 0.435 x3 (l.3_) 0.819 0.088 9.333 0.000 0.719 0.696 textual =~ x4 (l.4_) 0.950 0.090 10.553 0.000 0.914 0.836 x5 (l.5_) 1.069 0.117 9.122 0.000 1.029 0.866 x6 (l.6_) 0.824 0.092 8.926 0.000 0.793 0.772 speed =~ x7 (l.7_) 0.471 0.085 5.523 0.000 0.619 0.596 x8 (l.8_) 0.636 0.108 5.897 0.000 0.835 0.783 x9 (l.9_) 0.557 0.112 4.971 0.000 0.732 0.703 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual ~~ textul (p.2_) 0.462 0.137 3.379 0.001 0.547 0.547 speed (p.3_1) 0.732 0.231 3.177 0.001 0.636 0.636 textual ~~ speed (p.3_2) 0.473 0.241 1.966 0.049 0.375 0.375 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 (n.1.) 4.881 0.105 46.577 0.000 4.881 4.289 .x2 (n.2.) 6.159 0.103 59.876 0.000 6.159 5.664 .x3 (n.3.) 1.993 0.098 20.368 0.000 1.993 1.932 .x4 (n.4.) 3.299 0.103 31.949 0.000 3.299 3.017 .x5 (n.5.) 4.674 0.105 44.381 0.000 4.674 3.932 .x6 (n.6.) 2.436 0.101 24.159 0.000 2.436 2.371 .x7 (n.7.) 4.002 0.097 41.140 0.000 4.002 3.855 .x8 (n.8.) 5.571 0.099 56.335 0.000 5.571 5.226 .x9 (n.9.) 5.375 0.098 54.583 0.000 5.375 5.165 visual (a.1.) 0.000 0.000 0.000 textual (a.2.) 0.000 0.000 0.000 speed (a.3.) 0.000 0.000 0.000 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 (t.1_) 0.653 0.167 3.900 0.000 0.653 0.504 .x2 (t.2_) 0.958 0.186 5.165 0.000 0.958 0.811 .x3 (t.3_) 0.548 0.117 4.672 0.000 0.548 0.515 .x4 (t.4_) 0.360 0.089 4.046 0.000 0.360 0.301 .x5 (t.5_) 0.354 0.096 3.679 0.000 0.354 0.250 .x6 (t.6_) 0.427 0.103 4.149 0.000 0.427 0.404 .x7 (t.7_) 0.695 0.120 5.805 0.000 0.695 0.645 .x8 (t.8_) 0.439 0.204 2.149 0.032 0.439 0.387 .x9 (t.9_) 0.547 0.148 3.697 0.000 0.547 0.506 visual (p.1_) 0.770 0.210 3.663 0.000 1.000 1.000 textual (p.2_) 0.926 0.228 4.058 0.000 1.000 1.000 speed (p.3_) 1.724 0.512 3.364 0.001 1.000 1.000 R-Square: Estimate x1 0.496 x2 0.189 x3 0.485 x4 0.699 x5 0.750 x6 0.596 x7 0.355 x8 0.613 x9 0.494 Code summary( metricInvarianceModelFullSyntax_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 67 iterations Estimator ML Optimization method NLMINB Number of model parameters 63 Number of equality constraints 9 Number of observations per group: 2 132 1 127 Number of missing patterns per group: 2 48 1 52 Model Test User Model: Standard Scaled Test Statistic 78.011 75.195 Degrees of freedom 54 54 P-value (Chi-square) 0.018 0.030 Scaling correction factor 1.037 Yuan-Bentler correction (Mplus variant) Test statistic for each group: 2 47.087 45.387 1 30.924 29.808 Model Test Baseline Model: Test statistic 604.129 571.412 Degrees of freedom 72 72 P-value 0.000 0.000 Scaling correction factor 1.057 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.955 0.958 Tucker-Lewis Index (TLI) 0.940 0.943 Robust Comparative Fit Index (CFI) 0.946 Robust Tucker-Lewis Index (TLI) 0.928 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -2689.503 -2689.503 Scaling correction factor 0.939 for the MLR correction Loglikelihood unrestricted model (H1) -2650.498 -2650.498 Scaling correction factor 1.066 for the MLR correction Akaike (AIC) 5487.007 5487.007 Bayesian (BIC) 5679.075 5679.075 Sample-size adjusted Bayesian (SABIC) 5507.876 5507.876 Root Mean Square Error of Approximation: RMSEA 0.059 0.055 90 Percent confidence interval - lower 0.025 0.019 90 Percent confidence interval - upper 0.086 0.082 P-value H_0: RMSEA &lt;= 0.050 0.296 0.368 P-value H_0: RMSEA &gt;= 0.080 0.104 0.069 Robust RMSEA 0.075 90 Percent confidence interval - lower 0.030 90 Percent confidence interval - upper 0.110 P-value H_0: Robust RMSEA &lt;= 0.050 0.147 P-value H_0: Robust RMSEA &gt;= 0.080 0.431 Standardized Root Mean Square Residual: SRMR 0.073 0.073 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Group 1 [2]: Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual =~ x1 (lmb1) 0.913 0.130 7.016 0.000 0.913 0.769 x2 (lmb2) 0.539 0.097 5.557 0.000 0.539 0.450 x3 (lmb3) 0.819 0.088 9.333 0.000 0.819 0.714 textual =~ x4 (lmb4) 0.950 0.090 10.553 0.000 0.950 0.811 x5 (lmb5) 1.069 0.117 9.122 0.000 1.069 0.852 x6 (lmb6) 0.824 0.092 8.926 0.000 0.824 0.832 speed =~ x7 (lmb7) 0.471 0.085 5.523 0.000 0.471 0.465 x8 (lmb8) 0.636 0.108 5.897 0.000 0.636 0.679 x9 (lmb9) 0.557 0.112 4.971 0.000 0.557 0.563 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual ~~ textual 0.455 0.127 3.578 0.000 0.455 0.455 speed 0.396 0.139 2.837 0.005 0.396 0.396 textual ~~ speed 0.318 0.115 2.773 0.006 0.318 0.318 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual 0.000 0.000 0.000 textual 0.000 0.000 0.000 speed 0.000 0.000 0.000 .x1 4.974 0.113 44.143 0.000 4.974 4.189 .x2 5.977 0.110 54.446 0.000 5.977 4.983 .x3 2.375 0.106 22.343 0.000 2.375 2.069 .x4 2.817 0.105 26.778 0.000 2.817 2.404 .x5 4.022 0.119 33.914 0.000 4.022 3.206 .x6 1.971 0.087 22.590 0.000 1.971 1.990 .x7 4.362 0.093 46.812 0.000 4.362 4.303 .x8 5.464 0.088 61.938 0.000 5.464 5.834 .x9 5.402 0.094 57.768 0.000 5.402 5.456 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual 1.000 1.000 1.000 textual 1.000 1.000 1.000 speed 1.000 1.000 1.000 .x1 0.576 0.179 3.221 0.001 0.576 0.409 .x2 1.147 0.185 6.212 0.000 1.147 0.798 .x3 0.646 0.134 4.811 0.000 0.646 0.491 .x4 0.470 0.090 5.216 0.000 0.470 0.343 .x5 0.431 0.109 3.941 0.000 0.431 0.274 .x6 0.302 0.075 4.033 0.000 0.302 0.308 .x7 0.806 0.122 6.590 0.000 0.806 0.784 .x8 0.473 0.120 3.928 0.000 0.473 0.539 .x9 0.670 0.149 4.495 0.000 0.670 0.683 R-Square: Estimate x1 0.591 x2 0.202 x3 0.509 x4 0.657 x5 0.726 x6 0.692 x7 0.216 x8 0.461 x9 0.317 Group 2 [1]: Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual =~ x1 (lmb1) 0.913 0.130 7.016 0.000 0.801 0.704 x2 (lmb2) 0.539 0.097 5.557 0.000 0.473 0.435 x3 (lmb3) 0.819 0.088 9.333 0.000 0.719 0.696 textual =~ x4 (lmb4) 0.950 0.090 10.553 0.000 0.914 0.836 x5 (lmb5) 1.069 0.117 9.122 0.000 1.029 0.866 x6 (lmb6) 0.824 0.092 8.926 0.000 0.793 0.772 speed =~ x7 (lmb7) 0.471 0.085 5.523 0.000 0.619 0.596 x8 (lmb8) 0.636 0.108 5.897 0.000 0.835 0.783 x9 (lmb9) 0.557 0.112 4.971 0.000 0.732 0.703 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual ~~ textual 0.462 0.137 3.379 0.001 0.547 0.547 speed 0.732 0.231 3.177 0.001 0.636 0.636 textual ~~ speed 0.473 0.241 1.966 0.049 0.375 0.375 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual 0.000 0.000 0.000 textual 0.000 0.000 0.000 speed 0.000 0.000 0.000 .x1 4.881 0.105 46.577 0.000 4.881 4.289 .x2 6.159 0.103 59.876 0.000 6.159 5.664 .x3 1.993 0.098 20.368 0.000 1.993 1.932 .x4 3.299 0.103 31.949 0.000 3.299 3.017 .x5 4.674 0.105 44.381 0.000 4.674 3.932 .x6 2.436 0.101 24.159 0.000 2.436 2.371 .x7 4.002 0.097 41.140 0.000 4.002 3.855 .x8 5.571 0.099 56.335 0.000 5.571 5.226 .x9 5.375 0.098 54.583 0.000 5.375 5.165 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual 0.770 0.210 3.663 0.000 1.000 1.000 textual 0.926 0.228 4.058 0.000 1.000 1.000 speed 1.724 0.512 3.364 0.001 1.000 1.000 .x1 0.653 0.167 3.900 0.000 0.653 0.504 .x2 0.958 0.186 5.165 0.000 0.958 0.811 .x3 0.548 0.117 4.672 0.000 0.548 0.515 .x4 0.360 0.089 4.046 0.000 0.360 0.301 .x5 0.354 0.096 3.679 0.000 0.354 0.250 .x6 0.427 0.103 4.149 0.000 0.427 0.404 .x7 0.695 0.120 5.805 0.000 0.695 0.645 .x8 0.439 0.204 2.149 0.032 0.439 0.387 .x9 0.547 0.148 3.697 0.000 0.547 0.506 R-Square: Estimate x1 0.496 x2 0.189 x3 0.485 x4 0.699 x5 0.750 x6 0.596 x7 0.355 x8 0.613 x9 0.494 16.10.5.5 Model Fit You can specify the null model as the baseline model using: baseline.model = nullModelFit Code fitMeasures(metricInvarianceModel_fit) npar fmin 54.000 0.151 chisq df 78.011 54.000 pvalue chisq.scaled 0.018 75.195 df.scaled pvalue.scaled 54.000 0.030 chisq.scaling.factor baseline.chisq 1.037 604.129 baseline.df baseline.pvalue 72.000 0.000 baseline.chisq.scaled baseline.df.scaled 571.412 72.000 baseline.pvalue.scaled baseline.chisq.scaling.factor 0.000 1.057 cfi tli 0.955 0.940 cfi.scaled tli.scaled 0.958 0.943 cfi.robust tli.robust 0.946 0.928 nnfi rfi 0.940 0.828 nfi pnfi 0.871 0.653 ifi rni 0.956 0.955 nnfi.scaled rfi.scaled 0.943 0.825 nfi.scaled pnfi.scaled 0.868 0.651 ifi.scaled rni.scaled 0.959 0.958 nnfi.robust rni.robust 0.928 0.946 logl unrestricted.logl -2689.503 -2650.498 aic bic 5487.007 5679.075 ntotal bic2 259.000 5507.876 scaling.factor.h1 scaling.factor.h0 1.066 0.939 rmsea rmsea.ci.lower 0.059 0.025 rmsea.ci.upper rmsea.ci.level 0.086 0.900 rmsea.pvalue rmsea.close.h0 0.296 0.050 rmsea.notclose.pvalue rmsea.notclose.h0 0.104 0.080 rmsea.scaled rmsea.ci.lower.scaled 0.055 0.019 rmsea.ci.upper.scaled rmsea.pvalue.scaled 0.082 0.368 rmsea.notclose.pvalue.scaled rmsea.robust 0.069 0.075 rmsea.ci.lower.robust rmsea.ci.upper.robust 0.030 0.110 rmsea.pvalue.robust rmsea.notclose.pvalue.robust 0.147 0.431 rmr rmr_nomean 0.090 0.099 srmr srmr_bentler 0.073 0.073 srmr_bentler_nomean crmr 0.080 0.075 crmr_nomean srmr_mplus 0.084 0.078 srmr_mplus_nomean cn_05 0.078 240.551 cn_01 gfi 270.151 0.995 agfi pgfi 0.990 0.498 mfi ecvi 0.955 0.718 Code metricInvarianceModelFitIndices &lt;- fitMeasures( metricInvarianceModel_fit)[c( &quot;cfi.robust&quot;, &quot;rmsea.robust&quot;, &quot;srmr&quot;)] metricInvarianceModel_chisquare &lt;- fitMeasures( metricInvarianceModel_fit)[c(&quot;chisq.scaled&quot;)] metricInvarianceModel_chisquareScaling &lt;- fitMeasures( metricInvarianceModel_fit)[c(&quot;chisq.scaling.factor&quot;)] metricInvarianceModel_df &lt;- fitMeasures( metricInvarianceModel_fit)[c(&quot;df.scaled&quot;)] metricInvarianceModel_N &lt;- lavInspect( metricInvarianceModel_fit, what = &quot;ntotal&quot;) 16.10.5.6 Compare Model Fit 16.10.5.6.1 Nested Model (\\(\\chi^2\\)) Difference Test The configural invariance model and the metric (“weak factorial”) invariance model are considered “nested” models. The metric invariance model is nested within the configural invariance model because the configural invariance model includes all of the terms of the metric invariance model along with additional terms. Model fit of nested models can be compared with a chi-square difference test, also known as a likelihood ratio test or deviance test. A significant chi-square difference test would indicate that the simplified model with additional constraints (the metric invariance model) is significantly worse fitting than the more complex model that has fewer constraints (the configural invariance model). Code anova(configuralInvarianceModel_fit, metricInvarianceModel_fit) The metric invariance model did not fit significantly worse than the configural invariance model, so metric invariance held. This provides evidence of measurement invariance of factor loadings across groups. Measurement invariance of factor loadings across groups provides support for examining whether the groups show different associations of the factor with other constructs (Little et al., 2007). The petersenlab package (Petersen, 2024b) contains the satorraBentlerScaledChiSquareDifferenceTestStatistic() function that performs a Satorra-Bentler scaled chi-square difference test: Below is a Satorra-Bentler scaled chi-square difference test, where \\(c0\\) and \\(c1\\) are the scaling correction factor for the nested model and comparison model, respectively; \\(d0\\) and \\(d1\\) are the degrees of freedom of the nested model and comparison model, respectively; and \\(T0\\) and \\(T1\\) are the chi-square values of the nested model and comparison model, respectively. Code metricInvarianceModel_chisquareDiff &lt;- satorraBentlerScaledChiSquareDifferenceTestStatistic( T0 = metricInvarianceModel_chisquare, c0 = metricInvarianceModel_chisquareScaling, d0 = metricInvarianceModel_df, T1 = configuralInvarianceModel_chisquare, c1 = configuralInvarianceModel_chisquareScaling, d1 = configuralInvarianceModel_df) metricInvarianceModel_chisquareDiff chisq.scaled 5.146394 16.10.5.6.2 Compare Other Fit Criteria Code round(metricInvarianceModelFitIndices - configuralInvarianceModelFitIndices, digits = 3) cfi.robust rmsea.robust srmr -0.009 0.003 0.007 16.10.5.6.3 Score-Based Test Score-based tests of measurement invariance are implemented using the strucchange package and are described by T. Wang et al. (2014). Code coef(metricInvarianceModel_fit) lambda.1_1 lambda.2_1 lambda.3_1 lambda.4_2 lambda.5_2 lambda.6_2 0.913 0.539 0.819 0.950 1.069 0.824 lambda.7_3 lambda.8_3 lambda.9_3 nu.1.g1 nu.2.g1 nu.3.g1 0.471 0.636 0.557 4.974 5.977 2.375 nu.4.g1 nu.5.g1 nu.6.g1 nu.7.g1 nu.8.g1 nu.9.g1 2.817 4.022 1.971 4.362 5.464 5.402 theta.1_1.g1 theta.2_2.g1 theta.3_3.g1 theta.4_4.g1 theta.5_5.g1 theta.6_6.g1 0.576 1.147 0.646 0.470 0.431 0.302 theta.7_7.g1 theta.8_8.g1 theta.9_9.g1 psi.2_1.g1 psi.3_1.g1 psi.3_2.g1 0.806 0.473 0.670 0.455 0.396 0.318 lambda.1_1 lambda.2_1 lambda.3_1 lambda.4_2 lambda.5_2 lambda.6_2 0.913 0.539 0.819 0.950 1.069 0.824 lambda.7_3 lambda.8_3 lambda.9_3 nu.1.g2 nu.2.g2 nu.3.g2 0.471 0.636 0.557 4.881 6.159 1.993 nu.4.g2 nu.5.g2 nu.6.g2 nu.7.g2 nu.8.g2 nu.9.g2 3.299 4.674 2.436 4.002 5.571 5.375 theta.1_1.g2 theta.2_2.g2 theta.3_3.g2 theta.4_4.g2 theta.5_5.g2 theta.6_6.g2 0.653 0.958 0.548 0.360 0.354 0.427 theta.7_7.g2 theta.8_8.g2 theta.9_9.g2 psi.1_1.g2 psi.2_2.g2 psi.3_3.g2 0.695 0.439 0.547 0.770 0.926 1.724 psi.2_1.g2 psi.3_1.g2 psi.3_2.g2 0.462 0.732 0.473 Code coef(metricInvarianceModel_fit)[1:9] lambda.1_1 lambda.2_1 lambda.3_1 lambda.4_2 lambda.5_2 lambda.6_2 lambda.7_3 0.9131519 0.5394530 0.8190934 0.9499853 1.0692197 0.8238386 0.4712981 lambda.8_3 lambda.9_3 0.6358889 0.5573554 Code sctest( metricInvarianceModel_fit, order.by = HolzingerSwineford1939$school, parm = 1:9, functional = &quot;LMuo&quot;) Error in `[&lt;-`(`*tmp*`, wi, , value = -scores.H1 %*% Delta): subscript out of bounds A score-based test and expected parameter change (EPC) estimates (Oberski, 2014; Oberski et al., 2015) are provided by the lavaan package (Rosseel et al., 2022). Code lavTestScore( metricInvarianceModel_fit, epc = TRUE ) $test total score test: test X2 df p.value 1 score 5.672 9 0.772 $uni univariate score tests: lhs op rhs X2 df p.value 1 .p1. == .p37. 0.772 1 0.380 2 .p2. == .p38. 0.050 1 0.823 3 .p3. == .p39. 1.108 1 0.293 4 .p4. == .p40. 0.996 1 0.318 5 .p5. == .p41. 4.389 1 0.036 6 .p6. == .p42. 1.382 1 0.240 7 .p7. == .p43. 0.012 1 0.914 8 .p8. == .p44. 0.052 1 0.819 9 .p9. == .p45. 0.108 1 0.742 $epc expected parameter changes (epc) and expected parameter values (epv): lhs op rhs block group free label plabel est epc epv 1 visual =~ x1 1 1 1 lambda.1_1 .p1. 0.913 0.070 0.983 2 visual =~ x2 1 1 2 lambda.2_1 .p2. 0.539 0.014 0.554 3 visual =~ x3 1 1 3 lambda.3_1 .p3. 0.819 -0.079 0.740 4 textual =~ x4 1 1 4 lambda.4_2 .p4. 0.950 -0.050 0.900 5 textual =~ x5 1 1 5 lambda.5_2 .p5. 1.069 0.093 1.162 6 textual =~ x6 1 1 6 lambda.6_2 .p6. 0.824 -0.046 0.778 7 speed =~ x7 1 1 7 lambda.7_3 .p7. 0.471 -0.007 0.464 8 speed =~ x8 1 1 8 lambda.8_3 .p8. 0.636 -0.022 0.614 9 speed =~ x9 1 1 9 lambda.9_3 .p9. 0.557 0.030 0.588 10 x1 ~1 1 1 10 nu.1.g1 .p10. 4.974 0.000 4.974 11 x2 ~1 1 1 11 nu.2.g1 .p11. 5.977 0.000 5.977 12 x3 ~1 1 1 12 nu.3.g1 .p12. 2.375 0.000 2.375 13 x4 ~1 1 1 13 nu.4.g1 .p13. 2.817 0.000 2.817 14 x5 ~1 1 1 14 nu.5.g1 .p14. 4.022 0.000 4.022 15 x6 ~1 1 1 15 nu.6.g1 .p15. 1.971 0.000 1.971 16 x7 ~1 1 1 16 nu.7.g1 .p16. 4.362 0.000 4.362 17 x8 ~1 1 1 17 nu.8.g1 .p17. 5.464 0.000 5.464 18 x9 ~1 1 1 18 nu.9.g1 .p18. 5.402 0.000 5.402 19 x1 ~~ x1 1 1 19 theta.1_1.g1 .p19. 0.576 -0.090 0.486 20 x2 ~~ x2 1 1 20 theta.2_2.g1 .p20. 1.147 -0.003 1.144 21 x3 ~~ x3 1 1 21 theta.3_3.g1 .p21. 0.646 0.080 0.727 22 x4 ~~ x4 1 1 22 theta.4_4.g1 .p22. 0.470 0.031 0.501 23 x5 ~~ x5 1 1 23 theta.5_5.g1 .p23. 0.431 -0.077 0.354 24 x6 ~~ x6 1 1 24 theta.6_6.g1 .p24. 0.302 0.029 0.331 25 x7 ~~ x7 1 1 25 theta.7_7.g1 .p25. 0.806 0.002 0.808 26 x8 ~~ x8 1 1 26 theta.8_8.g1 .p26. 0.473 0.021 0.494 27 x9 ~~ x9 1 1 27 theta.9_9.g1 .p27. 0.670 -0.022 0.648 28 visual ~1 1 1 0 alpha.1.g1 .p28. 0.000 NA NA 29 textual ~1 1 1 0 alpha.2.g1 .p29. 0.000 NA NA 30 speed ~1 1 1 0 alpha.3.g1 .p30. 0.000 NA NA 31 visual ~~ visual 1 1 0 psi.1_1.g1 .p31. 1.000 NA NA 32 textual ~~ textual 1 1 0 psi.2_2.g1 .p32. 1.000 NA NA 33 speed ~~ speed 1 1 0 psi.3_3.g1 .p33. 1.000 NA NA 34 visual ~~ textual 1 1 28 psi.2_1.g1 .p34. 0.455 -0.004 0.451 35 visual ~~ speed 1 1 29 psi.3_1.g1 .p35. 0.396 -0.001 0.395 36 textual ~~ speed 1 1 30 psi.3_2.g1 .p36. 0.318 0.001 0.319 37 visual =~ x1 2 2 31 lambda.1_1 .p37. 0.913 -0.058 0.855 38 visual =~ x2 2 2 32 lambda.2_1 .p38. 0.539 -0.018 0.521 39 visual =~ x3 2 2 33 lambda.3_1 .p39. 0.819 0.068 0.887 40 textual =~ x4 2 2 34 lambda.4_2 .p40. 0.950 0.052 1.002 41 textual =~ x5 2 2 35 lambda.5_2 .p41. 1.069 -0.095 0.974 42 textual =~ x6 2 2 36 lambda.6_2 .p42. 0.824 0.063 0.887 43 speed =~ x7 2 2 37 lambda.7_3 .p43. 0.471 0.003 0.474 44 speed =~ x8 2 2 38 lambda.8_3 .p44. 0.636 0.007 0.643 45 speed =~ x9 2 2 39 lambda.9_3 .p45. 0.557 -0.011 0.547 46 x1 ~1 2 2 40 nu.1.g2 .p46. 4.881 0.000 4.881 47 x2 ~1 2 2 41 nu.2.g2 .p47. 6.159 0.000 6.159 48 x3 ~1 2 2 42 nu.3.g2 .p48. 1.993 0.000 1.993 49 x4 ~1 2 2 43 nu.4.g2 .p49. 3.299 0.000 3.299 50 x5 ~1 2 2 44 nu.5.g2 .p50. 4.674 0.000 4.674 51 x6 ~1 2 2 45 nu.6.g2 .p51. 2.436 0.000 2.436 52 x7 ~1 2 2 46 nu.7.g2 .p52. 4.002 0.000 4.002 53 x8 ~1 2 2 47 nu.8.g2 .p53. 5.571 0.000 5.571 54 x9 ~1 2 2 48 nu.9.g2 .p54. 5.375 0.000 5.375 55 x1 ~~ x1 2 2 49 theta.1_1.g2 .p55. 0.653 0.046 0.699 56 x2 ~~ x2 2 2 50 theta.2_2.g2 .p56. 0.958 0.005 0.964 57 x3 ~~ x3 2 2 51 theta.3_3.g2 .p57. 0.548 -0.042 0.506 58 x4 ~~ x4 2 2 52 theta.4_4.g2 .p58. 0.360 -0.041 0.319 59 x5 ~~ x5 2 2 53 theta.5_5.g2 .p59. 0.354 0.079 0.433 60 x6 ~~ x6 2 2 54 theta.6_6.g2 .p60. 0.427 -0.029 0.398 61 x7 ~~ x7 2 2 55 theta.7_7.g2 .p61. 0.695 -0.001 0.694 62 x8 ~~ x8 2 2 56 theta.8_8.g2 .p62. 0.439 -0.009 0.430 63 x9 ~~ x9 2 2 57 theta.9_9.g2 .p63. 0.547 0.009 0.557 64 visual ~1 2 2 0 alpha.1.g2 .p64. 0.000 NA NA 65 textual ~1 2 2 0 alpha.2.g2 .p65. 0.000 NA NA 66 speed ~1 2 2 0 alpha.3.g2 .p66. 0.000 NA NA 67 visual ~~ visual 2 2 58 psi.1_1.g2 .p67. 0.770 -0.004 0.766 68 textual ~~ textual 2 2 59 psi.2_2.g2 .p68. 0.926 0.000 0.926 69 speed ~~ speed 2 2 60 psi.3_3.g2 .p69. 1.724 0.000 1.724 70 visual ~~ textual 2 2 61 psi.2_1.g2 .p70. 0.462 0.000 0.461 71 visual ~~ speed 2 2 62 psi.3_1.g2 .p71. 0.732 -0.002 0.730 72 textual ~~ speed 2 2 63 psi.3_2.g2 .p72. 0.473 0.001 0.474 sepc.lv sepc.all sepc.nox 1 0.070 0.059 0.059 2 0.014 0.012 0.012 3 -0.079 -0.069 -0.069 4 -0.050 -0.043 -0.043 5 0.093 0.074 0.074 6 -0.046 -0.047 -0.047 7 -0.007 -0.007 -0.007 8 -0.022 -0.023 -0.023 9 0.030 0.031 0.031 10 0.000 0.000 0.000 11 0.000 0.000 0.000 12 0.000 0.000 0.000 13 0.000 0.000 0.000 14 0.000 0.000 0.000 15 0.000 0.000 0.000 16 0.000 0.000 0.000 17 0.000 0.000 0.000 18 0.000 0.000 0.000 19 -0.576 -0.409 -0.409 20 -1.147 -0.798 -0.798 21 0.646 0.491 0.491 22 0.470 0.343 0.343 23 -0.431 -0.274 -0.274 24 0.302 0.308 0.308 25 0.806 0.784 0.784 26 0.473 0.539 0.539 27 -0.670 -0.683 -0.683 28 NA NA NA 29 NA NA NA 30 NA NA NA 31 NA NA NA 32 NA NA NA 33 NA NA NA 34 -0.004 -0.004 -0.004 35 -0.001 -0.001 -0.001 36 0.001 0.001 0.001 37 -0.051 -0.045 -0.045 38 -0.016 -0.015 -0.015 39 0.059 0.058 0.058 40 0.050 0.046 0.046 41 -0.091 -0.077 -0.077 42 0.061 0.059 0.059 43 0.004 0.004 0.004 44 0.009 0.008 0.008 45 -0.014 -0.013 -0.013 46 0.000 0.000 0.000 47 0.000 0.000 0.000 48 0.000 0.000 0.000 49 0.000 0.000 0.000 50 0.000 0.000 0.000 51 0.000 0.000 0.000 52 0.000 0.000 0.000 53 0.000 0.000 0.000 54 0.000 0.000 0.000 55 0.653 0.504 0.504 56 0.958 0.811 0.811 57 -0.548 -0.515 -0.515 58 -0.360 -0.301 -0.301 59 0.354 0.250 0.250 60 -0.427 -0.404 -0.404 61 -0.695 -0.645 -0.645 62 -0.439 -0.387 -0.387 63 0.547 0.506 0.506 64 NA NA NA 65 NA NA NA 66 NA NA NA 67 -1.000 -1.000 -1.000 68 1.000 1.000 1.000 69 1.000 1.000 1.000 70 0.000 0.000 0.000 71 -0.002 -0.002 -0.002 72 0.001 0.001 0.001 16.10.5.6.4 Equivalence Test The petersenlab package (Petersen, 2024b) contains the equiv_chi() function from Counsell et al. (2020) that performs an equivalence test: https://osf.io/cqu8v. The chi-square equivalence test is non-significant, suggesting that the model fit is not acceptable. Code equiv_chi( alpha = .05, chi = metricInvarianceModel_chisquare, df = metricInvarianceModel_df, m = 2, N_sample = metricInvarianceModel_N, popRMSEA = .08) Moreover, the equivalence test of the chi-square difference test is non-significant, suggesting that the degree of worsening of model fit is not acceptable. In other words, metric invariance failed. Code metricInvarianceModel_dfDiff &lt;- metricInvarianceModel_df - configuralInvarianceModel_df equiv_chi( alpha = .05, chi = metricInvarianceModel_chisquareDiff, df = metricInvarianceModel_dfDiff, m = 2, N_sample = metricInvarianceModel_N, popRMSEA = .08) 16.10.5.6.5 Permutation Test Permutation procedures for testing measurement invariance are described in (Jorgensen et al., 2018). For reproducibility, I set the seed below. Using the same seed will yield the same answer every time. There is nothing special about this particular seed. You can specify the null model as the baseline model using: baseline.model = nullModelFit. Warning: this code takes a while to run based on \\(100\\) iterations. You can reduce the number of iterations to be faster. Code set.seed(52242) metricInvarianceTest &lt;- permuteMeasEq( nPermute = numPermutations, modelType = &quot;mgcfa&quot;, con = metricInvarianceModel_fit, uncon = configuralInvarianceModel_fit, AFIs = myAFIs, moreAFIs = moreAFIs, parallelType = &quot;multicore&quot;, #only &#39;snow&#39; works on Windows, but right now, it is throwing an error iseed = 52242) Code RNGkind(&quot;default&quot;, &quot;default&quot;, &quot;default&quot;) set.seed(52242) metricInvarianceTest Omnibus p value based on parametric chi-squared difference test: Chisq diff Df diff Pr(&gt;Chisq) 5.146 6.000 0.525 Omnibus p values based on nonparametric permutation method: AFI.Difference p.value chisq 6.568 0.51 chisq.scaled 4.291 0.64 rmsea -0.003 0.54 cfi -0.001 0.51 tli 0.006 0.55 srmr 0.007 0.47 rmsea.robust 0.003 0.12 cfi.robust -0.009 0.13 tli.robust -0.005 0.12 The p-values are non-significant, indicating that the model does not fit significantly worse than the configural invariance model. In other words, metric invariance held. 16.10.5.7 Internal Consistency Reliability Internal consistency reliability of items composing the latent factors, as quantified by omega (\\(\\omega\\)) and average variance extracted (AVE), was estimated using the semTools package (Jorgensen et al., 2021). Code compRelSEM(metricInvarianceModel_fit) Code AVE(metricInvarianceModel_fit) 16.10.5.8 Path Diagram A path diagram of the model generated using the semPlot package (Epskamp, 2022) is below. Code semPaths( metricInvarianceModel_fit, what = &quot;est&quot;, layout = &quot;tree2&quot;, ask = FALSE, edge.label.cex = 1.2) Figure 16.55: Metric Invariance Model in Confirmatory Factor Analysis. Figure 16.56: Metric Invariance Model in Confirmatory Factor Analysis. 16.10.6 Scalar (“Strong Factorial”) Invariance Model Specify invariance of factor loadings and intercepts across groups. 16.10.6.1 Model Syntax Code scalarInvarianceModel &lt;- measEq.syntax( configural.model = cfaModel, data = HolzingerSwineford1939, ID.fac = &quot;std.lv&quot;, group = &quot;school&quot;, group.equal = c(&quot;loadings&quot;,&quot;intercepts&quot;)) Code cat(as.character(scalarInvarianceModel)) ## LOADINGS: visual =~ c(NA, NA)*x1 + c(lambda.1_1, lambda.1_1)*x1 visual =~ c(NA, NA)*x2 + c(lambda.2_1, lambda.2_1)*x2 visual =~ c(NA, NA)*x3 + c(lambda.3_1, lambda.3_1)*x3 textual =~ c(NA, NA)*x4 + c(lambda.4_2, lambda.4_2)*x4 textual =~ c(NA, NA)*x5 + c(lambda.5_2, lambda.5_2)*x5 textual =~ c(NA, NA)*x6 + c(lambda.6_2, lambda.6_2)*x6 speed =~ c(NA, NA)*x7 + c(lambda.7_3, lambda.7_3)*x7 speed =~ c(NA, NA)*x8 + c(lambda.8_3, lambda.8_3)*x8 speed =~ c(NA, NA)*x9 + c(lambda.9_3, lambda.9_3)*x9 ## INTERCEPTS: x1 ~ c(NA, NA)*1 + c(nu.1, nu.1)*1 x2 ~ c(NA, NA)*1 + c(nu.2, nu.2)*1 x3 ~ c(NA, NA)*1 + c(nu.3, nu.3)*1 x4 ~ c(NA, NA)*1 + c(nu.4, nu.4)*1 x5 ~ c(NA, NA)*1 + c(nu.5, nu.5)*1 x6 ~ c(NA, NA)*1 + c(nu.6, nu.6)*1 x7 ~ c(NA, NA)*1 + c(nu.7, nu.7)*1 x8 ~ c(NA, NA)*1 + c(nu.8, nu.8)*1 x9 ~ c(NA, NA)*1 + c(nu.9, nu.9)*1 ## UNIQUE-FACTOR VARIANCES: x1 ~~ c(NA, NA)*x1 + c(theta.1_1.g1, theta.1_1.g2)*x1 x2 ~~ c(NA, NA)*x2 + c(theta.2_2.g1, theta.2_2.g2)*x2 x3 ~~ c(NA, NA)*x3 + c(theta.3_3.g1, theta.3_3.g2)*x3 x4 ~~ c(NA, NA)*x4 + c(theta.4_4.g1, theta.4_4.g2)*x4 x5 ~~ c(NA, NA)*x5 + c(theta.5_5.g1, theta.5_5.g2)*x5 x6 ~~ c(NA, NA)*x6 + c(theta.6_6.g1, theta.6_6.g2)*x6 x7 ~~ c(NA, NA)*x7 + c(theta.7_7.g1, theta.7_7.g2)*x7 x8 ~~ c(NA, NA)*x8 + c(theta.8_8.g1, theta.8_8.g2)*x8 x9 ~~ c(NA, NA)*x9 + c(theta.9_9.g1, theta.9_9.g2)*x9 ## LATENT MEANS/INTERCEPTS: visual ~ c(0, NA)*1 + c(alpha.1.g1, alpha.1.g2)*1 textual ~ c(0, NA)*1 + c(alpha.2.g1, alpha.2.g2)*1 speed ~ c(0, NA)*1 + c(alpha.3.g1, alpha.3.g2)*1 ## COMMON-FACTOR VARIANCES: visual ~~ c(1, NA)*visual + c(psi.1_1.g1, psi.1_1.g2)*visual textual ~~ c(1, NA)*textual + c(psi.2_2.g1, psi.2_2.g2)*textual speed ~~ c(1, NA)*speed + c(psi.3_3.g1, psi.3_3.g2)*speed ## COMMON-FACTOR COVARIANCES: visual ~~ c(NA, NA)*textual + c(psi.2_1.g1, psi.2_1.g2)*textual visual ~~ c(NA, NA)*speed + c(psi.3_1.g1, psi.3_1.g2)*speed textual ~~ c(NA, NA)*speed + c(psi.3_2.g1, psi.3_2.g2)*speed Code scalarInvarianceSyntax &lt;- as.character(scalarInvarianceModel) 16.10.6.1.1 Summary of Model Features Code summary(scalarInvarianceModel) This lavaan model syntax specifies a CFA with 9 manifest indicators of 3 common factor(s). To identify the location and scale of each common factor, the factor means and variances were fixed to 0 and 1, respectively, unless equality constraints on measurement parameters allow them to be freed. Pattern matrix indicating num(eric), ord(ered), and lat(ent) indicators per factor: visual textual speed x1 num x2 num x3 num x4 num x5 num x6 num x7 num x8 num x9 num The following types of parameter were constrained to equality across groups: loadings intercepts 16.10.6.2 Model Syntax in Table Form Code lavaanify(scalarInvarianceModel, ngroups = 2) Code lavaanify(cfaModel_scalarInvariance, ngroups = 2) 16.10.6.3 Fit the Model Code scalarInvarianceModel_fit &lt;- cfa( scalarInvarianceSyntax, data = HolzingerSwineford1939, group = &quot;school&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) Code scalarInvarianceModelFullSyntax_fit &lt;- lavaan( cfaModel_scalarInvariance, data = HolzingerSwineford1939, group = &quot;school&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) 16.10.6.4 Model Summary Code summary( scalarInvarianceModel_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 67 iterations Estimator ML Optimization method NLMINB Number of model parameters 66 Number of equality constraints 18 Number of observations per group: 2 132 1 127 Number of missing patterns per group: 2 48 1 52 Model Test User Model: Standard Scaled Test Statistic 97.784 94.366 Degrees of freedom 60 60 P-value (Chi-square) 0.001 0.003 Scaling correction factor 1.036 Yuan-Bentler correction (Mplus variant) Test statistic for each group: 2 57.155 55.157 1 40.629 39.209 Model Test Baseline Model: Test statistic 604.129 571.412 Degrees of freedom 72 72 P-value 0.000 0.000 Scaling correction factor 1.057 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.929 0.931 Tucker-Lewis Index (TLI) 0.915 0.917 Robust Comparative Fit Index (CFI) 0.928 Robust Tucker-Lewis Index (TLI) 0.914 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -2699.390 -2699.390 Scaling correction factor 0.803 for the MLR correction Loglikelihood unrestricted model (H1) -2650.498 -2650.498 Scaling correction factor 1.066 for the MLR correction Akaike (AIC) 5494.779 5494.779 Bayesian (BIC) 5665.507 5665.507 Sample-size adjusted Bayesian (SABIC) 5513.330 5513.330 Root Mean Square Error of Approximation: RMSEA 0.070 0.067 90 Percent confidence interval - lower 0.043 0.040 90 Percent confidence interval - upper 0.094 0.091 P-value H_0: RMSEA &lt;= 0.050 0.101 0.141 P-value H_0: RMSEA &gt;= 0.080 0.260 0.193 Robust RMSEA 0.082 90 Percent confidence interval - lower 0.046 90 Percent confidence interval - upper 0.114 P-value H_0: Robust RMSEA &lt;= 0.050 0.070 P-value H_0: Robust RMSEA &gt;= 0.080 0.561 Standardized Root Mean Square Residual: SRMR 0.079 0.079 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Group 1 [2]: Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual =~ x1 (l.1_) 0.900 0.131 6.892 0.000 0.900 0.760 x2 (l.2_) 0.516 0.097 5.320 0.000 0.516 0.429 x3 (l.3_) 0.835 0.090 9.300 0.000 0.835 0.718 textual =~ x4 (l.4_) 0.940 0.089 10.506 0.000 0.940 0.806 x5 (l.5_) 1.083 0.118 9.182 0.000 1.083 0.858 x6 (l.6_) 0.824 0.088 9.345 0.000 0.824 0.832 speed =~ x7 (l.7_) 0.460 0.094 4.903 0.000 0.460 0.447 x8 (l.8_) 0.615 0.106 5.794 0.000 0.615 0.655 x9 (l.9_) 0.571 0.116 4.933 0.000 0.571 0.578 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual ~~ textul (p.2_) 0.455 0.128 3.556 0.000 0.455 0.455 speed (p.3_1) 0.420 0.141 2.968 0.003 0.420 0.420 textual ~~ speed (p.3_2) 0.316 0.115 2.742 0.006 0.316 0.316 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 (nu.1) 5.017 0.106 47.479 0.000 5.017 4.235 .x2 (nu.2) 6.127 0.093 65.843 0.000 6.127 5.095 .x3 (nu.3) 2.258 0.103 21.837 0.000 2.258 1.941 .x4 (nu.4) 2.790 0.098 28.357 0.000 2.790 2.392 .x5 (nu.5) 4.043 0.116 34.780 0.000 4.043 3.203 .x6 (nu.6) 1.971 0.080 24.757 0.000 1.971 1.989 .x7 (nu.7) 4.191 0.080 52.551 0.000 4.191 4.069 .x8 (nu.8) 5.543 0.081 68.124 0.000 5.543 5.909 .x9 (nu.9) 5.411 0.086 63.043 0.000 5.411 5.475 visual (a.1.) 0.000 0.000 0.000 textual (a.2.) 0.000 0.000 0.000 speed (a.3.) 0.000 0.000 0.000 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 (t.1_) 0.593 0.183 3.249 0.001 0.593 0.423 .x2 (t.2_) 1.180 0.185 6.377 0.000 1.180 0.816 .x3 (t.3_) 0.655 0.145 4.510 0.000 0.655 0.484 .x4 (t.4_) 0.476 0.088 5.381 0.000 0.476 0.350 .x5 (t.5_) 0.420 0.110 3.828 0.000 0.420 0.264 .x6 (t.6_) 0.302 0.076 3.982 0.000 0.302 0.308 .x7 (t.7_) 0.849 0.135 6.312 0.000 0.849 0.800 .x8 (t.8_) 0.502 0.121 4.163 0.000 0.502 0.570 .x9 (t.9_) 0.651 0.152 4.289 0.000 0.651 0.666 visual (p.1_) 1.000 1.000 1.000 textual (p.2_) 1.000 1.000 1.000 speed (p.3_) 1.000 1.000 1.000 R-Square: Estimate x1 0.577 x2 0.184 x3 0.516 x4 0.650 x5 0.736 x6 0.692 x7 0.200 x8 0.430 x9 0.334 Group 2 [1]: Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual =~ x1 (l.1_) 0.900 0.131 6.892 0.000 0.790 0.697 x2 (l.2_) 0.516 0.097 5.320 0.000 0.452 0.415 x3 (l.3_) 0.835 0.090 9.300 0.000 0.733 0.703 textual =~ x4 (l.4_) 0.940 0.089 10.506 0.000 0.901 0.828 x5 (l.5_) 1.083 0.118 9.182 0.000 1.038 0.871 x6 (l.6_) 0.824 0.088 9.345 0.000 0.789 0.769 speed =~ x7 (l.7_) 0.460 0.094 4.903 0.000 0.605 0.576 x8 (l.8_) 0.615 0.106 5.794 0.000 0.808 0.758 x9 (l.9_) 0.571 0.116 4.933 0.000 0.750 0.720 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual ~~ textul (p.2_) 0.460 0.136 3.377 0.001 0.547 0.547 speed (p.3_1) 0.749 0.234 3.205 0.001 0.650 0.650 textual ~~ speed (p.3_2) 0.488 0.238 2.050 0.040 0.388 0.388 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 (nu.1) 5.017 0.106 47.479 0.000 5.017 4.425 .x2 (nu.2) 6.127 0.093 65.843 0.000 6.127 5.617 .x3 (nu.3) 2.258 0.103 21.837 0.000 2.258 2.165 .x4 (nu.4) 2.790 0.098 28.357 0.000 2.790 2.563 .x5 (nu.5) 4.043 0.116 34.780 0.000 4.043 3.392 .x6 (nu.6) 1.971 0.080 24.757 0.000 1.971 1.920 .x7 (nu.7) 4.191 0.080 52.551 0.000 4.191 3.992 .x8 (nu.8) 5.543 0.081 68.124 0.000 5.543 5.204 .x9 (nu.9) 5.411 0.086 63.043 0.000 5.411 5.194 visual (a.1.) -0.205 0.156 -1.314 0.189 -0.233 -0.233 textual (a.2.) 0.565 0.149 3.785 0.000 0.590 0.590 speed (a.3.) -0.075 0.190 -0.394 0.694 -0.057 -0.057 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 (t.1_) 0.662 0.167 3.973 0.000 0.662 0.515 .x2 (t.2_) 0.985 0.191 5.152 0.000 0.985 0.828 .x3 (t.3_) 0.551 0.126 4.386 0.000 0.551 0.506 .x4 (t.4_) 0.373 0.086 4.318 0.000 0.373 0.315 .x5 (t.5_) 0.344 0.096 3.576 0.000 0.344 0.242 .x6 (t.6_) 0.430 0.103 4.180 0.000 0.430 0.408 .x7 (t.7_) 0.737 0.129 5.722 0.000 0.737 0.669 .x8 (t.8_) 0.482 0.215 2.242 0.025 0.482 0.425 .x9 (t.9_) 0.522 0.153 3.409 0.001 0.522 0.481 visual (p.1_) 0.770 0.209 3.678 0.000 1.000 1.000 textual (p.2_) 0.918 0.231 3.977 0.000 1.000 1.000 speed (p.3_) 1.726 0.524 3.296 0.001 1.000 1.000 R-Square: Estimate x1 0.485 x2 0.172 x3 0.494 x4 0.685 x5 0.758 x6 0.592 x7 0.331 x8 0.575 x9 0.519 Code summary( scalarInvarianceModelFullSyntax_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 62 iterations Estimator ML Optimization method NLMINB Number of model parameters 66 Number of equality constraints 18 Number of observations per group: 2 132 1 127 Number of missing patterns per group: 2 48 1 52 Model Test User Model: Standard Scaled Test Statistic 97.784 94.366 Degrees of freedom 60 60 P-value (Chi-square) 0.001 0.003 Scaling correction factor 1.036 Yuan-Bentler correction (Mplus variant) Test statistic for each group: 2 57.155 55.157 1 40.629 39.209 Model Test Baseline Model: Test statistic 604.129 571.412 Degrees of freedom 72 72 P-value 0.000 0.000 Scaling correction factor 1.057 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.929 0.931 Tucker-Lewis Index (TLI) 0.915 0.917 Robust Comparative Fit Index (CFI) 0.928 Robust Tucker-Lewis Index (TLI) 0.914 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -2699.390 -2699.390 Scaling correction factor 0.803 for the MLR correction Loglikelihood unrestricted model (H1) -2650.498 -2650.498 Scaling correction factor 1.066 for the MLR correction Akaike (AIC) 5494.779 5494.779 Bayesian (BIC) 5665.507 5665.507 Sample-size adjusted Bayesian (SABIC) 5513.330 5513.330 Root Mean Square Error of Approximation: RMSEA 0.070 0.067 90 Percent confidence interval - lower 0.043 0.040 90 Percent confidence interval - upper 0.094 0.091 P-value H_0: RMSEA &lt;= 0.050 0.101 0.141 P-value H_0: RMSEA &gt;= 0.080 0.260 0.193 Robust RMSEA 0.082 90 Percent confidence interval - lower 0.046 90 Percent confidence interval - upper 0.114 P-value H_0: Robust RMSEA &lt;= 0.050 0.070 P-value H_0: Robust RMSEA &gt;= 0.080 0.561 Standardized Root Mean Square Residual: SRMR 0.079 0.079 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Group 1 [2]: Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual =~ x1 (lmb1) 0.900 0.131 6.892 0.000 0.900 0.760 x2 (lmb2) 0.516 0.097 5.320 0.000 0.516 0.429 x3 (lmb3) 0.835 0.090 9.300 0.000 0.835 0.718 textual =~ x4 (lmb4) 0.940 0.089 10.506 0.000 0.940 0.806 x5 (lmb5) 1.083 0.118 9.182 0.000 1.083 0.858 x6 (lmb6) 0.824 0.088 9.345 0.000 0.824 0.832 speed =~ x7 (lmb7) 0.460 0.094 4.903 0.000 0.460 0.447 x8 (lmb8) 0.615 0.106 5.794 0.000 0.615 0.655 x9 (lmb9) 0.571 0.116 4.934 0.000 0.571 0.578 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual ~~ textual 0.455 0.128 3.557 0.000 0.455 0.455 speed 0.420 0.141 2.968 0.003 0.420 0.420 textual ~~ speed 0.316 0.115 2.742 0.006 0.316 0.316 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual 0.000 0.000 0.000 textual 0.000 0.000 0.000 speed 0.000 0.000 0.000 .x1 (int1) 5.017 0.106 47.479 0.000 5.017 4.235 .x2 (int2) 6.127 0.093 65.843 0.000 6.127 5.095 .x3 (int3) 2.258 0.103 21.837 0.000 2.258 1.941 .x4 (int4) 2.790 0.098 28.357 0.000 2.790 2.392 .x5 (int5) 4.043 0.116 34.780 0.000 4.043 3.203 .x6 (int6) 1.971 0.080 24.757 0.000 1.971 1.989 .x7 (int7) 4.191 0.080 52.551 0.000 4.191 4.069 .x8 (int8) 5.543 0.081 68.124 0.000 5.543 5.909 .x9 (int9) 5.411 0.086 63.043 0.000 5.411 5.475 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual 1.000 1.000 1.000 textual 1.000 1.000 1.000 speed 1.000 1.000 1.000 .x1 0.593 0.183 3.249 0.001 0.593 0.423 .x2 1.180 0.185 6.377 0.000 1.180 0.816 .x3 0.655 0.145 4.510 0.000 0.655 0.484 .x4 0.476 0.088 5.381 0.000 0.476 0.350 .x5 0.420 0.110 3.828 0.000 0.420 0.264 .x6 0.302 0.076 3.982 0.000 0.302 0.308 .x7 0.849 0.135 6.312 0.000 0.849 0.800 .x8 0.502 0.121 4.163 0.000 0.502 0.570 .x9 0.651 0.152 4.289 0.000 0.651 0.666 R-Square: Estimate x1 0.577 x2 0.184 x3 0.516 x4 0.650 x5 0.736 x6 0.692 x7 0.200 x8 0.430 x9 0.334 Group 2 [1]: Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual =~ x1 (lmb1) 0.900 0.131 6.892 0.000 0.790 0.697 x2 (lmb2) 0.516 0.097 5.320 0.000 0.452 0.415 x3 (lmb3) 0.835 0.090 9.300 0.000 0.733 0.703 textual =~ x4 (lmb4) 0.940 0.089 10.506 0.000 0.901 0.828 x5 (lmb5) 1.083 0.118 9.182 0.000 1.038 0.871 x6 (lmb6) 0.824 0.088 9.345 0.000 0.789 0.769 speed =~ x7 (lmb7) 0.460 0.094 4.903 0.000 0.605 0.576 x8 (lmb8) 0.615 0.106 5.794 0.000 0.808 0.758 x9 (lmb9) 0.571 0.116 4.934 0.000 0.750 0.720 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual ~~ textual 0.460 0.136 3.377 0.001 0.547 0.547 speed 0.749 0.234 3.205 0.001 0.650 0.650 textual ~~ speed 0.488 0.238 2.050 0.040 0.388 0.388 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual -0.205 0.156 -1.314 0.189 -0.233 -0.233 textual 0.565 0.149 3.785 0.000 0.590 0.590 speed -0.075 0.190 -0.394 0.694 -0.057 -0.057 .x1 (int1) 5.017 0.106 47.479 0.000 5.017 4.425 .x2 (int2) 6.127 0.093 65.843 0.000 6.127 5.617 .x3 (int3) 2.258 0.103 21.837 0.000 2.258 2.165 .x4 (int4) 2.790 0.098 28.357 0.000 2.790 2.563 .x5 (int5) 4.043 0.116 34.780 0.000 4.043 3.392 .x6 (int6) 1.971 0.080 24.757 0.000 1.971 1.920 .x7 (int7) 4.191 0.080 52.551 0.000 4.191 3.992 .x8 (int8) 5.543 0.081 68.124 0.000 5.543 5.204 .x9 (int9) 5.411 0.086 63.043 0.000 5.411 5.194 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual 0.770 0.209 3.678 0.000 1.000 1.000 textual 0.918 0.231 3.977 0.000 1.000 1.000 speed 1.726 0.524 3.296 0.001 1.000 1.000 .x1 0.662 0.167 3.973 0.000 0.662 0.515 .x2 0.985 0.191 5.152 0.000 0.985 0.828 .x3 0.551 0.126 4.386 0.000 0.551 0.506 .x4 0.373 0.086 4.318 0.000 0.373 0.315 .x5 0.344 0.096 3.576 0.000 0.344 0.242 .x6 0.430 0.103 4.180 0.000 0.430 0.408 .x7 0.737 0.129 5.722 0.000 0.737 0.669 .x8 0.482 0.215 2.242 0.025 0.482 0.425 .x9 0.522 0.153 3.409 0.001 0.522 0.481 R-Square: Estimate x1 0.485 x2 0.172 x3 0.494 x4 0.685 x5 0.758 x6 0.592 x7 0.331 x8 0.575 x9 0.519 16.10.6.5 Model Fit You can specify the null model as the baseline model using: baseline.model = nullModelFit Code fitMeasures(scalarInvarianceModel_fit) npar fmin 48.000 0.189 chisq df 97.784 60.000 pvalue chisq.scaled 0.001 94.366 df.scaled pvalue.scaled 60.000 0.003 chisq.scaling.factor baseline.chisq 1.036 604.129 baseline.df baseline.pvalue 72.000 0.000 baseline.chisq.scaled baseline.df.scaled 571.412 72.000 baseline.pvalue.scaled baseline.chisq.scaling.factor 0.000 1.057 cfi tli 0.929 0.915 cfi.scaled tli.scaled 0.931 0.917 cfi.robust tli.robust 0.928 0.914 nnfi rfi 0.915 0.806 nfi pnfi 0.838 0.698 ifi rni 0.931 0.929 nnfi.scaled rfi.scaled 0.917 0.802 nfi.scaled pnfi.scaled 0.835 0.696 ifi.scaled rni.scaled 0.933 0.931 nnfi.robust rni.robust 0.914 0.928 logl unrestricted.logl -2699.390 -2650.498 aic bic 5494.779 5665.507 ntotal bic2 259.000 5513.330 scaling.factor.h1 scaling.factor.h0 1.066 0.803 rmsea rmsea.ci.lower 0.070 0.043 rmsea.ci.upper rmsea.ci.level 0.094 0.900 rmsea.pvalue rmsea.close.h0 0.101 0.050 rmsea.notclose.pvalue rmsea.notclose.h0 0.260 0.080 rmsea.scaled rmsea.ci.lower.scaled 0.067 0.040 rmsea.ci.upper.scaled rmsea.pvalue.scaled 0.091 0.141 rmsea.notclose.pvalue.scaled rmsea.robust 0.193 0.082 rmsea.ci.lower.robust rmsea.ci.upper.robust 0.046 0.114 rmsea.pvalue.robust rmsea.notclose.pvalue.robust 0.070 0.561 rmr rmr_nomean 0.095 0.097 srmr srmr_bentler 0.079 0.079 srmr_bentler_nomean crmr 0.079 0.081 crmr_nomean srmr_mplus 0.082 0.086 srmr_mplus_nomean cn_05 0.077 210.464 cn_01 gfi 235.091 0.994 agfi pgfi 0.990 0.552 mfi ecvi 0.930 0.748 Code scalarInvarianceModelFitIndices &lt;- fitMeasures( scalarInvarianceModel_fit)[c( &quot;cfi.robust&quot;, &quot;rmsea.robust&quot;, &quot;srmr&quot;)] scalarInvarianceModel_chisquare &lt;- fitMeasures( scalarInvarianceModel_fit)[c(&quot;chisq.scaled&quot;)] scalarInvarianceModel_chisquareScaling &lt;- fitMeasures( scalarInvarianceModel_fit)[c(&quot;chisq.scaling.factor&quot;)] scalarInvarianceModel_df &lt;- fitMeasures( scalarInvarianceModel_fit)[c(&quot;df.scaled&quot;)] scalarInvarianceModel_N &lt;- lavInspect( scalarInvarianceModel_fit, what = &quot;ntotal&quot;) 16.10.6.6 Compare Model Fit 16.10.6.6.1 Nested Model (\\(\\chi^2\\)) Difference Test The metric invariance model and the scalar (“strong factorial”) invariance model are considered “nested” models. The scalar invariance model is nested within the metric invariance model because the metric invariance model includes all of the terms of the metric invariance model along with additional terms. Model fit of nested models can be compared with a chi-square difference test, also known as a likelihood ratio test or deviance test. A significant chi-square difference test would indicate that the simplified model with additional constraints (the scalar invariance model) is significantly worse fitting than the more complex model that has fewer constraints (the metric invariance model). Code anova(metricInvarianceModel_fit, scalarInvarianceModel_fit) The scalar invariance model fit significantly worse than the configural invariance model, so scalar invariance did not hold. This provides evidence of measurement non-invariance of indicator intercepts across groups. Measurement non-invariance of indicator intercepts poses challenges to being able to meaningfully compare levels on the latent factor across groups (Little et al., 2007). The petersenlab package (Petersen, 2024b) contains the satorraBentlerScaledChiSquareDifferenceTestStatistic() function that performs a Satorra-Bentler scaled chi-square difference test: Code scalarInvarianceModel_chisquareDiff &lt;- satorraBentlerScaledChiSquareDifferenceTestStatistic( T0 = scalarInvarianceModel_chisquare, c0 = scalarInvarianceModel_chisquareScaling, d0 = scalarInvarianceModel_df, T1 = metricInvarianceModel_chisquare, c1 = metricInvarianceModel_chisquareScaling, d1 = metricInvarianceModel_df) scalarInvarianceModel_chisquareDiff chisq.scaled 19.28849 16.10.6.6.2 Compare Other Fit Criteria Code round( scalarInvarianceModelFitIndices - metricInvarianceModelFitIndices, digits = 3) cfi.robust rmsea.robust srmr -0.018 0.007 0.006 16.10.6.6.3 Score-Based Test Score-based tests of measurement invariance are implemented using the strucchange package and are described by T. Wang et al. (2014). Code coef(scalarInvarianceModel_fit) lambda.1_1 lambda.2_1 lambda.3_1 lambda.4_2 lambda.5_2 lambda.6_2 0.900 0.516 0.835 0.940 1.083 0.824 lambda.7_3 lambda.8_3 lambda.9_3 nu.1 nu.2 nu.3 0.460 0.615 0.571 5.017 6.127 2.258 nu.4 nu.5 nu.6 nu.7 nu.8 nu.9 2.790 4.043 1.971 4.191 5.543 5.411 theta.1_1.g1 theta.2_2.g1 theta.3_3.g1 theta.4_4.g1 theta.5_5.g1 theta.6_6.g1 0.593 1.180 0.655 0.476 0.420 0.302 theta.7_7.g1 theta.8_8.g1 theta.9_9.g1 psi.2_1.g1 psi.3_1.g1 psi.3_2.g1 0.849 0.502 0.651 0.455 0.420 0.316 lambda.1_1 lambda.2_1 lambda.3_1 lambda.4_2 lambda.5_2 lambda.6_2 0.900 0.516 0.835 0.940 1.083 0.824 lambda.7_3 lambda.8_3 lambda.9_3 nu.1 nu.2 nu.3 0.460 0.615 0.571 5.017 6.127 2.258 nu.4 nu.5 nu.6 nu.7 nu.8 nu.9 2.790 4.043 1.971 4.191 5.543 5.411 theta.1_1.g2 theta.2_2.g2 theta.3_3.g2 theta.4_4.g2 theta.5_5.g2 theta.6_6.g2 0.662 0.985 0.551 0.373 0.344 0.430 theta.7_7.g2 theta.8_8.g2 theta.9_9.g2 alpha.1.g2 alpha.2.g2 alpha.3.g2 0.737 0.482 0.522 -0.205 0.565 -0.075 psi.1_1.g2 psi.2_2.g2 psi.3_3.g2 psi.2_1.g2 psi.3_1.g2 psi.3_2.g2 0.770 0.918 1.726 0.460 0.749 0.488 Code coef(scalarInvarianceModel_fit)[10:18] nu.1 nu.2 nu.3 nu.4 nu.5 nu.6 nu.7 nu.8 5.017046 6.127367 2.258070 2.789714 4.042892 1.970548 4.191389 5.543168 nu.9 5.410889 Code sctest( scalarInvarianceModel_fit, order.by = HolzingerSwineford1939$school, parm = 10:18, functional = &quot;LMuo&quot;) Error in `[&lt;-`(`*tmp*`, wi, , value = -scores.H1 %*% Delta): subscript out of bounds A score-based test and expected parameter change (EPC) estimates (Oberski, 2014; Oberski et al., 2015) are provided by the lavaan package (Rosseel et al., 2022). Code lavTestScore( scalarInvarianceModel_fit, epc = TRUE ) $test total score test: test X2 df p.value 1 score 25.395 18 0.114 $uni univariate score tests: lhs op rhs X2 df p.value 1 .p1. == .p37. 1.089 1 0.297 2 .p2. == .p38. 0.323 1 0.570 3 .p3. == .p39. 2.160 1 0.142 4 .p4. == .p40. 0.548 1 0.459 5 .p5. == .p41. 3.142 1 0.076 6 .p6. == .p42. 1.243 1 0.265 7 .p7. == .p43. 0.147 1 0.702 8 .p8. == .p44. 0.009 1 0.925 9 .p9. == .p45. 0.202 1 0.653 10 .p10. == .p46. 1.340 1 0.247 11 .p11. == .p47. 4.970 1 0.026 12 .p12. == .p48. 6.462 1 0.011 13 .p13. == .p49. 0.449 1 0.503 14 .p14. == .p50. 0.400 1 0.527 15 .p15. == .p51. 0.000 1 0.986 16 .p16. == .p52. 9.223 1 0.002 17 .p17. == .p53. 4.666 1 0.031 18 .p18. == .p54. 0.030 1 0.862 $epc expected parameter changes (epc) and expected parameter values (epv): lhs op rhs block group free label plabel est epc epv 1 visual =~ x1 1 1 1 lambda.1_1 .p1. 0.900 0.088 0.988 2 visual =~ x2 1 1 2 lambda.2_1 .p2. 0.516 0.043 0.559 3 visual =~ x3 1 1 3 lambda.3_1 .p3. 0.835 -0.111 0.724 4 textual =~ x4 1 1 4 lambda.4_2 .p4. 0.940 -0.040 0.900 5 textual =~ x5 1 1 5 lambda.5_2 .p5. 1.083 0.082 1.166 6 textual =~ x6 1 1 6 lambda.6_2 .p6. 0.824 -0.046 0.778 7 speed =~ x7 1 1 7 lambda.7_3 .p7. 0.460 -0.035 0.425 8 speed =~ x8 1 1 8 lambda.8_3 .p8. 0.615 -0.012 0.602 9 speed =~ x9 1 1 9 lambda.9_3 .p9. 0.571 0.038 0.609 10 x1 ~1 1 1 10 nu.1 .p10. 5.017 -0.045 4.973 11 x2 ~1 1 1 11 nu.2 .p11. 6.127 -0.151 5.976 12 x3 ~1 1 1 12 nu.3 .p12. 2.258 0.116 2.374 13 x4 ~1 1 1 13 nu.4 .p13. 2.790 0.027 2.817 14 x5 ~1 1 1 14 nu.5 .p14. 4.043 -0.021 4.022 15 x6 ~1 1 1 15 nu.6 .p15. 1.971 0.001 1.971 16 x7 ~1 1 1 16 nu.7 .p16. 4.191 0.171 4.363 17 x8 ~1 1 1 17 nu.8 .p17. 5.543 -0.079 5.464 18 x9 ~1 1 1 18 nu.9 .p18. 5.411 -0.009 5.402 19 x1 ~~ x1 1 1 19 theta.1_1.g1 .p19. 0.593 -0.114 0.480 20 x2 ~~ x2 1 1 20 theta.2_2.g1 .p20. 1.180 -0.014 1.166 21 x3 ~~ x3 1 1 21 theta.3_3.g1 .p21. 0.655 0.116 0.771 22 x4 ~~ x4 1 1 22 theta.4_4.g1 .p22. 0.476 0.025 0.501 23 x5 ~~ x5 1 1 23 theta.5_5.g1 .p23. 0.420 -0.071 0.349 24 x6 ~~ x6 1 1 24 theta.6_6.g1 .p24. 0.302 0.029 0.331 25 x7 ~~ x7 1 1 25 theta.7_7.g1 .p25. 0.849 0.016 0.866 26 x8 ~~ x8 1 1 26 theta.8_8.g1 .p26. 0.502 0.013 0.515 27 x9 ~~ x9 1 1 27 theta.9_9.g1 .p27. 0.651 -0.028 0.623 28 visual ~1 1 1 0 alpha.1.g1 .p28. 0.000 NA NA 29 textual ~1 1 1 0 alpha.2.g1 .p29. 0.000 NA NA 30 speed ~1 1 1 0 alpha.3.g1 .p30. 0.000 NA NA 31 visual ~~ visual 1 1 0 psi.1_1.g1 .p31. 1.000 NA NA 32 textual ~~ textual 1 1 0 psi.2_2.g1 .p32. 1.000 NA NA 33 speed ~~ speed 1 1 0 psi.3_3.g1 .p33. 1.000 NA NA 34 visual ~~ textual 1 1 28 psi.2_1.g1 .p34. 0.455 -0.004 0.451 35 visual ~~ speed 1 1 29 psi.3_1.g1 .p35. 0.420 -0.002 0.417 36 textual ~~ speed 1 1 30 psi.3_2.g1 .p36. 0.316 0.000 0.315 37 visual =~ x1 2 2 31 lambda.1_1 .p37. 0.900 -0.044 0.856 38 visual =~ x2 2 2 32 lambda.2_1 .p38. 0.516 0.003 0.519 39 visual =~ x3 2 2 33 lambda.3_1 .p39. 0.835 0.041 0.876 40 textual =~ x4 2 2 34 lambda.4_2 .p40. 0.940 0.065 1.006 41 textual =~ x5 2 2 35 lambda.5_2 .p41. 1.083 -0.106 0.977 42 textual =~ x6 2 2 36 lambda.6_2 .p42. 0.824 0.067 0.891 43 speed =~ x7 2 2 37 lambda.7_3 .p43. 0.460 0.003 0.463 44 speed =~ x8 2 2 38 lambda.8_3 .p44. 0.615 0.011 0.626 45 speed =~ x9 2 2 39 lambda.9_3 .p45. 0.571 -0.014 0.557 46 x1 ~1 2 2 40 nu.1 .p46. 5.017 0.027 5.044 47 x2 ~1 2 2 41 nu.2 .p47. 6.127 0.132 6.259 48 x3 ~1 2 2 42 nu.3 .p48. 2.258 -0.096 2.162 49 x4 ~1 2 2 43 nu.4 .p49. 2.790 -0.059 2.731 50 x5 ~1 2 2 44 nu.5 .p50. 4.043 0.079 4.122 51 x6 ~1 2 2 45 nu.6 .p51. 1.971 -0.037 1.933 52 x7 ~1 2 2 46 nu.7 .p52. 4.191 -0.150 4.041 53 x8 ~1 2 2 47 nu.8 .p53. 5.543 0.082 5.625 54 x9 ~1 2 2 48 nu.9 .p54. 5.411 0.013 5.424 55 x1 ~~ x1 2 2 49 theta.1_1.g2 .p55. 0.662 0.033 0.694 56 x2 ~~ x2 2 2 50 theta.2_2.g2 .p56. 0.985 -0.001 0.985 57 x3 ~~ x3 2 2 51 theta.3_3.g2 .p57. 0.551 -0.027 0.523 58 x4 ~~ x4 2 2 52 theta.4_4.g2 .p58. 0.373 -0.049 0.324 59 x5 ~~ x5 2 2 53 theta.5_5.g2 .p59. 0.344 0.093 0.437 60 x6 ~~ x6 2 2 54 theta.6_6.g2 .p60. 0.430 -0.032 0.398 61 x7 ~~ x7 2 2 55 theta.7_7.g2 .p61. 0.737 -0.001 0.736 62 x8 ~~ x8 2 2 56 theta.8_8.g2 .p62. 0.482 -0.013 0.469 63 x9 ~~ x9 2 2 57 theta.9_9.g2 .p63. 0.522 0.014 0.536 64 visual ~1 2 2 58 alpha.1.g2 .p64. -0.205 0.012 -0.193 65 textual ~1 2 2 59 alpha.2.g2 .p65. 0.565 0.000 0.565 66 speed ~1 2 2 60 alpha.3.g2 .p66. -0.075 -0.011 -0.086 67 visual ~~ visual 2 2 61 psi.1_1.g2 .p67. 0.770 -0.001 0.769 68 textual ~~ textual 2 2 62 psi.2_2.g2 .p68. 0.918 0.000 0.918 69 speed ~~ speed 2 2 63 psi.3_3.g2 .p69. 1.726 0.000 1.726 70 visual ~~ textual 2 2 64 psi.2_1.g2 .p70. 0.460 0.001 0.461 71 visual ~~ speed 2 2 65 psi.3_1.g2 .p71. 0.749 -0.001 0.748 72 textual ~~ speed 2 2 66 psi.3_2.g2 .p72. 0.488 0.002 0.489 sepc.lv sepc.all sepc.nox 1 0.088 0.074 0.074 2 0.043 0.036 0.036 3 -0.111 -0.095 -0.095 4 -0.040 -0.034 -0.034 5 0.082 0.065 0.065 6 -0.046 -0.047 -0.047 7 -0.035 -0.034 -0.034 8 -0.012 -0.013 -0.013 9 0.038 0.039 0.039 10 -0.045 -0.038 -0.038 11 -0.151 -0.126 -0.126 12 0.116 0.100 0.100 13 0.027 0.023 0.023 14 -0.021 -0.016 -0.016 15 0.001 0.001 0.001 16 0.171 0.166 0.166 17 -0.079 -0.085 -0.085 18 -0.009 -0.009 -0.009 19 -0.593 -0.423 -0.423 20 -1.180 -0.816 -0.816 21 0.655 0.484 0.484 22 0.476 0.350 0.350 23 -0.420 -0.264 -0.264 24 0.302 0.308 0.308 25 0.849 0.800 0.800 26 0.502 0.570 0.570 27 -0.651 -0.666 -0.666 28 NA NA NA 29 NA NA NA 30 NA NA NA 31 NA NA NA 32 NA NA NA 33 NA NA NA 34 -0.004 -0.004 -0.004 35 -0.002 -0.002 -0.002 36 0.000 0.000 0.000 37 -0.039 -0.034 -0.034 38 0.003 0.002 0.002 39 0.036 0.035 0.035 40 0.063 0.058 0.058 41 -0.102 -0.085 -0.085 42 0.064 0.062 0.062 43 0.004 0.004 0.004 44 0.014 0.013 0.013 45 -0.018 -0.017 -0.017 46 0.027 0.024 0.024 47 0.132 0.121 0.121 48 -0.096 -0.092 -0.092 49 -0.059 -0.054 -0.054 50 0.079 0.066 0.066 51 -0.037 -0.036 -0.036 52 -0.150 -0.143 -0.143 53 0.082 0.077 0.077 54 0.013 0.012 0.012 55 0.662 0.515 0.515 56 -0.985 -0.828 -0.828 57 -0.551 -0.506 -0.506 58 -0.373 -0.315 -0.315 59 0.344 0.242 0.242 60 -0.430 -0.408 -0.408 61 -0.737 -0.669 -0.669 62 -0.482 -0.425 -0.425 63 0.522 0.481 0.481 64 0.014 0.014 0.014 65 0.000 0.000 0.000 66 -0.009 -0.009 -0.009 67 -1.000 -1.000 -1.000 68 1.000 1.000 1.000 69 -1.000 -1.000 -1.000 70 0.002 0.002 0.002 71 -0.001 -0.001 -0.001 72 0.001 0.001 0.001 16.10.6.6.4 Equivalence Test The petersenlab package (Petersen, 2024b) contains the equiv_chi() function from Counsell et al. (2020) that performs an equivalence test: https://osf.io/cqu8v. The chi-square equivalence test is non-significant, suggesting that the model fit is not acceptable. Code equiv_chi( alpha = .05, chi = scalarInvarianceModel_chisquare, df = scalarInvarianceModel_df, m = 2, N_sample = scalarInvarianceModel_N, popRMSEA = .08) Moreover, the equivalence test of the chi-square difference test is non-significant, suggesting that the degree of worsening of model fit is not acceptable. In other words, scalar invariance failed. Code scalarInvarianceModel_dfDiff &lt;- scalarInvarianceModel_df - metricInvarianceModel_df equiv_chi( alpha = .05, chi = scalarInvarianceModel_chisquareDiff, df = scalarInvarianceModel_dfDiff, m = 2, N_sample = scalarInvarianceModel_N, popRMSEA = .08) 16.10.6.6.5 Permutation Test Permutation procedures for testing measurement invariance are described in (Jorgensen et al., 2018). For reproducibility, I set the seed below. Using the same seed will yield the same answer every time. There is nothing special about this particular seed. You can specify the null model as the baseline model using: baseline.model = nullModelFit Warning: this code takes a while to run based on 100 iterations. You can reduce the number of iterations to be faster. Code set.seed(52242) scalarInvarianceTest &lt;- permuteMeasEq( nPermute = numPermutations, modelType = &quot;mgcfa&quot;, con = scalarInvarianceModel_fit, uncon = metricInvarianceModel_fit, AFIs = myAFIs, moreAFIs = moreAFIs, parallelType = &quot;multicore&quot;, #only &#39;snow&#39; works on Windows, but right now, it is throwing an error iseed = 52242) Code RNGkind(&quot;default&quot;, &quot;default&quot;, &quot;default&quot;) set.seed(52242) scalarInvarianceTest Omnibus p value based on parametric chi-squared difference test: Chisq diff Df diff Pr(&gt;Chisq) 19.288 6.000 0.004 Omnibus p values based on nonparametric permutation method: AFI.Difference p.value chisq 19.773 0.00 chisq.scaled 19.171 0.00 rmsea 0.011 0.00 cfi -0.026 0.00 tli -0.025 0.00 srmr 0.006 0.03 rmsea.robust 0.007 0.02 cfi.robust -0.018 0.00 tli.robust -0.015 0.01 The p-values are significant, indicating that the model fit significantly worse than the metric invariance model. In other words, scalar invariance failed. 16.10.6.7 Internal Consistency Reliability Internal consistency reliability of items composing the latent factors, as quantified by omega (\\(\\omega\\)) and average variance extracted (AVE), was estimated using the semTools package (Jorgensen et al., 2021). Code compRelSEM(scalarInvarianceModel_fit) Code AVE(scalarInvarianceModel_fit) 16.10.6.8 Path Diagram A path diagram of the model generated using the semPlot package (Epskamp, 2022) is below. Code semPaths( scalarInvarianceModel_fit, what = &quot;est&quot;, layout = &quot;tree2&quot;, ask = FALSE, edge.label.cex = 1.2) Figure 16.57: Scalar Invariance Model in Confirmatory Factor Analysis. Figure 16.58: Scalar Invariance Model in Confirmatory Factor Analysis. 16.10.7 Residual (“Strict Factorial”) Invariance Model Specify invariance of factor loadings, intercepts, and residuals across groups. 16.10.7.1 Model Syntax Code residualInvarianceModel &lt;- measEq.syntax( configural.model = cfaModel, data = HolzingerSwineford1939, ID.fac = &quot;std.lv&quot;, group = &quot;school&quot;, group.equal = c(&quot;loadings&quot;,&quot;intercepts&quot;,&quot;residuals&quot;)) Code cat(as.character(residualInvarianceModel)) ## LOADINGS: visual =~ c(NA, NA)*x1 + c(lambda.1_1, lambda.1_1)*x1 visual =~ c(NA, NA)*x2 + c(lambda.2_1, lambda.2_1)*x2 visual =~ c(NA, NA)*x3 + c(lambda.3_1, lambda.3_1)*x3 textual =~ c(NA, NA)*x4 + c(lambda.4_2, lambda.4_2)*x4 textual =~ c(NA, NA)*x5 + c(lambda.5_2, lambda.5_2)*x5 textual =~ c(NA, NA)*x6 + c(lambda.6_2, lambda.6_2)*x6 speed =~ c(NA, NA)*x7 + c(lambda.7_3, lambda.7_3)*x7 speed =~ c(NA, NA)*x8 + c(lambda.8_3, lambda.8_3)*x8 speed =~ c(NA, NA)*x9 + c(lambda.9_3, lambda.9_3)*x9 ## INTERCEPTS: x1 ~ c(NA, NA)*1 + c(nu.1, nu.1)*1 x2 ~ c(NA, NA)*1 + c(nu.2, nu.2)*1 x3 ~ c(NA, NA)*1 + c(nu.3, nu.3)*1 x4 ~ c(NA, NA)*1 + c(nu.4, nu.4)*1 x5 ~ c(NA, NA)*1 + c(nu.5, nu.5)*1 x6 ~ c(NA, NA)*1 + c(nu.6, nu.6)*1 x7 ~ c(NA, NA)*1 + c(nu.7, nu.7)*1 x8 ~ c(NA, NA)*1 + c(nu.8, nu.8)*1 x9 ~ c(NA, NA)*1 + c(nu.9, nu.9)*1 ## UNIQUE-FACTOR VARIANCES: x1 ~~ c(NA, NA)*x1 + c(theta.1_1, theta.1_1)*x1 x2 ~~ c(NA, NA)*x2 + c(theta.2_2, theta.2_2)*x2 x3 ~~ c(NA, NA)*x3 + c(theta.3_3, theta.3_3)*x3 x4 ~~ c(NA, NA)*x4 + c(theta.4_4, theta.4_4)*x4 x5 ~~ c(NA, NA)*x5 + c(theta.5_5, theta.5_5)*x5 x6 ~~ c(NA, NA)*x6 + c(theta.6_6, theta.6_6)*x6 x7 ~~ c(NA, NA)*x7 + c(theta.7_7, theta.7_7)*x7 x8 ~~ c(NA, NA)*x8 + c(theta.8_8, theta.8_8)*x8 x9 ~~ c(NA, NA)*x9 + c(theta.9_9, theta.9_9)*x9 ## LATENT MEANS/INTERCEPTS: visual ~ c(0, NA)*1 + c(alpha.1.g1, alpha.1.g2)*1 textual ~ c(0, NA)*1 + c(alpha.2.g1, alpha.2.g2)*1 speed ~ c(0, NA)*1 + c(alpha.3.g1, alpha.3.g2)*1 ## COMMON-FACTOR VARIANCES: visual ~~ c(1, NA)*visual + c(psi.1_1.g1, psi.1_1.g2)*visual textual ~~ c(1, NA)*textual + c(psi.2_2.g1, psi.2_2.g2)*textual speed ~~ c(1, NA)*speed + c(psi.3_3.g1, psi.3_3.g2)*speed ## COMMON-FACTOR COVARIANCES: visual ~~ c(NA, NA)*textual + c(psi.2_1.g1, psi.2_1.g2)*textual visual ~~ c(NA, NA)*speed + c(psi.3_1.g1, psi.3_1.g2)*speed textual ~~ c(NA, NA)*speed + c(psi.3_2.g1, psi.3_2.g2)*speed Code residualInvarianceSyntax &lt;- as.character(residualInvarianceModel) 16.10.7.1.1 Summary of Model Features Code summary(residualInvarianceModel) This lavaan model syntax specifies a CFA with 9 manifest indicators of 3 common factor(s). To identify the location and scale of each common factor, the factor means and variances were fixed to 0 and 1, respectively, unless equality constraints on measurement parameters allow them to be freed. Pattern matrix indicating num(eric), ord(ered), and lat(ent) indicators per factor: visual textual speed x1 num x2 num x3 num x4 num x5 num x6 num x7 num x8 num x9 num The following types of parameter were constrained to equality across groups: loadings intercepts residuals 16.10.7.2 Model Syntax in Table Form Code lavaanify(residualInvarianceModel, ngroups = 2) Code lavaanify(cfaModel_residualInvariance, ngroups = 2) 16.10.7.3 Fit the Model Code residualInvarianceModel_fit &lt;- cfa( residualInvarianceSyntax, data = HolzingerSwineford1939, group = &quot;school&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) Code residualInvarianceModelFullSyntax_fit &lt;- lavaan( cfaModel_residualInvariance, data = HolzingerSwineford1939, group = &quot;school&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) 16.10.7.4 Model Summary Code summary( residualInvarianceModel_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 61 iterations Estimator ML Optimization method NLMINB Number of model parameters 66 Number of equality constraints 27 Number of observations per group: 2 132 1 127 Number of missing patterns per group: 2 48 1 52 Model Test User Model: Standard Scaled Test Statistic 102.581 97.522 Degrees of freedom 69 69 P-value (Chi-square) 0.005 0.014 Scaling correction factor 1.052 Yuan-Bentler correction (Mplus variant) Test statistic for each group: 2 58.408 55.528 1 44.173 41.994 Model Test Baseline Model: Test statistic 604.129 571.412 Degrees of freedom 72 72 P-value 0.000 0.000 Scaling correction factor 1.057 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.937 0.943 Tucker-Lewis Index (TLI) 0.934 0.940 Robust Comparative Fit Index (CFI) 0.940 Robust Tucker-Lewis Index (TLI) 0.938 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -2701.788 -2701.788 Scaling correction factor 0.645 for the MLR correction Loglikelihood unrestricted model (H1) -2650.498 -2650.498 Scaling correction factor 1.066 for the MLR correction Akaike (AIC) 5481.576 5481.576 Bayesian (BIC) 5620.293 5620.293 Sample-size adjusted Bayesian (SABIC) 5496.648 5496.648 Root Mean Square Error of Approximation: RMSEA 0.061 0.056 90 Percent confidence interval - lower 0.034 0.028 90 Percent confidence interval - upper 0.085 0.080 P-value H_0: RMSEA &lt;= 0.050 0.221 0.322 P-value H_0: RMSEA &gt;= 0.080 0.103 0.054 Robust RMSEA 0.070 90 Percent confidence interval - lower 0.030 90 Percent confidence interval - upper 0.101 P-value H_0: Robust RMSEA &lt;= 0.050 0.173 P-value H_0: Robust RMSEA &gt;= 0.080 0.320 Standardized Root Mean Square Residual: SRMR 0.082 0.082 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Group 1 [2]: Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual =~ x1 (l.1_) 0.892 0.123 7.263 0.000 0.892 0.743 x2 (l.2_) 0.523 0.099 5.294 0.000 0.523 0.449 x3 (l.3_) 0.848 0.092 9.224 0.000 0.848 0.742 textual =~ x4 (l.4_) 0.945 0.089 10.651 0.000 0.945 0.826 x5 (l.5_) 1.089 0.115 9.473 0.000 1.089 0.870 x6 (l.6_) 0.828 0.084 9.871 0.000 0.828 0.807 speed =~ x7 (l.7_) 0.469 0.095 4.939 0.000 0.469 0.465 x8 (l.8_) 0.631 0.103 6.129 0.000 0.631 0.670 x9 (l.9_) 0.586 0.112 5.251 0.000 0.586 0.607 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual ~~ textul (p.2_) 0.430 0.132 3.262 0.001 0.430 0.430 speed (p.3_1) 0.414 0.139 2.975 0.003 0.414 0.414 textual ~~ speed (p.3_2) 0.303 0.112 2.712 0.007 0.303 0.303 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 (nu.1) 5.023 0.105 47.688 0.000 5.023 4.182 .x2 (nu.2) 6.117 0.090 68.293 0.000 6.117 5.249 .x3 (nu.3) 2.270 0.104 21.872 0.000 2.270 1.986 .x4 (nu.4) 2.796 0.098 28.415 0.000 2.796 2.445 .x5 (nu.5) 4.041 0.115 35.290 0.000 4.041 3.225 .x6 (nu.6) 1.971 0.080 24.742 0.000 1.971 1.920 .x7 (nu.7) 4.203 0.078 54.170 0.000 4.203 4.169 .x8 (nu.8) 5.542 0.082 67.222 0.000 5.542 5.883 .x9 (nu.9) 5.411 0.086 62.848 0.000 5.411 5.606 visual (a.1.) 0.000 0.000 0.000 textual (a.2.) 0.000 0.000 0.000 speed (a.3.) 0.000 0.000 0.000 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 (t.1_) 0.647 0.141 4.573 0.000 0.647 0.448 .x2 (t.2_) 1.084 0.133 8.131 0.000 1.084 0.798 .x3 (t.3_) 0.588 0.109 5.404 0.000 0.588 0.450 .x4 (t.4_) 0.415 0.066 6.324 0.000 0.415 0.317 .x5 (t.5_) 0.383 0.084 4.549 0.000 0.383 0.244 .x6 (t.6_) 0.368 0.064 5.787 0.000 0.368 0.349 .x7 (t.7_) 0.796 0.098 8.157 0.000 0.796 0.784 .x8 (t.8_) 0.489 0.128 3.830 0.000 0.489 0.551 .x9 (t.9_) 0.588 0.122 4.838 0.000 0.588 0.631 visual (p.1_) 1.000 1.000 1.000 textual (p.2_) 1.000 1.000 1.000 speed (p.3_) 1.000 1.000 1.000 R-Square: Estimate x1 0.552 x2 0.202 x3 0.550 x4 0.683 x5 0.756 x6 0.651 x7 0.216 x8 0.449 x9 0.369 Group 2 [1]: Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual =~ x1 (l.1_) 0.892 0.123 7.263 0.000 0.772 0.692 x2 (l.2_) 0.523 0.099 5.294 0.000 0.453 0.399 x3 (l.3_) 0.848 0.092 9.224 0.000 0.733 0.691 textual =~ x4 (l.4_) 0.945 0.089 10.651 0.000 0.900 0.813 x5 (l.5_) 1.089 0.115 9.473 0.000 1.038 0.859 x6 (l.6_) 0.828 0.084 9.871 0.000 0.789 0.793 speed =~ x7 (l.7_) 0.469 0.095 4.939 0.000 0.594 0.554 x8 (l.8_) 0.631 0.103 6.129 0.000 0.799 0.753 x9 (l.9_) 0.586 0.112 5.251 0.000 0.742 0.695 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual ~~ textul (p.2_) 0.460 0.135 3.412 0.001 0.558 0.558 speed (p.3_1) 0.729 0.223 3.268 0.001 0.665 0.665 textual ~~ speed (p.3_2) 0.465 0.226 2.053 0.040 0.385 0.385 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 (nu.1) 5.023 0.105 47.688 0.000 5.023 4.507 .x2 (nu.2) 6.117 0.090 68.293 0.000 6.117 5.388 .x3 (nu.3) 2.270 0.104 21.872 0.000 2.270 2.140 .x4 (nu.4) 2.796 0.098 28.415 0.000 2.796 2.525 .x5 (nu.5) 4.041 0.115 35.290 0.000 4.041 3.344 .x6 (nu.6) 1.971 0.080 24.742 0.000 1.971 1.980 .x7 (nu.7) 4.203 0.078 54.170 0.000 4.203 3.921 .x8 (nu.8) 5.542 0.082 67.222 0.000 5.542 5.218 .x9 (nu.9) 5.411 0.086 62.848 0.000 5.411 5.070 visual (a.1.) -0.211 0.157 -1.342 0.179 -0.244 -0.244 textual (a.2.) 0.559 0.147 3.809 0.000 0.587 0.587 speed (a.3.) -0.073 0.188 -0.390 0.696 -0.058 -0.058 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .x1 (t.1_) 0.647 0.141 4.573 0.000 0.647 0.521 .x2 (t.2_) 1.084 0.133 8.131 0.000 1.084 0.841 .x3 (t.3_) 0.588 0.109 5.404 0.000 0.588 0.522 .x4 (t.4_) 0.415 0.066 6.324 0.000 0.415 0.339 .x5 (t.5_) 0.383 0.084 4.549 0.000 0.383 0.262 .x6 (t.6_) 0.368 0.064 5.787 0.000 0.368 0.372 .x7 (t.7_) 0.796 0.098 8.157 0.000 0.796 0.693 .x8 (t.8_) 0.489 0.128 3.830 0.000 0.489 0.434 .x9 (t.9_) 0.588 0.122 4.838 0.000 0.588 0.516 visual (p.1_) 0.748 0.185 4.050 0.000 1.000 1.000 textual (p.2_) 0.908 0.226 4.009 0.000 1.000 1.000 speed (p.3_) 1.604 0.491 3.266 0.001 1.000 1.000 R-Square: Estimate x1 0.479 x2 0.159 x3 0.478 x4 0.661 x5 0.738 x6 0.628 x7 0.307 x8 0.566 x9 0.484 Code summary( residualInvarianceModelFullSyntax_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) lavaan 0.6.17 ended normally after 62 iterations Estimator ML Optimization method NLMINB Number of model parameters 66 Number of equality constraints 27 Number of observations per group: 2 132 1 127 Number of missing patterns per group: 2 48 1 52 Model Test User Model: Standard Scaled Test Statistic 102.581 97.522 Degrees of freedom 69 69 P-value (Chi-square) 0.005 0.014 Scaling correction factor 1.052 Yuan-Bentler correction (Mplus variant) Test statistic for each group: 2 58.408 55.528 1 44.173 41.994 Model Test Baseline Model: Test statistic 604.129 571.412 Degrees of freedom 72 72 P-value 0.000 0.000 Scaling correction factor 1.057 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.937 0.943 Tucker-Lewis Index (TLI) 0.934 0.940 Robust Comparative Fit Index (CFI) 0.940 Robust Tucker-Lewis Index (TLI) 0.938 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -2701.788 -2701.788 Scaling correction factor 0.645 for the MLR correction Loglikelihood unrestricted model (H1) -2650.498 -2650.498 Scaling correction factor 1.066 for the MLR correction Akaike (AIC) 5481.576 5481.576 Bayesian (BIC) 5620.293 5620.293 Sample-size adjusted Bayesian (SABIC) 5496.648 5496.648 Root Mean Square Error of Approximation: RMSEA 0.061 0.056 90 Percent confidence interval - lower 0.034 0.028 90 Percent confidence interval - upper 0.085 0.080 P-value H_0: RMSEA &lt;= 0.050 0.221 0.322 P-value H_0: RMSEA &gt;= 0.080 0.103 0.054 Robust RMSEA 0.070 90 Percent confidence interval - lower 0.030 90 Percent confidence interval - upper 0.101 P-value H_0: Robust RMSEA &lt;= 0.050 0.173 P-value H_0: Robust RMSEA &gt;= 0.080 0.320 Standardized Root Mean Square Residual: SRMR 0.082 0.082 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Group 1 [2]: Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual =~ x1 (lmb1) 0.892 0.123 7.263 0.000 0.892 0.743 x2 (lmb2) 0.523 0.099 5.294 0.000 0.523 0.449 x3 (lmb3) 0.848 0.092 9.224 0.000 0.848 0.742 textual =~ x4 (lmb4) 0.945 0.089 10.651 0.000 0.945 0.826 x5 (lmb5) 1.089 0.115 9.473 0.000 1.089 0.870 x6 (lmb6) 0.828 0.084 9.871 0.000 0.828 0.807 speed =~ x7 (lmb7) 0.469 0.095 4.939 0.000 0.469 0.465 x8 (lmb8) 0.631 0.103 6.129 0.000 0.631 0.670 x9 (lmb9) 0.586 0.112 5.251 0.000 0.586 0.607 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual ~~ textual 0.430 0.132 3.262 0.001 0.430 0.430 speed 0.414 0.139 2.975 0.003 0.414 0.414 textual ~~ speed 0.303 0.112 2.712 0.007 0.303 0.303 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual 0.000 0.000 0.000 textual 0.000 0.000 0.000 speed 0.000 0.000 0.000 .x1 (int1) 5.023 0.105 47.689 0.000 5.023 4.182 .x2 (int2) 6.117 0.090 68.293 0.000 6.117 5.249 .x3 (int3) 2.270 0.104 21.872 0.000 2.270 1.986 .x4 (int4) 2.796 0.098 28.415 0.000 2.796 2.445 .x5 (int5) 4.041 0.115 35.290 0.000 4.041 3.225 .x6 (int6) 1.971 0.080 24.742 0.000 1.971 1.920 .x7 (int7) 4.203 0.078 54.170 0.000 4.203 4.169 .x8 (int8) 5.542 0.082 67.222 0.000 5.542 5.883 .x9 (int9) 5.411 0.086 62.848 0.000 5.411 5.606 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual 1.000 1.000 1.000 textual 1.000 1.000 1.000 speed 1.000 1.000 1.000 .x1 (rsd1) 0.647 0.141 4.573 0.000 0.647 0.448 .x2 (rsd2) 1.084 0.133 8.131 0.000 1.084 0.798 .x3 (rsd3) 0.588 0.109 5.404 0.000 0.588 0.450 .x4 (rsd4) 0.415 0.066 6.324 0.000 0.415 0.317 .x5 (rsd5) 0.383 0.084 4.549 0.000 0.383 0.244 .x6 (rsd6) 0.368 0.064 5.787 0.000 0.368 0.349 .x7 (rsd7) 0.796 0.098 8.157 0.000 0.796 0.784 .x8 (rsd8) 0.489 0.128 3.830 0.000 0.489 0.551 .x9 (rsd9) 0.588 0.122 4.838 0.000 0.588 0.631 R-Square: Estimate x1 0.552 x2 0.202 x3 0.550 x4 0.683 x5 0.756 x6 0.651 x7 0.216 x8 0.449 x9 0.369 Group 2 [1]: Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual =~ x1 (lmb1) 0.892 0.123 7.263 0.000 0.772 0.692 x2 (lmb2) 0.523 0.099 5.294 0.000 0.453 0.399 x3 (lmb3) 0.848 0.092 9.224 0.000 0.733 0.691 textual =~ x4 (lmb4) 0.945 0.089 10.651 0.000 0.900 0.813 x5 (lmb5) 1.089 0.115 9.473 0.000 1.038 0.859 x6 (lmb6) 0.828 0.084 9.871 0.000 0.789 0.793 speed =~ x7 (lmb7) 0.469 0.095 4.939 0.000 0.594 0.554 x8 (lmb8) 0.631 0.103 6.129 0.000 0.799 0.753 x9 (lmb9) 0.586 0.112 5.251 0.000 0.742 0.695 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual ~~ textual 0.460 0.135 3.412 0.001 0.558 0.558 speed 0.729 0.223 3.268 0.001 0.665 0.665 textual ~~ speed 0.465 0.226 2.053 0.040 0.385 0.385 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual -0.211 0.157 -1.342 0.179 -0.244 -0.244 textual 0.559 0.147 3.809 0.000 0.587 0.587 speed -0.073 0.188 -0.390 0.696 -0.058 -0.058 .x1 (int1) 5.023 0.105 47.689 0.000 5.023 4.507 .x2 (int2) 6.117 0.090 68.293 0.000 6.117 5.388 .x3 (int3) 2.270 0.104 21.872 0.000 2.270 2.140 .x4 (int4) 2.796 0.098 28.415 0.000 2.796 2.525 .x5 (int5) 4.041 0.115 35.290 0.000 4.041 3.344 .x6 (int6) 1.971 0.080 24.742 0.000 1.971 1.980 .x7 (int7) 4.203 0.078 54.170 0.000 4.203 3.921 .x8 (int8) 5.542 0.082 67.222 0.000 5.542 5.218 .x9 (int9) 5.411 0.086 62.848 0.000 5.411 5.070 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all visual 0.748 0.185 4.050 0.000 1.000 1.000 textual 0.908 0.226 4.009 0.000 1.000 1.000 speed 1.604 0.491 3.266 0.001 1.000 1.000 .x1 (rsd1) 0.647 0.141 4.573 0.000 0.647 0.521 .x2 (rsd2) 1.084 0.133 8.131 0.000 1.084 0.841 .x3 (rsd3) 0.588 0.109 5.404 0.000 0.588 0.522 .x4 (rsd4) 0.415 0.066 6.324 0.000 0.415 0.339 .x5 (rsd5) 0.383 0.084 4.549 0.000 0.383 0.262 .x6 (rsd6) 0.368 0.064 5.787 0.000 0.368 0.372 .x7 (rsd7) 0.796 0.098 8.157 0.000 0.796 0.693 .x8 (rsd8) 0.489 0.128 3.830 0.000 0.489 0.434 .x9 (rsd9) 0.588 0.122 4.838 0.000 0.588 0.516 R-Square: Estimate x1 0.479 x2 0.159 x3 0.478 x4 0.661 x5 0.738 x6 0.628 x7 0.307 x8 0.566 x9 0.484 16.10.7.5 Model Fit You can specify the null model as the baseline model using: baseline.model = nullModelFit Code fitMeasures(residualInvarianceModel_fit) npar fmin 39.000 0.198 chisq df 102.581 69.000 pvalue chisq.scaled 0.005 97.522 df.scaled pvalue.scaled 69.000 0.014 chisq.scaling.factor baseline.chisq 1.052 604.129 baseline.df baseline.pvalue 72.000 0.000 baseline.chisq.scaled baseline.df.scaled 571.412 72.000 baseline.pvalue.scaled baseline.chisq.scaling.factor 0.000 1.057 cfi tli 0.937 0.934 cfi.scaled tli.scaled 0.943 0.940 cfi.robust tli.robust 0.940 0.938 nnfi rfi 0.934 0.823 nfi pnfi 0.830 0.796 ifi rni 0.937 0.937 nnfi.scaled rfi.scaled 0.940 0.822 nfi.scaled pnfi.scaled 0.829 0.795 ifi.scaled rni.scaled 0.943 0.943 nnfi.robust rni.robust 0.938 0.940 logl unrestricted.logl -2701.788 -2650.498 aic bic 5481.576 5620.293 ntotal bic2 259.000 5496.648 scaling.factor.h1 scaling.factor.h0 1.066 0.645 rmsea rmsea.ci.lower 0.061 0.034 rmsea.ci.upper rmsea.ci.level 0.085 0.900 rmsea.pvalue rmsea.close.h0 0.221 0.050 rmsea.notclose.pvalue rmsea.notclose.h0 0.103 0.080 rmsea.scaled rmsea.ci.lower.scaled 0.056 0.028 rmsea.ci.upper.scaled rmsea.pvalue.scaled 0.080 0.322 rmsea.notclose.pvalue.scaled rmsea.robust 0.054 0.070 rmsea.ci.lower.robust rmsea.ci.upper.robust 0.030 0.101 rmsea.pvalue.robust rmsea.notclose.pvalue.robust 0.173 0.320 rmr rmr_nomean 0.098 0.101 srmr srmr_bentler 0.082 0.082 srmr_bentler_nomean crmr 0.082 0.082 crmr_nomean srmr_mplus 0.083 0.100 srmr_mplus_nomean cn_05 0.081 226.698 cn_01 gfi 251.533 0.994 agfi pgfi 0.990 0.635 mfi ecvi 0.937 0.697 Code residualInvarianceModelFitIndices &lt;- fitMeasures( residualInvarianceModel_fit)[c( &quot;cfi.robust&quot;, &quot;rmsea.robust&quot;, &quot;srmr&quot;)] residualInvarianceModel_chisquare &lt;- fitMeasures( residualInvarianceModel_fit)[c(&quot;chisq.scaled&quot;)] residualInvarianceModel_chisquareScaling &lt;- fitMeasures( residualInvarianceModel_fit)[c(&quot;chisq.scaling.factor&quot;)] residualInvarianceModel_df &lt;- fitMeasures( residualInvarianceModel_fit)[c(&quot;df.scaled&quot;)] residualInvarianceModel_N &lt;- lavInspect( residualInvarianceModel_fit, what = &quot;ntotal&quot;) 16.10.7.6 Compare Model Fit 16.10.7.6.1 Nested Model (\\(\\chi^2\\)) Difference Test The scalar invariance model and the residual invariance model are considered “nested” models. The residual invariance model is nested within the scalar invariance model because the scalar invariance model includes all of the terms of the residual invariance model along with additional terms. Model fit of nested models can be compared with a chi-square difference test, also known as a likelihood ratio test or deviance test. A significant chi-square difference test would indicate that the simplified model with additional constraints (the residual invariance model) is significantly worse fitting than the more complex model that has fewer constraints (the scalar invariance model). Code anova(scalarInvarianceModel_fit, residualInvarianceModel_fit) In this instance, you do not need to examine whether residual invariance held because the preceding level of measurement invariance (scalar invariance) did not hold. The petersenlab package (Petersen, 2024b) contains the satorraBentlerScaledChiSquareDifferenceTestStatistic() function that performs a Satorra-Bentler scaled chi-square difference test: Code residualInvarianceModel_chisquareDiff &lt;- satorraBentlerScaledChiSquareDifferenceTestStatistic( T0 = residualInvarianceModel_chisquare, c0 = residualInvarianceModel_chisquareScaling, d0 = residualInvarianceModel_df, T1 = scalarInvarianceModel_chisquare, c1 = scalarInvarianceModel_chisquareScaling, d1 = scalarInvarianceModel_df) residualInvarianceModel_chisquareDiff chisq.scaled 4.148739 16.10.7.6.2 Compare Other Fit Criteria Code round(residualInvarianceModelFitIndices - scalarInvarianceModelFitIndices, digits = 3) cfi.robust rmsea.robust srmr 0.012 -0.012 0.003 16.10.7.6.3 Score-Based Test Score-based tests of measurement invariance are implemented using the strucchange package and are described by T. Wang et al. (2014). Code coef(residualInvarianceModel_fit) lambda.1_1 lambda.2_1 lambda.3_1 lambda.4_2 lambda.5_2 lambda.6_2 lambda.7_3 0.892 0.523 0.848 0.945 1.089 0.828 0.469 lambda.8_3 lambda.9_3 nu.1 nu.2 nu.3 nu.4 nu.5 0.631 0.586 5.023 6.117 2.270 2.796 4.041 nu.6 nu.7 nu.8 nu.9 theta.1_1 theta.2_2 theta.3_3 1.971 4.203 5.542 5.411 0.647 1.084 0.588 theta.4_4 theta.5_5 theta.6_6 theta.7_7 theta.8_8 theta.9_9 psi.2_1.g1 0.415 0.383 0.368 0.796 0.489 0.588 0.430 psi.3_1.g1 psi.3_2.g1 lambda.1_1 lambda.2_1 lambda.3_1 lambda.4_2 lambda.5_2 0.414 0.303 0.892 0.523 0.848 0.945 1.089 lambda.6_2 lambda.7_3 lambda.8_3 lambda.9_3 nu.1 nu.2 nu.3 0.828 0.469 0.631 0.586 5.023 6.117 2.270 nu.4 nu.5 nu.6 nu.7 nu.8 nu.9 theta.1_1 2.796 4.041 1.971 4.203 5.542 5.411 0.647 theta.2_2 theta.3_3 theta.4_4 theta.5_5 theta.6_6 theta.7_7 theta.8_8 1.084 0.588 0.415 0.383 0.368 0.796 0.489 theta.9_9 alpha.1.g2 alpha.2.g2 alpha.3.g2 psi.1_1.g2 psi.2_2.g2 psi.3_3.g2 0.588 -0.211 0.559 -0.073 0.748 0.908 1.604 psi.2_1.g2 psi.3_1.g2 psi.3_2.g2 0.460 0.729 0.465 Code coef(residualInvarianceModel_fit)[19:27] theta.1_1 theta.2_2 theta.3_3 theta.4_4 theta.5_5 theta.6_6 theta.7_7 theta.8_8 0.6466924 1.0835856 0.5875180 0.4152642 0.3828318 0.3681839 0.7964128 0.4892110 theta.9_9 0.5881342 Code sctest( residualInvarianceModel_fit, order.by = HolzingerSwineford1939$school, parm = 19:27, functional = &quot;LMuo&quot;) Error in `[&lt;-`(`*tmp*`, wi, , value = -scores.H1 %*% Delta): subscript out of bounds A score-based test and expected parameter change (EPC) estimates (Oberski, 2014; Oberski et al., 2015) are provided by the lavaan package (Rosseel et al., 2022). Code lavTestScore( residualInvarianceModel_fit, epc = TRUE ) $test total score test: test X2 df p.value 1 score 30.137 27 0.308 $uni univariate score tests: lhs op rhs X2 df p.value 1 .p1. == .p37. 0.084 1 0.773 2 .p2. == .p38. 0.626 1 0.429 3 .p3. == .p39. 0.582 1 0.446 4 .p4. == .p40. 0.116 1 0.734 5 .p5. == .p41. 3.263 1 0.071 6 .p6. == .p42. 2.855 1 0.091 7 .p7. == .p43. 0.083 1 0.774 8 .p8. == .p44. 0.133 1 0.716 9 .p9. == .p45. 0.373 1 0.541 10 .p10. == .p46. 1.251 1 0.263 11 .p11. == .p47. 5.160 1 0.023 12 .p12. == .p48. 6.367 1 0.012 13 .p13. == .p49. 0.426 1 0.514 14 .p14. == .p50. 0.352 1 0.553 15 .p15. == .p51. 0.000 1 0.991 16 .p16. == .p52. 9.164 1 0.002 17 .p17. == .p53. 4.677 1 0.031 18 .p18. == .p54. 0.022 1 0.882 19 .p19. == .p55. 0.082 1 0.774 20 .p20. == .p56. 0.682 1 0.409 21 .p21. == .p57. 0.262 1 0.609 22 .p22. == .p58. 0.722 1 0.396 23 .p23. == .p59. 0.198 1 0.656 24 .p24. == .p60. 1.123 1 0.289 25 .p25. == .p61. 0.362 1 0.548 26 .p26. == .p62. 0.041 1 0.839 27 .p27. == .p63. 0.798 1 0.372 $epc expected parameter changes (epc) and expected parameter values (epv): lhs op rhs block group free label plabel est epc epv 1 visual =~ x1 1 1 1 lambda.1_1 .p1. 0.892 0.090 0.982 2 visual =~ x2 1 1 2 lambda.2_1 .p2. 0.523 0.044 0.567 3 visual =~ x3 1 1 3 lambda.3_1 .p3. 0.848 -0.117 0.731 4 textual =~ x4 1 1 4 lambda.4_2 .p4. 0.945 -0.042 0.903 5 textual =~ x5 1 1 5 lambda.5_2 .p5. 1.089 0.080 1.169 6 textual =~ x6 1 1 6 lambda.6_2 .p6. 0.828 -0.056 0.772 7 speed =~ x7 1 1 7 lambda.7_3 .p7. 0.469 -0.038 0.431 8 speed =~ x8 1 1 8 lambda.8_3 .p8. 0.631 -0.028 0.603 9 speed =~ x9 1 1 9 lambda.9_3 .p9. 0.586 0.022 0.608 10 x1 ~1 1 1 10 nu.1 .p10. 5.023 -0.047 4.976 11 x2 ~1 1 1 11 nu.2 .p11. 6.117 -0.141 5.976 12 x3 ~1 1 1 12 nu.3 .p12. 2.270 0.104 2.374 13 x4 ~1 1 1 13 nu.4 .p13. 2.796 0.023 2.819 14 x5 ~1 1 1 14 nu.5 .p14. 4.041 -0.018 4.023 15 x6 ~1 1 1 15 nu.6 .p15. 1.971 0.000 1.970 16 x7 ~1 1 1 16 nu.7 .p16. 4.203 0.160 4.363 17 x8 ~1 1 1 17 nu.8 .p17. 5.542 -0.078 5.465 18 x9 ~1 1 1 18 nu.9 .p18. 5.411 -0.007 5.404 19 x1 ~~ x1 1 1 19 theta.1_1 .p19. 0.647 -0.154 0.493 20 x2 ~~ x2 1 1 20 theta.2_2 .p20. 1.084 0.070 1.153 21 x3 ~~ x3 1 1 21 theta.3_3 .p21. 0.588 0.173 0.761 22 x4 ~~ x4 1 1 22 theta.4_4 .p22. 0.415 0.080 0.495 23 x5 ~~ x5 1 1 23 theta.5_5 .p23. 0.383 -0.045 0.338 24 x6 ~~ x6 1 1 24 theta.6_6 .p24. 0.368 -0.029 0.339 25 x7 ~~ x7 1 1 25 theta.7_7 .p25. 0.796 0.062 0.858 26 x8 ~~ x8 1 1 26 theta.8_8 .p26. 0.489 0.026 0.515 27 x9 ~~ x9 1 1 27 theta.9_9 .p27. 0.588 0.036 0.624 28 visual ~1 1 1 0 alpha.1.g1 .p28. 0.000 NA NA 29 textual ~1 1 1 0 alpha.2.g1 .p29. 0.000 NA NA 30 speed ~1 1 1 0 alpha.3.g1 .p30. 0.000 NA NA 31 visual ~~ visual 1 1 0 psi.1_1.g1 .p31. 1.000 NA NA 32 textual ~~ textual 1 1 0 psi.2_2.g1 .p32. 1.000 NA NA 33 speed ~~ speed 1 1 0 psi.3_3.g1 .p33. 1.000 NA NA 34 visual ~~ textual 1 1 28 psi.2_1.g1 .p34. 0.430 0.002 0.432 35 visual ~~ speed 1 1 29 psi.3_1.g1 .p35. 0.414 0.011 0.424 36 textual ~~ speed 1 1 30 psi.3_2.g1 .p36. 0.303 0.006 0.309 37 visual =~ x1 2 2 31 lambda.1_1 .p37. 0.892 -0.028 0.864 38 visual =~ x2 2 2 32 lambda.2_1 .p38. 0.523 -0.005 0.518 39 visual =~ x3 2 2 33 lambda.3_1 .p39. 0.848 0.032 0.880 40 textual =~ x4 2 2 34 lambda.4_2 .p40. 0.945 0.070 1.015 41 textual =~ x5 2 2 35 lambda.5_2 .p41. 1.089 -0.115 0.975 42 textual =~ x6 2 2 36 lambda.6_2 .p42. 0.828 0.069 0.897 43 speed =~ x7 2 2 37 lambda.7_3 .p43. 0.469 0.008 0.477 44 speed =~ x8 2 2 38 lambda.8_3 .p44. 0.631 0.016 0.647 45 speed =~ x9 2 2 39 lambda.9_3 .p45. 0.586 -0.006 0.580 46 x1 ~1 2 2 40 nu.1 .p46. 5.023 0.027 5.050 47 x2 ~1 2 2 41 nu.2 .p47. 6.117 0.146 6.262 48 x3 ~1 2 2 42 nu.3 .p48. 2.270 -0.103 2.167 49 x4 ~1 2 2 43 nu.4 .p49. 2.796 -0.063 2.733 50 x5 ~1 2 2 44 nu.5 .p50. 4.041 0.083 4.124 51 x6 ~1 2 2 45 nu.6 .p51. 1.971 -0.037 1.933 52 x7 ~1 2 2 46 nu.7 .p52. 4.203 -0.160 4.043 53 x8 ~1 2 2 47 nu.8 .p53. 5.542 0.085 5.627 54 x9 ~1 2 2 48 nu.9 .p54. 5.411 0.014 5.426 55 x1 ~~ x1 2 2 49 theta.1_1 .p55. 0.647 0.043 0.690 56 x2 ~~ x2 2 2 50 theta.2_2 .p56. 1.084 -0.090 0.993 57 x3 ~~ x3 2 2 51 theta.3_3 .p57. 0.588 -0.058 0.529 58 x4 ~~ x4 2 2 52 theta.4_4 .p58. 0.415 -0.097 0.319 59 x5 ~~ x5 2 2 53 theta.5_5 .p59. 0.383 0.067 0.450 60 x6 ~~ x6 2 2 54 theta.6_6 .p60. 0.368 0.025 0.393 61 x7 ~~ x7 2 2 55 theta.7_7 .p61. 0.796 -0.052 0.745 62 x8 ~~ x8 2 2 56 theta.8_8 .p62. 0.489 -0.020 0.469 63 x9 ~~ x9 2 2 57 theta.9_9 .p63. 0.588 -0.058 0.530 64 visual ~1 2 2 58 alpha.1.g2 .p64. -0.211 0.013 -0.198 65 textual ~1 2 2 59 alpha.2.g2 .p65. 0.559 0.000 0.560 66 speed ~1 2 2 60 alpha.3.g2 .p66. -0.073 -0.013 -0.086 67 visual ~~ visual 2 2 61 psi.1_1.g2 .p67. 0.748 0.007 0.755 68 textual ~~ textual 2 2 62 psi.2_2.g2 .p68. 0.908 0.001 0.909 69 speed ~~ speed 2 2 63 psi.3_3.g2 .p69. 1.604 0.006 1.610 70 visual ~~ textual 2 2 64 psi.2_1.g2 .p70. 0.460 -0.001 0.459 71 visual ~~ speed 2 2 65 psi.3_1.g2 .p71. 0.729 -0.010 0.719 72 textual ~~ speed 2 2 66 psi.3_2.g2 .p72. 0.465 -0.006 0.459 sepc.lv sepc.all sepc.nox 1 0.090 0.075 0.075 2 0.044 0.038 0.038 3 -0.117 -0.102 -0.102 4 -0.042 -0.037 -0.037 5 0.080 0.064 0.064 6 -0.056 -0.055 -0.055 7 -0.038 -0.038 -0.038 8 -0.028 -0.029 -0.029 9 0.022 0.023 0.023 10 -0.047 -0.039 -0.039 11 -0.141 -0.121 -0.121 12 0.104 0.091 0.091 13 0.023 0.020 0.020 14 -0.018 -0.014 -0.014 15 0.000 0.000 0.000 16 0.160 0.159 0.159 17 -0.078 -0.082 -0.082 18 -0.007 -0.007 -0.007 19 -0.647 -0.448 -0.448 20 1.084 0.798 0.798 21 0.588 0.450 0.450 22 0.415 0.317 0.317 23 -0.383 -0.244 -0.244 24 -0.368 -0.349 -0.349 25 0.796 0.784 0.784 26 0.489 0.551 0.551 27 0.588 0.631 0.631 28 NA NA NA 29 NA NA NA 30 NA NA NA 31 NA NA NA 32 NA NA NA 33 NA NA NA 34 0.002 0.002 0.002 35 0.011 0.011 0.011 36 0.006 0.006 0.006 37 -0.024 -0.022 -0.022 38 -0.004 -0.004 -0.004 39 0.028 0.026 0.026 40 0.067 0.061 0.061 41 -0.109 -0.090 -0.090 42 0.066 0.066 0.066 43 0.010 0.010 0.010 44 0.021 0.020 0.020 45 -0.007 -0.007 -0.007 46 0.027 0.025 0.025 47 0.146 0.128 0.128 48 -0.103 -0.097 -0.097 49 -0.063 -0.057 -0.057 50 0.083 0.069 0.069 51 -0.037 -0.038 -0.038 52 -0.160 -0.150 -0.150 53 0.085 0.080 0.080 54 0.014 0.013 0.013 55 0.647 0.521 0.521 56 -1.084 -0.841 -0.841 57 -0.588 -0.522 -0.522 58 -0.415 -0.339 -0.339 59 0.383 0.262 0.262 60 0.368 0.372 0.372 61 -0.796 -0.693 -0.693 62 -0.489 -0.434 -0.434 63 -0.588 -0.516 -0.516 64 0.015 0.015 0.015 65 0.000 0.000 0.000 66 -0.010 -0.010 -0.010 67 1.000 1.000 1.000 68 1.000 1.000 1.000 69 1.000 1.000 1.000 70 -0.001 -0.001 -0.001 71 -0.009 -0.009 -0.009 72 -0.005 -0.005 -0.005 16.10.7.6.4 Equivalence Test The petersenlab package (Petersen, 2024b) contains the equiv_chi() function from Counsell et al. (2020) that performs an equivalence test: https://osf.io/cqu8v. The chi-square equivalence test is non-significant, suggesting that the model fit is not acceptable. The equivalence test of the chi-square difference test is non-significant, suggesting that the degree of worsening of model fit is not acceptable. Code equiv_chi( alpha = .05, chi = residualInvarianceModel_chisquare, df = residualInvarianceModel_df, m = 2, N_sample = residualInvarianceModel_N, popRMSEA = .08) Moreover, the equivalence test of the chi-square difference test is non-significant, suggesting that the degree of worsening of model fit is not acceptable. In other words, residual invariance failed. Code residualInvarianceModel_dfDiff &lt;- residualInvarianceModel_df - scalarInvarianceModel_df equiv_chi( alpha = .05, chi = residualInvarianceModel_chisquareDiff, df = residualInvarianceModel_dfDiff, m = 2, N_sample = residualInvarianceModel_N, popRMSEA = .08) 16.10.7.6.5 Permutation Test Permutation procedures for testing measurement invariance are described in (Jorgensen et al., 2018). For reproducibility, I set the seed below. Using the same seed will yield the same answer every time. There is nothing special about this particular seed. You can specify the null model as the baseline model using: baseline.model = nullModelFit. Warning: this code takes a while to run based on \\(100\\) iterations. You can reduce the number of iterations to be faster. Code set.seed(52242) residualInvarianceTest &lt;- permuteMeasEq( nPermute = numPermutations, modelType = &quot;mgcfa&quot;, con = residualInvarianceModel_fit, uncon = scalarInvarianceModel_fit, AFIs = myAFIs, moreAFIs = moreAFIs, parallelType = &quot;multicore&quot;, #only &#39;snow&#39; works on Windows, but right now, it is throwing an error iseed = 52242) Code RNGkind(&quot;default&quot;, &quot;default&quot;, &quot;default&quot;) set.seed(52242) residualInvarianceTest Omnibus p value based on parametric chi-squared difference test: Chisq diff Df diff Pr(&gt;Chisq) 4.149 9.000 0.901 Omnibus p values based on nonparametric permutation method: AFI.Difference p.value chisq 4.797 0.94 chisq.scaled 3.156 0.95 rmsea -0.008 0.93 cfi 0.008 0.94 tli 0.019 0.96 srmr 0.003 0.79 rmsea.robust -0.012 0.97 cfi.robust 0.012 0.97 tli.robust 0.024 1.00 The p-values are non-significant, indicating that the model did not fit significantly worse than the scalar invariance model. 16.10.7.7 Internal Consistency Reliability Internal consistency reliability of items composing the latent factors, as quantified by omega (\\(\\omega\\)) and average variance extracted (AVE), was estimated using the semTools package (Jorgensen et al., 2021). Code compRelSEM(residualInvarianceModel_fit) Code AVE(residualInvarianceModel_fit) 16.10.7.8 Path Diagram A path diagram of the model generated using the semPlot package (Epskamp, 2022) is below. Code semPaths( residualInvarianceModel_fit, what = &quot;est&quot;, layout = &quot;tree2&quot;, ask = FALSE, edge.label.cex = 1.2) Figure 16.59: Residual Invariance Model in Confirmatory Factor Analysis. Figure 16.60: Residual Invariance Model in Confirmatory Factor Analysis. 16.10.8 Addressing Measurement Non-Invariance In the example above, we detected measurement non-invariance of intercepts, but not factor loadings. Thus, we would want to identify which item(s) show non-invariant intercepts across groups. When detecting measurement non-invariance in a given parameter (e.g., factor loadings, intercepts, or residuals), one can identify the specific items that show measurement non-invariance in one of two primary ways: (1) starting with a model that allows the given parameter to differ across groups, a researcher can iteratively add constraints to identify the item(s) for which measurement invariance fails, or (2) starting with a model that constrains the given parameter to be the same across groups, a researcher can iteratively remove constraints to identify the item(s) for which measurement invariance becomes established (and by process of elimination, the items for which measurement invariance does not become established). 16.10.8.1 Iteratively add constraints to identify measurement non-invariance In the example above, we detected measurement non-invariance of intercepts. One approach to identify measurement non-invariance is to iteratively add constraints to a model that allows the given parameter (intercepts) to differ across groups. So, we will use the metric invariance model as the baseline model (in which item intercepts are allowed to differ across groups), and iteratively add constraints to identify which item(s) show non-invariant intercepts across groups. 16.10.8.1.1 Variable 1 Variable 1 does not have non-invariant intercepts. Code cfaModel_metricInvarianceV1 &lt;- &#39; #Fix factor loadings to be the same across groups visual =~ c(lambdax1,lambdax1)*x1 + c(lambdax2,lambdax2)*x2 + c(lambdax3,lambdax3)*x3 textual =~ c(lambdax4,lambdax4)*x4 + c(lambdax5,lambdax5)*x5 + c(lambdax6,lambdax6)*x6 speed =~ c(lambdax7,lambdax7)*x7 + c(lambdax8,lambdax8)*x8 + c(lambdax9,lambdax9)*x9 #Fix latent means to zero visual ~ 0 textual ~ 0 speed ~ 0 #Fix latent variances to one in group 1; free latent variances in group 2 visual ~~ c(1, NA)*visual textual ~~ c(1, NA)*textual speed ~~ c(1, NA)*speed #Estimate covariances among latent variables visual ~~ textual visual ~~ speed textual ~~ speed #Estimate residual variances of manifest variables x1 ~~ x1 x2 ~~ x2 x3 ~~ x3 x4 ~~ x4 x5 ~~ x5 x6 ~~ x6 x7 ~~ x7 x8 ~~ x8 x9 ~~ x9 #Iteratively fix intercepts of manifest variables across groups x1 ~ c(intx1, intx1)*1 x2 ~ NA*1 x3 ~ NA*1 x4 ~ NA*1 x5 ~ NA*1 x6 ~ NA*1 x7 ~ NA*1 x8 ~ NA*1 x9 ~ NA*1 &#39; cfaModel_metricInvarianceV1_fit &lt;- lavaan( cfaModel_metricInvarianceV1, data = HolzingerSwineford1939, group = &quot;school&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) anova(metricInvarianceModel_fit, cfaModel_metricInvarianceV1_fit) 16.10.8.1.2 Variable 2 Variable 2 has non-invariant intercepts. Code cfaModel_metricInvarianceV2 &lt;- &#39; #Fix factor loadings to be the same across groups visual =~ c(lambdax1,lambdax1)*x1 + c(lambdax2,lambdax2)*x2 + c(lambdax3,lambdax3)*x3 textual =~ c(lambdax4,lambdax4)*x4 + c(lambdax5,lambdax5)*x5 + c(lambdax6,lambdax6)*x6 speed =~ c(lambdax7,lambdax7)*x7 + c(lambdax8,lambdax8)*x8 + c(lambdax9,lambdax9)*x9 #Fix latent means to zero visual ~ 0 textual ~ 0 speed ~ 0 #Fix latent variances to one in group 1; free latent variances in group 2 visual ~~ c(1, NA)*visual textual ~~ c(1, NA)*textual speed ~~ c(1, NA)*speed #Estimate covariances among latent variables visual ~~ textual visual ~~ speed textual ~~ speed #Estimate residual variances of manifest variables x1 ~~ x1 x2 ~~ x2 x3 ~~ x3 x4 ~~ x4 x5 ~~ x5 x6 ~~ x6 x7 ~~ x7 x8 ~~ x8 x9 ~~ x9 #Iteratively fix intercepts of manifest variables across groups x1 ~ c(intx1, intx1)*1 x2 ~ c(intx1, intx1)*1 x3 ~ NA*1 x4 ~ NA*1 x5 ~ NA*1 x6 ~ NA*1 x7 ~ NA*1 x8 ~ NA*1 x9 ~ NA*1 &#39; cfaModel_metricInvarianceV2_fit &lt;- lavaan( cfaModel_metricInvarianceV2, data = HolzingerSwineford1939, group = &quot;school&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) anova(metricInvarianceModel_fit, cfaModel_metricInvarianceV2_fit) Variable 2 appears to have larger intercepts in group 2 than in group 1: Code summary(metricInvarianceModel_fit) lavaan 0.6.17 ended normally after 67 iterations Estimator ML Optimization method NLMINB Number of model parameters 63 Number of equality constraints 9 Number of observations per group: 2 132 1 127 Number of missing patterns per group: 2 48 1 52 Model Test User Model: Standard Scaled Test Statistic 78.011 75.195 Degrees of freedom 54 54 P-value (Chi-square) 0.018 0.030 Scaling correction factor 1.037 Yuan-Bentler correction (Mplus variant) Test statistic for each group: 2 47.087 45.387 1 30.924 29.808 Parameter Estimates: Standard errors Sandwich Information bread Observed Observed information based on Hessian Group 1 [2]: Latent Variables: Estimate Std.Err z-value P(&gt;|z|) visual =~ x1 (l.1_) 0.913 0.130 7.016 0.000 x2 (l.2_) 0.539 0.097 5.557 0.000 x3 (l.3_) 0.819 0.088 9.333 0.000 textual =~ x4 (l.4_) 0.950 0.090 10.553 0.000 x5 (l.5_) 1.069 0.117 9.122 0.000 x6 (l.6_) 0.824 0.092 8.926 0.000 speed =~ x7 (l.7_) 0.471 0.085 5.523 0.000 x8 (l.8_) 0.636 0.108 5.897 0.000 x9 (l.9_) 0.557 0.112 4.971 0.000 Covariances: Estimate Std.Err z-value P(&gt;|z|) visual ~~ textul (p.2_) 0.455 0.127 3.578 0.000 speed (p.3_1) 0.396 0.139 2.837 0.005 textual ~~ speed (p.3_2) 0.318 0.115 2.773 0.006 Intercepts: Estimate Std.Err z-value P(&gt;|z|) .x1 (n.1.) 4.974 0.113 44.143 0.000 .x2 (n.2.) 5.977 0.110 54.446 0.000 .x3 (n.3.) 2.375 0.106 22.343 0.000 .x4 (n.4.) 2.817 0.105 26.778 0.000 .x5 (n.5.) 4.022 0.119 33.914 0.000 .x6 (n.6.) 1.971 0.087 22.590 0.000 .x7 (n.7.) 4.362 0.093 46.812 0.000 .x8 (n.8.) 5.464 0.088 61.938 0.000 .x9 (n.9.) 5.402 0.094 57.768 0.000 visual (a.1.) 0.000 textual (a.2.) 0.000 speed (a.3.) 0.000 Variances: Estimate Std.Err z-value P(&gt;|z|) .x1 (t.1_) 0.576 0.179 3.221 0.001 .x2 (t.2_) 1.147 0.185 6.212 0.000 .x3 (t.3_) 0.646 0.134 4.811 0.000 .x4 (t.4_) 0.470 0.090 5.216 0.000 .x5 (t.5_) 0.431 0.109 3.941 0.000 .x6 (t.6_) 0.302 0.075 4.033 0.000 .x7 (t.7_) 0.806 0.122 6.590 0.000 .x8 (t.8_) 0.473 0.120 3.928 0.000 .x9 (t.9_) 0.670 0.149 4.495 0.000 visual (p.1_) 1.000 textual (p.2_) 1.000 speed (p.3_) 1.000 Group 2 [1]: Latent Variables: Estimate Std.Err z-value P(&gt;|z|) visual =~ x1 (l.1_) 0.913 0.130 7.016 0.000 x2 (l.2_) 0.539 0.097 5.557 0.000 x3 (l.3_) 0.819 0.088 9.333 0.000 textual =~ x4 (l.4_) 0.950 0.090 10.553 0.000 x5 (l.5_) 1.069 0.117 9.122 0.000 x6 (l.6_) 0.824 0.092 8.926 0.000 speed =~ x7 (l.7_) 0.471 0.085 5.523 0.000 x8 (l.8_) 0.636 0.108 5.897 0.000 x9 (l.9_) 0.557 0.112 4.971 0.000 Covariances: Estimate Std.Err z-value P(&gt;|z|) visual ~~ textul (p.2_) 0.462 0.137 3.379 0.001 speed (p.3_1) 0.732 0.231 3.177 0.001 textual ~~ speed (p.3_2) 0.473 0.241 1.966 0.049 Intercepts: Estimate Std.Err z-value P(&gt;|z|) .x1 (n.1.) 4.881 0.105 46.577 0.000 .x2 (n.2.) 6.159 0.103 59.876 0.000 .x3 (n.3.) 1.993 0.098 20.368 0.000 .x4 (n.4.) 3.299 0.103 31.949 0.000 .x5 (n.5.) 4.674 0.105 44.381 0.000 .x6 (n.6.) 2.436 0.101 24.159 0.000 .x7 (n.7.) 4.002 0.097 41.140 0.000 .x8 (n.8.) 5.571 0.099 56.335 0.000 .x9 (n.9.) 5.375 0.098 54.583 0.000 visual (a.1.) 0.000 textual (a.2.) 0.000 speed (a.3.) 0.000 Variances: Estimate Std.Err z-value P(&gt;|z|) .x1 (t.1_) 0.653 0.167 3.900 0.000 .x2 (t.2_) 0.958 0.186 5.165 0.000 .x3 (t.3_) 0.548 0.117 4.672 0.000 .x4 (t.4_) 0.360 0.089 4.046 0.000 .x5 (t.5_) 0.354 0.096 3.679 0.000 .x6 (t.6_) 0.427 0.103 4.149 0.000 .x7 (t.7_) 0.695 0.120 5.805 0.000 .x8 (t.8_) 0.439 0.204 2.149 0.032 .x9 (t.9_) 0.547 0.148 3.697 0.000 visual (p.1_) 0.770 0.210 3.663 0.000 textual (p.2_) 0.926 0.228 4.058 0.000 speed (p.3_) 1.724 0.512 3.364 0.001 Thus, in subsequent models, we would allow variable 2 to have different intercepts across groups. 16.10.8.1.3 Variable 3 Variable 3 has non-invariant intercepts. Code cfaModel_metricInvarianceV3 &lt;- &#39; #Fix factor loadings to be the same across groups visual =~ c(lambdax1,lambdax1)*x1 + c(lambdax2,lambdax2)*x2 + c(lambdax3,lambdax3)*x3 textual =~ c(lambdax4,lambdax4)*x4 + c(lambdax5,lambdax5)*x5 + c(lambdax6,lambdax6)*x6 speed =~ c(lambdax7,lambdax7)*x7 + c(lambdax8,lambdax8)*x8 + c(lambdax9,lambdax9)*x9 #Fix latent means to zero visual ~ 0 textual ~ 0 speed ~ 0 #Fix latent variances to one in group 1; free latent variances in group 2 visual ~~ c(1, NA)*visual textual ~~ c(1, NA)*textual speed ~~ c(1, NA)*speed #Estimate covariances among latent variables visual ~~ textual visual ~~ speed textual ~~ speed #Estimate residual variances of manifest variables x1 ~~ x1 x2 ~~ x2 x3 ~~ x3 x4 ~~ x4 x5 ~~ x5 x6 ~~ x6 x7 ~~ x7 x8 ~~ x8 x9 ~~ x9 #Iteratively fix intercepts of manifest variables across groups x1 ~ c(intx1, intx1)*1 x2 ~ NA*1 x3 ~ c(intx1, intx1)*1 x4 ~ NA*1 x5 ~ NA*1 x6 ~ NA*1 x7 ~ NA*1 x8 ~ NA*1 x9 ~ NA*1 &#39; cfaModel_metricInvarianceV3_fit &lt;- lavaan( cfaModel_metricInvarianceV3, data = HolzingerSwineford1939, group = &quot;school&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) anova(metricInvarianceModel_fit, cfaModel_metricInvarianceV3_fit) 16.10.8.1.4 Variable 4 Variable 4 has non-invariant intercepts. Code cfaModel_metricInvarianceV4 &lt;- &#39; #Fix factor loadings to be the same across groups visual =~ c(lambdax1,lambdax1)*x1 + c(lambdax2,lambdax2)*x2 + c(lambdax3,lambdax3)*x3 textual =~ c(lambdax4,lambdax4)*x4 + c(lambdax5,lambdax5)*x5 + c(lambdax6,lambdax6)*x6 speed =~ c(lambdax7,lambdax7)*x7 + c(lambdax8,lambdax8)*x8 + c(lambdax9,lambdax9)*x9 #Fix latent means to zero visual ~ 0 textual ~ 0 speed ~ 0 #Fix latent variances to one in group 1; free latent variances in group 2 visual ~~ c(1, NA)*visual textual ~~ c(1, NA)*textual speed ~~ c(1, NA)*speed #Estimate covariances among latent variables visual ~~ textual visual ~~ speed textual ~~ speed #Estimate residual variances of manifest variables x1 ~~ x1 x2 ~~ x2 x3 ~~ x3 x4 ~~ x4 x5 ~~ x5 x6 ~~ x6 x7 ~~ x7 x8 ~~ x8 x9 ~~ x9 #Iteratively fix intercepts of manifest variables across groups x1 ~ c(intx1, intx1)*1 x2 ~ NA*1 x3 ~ NA*1 x4 ~ c(intx1, intx1)*1 x5 ~ NA*1 x6 ~ NA*1 x7 ~ NA*1 x8 ~ NA*1 x9 ~ NA*1 &#39; cfaModel_metricInvarianceV4_fit &lt;- lavaan( cfaModel_metricInvarianceV4, data = HolzingerSwineford1939, group = &quot;school&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) anova(metricInvarianceModel_fit, cfaModel_metricInvarianceV4_fit) 16.10.8.1.5 Variable 5 Variable 5 has non-invariant intercepts. Code cfaModel_metricInvarianceV5 &lt;- &#39; #Fix factor loadings to be the same across groups visual =~ c(lambdax1,lambdax1)*x1 + c(lambdax2,lambdax2)*x2 + c(lambdax3,lambdax3)*x3 textual =~ c(lambdax4,lambdax4)*x4 + c(lambdax5,lambdax5)*x5 + c(lambdax6,lambdax6)*x6 speed =~ c(lambdax7,lambdax7)*x7 + c(lambdax8,lambdax8)*x8 + c(lambdax9,lambdax9)*x9 #Fix latent means to zero visual ~ 0 textual ~ 0 speed ~ 0 #Fix latent variances to one in group 1; free latent variances in group 2 visual ~~ c(1, NA)*visual textual ~~ c(1, NA)*textual speed ~~ c(1, NA)*speed #Estimate covariances among latent variables visual ~~ textual visual ~~ speed textual ~~ speed #Estimate residual variances of manifest variables x1 ~~ x1 x2 ~~ x2 x3 ~~ x3 x4 ~~ x4 x5 ~~ x5 x6 ~~ x6 x7 ~~ x7 x8 ~~ x8 x9 ~~ x9 #Iteratively fix intercepts of manifest variables across groups x1 ~ c(intx1, intx1)*1 x2 ~ NA*1 x3 ~ NA*1 x4 ~ NA*1 x5 ~ c(intx1, intx1)*1 x6 ~ NA*1 x7 ~ NA*1 x8 ~ NA*1 x9 ~ NA*1 &#39; cfaModel_metricInvarianceV5_fit &lt;- lavaan( cfaModel_metricInvarianceV5, data = HolzingerSwineford1939, group = &quot;school&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) anova(metricInvarianceModel_fit, cfaModel_metricInvarianceV5_fit) 16.10.8.1.6 Variable 6 Variable 6 has non-invariant intercepts. Code cfaModel_metricInvarianceV6 &lt;- &#39; #Fix factor loadings to be the same across groups visual =~ c(lambdax1,lambdax1)*x1 + c(lambdax2,lambdax2)*x2 + c(lambdax3,lambdax3)*x3 textual =~ c(lambdax4,lambdax4)*x4 + c(lambdax5,lambdax5)*x5 + c(lambdax6,lambdax6)*x6 speed =~ c(lambdax7,lambdax7)*x7 + c(lambdax8,lambdax8)*x8 + c(lambdax9,lambdax9)*x9 #Fix latent means to zero visual ~ 0 textual ~ 0 speed ~ 0 #Fix latent variances to one in group 1; free latent variances in group 2 visual ~~ c(1, NA)*visual textual ~~ c(1, NA)*textual speed ~~ c(1, NA)*speed #Estimate covariances among latent variables visual ~~ textual visual ~~ speed textual ~~ speed #Estimate residual variances of manifest variables x1 ~~ x1 x2 ~~ x2 x3 ~~ x3 x4 ~~ x4 x5 ~~ x5 x6 ~~ x6 x7 ~~ x7 x8 ~~ x8 x9 ~~ x9 #Iteratively fix intercepts of manifest variables across groups x1 ~ c(intx1, intx1)*1 x2 ~ NA*1 x3 ~ NA*1 x4 ~ NA*1 x5 ~ NA*1 x6 ~ c(intx1, intx1)*1 x7 ~ NA*1 x8 ~ NA*1 x9 ~ NA*1 &#39; cfaModel_metricInvarianceV6_fit &lt;- lavaan( cfaModel_metricInvarianceV6, data = HolzingerSwineford1939, group = &quot;school&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) anova(metricInvarianceModel_fit, cfaModel_metricInvarianceV6_fit) 16.10.8.1.7 Variable 7 Variable 7 has non-invariant intercepts. Code cfaModel_metricInvarianceV7 &lt;- &#39; #Fix factor loadings to be the same across groups visual =~ c(lambdax1,lambdax1)*x1 + c(lambdax2,lambdax2)*x2 + c(lambdax3,lambdax3)*x3 textual =~ c(lambdax4,lambdax4)*x4 + c(lambdax5,lambdax5)*x5 + c(lambdax6,lambdax6)*x6 speed =~ c(lambdax7,lambdax7)*x7 + c(lambdax8,lambdax8)*x8 + c(lambdax9,lambdax9)*x9 #Fix latent means to zero visual ~ 0 textual ~ 0 speed ~ 0 #Fix latent variances to one in group 1; free latent variances in group 2 visual ~~ c(1, NA)*visual textual ~~ c(1, NA)*textual speed ~~ c(1, NA)*speed #Estimate covariances among latent variables visual ~~ textual visual ~~ speed textual ~~ speed #Estimate residual variances of manifest variables x1 ~~ x1 x2 ~~ x2 x3 ~~ x3 x4 ~~ x4 x5 ~~ x5 x6 ~~ x6 x7 ~~ x7 x8 ~~ x8 x9 ~~ x9 #Iteratively fix intercepts of manifest variables across groups x1 ~ c(intx1, intx1)*1 x2 ~ NA*1 x3 ~ NA*1 x4 ~ NA*1 x5 ~ NA*1 x6 ~ NA*1 x7 ~ c(intx1, intx1)*1 x8 ~ NA*1 x9 ~ NA*1 &#39; cfaModel_metricInvarianceV7_fit &lt;- lavaan( cfaModel_metricInvarianceV7, data = HolzingerSwineford1939, group = &quot;school&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) anova(metricInvarianceModel_fit, cfaModel_metricInvarianceV7_fit) 16.10.8.1.8 Variable 8 Variable 8 has non-invariant intercepts. Code cfaModel_metricInvarianceV8 &lt;- &#39; #Fix factor loadings to be the same across groups visual =~ c(lambdax1,lambdax1)*x1 + c(lambdax2,lambdax2)*x2 + c(lambdax3,lambdax3)*x3 textual =~ c(lambdax4,lambdax4)*x4 + c(lambdax5,lambdax5)*x5 + c(lambdax6,lambdax6)*x6 speed =~ c(lambdax7,lambdax7)*x7 + c(lambdax8,lambdax8)*x8 + c(lambdax9,lambdax9)*x9 #Fix latent means to zero visual ~ 0 textual ~ 0 speed ~ 0 #Fix latent variances to one in group 1; free latent variances in group 2 visual ~~ c(1, NA)*visual textual ~~ c(1, NA)*textual speed ~~ c(1, NA)*speed #Estimate covariances among latent variables visual ~~ textual visual ~~ speed textual ~~ speed #Estimate residual variances of manifest variables x1 ~~ x1 x2 ~~ x2 x3 ~~ x3 x4 ~~ x4 x5 ~~ x5 x6 ~~ x6 x7 ~~ x7 x8 ~~ x8 x9 ~~ x9 #Iteratively fix intercepts of manifest variables across groups x1 ~ c(intx1, intx1)*1 x2 ~ NA*1 x3 ~ NA*1 x4 ~ NA*1 x5 ~ NA*1 x6 ~ NA*1 x7 ~ NA*1 x8 ~ c(intx1, intx1)*1 x9 ~ NA*1 &#39; cfaModel_metricInvarianceV8_fit &lt;- lavaan( cfaModel_metricInvarianceV8, data = HolzingerSwineford1939, group = &quot;school&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) anova(metricInvarianceModel_fit, cfaModel_metricInvarianceV8_fit) 16.10.8.1.9 Variable 9 Variable 9 has non-invariant intercepts. Code cfaModel_metricInvarianceV9 &lt;- &#39; #Fix factor loadings to be the same across groups visual =~ c(lambdax1,lambdax1)*x1 + c(lambdax2,lambdax2)*x2 + c(lambdax3,lambdax3)*x3 textual =~ c(lambdax4,lambdax4)*x4 + c(lambdax5,lambdax5)*x5 + c(lambdax6,lambdax6)*x6 speed =~ c(lambdax7,lambdax7)*x7 + c(lambdax8,lambdax8)*x8 + c(lambdax9,lambdax9)*x9 #Fix latent means to zero visual ~ 0 textual ~ 0 speed ~ 0 #Fix latent variances to one in group 1; free latent variances in group 2 visual ~~ c(1, NA)*visual textual ~~ c(1, NA)*textual speed ~~ c(1, NA)*speed #Estimate covariances among latent variables visual ~~ textual visual ~~ speed textual ~~ speed #Estimate residual variances of manifest variables x1 ~~ x1 x2 ~~ x2 x3 ~~ x3 x4 ~~ x4 x5 ~~ x5 x6 ~~ x6 x7 ~~ x7 x8 ~~ x8 x9 ~~ x9 #Iteratively fix intercepts of manifest variables across groups x1 ~ c(intx1, intx1)*1 x2 ~ NA*1 x3 ~ NA*1 x4 ~ NA*1 x5 ~ NA*1 x6 ~ NA*1 x7 ~ NA*1 x8 ~ NA*1 x9 ~ c(intx1, intx1)*1 &#39; cfaModel_metricInvarianceV9_fit &lt;- lavaan( cfaModel_metricInvarianceV9, data = HolzingerSwineford1939, group = &quot;school&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) anova(metricInvarianceModel_fit, cfaModel_metricInvarianceV9_fit) 16.10.8.1.10 Summary When iteratively adding constraints, item 1 showed measurement invariance of intercepts, but all other items showed measurement non-invariance of intercepts. This could be due to the selection of starting with item 1 as the first constraint. We could try applying constraints in a different order to see the extent to which the order influences which items are identified as showing measurement non-invariance. 16.10.8.2 Iteratively drop constraints to identify measurement non-invariance Another approach to identify measurement non-invariance is to iteratively drop constraints to a model that constrains the given parameter (intercepts) to be the same across groups. So, we will use the scalar invariance model as the baseline model (in which item intercepts are constrained to be the same across groups), and we will iteratively drop constraints to identify the items for which measurement invariance becomes established (relative to the metric invariance model which allows item intercepts to differ across groups), and therefore, identifies which item(s) show non-invariant intercepts across groups. 16.10.8.2.1 Variable 1 Measurement invariance does not yet become established when freeing intercepts of variable 1 across groups. This suggests that, whether or not variable 1 shows non-invariant intercepts, there are other items that show non-invariant intercepts. Code cfaModel_scalarInvarianceV1 &lt;- &#39; #Fix factor loadings to be the same across groups visual =~ c(lambdax1,lambdax1)*x1 + c(lambdax2,lambdax2)*x2 + c(lambdax3,lambdax3)*x3 textual =~ c(lambdax4,lambdax4)*x4 + c(lambdax5,lambdax5)*x5 + c(lambdax6,lambdax6)*x6 speed =~ c(lambdax7,lambdax7)*x7 + c(lambdax8,lambdax8)*x8 + c(lambdax9,lambdax9)*x9 #Fix latent means to zero visual ~ 0 textual ~ 0 speed ~ 0 #Fix latent variances to one in group 1; free latent variances in group 2 visual ~~ c(1, NA)*visual textual ~~ c(1, NA)*textual speed ~~ c(1, NA)*speed #Estimate covariances among latent variables visual ~~ textual visual ~~ speed textual ~~ speed #Estimate residual variances of manifest variables x1 ~~ x1 x2 ~~ x2 x3 ~~ x3 x4 ~~ x4 x5 ~~ x5 x6 ~~ x6 x7 ~~ x7 x8 ~~ x8 x9 ~~ x9 #Iteratively free intercepts across groups x1 ~ NA*1 x2 ~ c(intx2, intx2)*1 x3 ~ c(intx3, intx3)*1 x4 ~ c(intx4, intx4)*1 x5 ~ c(intx5, intx5)*1 x6 ~ c(intx6, intx6)*1 x7 ~ c(intx7, intx7)*1 x8 ~ c(intx8, intx8)*1 x9 ~ c(intx9, intx9)*1 &#39; cfaModel_scalarInvarianceV1_fit &lt;- lavaan( cfaModel_scalarInvarianceV1, data = HolzingerSwineford1939, group = &quot;school&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) anova(metricInvarianceModel_fit, cfaModel_scalarInvarianceV1_fit) 16.10.8.2.2 Variable 2 Measurement invariance does not yet become established when freeing intercepts of variable 2 across groups. This suggests that, whether or not variables 1 and 2 show non-invariant intercepts, there are other items that show non-invariant intercepts. Code cfaModel_scalarInvarianceV2 &lt;- &#39; #Fix factor loadings to be the same across groups visual =~ c(lambdax1,lambdax1)*x1 + c(lambdax2,lambdax2)*x2 + c(lambdax3,lambdax3)*x3 textual =~ c(lambdax4,lambdax4)*x4 + c(lambdax5,lambdax5)*x5 + c(lambdax6,lambdax6)*x6 speed =~ c(lambdax7,lambdax7)*x7 + c(lambdax8,lambdax8)*x8 + c(lambdax9,lambdax9)*x9 #Fix latent means to zero visual ~ 0 textual ~ 0 speed ~ 0 #Fix latent variances to one in group 1; free latent variances in group 2 visual ~~ c(1, NA)*visual textual ~~ c(1, NA)*textual speed ~~ c(1, NA)*speed #Estimate covariances among latent variables visual ~~ textual visual ~~ speed textual ~~ speed #Estimate residual variances of manifest variables x1 ~~ x1 x2 ~~ x2 x3 ~~ x3 x4 ~~ x4 x5 ~~ x5 x6 ~~ x6 x7 ~~ x7 x8 ~~ x8 x9 ~~ x9 #Iteratively free intercepts across groups x1 ~ NA*1 x2 ~ NA*1 x3 ~ c(intx3, intx3)*1 x4 ~ c(intx4, intx4)*1 x5 ~ c(intx5, intx5)*1 x6 ~ c(intx6, intx6)*1 x7 ~ c(intx7, intx7)*1 x8 ~ c(intx8, intx8)*1 x9 ~ c(intx9, intx9)*1 &#39; cfaModel_scalarInvarianceV2_fit &lt;- lavaan( cfaModel_scalarInvarianceV2, data = HolzingerSwineford1939, group = &quot;school&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) anova(metricInvarianceModel_fit, cfaModel_scalarInvarianceV2_fit) 16.10.8.2.3 Variable 3 Measurement invariance does not yet become established when freeing intercepts of variable 3 across groups. This suggests that, whether or not variables 1, 2, and 3 show non-invariant intercepts, there are other items that show non-invariant intercepts. Code cfaModel_scalarInvarianceV3 &lt;- &#39; #Fix factor loadings to be the same across groups visual =~ c(lambdax1,lambdax1)*x1 + c(lambdax2,lambdax2)*x2 + c(lambdax3,lambdax3)*x3 textual =~ c(lambdax4,lambdax4)*x4 + c(lambdax5,lambdax5)*x5 + c(lambdax6,lambdax6)*x6 speed =~ c(lambdax7,lambdax7)*x7 + c(lambdax8,lambdax8)*x8 + c(lambdax9,lambdax9)*x9 #Fix latent means to zero visual ~ 0 textual ~ 0 speed ~ 0 #Fix latent variances to one in group 1; free latent variances in group 2 visual ~~ c(1, NA)*visual textual ~~ c(1, NA)*textual speed ~~ c(1, NA)*speed #Estimate covariances among latent variables visual ~~ textual visual ~~ speed textual ~~ speed #Estimate residual variances of manifest variables x1 ~~ x1 x2 ~~ x2 x3 ~~ x3 x4 ~~ x4 x5 ~~ x5 x6 ~~ x6 x7 ~~ x7 x8 ~~ x8 x9 ~~ x9 #Iteratively free intercepts across groups x1 ~ NA*1 x2 ~ NA*1 x3 ~ NA*1 x4 ~ c(intx4, intx4)*1 x5 ~ c(intx5, intx5)*1 x6 ~ c(intx6, intx6)*1 x7 ~ c(intx7, intx7)*1 x8 ~ c(intx8, intx8)*1 x9 ~ c(intx9, intx9)*1 &#39; cfaModel_scalarInvarianceV3_fit &lt;- lavaan( cfaModel_scalarInvarianceV3, data = HolzingerSwineford1939, group = &quot;school&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) anova(metricInvarianceModel_fit, cfaModel_scalarInvarianceV3_fit) 16.10.8.2.4 Variable 4 Measurement invariance does not yet become established when freeing intercepts of variable 4 across groups. This suggests that, whether or not variables 1, 2, 3, and 4 show non-invariant intercepts, there are other items that show non-invariant intercepts. Code cfaModel_scalarInvarianceV4 &lt;- &#39; #Fix factor loadings to be the same across groups visual =~ c(lambdax1,lambdax1)*x1 + c(lambdax2,lambdax2)*x2 + c(lambdax3,lambdax3)*x3 textual =~ c(lambdax4,lambdax4)*x4 + c(lambdax5,lambdax5)*x5 + c(lambdax6,lambdax6)*x6 speed =~ c(lambdax7,lambdax7)*x7 + c(lambdax8,lambdax8)*x8 + c(lambdax9,lambdax9)*x9 #Fix latent means to zero visual ~ 0 textual ~ 0 speed ~ 0 #Fix latent variances to one in group 1; free latent variances in group 2 visual ~~ c(1, NA)*visual textual ~~ c(1, NA)*textual speed ~~ c(1, NA)*speed #Estimate covariances among latent variables visual ~~ textual visual ~~ speed textual ~~ speed #Estimate residual variances of manifest variables x1 ~~ x1 x2 ~~ x2 x3 ~~ x3 x4 ~~ x4 x5 ~~ x5 x6 ~~ x6 x7 ~~ x7 x8 ~~ x8 x9 ~~ x9 #Iteratively free intercepts across groups x1 ~ NA*1 x2 ~ NA*1 x3 ~ NA*1 x4 ~ NA*1 x5 ~ c(intx5, intx5)*1 x6 ~ c(intx6, intx6)*1 x7 ~ c(intx7, intx7)*1 x8 ~ c(intx8, intx8)*1 x9 ~ c(intx9, intx9)*1 &#39; cfaModel_scalarInvarianceV4_fit &lt;- lavaan( cfaModel_scalarInvarianceV4, data = HolzingerSwineford1939, group = &quot;school&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) anova(metricInvarianceModel_fit, cfaModel_scalarInvarianceV4_fit) 16.10.8.2.5 Variable 5 Measurement invariance does not yet become established when freeing intercepts of variable 5 across groups. This suggests that, whether or not variables 1, 2, 3, 4, and 5 show non-invariant intercepts, there are other items that show non-invariant intercepts. Code cfaModel_scalarInvarianceV5 &lt;- &#39; #Fix factor loadings to be the same across groups visual =~ c(lambdax1,lambdax1)*x1 + c(lambdax2,lambdax2)*x2 + c(lambdax3,lambdax3)*x3 textual =~ c(lambdax4,lambdax4)*x4 + c(lambdax5,lambdax5)*x5 + c(lambdax6,lambdax6)*x6 speed =~ c(lambdax7,lambdax7)*x7 + c(lambdax8,lambdax8)*x8 + c(lambdax9,lambdax9)*x9 #Fix latent means to zero visual ~ 0 textual ~ 0 speed ~ 0 #Fix latent variances to one in group 1; free latent variances in group 2 visual ~~ c(1, NA)*visual textual ~~ c(1, NA)*textual speed ~~ c(1, NA)*speed #Estimate covariances among latent variables visual ~~ textual visual ~~ speed textual ~~ speed #Estimate residual variances of manifest variables x1 ~~ x1 x2 ~~ x2 x3 ~~ x3 x4 ~~ x4 x5 ~~ x5 x6 ~~ x6 x7 ~~ x7 x8 ~~ x8 x9 ~~ x9 #Iteratively free intercepts across groups x1 ~ NA*1 x2 ~ NA*1 x3 ~ NA*1 x4 ~ NA*1 x5 ~ NA*1 x6 ~ c(intx6, intx6)*1 x7 ~ c(intx7, intx7)*1 x8 ~ c(intx8, intx8)*1 x9 ~ c(intx9, intx9)*1 &#39; cfaModel_scalarInvarianceV5_fit &lt;- lavaan( cfaModel_scalarInvarianceV5, data = HolzingerSwineford1939, group = &quot;school&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) anova(metricInvarianceModel_fit, cfaModel_scalarInvarianceV5_fit) 16.10.8.2.6 Variable 6 Measurement invariance does not yet become established when freeing intercepts of variable 6 across groups. This suggests that, whether or not variables 1, 2, 3, 4, 5, and 6 show non-invariant intercepts, there are other items that show non-invariant intercepts. Code cfaModel_scalarInvarianceV6 &lt;- &#39; #Fix factor loadings to be the same across groups visual =~ c(lambdax1,lambdax1)*x1 + c(lambdax2,lambdax2)*x2 + c(lambdax3,lambdax3)*x3 textual =~ c(lambdax4,lambdax4)*x4 + c(lambdax5,lambdax5)*x5 + c(lambdax6,lambdax6)*x6 speed =~ c(lambdax7,lambdax7)*x7 + c(lambdax8,lambdax8)*x8 + c(lambdax9,lambdax9)*x9 #Fix latent means to zero visual ~ 0 textual ~ 0 speed ~ 0 #Fix latent variances to one in group 1; free latent variances in group 2 visual ~~ c(1, NA)*visual textual ~~ c(1, NA)*textual speed ~~ c(1, NA)*speed #Estimate covariances among latent variables visual ~~ textual visual ~~ speed textual ~~ speed #Estimate residual variances of manifest variables x1 ~~ x1 x2 ~~ x2 x3 ~~ x3 x4 ~~ x4 x5 ~~ x5 x6 ~~ x6 x7 ~~ x7 x8 ~~ x8 x9 ~~ x9 #Iteratively free intercepts across groups x1 ~ NA*1 x2 ~ NA*1 x3 ~ NA*1 x4 ~ NA*1 x5 ~ NA*1 x6 ~ NA*1 x7 ~ c(intx7, intx7)*1 x8 ~ c(intx8, intx8)*1 x9 ~ c(intx9, intx9)*1 &#39; cfaModel_scalarInvarianceV6_fit &lt;- lavaan( cfaModel_scalarInvarianceV6, data = HolzingerSwineford1939, group = &quot;school&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) anova(metricInvarianceModel_fit, cfaModel_scalarInvarianceV6_fit) 16.10.8.2.7 Variable 7 Measurement invariance becomes established when freeing intercepts of variable 7 across groups. This suggests that item 7 shows non-invariant intercepts across groups, and that measurement invariance holds when constraining items 8 and 9 to be invariant across groups. There may also be other items, especially among the preceding items (variables 1–6), that show non-invariant intercepts across groups. Code cfaModel_scalarInvarianceV7 &lt;- &#39; #Fix factor loadings to be the same across groups visual =~ c(lambdax1,lambdax1)*x1 + c(lambdax2,lambdax2)*x2 + c(lambdax3,lambdax3)*x3 textual =~ c(lambdax4,lambdax4)*x4 + c(lambdax5,lambdax5)*x5 + c(lambdax6,lambdax6)*x6 speed =~ c(lambdax7,lambdax7)*x7 + c(lambdax8,lambdax8)*x8 + c(lambdax9,lambdax9)*x9 #Fix latent means to zero visual ~ 0 textual ~ 0 speed ~ 0 #Fix latent variances to one in group 1; free latent variances in group 2 visual ~~ c(1, NA)*visual textual ~~ c(1, NA)*textual speed ~~ c(1, NA)*speed #Estimate covariances among latent variables visual ~~ textual visual ~~ speed textual ~~ speed #Estimate residual variances of manifest variables x1 ~~ x1 x2 ~~ x2 x3 ~~ x3 x4 ~~ x4 x5 ~~ x5 x6 ~~ x6 x7 ~~ x7 x8 ~~ x8 x9 ~~ x9 #Iteratively free intercepts across groups x1 ~ NA*1 x2 ~ NA*1 x3 ~ NA*1 x4 ~ NA*1 x5 ~ NA*1 x6 ~ NA*1 x7 ~ NA*1 x8 ~ c(intx8, intx8)*1 x9 ~ c(intx9, intx9)*1 &#39; cfaModel_scalarInvarianceV7_fit &lt;- lavaan( cfaModel_scalarInvarianceV7, data = HolzingerSwineford1939, group = &quot;school&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) anova(metricInvarianceModel_fit, cfaModel_scalarInvarianceV7_fit) 16.10.8.2.8 Variable 8 Measurement invariance continues to hold when freeing intercepts of variable 8 across groups. This suggests that variable 8 does not show non-invariant intercepts across groups, at least when the other intercepts have been freed. Code cfaModel_scalarInvarianceV8 &lt;- &#39; #Fix factor loadings to be the same across groups visual =~ c(lambdax1,lambdax1)*x1 + c(lambdax2,lambdax2)*x2 + c(lambdax3,lambdax3)*x3 textual =~ c(lambdax4,lambdax4)*x4 + c(lambdax5,lambdax5)*x5 + c(lambdax6,lambdax6)*x6 speed =~ c(lambdax7,lambdax7)*x7 + c(lambdax8,lambdax8)*x8 + c(lambdax9,lambdax9)*x9 #Fix latent means to zero visual ~ 0 textual ~ 0 speed ~ 0 #Fix latent variances to one in group 1; free latent variances in group 2 visual ~~ c(1, NA)*visual textual ~~ c(1, NA)*textual speed ~~ c(1, NA)*speed #Estimate covariances among latent variables visual ~~ textual visual ~~ speed textual ~~ speed #Estimate residual variances of manifest variables x1 ~~ x1 x2 ~~ x2 x3 ~~ x3 x4 ~~ x4 x5 ~~ x5 x6 ~~ x6 x7 ~~ x7 x8 ~~ x8 x9 ~~ x9 #Iteratively free intercepts across groups x1 ~ NA*1 x2 ~ NA*1 x3 ~ NA*1 x4 ~ NA*1 x5 ~ NA*1 x6 ~ NA*1 x7 ~ NA*1 x8 ~ NA*1 x9 ~ c(intx9, intx9)*1 &#39; cfaModel_scalarInvarianceV8_fit &lt;- lavaan( cfaModel_scalarInvarianceV8, data = HolzingerSwineford1939, group = &quot;school&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) anova(metricInvarianceModel_fit, cfaModel_scalarInvarianceV8_fit) 16.10.8.2.9 Variable 9 Measurement invariance continues to hold when freeing intercepts of variable 9 across groups (even after constraining intercepts of variable 8 across groups). This suggests that variable 9 does not show non-invariant intercepts across groups, at least when the other intercepts have been freed. Code cfaModel_scalarInvarianceV9 &lt;- &#39; #Fix factor loadings to be the same across groups visual =~ c(lambdax1,lambdax1)*x1 + c(lambdax2,lambdax2)*x2 + c(lambdax3,lambdax3)*x3 textual =~ c(lambdax4,lambdax4)*x4 + c(lambdax5,lambdax5)*x5 + c(lambdax6,lambdax6)*x6 speed =~ c(lambdax7,lambdax7)*x7 + c(lambdax8,lambdax8)*x8 + c(lambdax9,lambdax9)*x9 #Fix latent means to zero visual ~ 0 textual ~ 0 speed ~ 0 #Fix latent variances to one in group 1; free latent variances in group 2 visual ~~ c(1, NA)*visual textual ~~ c(1, NA)*textual speed ~~ c(1, NA)*speed #Estimate covariances among latent variables visual ~~ textual visual ~~ speed textual ~~ speed #Estimate residual variances of manifest variables x1 ~~ x1 x2 ~~ x2 x3 ~~ x3 x4 ~~ x4 x5 ~~ x5 x6 ~~ x6 x7 ~~ x7 x8 ~~ x8 x9 ~~ x9 #Iteratively free intercepts across groups x1 ~ NA*1 x2 ~ NA*1 x3 ~ NA*1 x4 ~ NA*1 x5 ~ NA*1 x6 ~ NA*1 x7 ~ NA*1 x8 ~ c(intx8, intx8)*1 x9 ~ NA*1 &#39; cfaModel_scalarInvarianceV9_fit &lt;- lavaan( cfaModel_scalarInvarianceV9, data = HolzingerSwineford1939, group = &quot;school&quot;, missing = &quot;ML&quot;, estimator = &quot;MLR&quot;) anova(metricInvarianceModel_fit, cfaModel_scalarInvarianceV9_fit) 16.10.8.2.10 Summary When iteratively dropping constraints, measurement invariance became established when freeing the intercepts for items 1–7 across groups. This suggests that items 7, 8, and 9 show measurement invariance of intercepts across groups. This could be due to the selection of starting with item 1 as the first constraint to be freed. We could try freeing constraints in a different order to see the extent to which the order influences which items are identified as showing measurement non-invariance. 16.10.8.3 Next Steps After identifying which item(s) show measurement non-invariance, we would evaluate the measurement non-invariance according to theory and effect sizes. We might start with the item with the largest measurement non-invariance. If we deem the measurement non-invariance for this item is non-negligible, we have three primary options: (1) drop the item for both groups, (2) drop the item for one group but keep it for the other group, or (3) freely estimate parameters for the item across groups. We would do this iteratively for the remaining items, by magnitude, that show non-negligible measurement non-invariance. Thus, our model might show partial scalar invariance—some items might have intercepts that are constrained to be the same across groups, whereas other items might have intercepts that are allowed to differ across groups. Then, we would test subsequent measurement invariance models (e.g., residual invariance) while keeping the partially freed constaints from the partial scalar invariance model. Once we establish the best fitting model that makes as many constraints that are theoretically and empirically justified for the purposes of the study, we would use that model in subsequent tests. As a reminder, full or partial metric invariance is helpful for comparing associations across groups, whereas full or partial scalar invariance is helpful for comparing mean levels across groups. 16.11 Conclusion Bias is a systematic error. Test bias refers to a systematic error (in measurement, prediction, etc.) as a function of group membership. The two broad categories of test bias include predictive bias and test structure bias. Predictive bias refers to differences between groups in the relation between the test and criterion—i.e., different intercepts and/or slopes of the regression line between the test and the criterion. Test structure bias refers to differences in the internal test characteristics across groups, and can be evaluated with approaches such as measurement invariance and differential item function. When detecting test bias, it is important to address it, and there are a number of ways of correcting for bias, including score adjustment and other approaches that do not involve score adjustment. However, even if a test does not show bias, it does not mean that the test is fair. Fairness is not a scientific question but rather a moral, societal, and ethical question; there are many different ways of operationalizing fairness. Fairness is a complex question, so do the best you can and try to minimize any negative impact of the assessment procedures. 16.12 Suggested Readings Putnick &amp; Bornstein (2016); N. S. Cole (1981); Fernández &amp; Abe (2018); Reynolds &amp; Suzuki (2012); Sackett &amp; Wilk (1994); Jonson &amp; Geisinger (2022) 16.13 Exercises 16.13.1 Questions Note: Several of the following questions use data from the Children of the National Longitudinal Survey of Youth Survey (CNLSY). The CNLSY is a publicly available longitudinal data set provided by the Bureau of Labor Statistics (https://www.bls.gov/nls/nlsy79-children.htm#topical-guide; archived at https://perma.cc/EH38-HDRN). The CNLSY data file for these exercises is located on the book’s page of the Open Science Framework (https://osf.io/3pwza). Children’s behavior problems were rated in 1988 (time 1: T1) and then again in 1990 (time 2: T2) on the Behavior Problems Index (BPI). Below are the items corresponding to the Antisocial subscale of the BPI: cheats or tells lies bullies or is cruel/mean to others does not seem to feel sorry after misbehaving breaks things deliberately is disobedient at school has trouble getting along with teachers has sudden changes in mood or feeling Determine whether there is predictive bias, as a function of sex, in predicting antisocial behavior at T2 using antisocial behavior at T2. Describe any predictive bias observed. How could you address this predictive bias? Assume that children are selected to receive preventative services if their score on the Antisocial scale at T1 is &gt; 6. Assume that children at T2 have an antisocial disorder if their score at T2 is &gt; 6. Does the Antisocial scale show fairness in terms of equal outcomes between boys and girls? A lawyer argues in court that a test that is used to select students for college admissions should be able to be used because it has been demonstrated to show no evidence of bias against particular groups. However, you know that just because a measure may be unbiased does not mean that using the measure’s scores for that purpose is fair. How could such a measure be unbiased yet unfair? Fit a multi-group item response theory model to the Antisocial subscale at time 2, with the participant’s sex as a grouping factor. Does a model with item parameters constrained across sexes fit significantly worse than a model with item parameters that are allowed to vary across sexes? Starting with an unconstrained model that allows item parameters to vary across sexes, sequentially add constraints to item parameters across groups. Which items differed in discrimination across groups? Which items differed in severity across groups? Describe any differences. How could any differential item functioning be addressed? Examine the longitudinal measurement invariance of the Antisocial scale across T1 and T2. Use full information maximum likelihood (FIML) to account for missing data. Use robust standard errors to account for non-normally distributed data. Which type(s) of longitudinal measurement invariance failed? Which items and parameters appeared to differ across ages? 16.13.2 Answers There is predictive bias in terms of different slopes across sexes. The slope is steeper for boys than girls, indicating that the measure is not as predictively accurate for girls as it is for boys. You could address this predictive bias by using a different predictive measure for girls than boys, changing the criterion, or by using within-group norming. No, the Antisocial scale does not show equal outcomes in terms of selection rate. The selection ratio is much lower in girls \\((0.10)\\) compared to boys \\((0.05)\\). The measure could be unbiased if it shows the same intercepts and slopes across groups in predicting college performance. However, a measure can still be unfair even it is unbiased. If there are different types of errors in different groups, the measure may not be fair. For instance, if there are more false negative errors in Black applicants compared to White applicants and more false positive errors in White applicants than Black applicants, the measure would be unfair against Black applicants. Even if the percent accuracy of prediction is the same for Black and White applicants, the errors have different implications for each group—White applicants who would have failed in college are more likely to be accepted whereas strong Black applicants who would have succeeded are less likely to be accepted. To address the unfairness of the test, it will be important to use one of the various operationalizations of fairness: equal outcomes, equal opportunity, or equal odds. The model with item parameters constrained across sexes fit significantly worse than a model with item parameters that were allowed to vary across sexes, \\(\\chi^2[-19] = -119.15, p &lt; .001\\). This suggests that the item parameters show differential item functioning across boys and girls. No items showed significant differences in discrimination across sexes, suggesting that items did not differ in their relation to the latent factor across boys and girls. Items 1, 2, 3, 4, 5, and 6 differed in severity across sexes. Items showed greater severity for girls than boys. It took a higher level of antisocial behavior for a girl to be endorsed as engaging in one of the following behaviors: cheats or tells lies, bullies or is cruel/mean to others, does not seem to feel sorry after misbehaving, breaks things deliberately, is disobedient at school, and has trouble getting along with teachers. The differences in item severity across boys and girls could be addressed by resolving DIF. That is, the items that show severity DIF (items 1–6) would be allowed to differ in their severity parameter across boys and girls, but these items would be constrained to have the same discrimination across groups; non-DIF items would have severity and discrimination parameters constrained to be the same across groups. The configural invariance model fit well according to RMSEA \\((0.09)\\) and SRMR \\((0.06)\\), but fit less well according to CFI \\((0.77)\\). The metric invariance model did not fit significantly worse than the configural invariance model \\((\\Delta\\chi^2[6] = 8.77, p = 0.187)\\). However, the scalar invariance model fit significantly worse than the metric invariance model \\((\\Delta\\chi^2[6] = 21.28, p = 0.002)\\), and the residual invariance model fit significantly worse than the scalar invariance model \\((\\Delta\\chi^2[7] = 42.80, p &lt; .001)\\). Therefore, scalar and residual invariance failed. The following items showed larger intercepts at T1 compared to T2: items 1, 2, 3, 4, 5, and 7. Only item 6 showed larger intercepts at T2 than T1. The following items showed a larger residual at T1 compared to T2: items 2–7. Only item 1 showed a larger residual at T2 compared to T1. References Aguinis, H., Culpepper, S. A., &amp; Pierce, C. A. (2010). Revival of test bias research in preemployment testing. Journal of Applied Psychology, 95(4), 648–680. https://doi.org/10.1037/a0018714 American Educational Research Association, American Psychological Association, &amp; National Council on Measurement in Education. (2014). Standards for educational and psychological testing. American Educational Research Association. Barrash, J., Stillman, A., Anderson, S. W., Uc, E. Y., Dawson, J. D., &amp; Rizzo, M. (2010). Prediction of driving ability with neuropsychological tests: Demographic adjustments diminish accuracy. Journal of the International Neuropsychological Society, 16(4), 679–686. https://doi.org/10.1017/S1355617710000470 Bauer, D. J., Belzak, W. C. M., &amp; Cole, V. T. (2020). Simplifying the assessment of measurement invariance over multiple background variables: Using regularized moderated nonlinear factor analysis to detect differential item functioning. Structural Equation Modeling: A Multidisciplinary Journal, 27(1), 43–55. https://doi.org/10.1080/10705511.2019.1642754 Belzak, W. C. M., &amp; Bauer, D. J. (2020). Improving the assessment of measurement invariance: Using regularization to select anchor items and identify differential item functioning. Psychological Methods, 25(6), 673–690. https://doi.org/10.1037/met0000253 Brown, R. T., Reynolds, C. R., &amp; Whitaker, J. S. (1999). Bias in mental testing since bias in mental testing. School Psychology Quarterly, 14(3), 208–238. https://doi.org/10.1037/h0089007 Burlew, A. K., Peteet, B. J., McCuistian, C., &amp; Miller-Roenigk, B. D. (2019). Best practices for researching diverse groups. American Journal of Orthopsychiatry, 89(3), 354–368. https://doi.org/10.1037/ort0000350 Camilli, G. (2013). Ongoing issues in test fairness. Educational Research and Evaluation, 19(2–3), 104–120. https://doi.org/10.1080/13803611.2013.767602 Chalmers, P. (2020). mirt: Multidimensional item response theory. https://CRAN.R-project.org/package=mirt Chen, F. F. (2007). Sensitivity of goodness of fit indexes to lack of measurement invariance. Structural Equation Modeling: A Multidisciplinary Journal, 14(3), 464–504. https://doi.org/10.1080/10705510701301834 Cheng, Y., Shao, C., &amp; Lathrop, Q. N. (2016). The mediated MIMIC model for understanding the underlying mechanism of DIF. Educational and Psychological Measurement, 76(1), 43–63. https://doi.org/10.1177/0013164415576187 Cheung, G. W., &amp; Rensvold, R. B. (2002). Evaluating goodness-of-fit indexes for testing measurement invariance. Structural Equation Modeling: A Multidisciplinary Journal, 9(2), 233–255. https://doi.org/10.1207/s15328007sem0902_5 Clark, M. J., &amp; Grandy, J. (1984). Sex differences in the academic performance of Scholastic Aptitude Test takers: College board report no. 84-8. College Board Publications. Cole, N. S. (1981). Bias in testing. American Psychologist, 36(10), 1067–1077. https://doi.org/10.1037/0003-066X.36.10.1067 Cole, V., Gottfredson, N., &amp; Giordano, M. (2018). aMNLFA: Automated fitting of moderated nonlinear factor analysis through the Mplus program. https://CRAN.R-project.org/package=aMNLFA Committee on the General Aptitude Test Battery, Commission on Behavioral and Social Sciences and Education, &amp; National Research Council. (1989). Fairness in employment testing: Validity generalization, minority issues, and the general aptitude test battery. National Academies Press. Counsell, A., Cribbie, R. A., &amp; Flora, D. B. (2020). Evaluating equivalence testing methods for measurement invariance. Multivariate Behavioral Research, 55(2), 312–328. https://doi.org/10.1080/00273171.2019.1633617 Curran, P. J., Howard, A. L., Bainter, S. A., Lane, S. T., &amp; McGinley, J. S. (2014). The separation of between-person and within-person components of individual change over time: A latent curve model with structured residuals. Journal of Consulting and Clinical Psychology, 82, 8–94. https://doi.org/10.1037/a0035297 Dorans, N. J. (2017). Contributions to the quantitative assessment of item, test, and score fairness. In R. E. Bennett &amp; M. von Davier (Eds.), Advancing human assessment (pp. 201–230). Springer, Cham. Dueber, D. (2019). dmacs: Measurement nonequivalence effect size calculator. https://github.com/ddueber/dmacs Epskamp, S. (2022). semPlot: Path diagrams and visual analysis of various SEM packages’ output. https://github.com/SachaEpskamp/semPlot Fernández, A. L., &amp; Abe, J. (2018). Bias in cross-cultural neuropsychological testing: Problems and possible solutions. Culture and Brain, 6(1), 1–35. https://doi.org/10.1007/s40167-017-0050-2 Fletcher, R. R., Nakeshimana, A., &amp; Olubeko, O. (2021). Addressing fairness, bias, and appropriate use of artificial intelligence and machine learning in global health. Frontiers in Artificial Intelligence, 3(116). https://doi.org/10.3389/frai.2020.561802 Gipps, C., &amp; Stobart, G. (2009). Fairness in assessment. In C. Wyatt-Smith &amp; J. J. Cumming (Eds.), Educational assessment in the 21st century: Connecting theory and practice (pp. 105–118). Springer Netherlands. https://doi.org/10.1007/978-1-4020-9964-9_6 Gonzalez, O., &amp; Pelham, W. E. (2021). When does differential item functioning matter for screening? A method for empirical evaluation. Assessment, 28(2), 446–456. https://doi.org/10.1177/1073191120913618 Gottfredson, L. S. (1994). The science and politics of race-norming. American Psychologist, 49(11), 955–963. https://doi.org/10.1037/0003-066X.49.11.955 Gottfredson, N. C., Cole, V. T., Giordano, M. L., Bauer, D. J., Hussong, A. M., &amp; Ennett, S. T. (2019). Simplifying the implementation of modern scale scoring methods with an automated R package: Automated moderated nonlinear factor analysis (aMNLFA). Addictive Behaviors, 94, 65–73. https://doi.org/10.1016/j.addbeh.2018.10.031 Gunn, H. J., Grimm, K. J., &amp; Edwards, M. C. (2020). Evaluation of six effect size measures of measurement non-invariance for continuous outcomes. Structural Equation Modeling: A Multidisciplinary Journal, 27(4), 503–514. https://doi.org/10.1080/10705511.2019.1689507 Hagquist, C. (2019). Explaining differential item functioning focusing on the crucial role of external information – an example from the measurement of adolescent mental health. BMC Medical Research Methodology, 19(1), 185. https://doi.org/10.1186/s12874-019-0828-3 Hagquist, C., &amp; Andrich, D. (2017). Recent advances in analysis of differential item functioning in health research using the Rasch model. Health and Quality of Life Outcomes, 15(1), 181. https://doi.org/10.1186/s12955-017-0755-0 Hall, G. C. N., Bansal, A., &amp; Lopez, I. R. (1999). Ethnicity and psychopathology: A meta-analytic review of 31 years of comparative MMPI/MMPI-2 research. Psychological Assessment, 11(2), 186–197. https://doi.org/10.1037/1040-3590.11.2.186 Han, K., Colarelli, S. M., &amp; Weed, N. C. (2019). Methodological and statistical advances in the consideration of cultural diversity in assessment: A critical review of group classification and measurement invariance testing. Psychological Assessment, 31(12), 1481–1496. https://doi.org/10.1037/pas0000731 Helms, J. E. (2006). Fairness is not validity or cultural bias in racial-group assessment: A quantitative perspective. American Psychologist, 61(8), 845–859. https://doi.org/10.1037/0003-066X.61.8.845 Jensen, A. R. (1980). Précis of bias in mental testing. Behavioral and Brain Sciences, 3(3), 325–333. https://doi.org/10.1017/S0140525X00005161 Jonson, J. L., &amp; Geisinger, K. F. (2022). Fairness in educational and psychological testing: Examining theoretical, research, practice, and policy implications of the 2014 standards. American Educational Research Association,. Jorgensen, T. D., Kite, B. A., Chen, P.-Y., &amp; Short, S. D. (2018). Permutation randomization methods for testing measurement equivalence and detecting differential item functioning in multiple-group confirmatory factor analysis. Psychological Methods, 23(4), 708–728. https://doi.org/10.1037/met0000152 Jorgensen, T. D., Pornprasertmanit, S., Schoemann, A. M., &amp; Rosseel, Y. (2021). semTools: Useful tools for structural equation modeling. https://github.com/simsem/semTools/wiki Kuncel, N. R., &amp; Hezlett, S. A. (2010). Fact and fiction in cognitive ability testing for admissions and hiring decisions. Current Directions in Psychological Science, 19(6), 339–345. https://doi.org/10.1177/0963721410389459 Lai, M. H. C. (2021). Adjusting for measurement noninvariance with alignment in growth modeling. Multivariate Behavioral Research, 1–18. https://doi.org/10.1080/00273171.2021.1941730 Lee, K., Bull, R., &amp; Ho, R. M. H. (2013). Developmental changes in executive functioning. Child Development, 84(6), 1933–1953. https://doi.org/10.1111/cdev.12096 Little, T. D. (2013). Longitudinal structural equation modeling. The Guilford Press. Little, T. D., Preacher, K. J., Selig, J. P., &amp; Card, N. A. (2007). New developments in latent variable panel analyses of longitudinal data. International Journal of Behavioral Development, 31(4), 357–365. https://doi.org/10.1177/0165025407077757 Little, T. D., Slegers, D. W., &amp; Card, N. A. (2006). A non-arbitrary method of identifying and scaling latent variables in SEM and MACS models. Structural Equation Modeling, 13(1), 59–72. https://doi.org/10.1207/s15328007sem1301_3 Liu, Y., Millsap, R. E., West, S. G., Tein, J.-Y., Tanaka, R., &amp; Grimm, K. J. (2017). Testing measurement invariance in longitudinal data with ordered-categorical measures. Psychological Methods, 22(3), 486–506. https://doi.org/10.1037/met0000075 Manly, J. J. (2005). Advantages and disadvantages of separate norms for African Americans. The Clinical Neuropsychologist, 19(2), 270–275. https://doi.org/10.1080/13854040590945346 Manly, J. J., &amp; Echemendia, R. J. (2007). Race-specific norms: Using the model of hypertension to understand issues of race, culture, and education in neuropsychology. Archives of Clinical Neuropsychology, 22(3), 319–325. https://doi.org/10.1016/j.acn.2007.01.006 McClelland, D. C. (1973). Testing for competence rather than for “intelligence.” American Psychologist, 28, 1–14. https://doi.org/10.1037/h0034092 Meade, A. W. (2010). A taxonomy of effect size measures for the differential functioning of items and scales. Journal of Applied Psychology, 95(4), 728–743. https://doi.org/10.1037/a0018966 Melikyan, Z. A., Agranovich, A. V., &amp; Puente, A. E. (2019). Fairness in psychological testing. In G. Goldstein, D. N. Allen, &amp; J. DeLuca (Eds.), Handbook of psychological assessment (fourth edition) (pp. 551–572). Academic Press. https://doi.org/10.1016/B978-0-12-802203-0.00018-3 Millsap, R. E. (2011). Statistical approaches to measurement invariance. Taylor &amp; Francis. Muthén, L. K., &amp; Muthén, B. O. (2019). Mplus version 8.4. Muthén &amp; Muthén. Nye, C. D., Bradburn, J., Olenick, J., Bialko, C., &amp; Drasgow, F. (2019). How big are my effects? Examining the magnitude of effect sizes in studies of measurement equivalence. Organizational Research Methods, 22(3), 678–709. https://doi.org/10.1177/1094428118761122 Oberski, D. L. (2014). Evaluating sensitivity of parameters of interest to measurement invariance in latent variable models. Political Analysis, 22(1), 45–60. https://doi.org/10.1093/pan/mpt014 Oberski, D. L., Vermunt, J. K., &amp; Moors, G. B. D. (2015). Evaluating measurement invariance in categorical data latent variable models with the EPC-interest. Political Analysis, 23(4), 550–563. https://doi.org/10.1093/pan/mpv020 Paulus, J. K., &amp; Kent, D. M. (2020). Predictably unequal: Understanding and addressing concerns that algorithmic clinical prediction may increase health disparities. Npj Digital Medicine, 3(1), 99. https://doi.org/10.1038/s41746-020-0304-9 Petersen, I. T. (2024b). petersenlab: Package of R functions for the Petersen Lab. https://doi.org/10.5281/zenodo.7602890 Petersen, I. T., Choe, D. E., &amp; LeBeau, B. (2020). Studying a moving target in development: The challenge and opportunity of heterotypic continuity. Developmental Review, 58, 100935. https://doi.org/10.1016/j.dr.2020.100935 Putnick, D. L., &amp; Bornstein, M. H. (2016). Measurement invariance conventions and reporting: The state of the art and future directions for psychological research. Developmental Review, 41, 71–90. https://doi.org/10.1016/j.dr.2016.06.004 Raykov, T., Marcoulides, G. A., Harrison, M., &amp; Zhang, M. (2020). On the dependability of a popular procedure for studying measurement invariance: A cause for concern? Structural Equation Modeling: A Multidisciplinary Journal, 27(4), 649–656. https://doi.org/10.1080/10705511.2019.1610409 Reynolds, C. R., Altmann, R. A., &amp; Allen, D. N. (2021). The problem of bias in psychological assessment. In C. R. Reynolds, R. A. Altmann, &amp; D. N. Allen (Eds.), Mastering modern psychological testing: Theory and methods (pp. 573–613). Springer International Publishing. https://doi.org/10.1007/978-3-030-59455-8_15 Reynolds, C. R., &amp; Suzuki, L. A. (2012). Bias in psychological assessment: An empirical review and recommendations. In I. B. Weiner, J. R. Graham, &amp; J. A. Naglieri (Eds.), Handbook of psychology, Vol. 10: Assessment psychology, Part 1: Assessment issues (2nd ed., pp. 82–113). Robitzsch, A. (2019). mnlfa: Moderated nonlinear factor analysis. https://CRAN.R-project.org/package=mnlfa Rosseel, Y., Jorgensen, T. D., &amp; Rockwood, N. (2022). lavaan: Latent variable analysis. https://lavaan.ugent.be Sackett, P. R., Borneman, M. J., &amp; Connelly, B. S. (2008). High stakes testing in higher education and employment: Appraising the evidence for validity and fairness. American Psychologist, 63, 215–227. https://doi.org/10.1037/0003-066X.63.4.215 Sackett, P. R., Schmitt, N., Ellingson, J. E., &amp; Kabin, M. B. (2001). High-stakes testing in employment, credentialing, and higher education. American Psychologist, 56, 301–318. https://doi.org/10.1037/0003-066X.56.4.302 Sackett, P. R., &amp; Wilk, S. L. (1994). Within-group norming and other forms of score adjustment in preemployment testing. American Psychologist, 49(11), 929–954. https://doi.org/10.1037/0003-066X.49.11.929 Schmidt, F. L., &amp; Hunter, J. E. (1981). Employment testing: Old theories and new research findings. American Psychologist, 36(10), 1128–1137. https://doi.org/10.1037/0003-066X.36.10.1128 Silverberg, N. D., &amp; Millis, S. R. (2009). Impairment versus deficiency in neuropsychological assessment: Implications for ecological validity. Journal of the International Neuropsychological Society, 15(1), 94–102. https://doi.org/10.1017/S1355617708090139 Thorndike, R. L. (1971). Concepts of culture-fairness. Journal of Educational Measurement, 8(2), 63–70. https://doi.org/10.1111/j.1745-3984.1971.tb00907.x Van De Schoot, R., Kluytmans, A., Tummers, L., Lugtig, P., Hox, J., &amp; Muthen, B. (2013). Facing off with scylla and charybdis: A comparison of scalar, partial, and the novel possibility of approximate measurement invariance. Frontiers in Psychology, 4(770). https://doi.org/10.3389/fpsyg.2013.00770 Van De Schoot, R., Schmidt, P., De Beuckelaer, A., Lek, K., &amp; Zondervan-Zwijnenburg, M. (2015). Editorial: Measurement invariance. Frontiers in Psychology, 6(1064). https://doi.org/10.3389/fpsyg.2015.01064 Wang, T., Merkle, E. C., &amp; Zeileis, A. (2014). Score-based tests of measurement invariance: Use in practice. Frontiers in Psychology, 5. https://doi.org/10.3389/fpsyg.2014.00438 Wang, W.-C., Shih, C.-L., &amp; Yang, C.-C. (2009). The MIMIC method with scale purification for detecting differential item functioning. Educational and Psychological Measurement, 69(5), 713–731. https://doi.org/10.1177/0013164409332228 Youngstrom, E. A., &amp; Van Meter, A. (2016). Empirically supported assessment of children and adolescents. Clinical Psychology: Science and Practice, 23(4), 327–347. https://doi.org/10.1111/cpsp.12172 Zieky, M. J. (2006). Fairness review in assessment. In S. M. Downing &amp; T. M. Haladyna (Eds.), Handbook of test development (pp. 359–376). Routledge. https://doi.org/10.4324/9780203874776.ch16 Zieky, M. J. (2013). Fairness review in assessment. In K. F. Geisinger, B. A. Bracken, J. F. Carlson, J.-I. C. Hansen, N. R. Kuncel, S. P. Reise, &amp; M. C. Rodriguez (Eds.), APA handbook of testing and assessment in psychology, Vol. 1: Test theory and testing and assessment in industrial and organizational psychology (pp. 293–302). American Psychological Association. https://doi.org/10.1037/14047-017 "],["interview.html", "Chapter 17 The Interview and the DSM 17.1 Overview of Clinical Interviews 17.2 Two Traditions: Unstructured and Structured Interviews 17.3 Other Findings Regarding Interviews 17.4 Best Practice for Diagnostic Assessment 17.5 The DSM and ICD 17.6 Conclusion 17.7 Suggested Readings", " Chapter 17 The Interview and the DSM 17.1 Overview of Clinical Interviews Clinical interviews are a form of conversation with a client, in which the conversation has explicit clinical goals including information gathering on the client. Sharp et al. (2013) provide an overview of the clinical interview. Interviews are the most widely used assessment technique in clinical psychology, yet there is very little done to explore their validity or ways to improve their validity. There is very little data on the validity of interviews. It is likely that there has been little research on the validity of interviews because the field has been riding the coattails of biological psychiatry, in which the tradition has been that the clinical interview is just assumed to be the “gold standard”. However, this is based on theory, not data. The problem is that we do not have a “gold standard” (Lilienfeld et al., 2015). We do not have any robust measure to compare interviews against to verify their accuracy—i.e., we do not have true knowledge about the pathophysiology as a strong indicator of disease status. Diagnoses are not directly observable concepts. Comparisons from one interviewer to another interviewer establishes inter-rater reliability (i.e., diagnostic agreement), but it does not establish diagnostic validity—i.e., the correspondence to truth about illness status. To estimate diagnostic accuracy in the absence of a gold standard, one can use latent class models (Faraone &amp; Tsuang, 1994), where you identify latent (unmeasured) class membership among participants using multiple observed variables. For example, you can identify a latent class membership based on multiple symptoms, raters or observers, and/or at multiple time points. High scores across symptoms, raters, and time points are more likely to reflect true illness. This is consistent with making diagnostic decisions using the LEAD standard: LEAD is an acronym for: Longitudinal, Expert, All Data. So, the best approach may be to use LEAD data to evaluate diagnostic validity. The latent class would be the illness status—i.e., whether the client “has” the disorder or not. Illness status is not directly observable, so we use observables to infer latent classes. Each person is assigned to a latent class with different probabilities to account for uncertainty. For example, Person A may be assigned a 75% probability of “having” depression, whereas the probability may be 13% for Person B. Then, we could treat the latent class estimate of illness status as the latent gold standard. Diagnoses based on LEAD data are not 100% valid; thus, they are a LEAD standard, not a gold standard. However, LEAD approaches are expensive and time-consuming, and they are not widely used. 17.2 Two Traditions: Unstructured and Structured Interviews There are two primary traditions to the clinical interview: the unstructured interview and the structured interview. 17.2.1 Unstructured Interview The unstructured interview is the oldest tradition of the clinical interview. It was the most widely used interview method until the late 1970s. The unstructured clinical interview came from a psychoanalytic theoretical orientation. In an unstructured interview, the clinician keeps the framework “in the head” of the information they want to extract from the client, including presenting problems, past treatments obtained, comorbid symptoms, etc. An unstructured interview is an open-ended, free flowing conversation between the clinician and client. According to psychoanalysis, the nondirective interview was believed to serve as a catalyst for the client’s expression of their unconscious, for example by means of transference and free associations. Transference is when the client treats the therapist as if the therapist was an important figure in the client’s past—for example, if the client treats the therapist as if the therapist were the client’s parent. Notes were frowned upon because they were viewed as disrespectful to the client and a disruption of the flow of session. A problem of the unstructured interview is that taking a different “conversation path” (i.e., asking different questions) or having a different clinician led to different information extracted from the client and to low inter-rater and test–retest reliability. The unstructured interview is susceptible to multiple forms of bias, such as halo effects when dealing with a polite client. Another important form of (cognitive) bias is confirmation bias, in which clinicians look for evidence that confirms their hypothesis about the client, which leads to the tendency to stop the interview once the first diagnosis is identified, and to fewer diagnoses. This is known as diagnostic overshadowing. The unstructured interview is also susceptible to race, gender, class bias, etc. In general, unstructured interviews show low reliability. The low reliability of unstructured interviews is a result of several factors. One factor is information variance: different information is extracted from the client in each interview. Another factor that contributes to the low reliability of unstructured interviews is criterion variance, where different criteria are used to determine the presence or absence of a condition. Criterion variance was inadequate once criterion-based diagnosis came along. A third factor that contributes to the low reliability of unstructured interviews is varying levels of skill and knowledge of interviewers. A potential advantage of the unstructured interview is that it can be good for non-specific aspects of interview. For instance, an unstructured interview can potentially be helpful for building rapport because the client feels the clinician’s first goal is to understand them and their situation in order to provide help. Classic examples of an unstructured interview are provided in “The Psychiatric Interview” (1970) by Harry Stack Sullivan, who is a psychodynamically oriented clinician. Sullivan (1970)’s book provides a guide to understanding the unstructured interview. He described processes such as “reciprocal emotion,” which he described as how the interviewer and interviewee influence each other and responses of the other through one’s emotional tone. 17.2.2 Structured Interview An important movement in the development of interviews was behavioral interviews that began in the 1960s. Behavioral interviews came from a cognitive-behavioral theoretical orientation. They started out as unstructured. However, behavioral interviews had a different set of goals from traditional unstructured interviews. Behavioral interviews sought to quantify aspects of behavior—for example, how often the symptoms occur. Behavioral interviews applied the A-B-C (antecedent-behavior-consequence) model. The A-B-C model tries to identify patterns of sequences that include the problem behavior. Specifically, the A-B-C model identifies things that came before the behavior (antecedents), and things that came after the behavior (consequences). This helps to identify precipitating and maintaining factors for the problem behavior, as well as strengths to bolster in treatment. It provides helpful ways to generate information and helps with case conceptualization. The behavior interviews eventually led to more structured versions, called structured interviews. An example of a structured clinical interview is the Structured Clinical Interview for DSM (SCID). Structured interviews were developed to solve the problem of reliability. In a structured interview, a pre-determined domain of questions is asked in a pre-defined way. A fully structured interview provides a script to use to ask each question (to standardize wording), and in a particular order, with a decision tree and branching logic to choose subsequent questions based on the responses given. For a structured diagnostic interview, the diagnostic criteria for a disorder are codified, and the exact language is provided to ask about each symptom. Then, the responses are scored and integrated in a systematic way, and the diagnosis is determined in a systematic way. Structured interviews show higher reliability compared to unstructured interviews due to: less information variance because the clinician extracts similar information from a given client, and less criterion variance because the clinician makes diagnostic decisions using the same criteria. Interviews differ in their continuum or degree of structure, from semi-structured to structured. In general, convergent validity improves as the degree of structure for an interview increases (Widiger, 2002). When very structured, even a layperson could perform the interview. Taken to the extreme, a human interviewer becomes unnecessary; the entire interview can be recorded and done by a computer. In these cases, inter-rater reliability approaches perfection. However, this removes the role of clinical judgment, which can lead to errors if the client misinterprets the computer-administered question or describes a behavior that is not actually clinically concerning. Therefore, structured interviews reduce the amount of training necessary for a clinician to administer, but they lose the potential positive qualities of unstructured interview: e.g., the capacity for better rapport; clients may not feel understood, cared about, or that they are working with an expert. Therefore, semi-structured interviews were developed. 17.2.2.1 Semi-Structured Interview Semi-structured interviews provide some leeway in how the clinician asks questions and the pace of the questions, so the clinician can ask questions in their own words and ask follow-up questions or restate questions using the client’s own words. Semi-structured interviews are still structured in that they are still asking about the same range of problems as a structured interview, to avoid confirmation bias, and they provide a structured way to score diagnostic criteria and assign diagnoses, to ensure consistency across therapists. A well-designed semi-structured interview balances improvements in validity—due to greater clinical judgment and probing—with the tradeoff in lower inter-rater reliability due to idiosyncratic decisions. A semi-structured interview also allows clarifying the meaning of questions and response choices. Conducting semi-structured interviews requires more training and there is frequent discussion, in the literature, of their problems, and it takes a long time to become really skilled. This can become an issue with highly trained individuals because they sometimes “screw around” with the interview (i.e., do things differently because they believe their approach is superior) and bypass the structure, leading to an unstructured interview. In addition to degree of structure, interviews also differ in their degree of specialization. Some interviews are general/broad, including the SCID. Other interviews are specific to particular domains, such as the Semi-Structured Assessment for the Genetics of Alcoholism (SSAGA), which assesses alcohol use disorder. As you get further away from the construct, you lose valued information and the interview becomes less and less helpful. Specialized interviews provide the maximum amount of information in the area of interest, and they take less time, which is why they are popular in research. A review of structured and semi-structured diagnostic interviews is provided by Summerfeldt et al. (2010). 17.2.3 Structured Versus Unstructured Interviews Structured interviews show higher reliability than unstructured interviews, in terms of both inter-rater and test–retest reliability. Inter-rater reliability of a clinical interview is typically estimated based on the degree of agreement between an interviewer and an observer of the interview. That is, most often inter-rater reliability of interviews is estimated based on two raters of the same interview content. However, this does not necessarily capture different styles of different interviewers. Therefore, the typical estimates of inter-rater reliability of interviews do not capture all parts of the reliability of the measure. Test–retest reliability of a clinical interview, is usually evaluated by conducting interviews of the same client separated (typically) by 2 weeks, often by a different interviewer. However, different ratings might be made at each time point if the diagnosis (e.g., depression) fluctuates over time or especially if the disorder is influenced by variation of the context. Therefore, changes in symptoms can contribute to differences in reliability over time—or, alternatively, differences could be due to differences in interviewers. Because of conflating differences in raters and time, test–retest reliability tends to be lower than inter-rater reliability in evaluating the reliability of interviews. Fully structured diagnostic interviews typically deliver inter-rater reliability Cohen’s kappa (\\(\\kappa\\)) estimates of .80 or higher. By contrast, estimates of inter-rater agreement between clinicians conducting unstructured interviews usually are near zero, which indicates that their agreement is no better than chance. Cohen’s kappa ranges from \\(-1\\) (perfect disagreement) to \\(+1\\) (perfect agreement). A kappa greater than .75 would be considered good inter-rater reliability. Despite the increased reliability of using structured and semi-structured interviews compared to unstructured interviews, many clinicians are reluctant to use structured and semi-structured interviews. Clinicians often feel that structured interviews impinge upon their professional autonomy and believe that structured interviews will damage rapport, even though semi-structured interviews have been shown to increase rapport. Clients prefer structured approaches, feeling that the clinician has a more comprehensive understanding of the client’s needs. Clinicians prefer to use their expertise, insight, and judgment. However, their judgment is what results in lower reliability and validity. Unstructured interviews are much more widely used in practice despite the advantages of structured interviews in terms of reliability and validity, because clinicians tend to prefer them. Clinicians using unstructured approaches tend to diagnose fewer conditions than structured interviews detect. This likely reflects confirmation bias and diagnostic overshadowing. 17.3 Other Findings Regarding Interviews There are other notable patterns regarding clinical interviews. Inter-rater reliability of Diagnostic and Statistical Manual of Mental Disorders (DSM) diagnoses is better for some conditions than others, but in general it is not very good. This provides further evidence that DSM-defined diagnostic categories are not “real” in a phenomenological sense. Moreover, there is considerable evidence that these diagnostic categories are better conceptualized as dimensional than categorical (Markon et al., 2011). Therefore, a better understanding of the structure of psychopathology may lead to a more valid diagnostic system. Low inter-rater reliability of interviews could be a result of either low sensitivity or low specificity. Even if one is high but the other is low, you will get low inter-rater reliability. It is worth noting that the clinical interview is not merely a tool for assessment. A clinical interview can be used as a form of intervention (i.e., therapeutic interview). For instance, motivational interviewing is a therapeutic interview that is often used as a treatment for substance use. 17.4 Best Practice for Diagnostic Assessment There are several best practice approaches for conducting diagnostic assessment. As discussed earlier, when possible, it is best to follow the LEAD standard: longitudinal, expert, all data. Have the assessment conducted by an expert or multiple experts in the domain(s) of interest. Have multiple people who are experts on the client (e.g., parent, teacher, friend, sibling, self) report on the person’s behavior. Use rating scales to generate the contending hypotheses, and then pick structured or semi-structured interview modules to use for follow-up to examine and rule out hypotheses. Incorporate multiple forms of information, e.g., interviews, rating scales, observations, objective/direct/performance-based measures, and school/work/legal records. If possible, include an observational measure and an objective/direct/performance-based measure of diagnostic-relevant behavior. Examples of observational measures include parent–child play in the clinic paired with a cleanup command to see how parents give commands, how children respond to the parent’s commands, and how parents respond to their child’s noncompliance. Examples of more objective measures could include biological assessments, such as polysomnography, or performance-based assessments, such as intelligence tests and academic achievement tests. There are many important skills when conducting a clinical interview, including: Taking a nonjudgmental stance Active listening Empathy Authenticity Effective use of silence Paraphrasing Summarizing Providing a sense of hope 17.5 The DSM and ICD The Diagnostic and Statistical Manual of Mental Disorders (DSM) provides the list of mental disorders and the diagnostic criteria for mental health treatment providers. It is published by the American Psychiatric Association and is used in the United States. The equivalent list of mental disorders and diagnostic criteria for mental health providers outside of the United States is the International Classification of Diseases (ICD). However, the ICD also includes diseases that are not considered mental disorders. Collectively, the DSM and ICD represent the diagnostic system for mental disorders. A goal of DSM and ICD is to define different “mental disorders” to make sure people get the services they need. 17.5.1 Strengths There are several strengths of the DSM and ICD. First, they can facilitate communication about disorders with other mental health providers; they provide a common language to describe complex behavioral presentations. Second, ideally, the DSM and ICD also guide treatment selection, especially if people are homogeneous within a disorder in terms of their symptoms, course, etiology, and treatment response. Third, diagnosis is used to justify payment for services from third-party payers (e.g., insurance companies, government). Fourth, diagnosis can be normalizing and empowering to some clients. It may help people learn that other people are experiencing similar challenges, and it may give them hope that there is something that can be done to address it. Fifth, the DSM/ICD promotes research in psychopathology, in terms of epidemiology (the disorder distribution in the population), etiology (causes of the disorder), course (how the disorder plays out over time), and treatment (including development and evaluation of interventions). 17.5.2 Concerns However, there are also key concerns with the DSM and ICD. One potential concern with the DSM and ICD is stigmatization. The goal of the DSM/ICD is not to label people. Labeling people can be an unfortunate consequence, however, and it can be stigmatizing because mental disorders often carry a stigma. A second concern is that we may be pathologizing normality. Mental disorders are not infrequent. Estimates of lifetime prevalence of mental disorders are around 75% (Schaefer et al., 2017). That is, three out of four people will experience a mental disorder at some point in their life. Thus, abnormality as defined by the DSM and ICD are normal. Another concern with the DSM and ICD is that they medicalize and pathologize problems in living as mental illnesses, and they obscure the role of environmental factors such as poverty (Gambrill, 2014). Moreover, diagnoses specified in the DSM and ICD ignore causes and etiology—they do not provide explanations for behavior (Fried, 2022). Ideally, a diagnostic system should have validity and utility. For instance, the diagnoses in a diagnostic system should have construct validity. That is, the diagnostic categories should reflect truth, reality, or the existence of the construct (i.e., diagnostic validity). Diagnostic validity is the extent to which the diagnostic category accurately captures the abnormal phenomenon of interest. Typical indicators of diagnostic validity are homogeneity (similarity) across people receiving a given diagnosis in terms of etiology, course, treatment response, etc. Utility means that the diagnostic system should help clinical decision-making, in terms of ease of use (e.g., saving time), facilitating communication with others, and in treatment planning. However, diagnostic systems (DSM and ICD) have serious concerns with both validity and utility. The diagnostic system has serious concerns with validity, in terms of whether the diagnostic categories defined by the DSM/ICD are actually real. There is great heterogeneity (variability) between people with the same diagnosis, in terms of symptom profiles, etiologies, course, and treatment outcomes (Fried, 2022). For instance, there are over 600,000 ways in terms of differing symptom profiles that a person can meet criteria for post-traumatic stress disorder (Galatzer-Levy &amp; Bryant, 2013). As another example, two people diagnosed with conduct disorder can share zero symptoms. Subtypes can attempt to address this problem, but there is little support for the validity of these subtypes. Many disorder subtypes and features are not based on empirical data—instead, they are based on expert judgment, consensus, and politics. Moreover, which diagnoses a person experiences at a given time do not portend the types of diagnoses they will experience in the future—people show considerable diagnosis switching from one disorder to another (and even across diagnostic families—e.g., from an externalizing disorder to an internalizing or thought disorder) across development (Caspi et al., 2020). There is also strong comorbidity (co-occurence) across disorders. A person who meets criteria for one disorder is likely to also meet criteria for other disorders (Caspi et al., 2020). Many disorders often co-occur within an individual, including major depressive disorder and generalized anxiety disorder, which suggests that they may share an underlying liability and do not reflect distinct categories. This complicates treatment and challenges the validity of the diagnostic system. Another challenge is regarding the coverage of the diagnostic system, including the number of categories and which are represented. How many disorders/categories should there be? Do we have too many categories? There are hundreds of possible diagnoses, including 541 disorders in the DSM-5 (Blashfield et al., 2014). Do we have too few diagnoses? Clinicians often use “Not Otherwise Specified” or “Unspecified” for diagnoses, which suggests that the current diagnostic categories do not cover the variability of clients’ presentations. How thinly should we “split” the categories? Some people are “splitters” and tend to split categories into as many categories as possible. Other people are “lumpers” and tend to lump categories together so there are fewer categories. The diagnostic system also has serious concerns with utility, in terms of lengthy criterion sets that are time-consuming and difficult to assess in practice (Mullins-Sweatt &amp; Widiger, 2009). One possible way to address the time-consuming nature of using lengthy criterion sets is to use prototypal matching instead of criterion sets. Prototypal matching involves having the clinician rate the similarity of the client to a narrative description of a prototypical person with the disorder. However, prototypal matching could result in lower inter-rater reliability (compared to using criterion sets) because of idiosyncratic judgments. There is a tradeoff between assessment time and validity. The clinician’s time can be saved by using computerized interviews or computerized tests. Another way to save the clinician’s time is to use self-report inventories as a screening device followed by an interview that is specific to the most relevant conditions identified by the screen. The diagnostic system includes arbitrary thresholds that do not facilitate social and clinical decision-making. Current diagnostic categories do not provide much utility in terms of determining the appropriate treatment. No single biological substrate has been identified for any psychological disorder that has high enough sensitivity and specificity to be useful for diagnosis or for predicting response to treatment (Tiego et al., 2023). This is because the DSM/ICD-based psychological disorders are fictive categories. The DSM/ICD-defined categories do not exist in nature. This does not dismiss the importance of what people are experiencing, which is very real. A person’s experience of depression is real, but the construct of major depressive disorder—as defined by the DSM/ICD—is not real. Mental disorders are not a concrete thing. Psychological disorders are social constructs. Mental disorders are not something that people “have”; rather, they are something that people “do” or “experience”. The mental disorders in the DSM/ICD are merely a description of behaviors. They are fuzzy concepts with vague boundaries. They are subjective, often without a clear “yes” or “no” as to whether someone meets criteria for a particular disorder. Mental disorders are defined in relation to cultural, social, and familial norms and values. That is, they are socially defined not biologically. The boundaries between normality and abnormality vary across cultures. Thus, the diagnostic categories in the DSM/ICD may not involve “biological pathology”—they do not carve nature at its joints. Researchers have examined biology-related differences in people with versus without mental disorders, and they have found some differences, but these biology-related differences are not able to classify perfectly because the categories are defined socially in terms of behaviors, and not in terms of biology. Another concern with the DSM and ICD is their conceptualization of psychopathology as binary. The DSM and ICD treat most disorders as categorical phenomena that are binary in nature: either you have a disorder or you do not. There is no gray in this conceptualization of psychopathology. However, research has shown that the difference between “normal” and “abnormal” behavior frequently is one of degree rather than kind (Markon et al., 2011). Thus, a dimensional approach to classification may provide more valid portrayal of many clinical phenomena than the categorical approach used by the DSM and ICD. There is also potential for bias in the diagnostic system or its application, especially against people with particular disorders. For instance, some therapists will not treat people with particular disorders, especially certain types of personality disorders (e.g., borderline personality disorder, antisocial personality disorder), because of perceived challenges. The diagnostic system also might overestimate abnormality in women (e.g., premenstrual dysphoric disorder) or racial/ethnic minorities. In general, reliability of diagnoses is low. The inter-rater reliability of diagnoses tends to be low (Lobbestael et al., 2011). Moreover, test–retest reliability is also relatively low. A client might cycle back and forth between diagnosis and no diagnosis based on one or a few symptoms. For instance, major depressive disorder is given a diagnosis with 5 or more symptoms, but no diagnosis is given with 4 or fewer symptoms. The fluctuation of symptoms and disorder status across time suggests that a binary approach to diagnosis is inaccurate. It will thus be important to improve our diagnostic system. For instance, it will be important for the diagnostic system to account for the dimensional nature of constructs so that the diagnostic system better carves nature at its joints (better validity) and is flexible to allow different optimal thresholds for different social and clinical decisions (better utility). One argument against dimensional diagnostic systems is that treatment decisions are often binary (e.g., whether or not to provide treatment). However, decisions in practice can be dimensional too, and can reflect different levels of treatment, including, for example, the dosage of medication, the frequency and intensity of therapy, and the degree of hospitalization. 17.5.3 Alternative Structures of Psychopathology Emerging evidence suggests that alternative models of psychopathology may provide a better fit to its structure. Models that have captured the greatest attention include hierarchical (higher-order) models of psychopathology, including the p-factor (Caspi et al., 2014; Smith et al., 2020) and the hierarchical taxonomy of psychopathology [HiTOP; Kotov et al. (2017); Kotov et al. (2021)]. The p-factor is the general factor of psychopathology, akin to the general factor of intelligence (g), and it attempts to account for the fact that many forms of psychopathology show strong covariation and co-occurrence. The p-factor (Caspi et al., 2014; Smith et al., 2020) is a hierarchical model of psychopathology in which the p-factor subsumes three lower-order dimensions of psychopathology: internalizing problems (e.g., depression, anxiety, obsessive-compulsive disorder), externalizing problems (e.g., conduct disorder, ADHD), and thought-disordered problems (e.g., autism, schizophrenia). In the HiTOP model (Kotov et al., 2017, 2021), a p-factor influences super spectra, including emotional dysfunction, psychosis, and externalizing problems. Emotional dysfunction subsumes somatoform and internalizing problems (e.g., sexual problems, eating pathology, fear, distress, and mania). Psychosis subsumes thought disorder and detachment. Externalizing problems are subdivided into disinhibited externalizing and antagonistic externalizing. Both disinhibited and antagonistic externalizing subsume antisocial behavior, whereas disinhibited externalizing subsumes heavy substance use. A third alternative conceptualization of psychopathology is the National Institute of Mental Health (NIMH) Research Domain Criteria (RDoC). RDoC is described in Chapter 20. Another emerging technique is network approaches that model the covariation among symptoms (McNally, 2021). 17.6 Conclusion Interviews can be administered (a) in a free-flowing way as unstructured interviews, (b) in a way that asks questions in a pre-defined way with a pre-defined order and pre-defined scoring criteria as structured interviews, or (c) as semi-structured interviews that blend structured interviews with freedom to ask follow-up questions. In general, reliability and validity improves as the degree of structure for an interview increases. The Diagnostic and Statistical Manual of Mental Disorders (DSM) and the International Classification of Diseases (ICD) provide the list of mental disorders and the diagnostic criteria for mental health treatment providers. The DSM and ICD have potential strengths, including (ideally) facilitating communication, guiding treatment selection, providing justification for payment for services, providing a normalizing and empowering effect for some clients, and promoting research in psychopathology. However, there are key concerns of the DSM and ICD, including concerns with stigmatization, pathologizing normality, poor coverage, binary classification, obscuring environmental factors, no biological criteria, potential bias, and low reliability, validity, and utility. There are alternatives to the DSM and ICD for conceptualizing psychopathology. Alternative structures of psychopathology include hierarchical structures such as the p-factor and the hierarchical taxonomy of psychopathology (HiTOP). 17.7 Suggested Readings Sommers-Flanagan &amp; Sommers-Flanagan (2016) References Blashfield, R. K., Keeley, J. W., Flanagan, E. H., &amp; Miles, S. R. (2014). The cycle of classification: DSM-I through DSM-5. Annual Review of Clinical Psychology, 10(1), 25–51. https://doi.org/10.1146/annurev-clinpsy-032813-153639 Caspi, A., Houts, R. M., Ambler, A., Danese, A., Elliott, M. L., Hariri, A., Harrington, H., Hogan, S., Poulton, R., Ramrakha, S., Rasmussen, L. J. H., Reuben, A., Richmond-Rakerd, L., Sugden, K., Wertz, J., Williams, B. S., &amp; Moffitt, T. E. (2020). Longitudinal assessment of mental health disorders and comorbidities across 4 decades among participants in the Dunedin Birth Cohort Study. JAMA Network Open, 3(4), e203221–e203221. https://doi.org/10.1001/jamanetworkopen.2020.3221 Caspi, A., Houts, R. M., Belsky, D. W., Goldman-Mellor, S. J., Harrington, H., Israel, S., Meier, M. H., Ramrakha, S., Shalev, I., Poulton, R., &amp; Moffitt, T. E. (2014). The p factor: One general psychopathology factor in the structure of psychiatric disorders? Clinical Psychological Science, 2(2), 119–137. https://doi.org/10.1177/2167702613497473 Faraone, S. V., &amp; Tsuang, M. T. (1994). Measuring diagnostic accuracy in the absence of a “gold standard.” American Journal of Psychiatry, 151, 650–657. https://doi.org/10.1176/ajp.151.5.650 Fried, E. I. (2022). Studying mental health problems as systems, not syndromes. Current Directions in Psychological Science, 31(6), 500–508. https://doi.org/10.1177/09637214221114089 Galatzer-Levy, I. R., &amp; Bryant, R. A. (2013). 636,120 ways to have posttraumatic stress disorder. Perspectives on Psychological Science, 8(6), 651–662. https://doi.org/10.1177/1745691613504115 Gambrill, E. (2014). The diagnostic and statistical manual of mental disorders as a major form of dehumanization in the modern world. Research on Social Work Practice, 24(1), 13–36. https://doi.org/10.1177/1049731513499411 Kotov, R., Krueger, R. F., Watson, D., Achenbach, T. M., Althoff, R. R., Bagby, R. M., Brown, T. A., Carpenter, W. T., Caspi, A., Clark, L. A., Eaton, N. R., Forbes, M. K., Forbush, K. T., Goldberg, D., Hasin, D., Hyman, S. E., Ivanova, M. Y., Lynam, D. R., Markon, K., … Zimmerman, M. (2017). The hierarchical taxonomy of psychopathology (HiTOP): A dimensional alternative to traditional nosologies. Journal of Abnormal Psychology, 126(4), 454–477. https://doi.org/10.1037/abn0000258 Kotov, R., Krueger, R. F., Watson, D., Cicero, D. C., Conway, C. C., DeYoung, C. G., Eaton, N. R., Forbes, M. K., Hallquist, M. N., Latzman, R. D., Mullins-Sweatt, S. N., Ruggero, C. J., Simms, L. J., Waldman, I. D., Waszczuk, M. A., &amp; Wright, A. G. C. (2021). The hierarchical taxonomy of psychopathology (HiTOP): A quantitative nosology based on consensus of evidence. Annual Review of Clinical Psychology, 17(1), 83–108. https://doi.org/10.1146/annurev-clinpsy-081219-093304 Lilienfeld, S. O., Sauvigne, K., Lynn, S. J., Latzman, R. D., Cautin, R., &amp; Waldman, I. D. (2015). Fifty psychological and psychiatric terms to avoid: A list of inaccurate, misleading, misused, ambiguous, and logically confused words and phrases. Frontiers in Psychology, 6. https://doi.org/10.3389/fpsyg.2015.01100 Lobbestael, J., Leurgans, M., &amp; Arntz, A. (2011). Inter-rater reliability of the Structured Clinical Interview for DSM-IV Axis I Disorders (SCID I) and Axis II Disorders (SCID II). Clinical Psychology &amp; Psychotherapy, 18(1), 75–79. https://doi.org/10.1002/cpp.693 Markon, K. E., Chmielewski, M., &amp; Miller, C. J. (2011). The reliability and validity of discrete and continuous measures of psychopathology: A quantitative review. Psychological Bulletin, 137(5), 856–879. https://doi.org/10.1037/a0023678 McNally, R. J. (2021). Network analysis of psychopathology: Controversies and challenges. Annual Review of Clinical Psychology, 17(1), 31–53. https://doi.org/10.1146/annurev-clinpsy-081219-092850 Mullins-Sweatt, S. N., &amp; Widiger, T. A. (2009). Clinical utility and DSM-V. Psychological Assessment, 21(3), 302–312. https://doi.org/10.1037/a0016607 Schaefer, J. D., Caspi, A., Belsky, D. W., Harrington, H., Houts, R., Horwood, L. J., Hussong, A., Ramrakha, S., Poulton, R., &amp; Moffitt, T. E. (2017). Enduring mental health: Prevalence and prediction. Journal of Abnormal Psychology, 126(2), 212–224. https://doi.org/10.1037/abn0000232 Sharp, K. L., Williams, A. J., Rhyner, K. T., &amp; Ilardi, S. S. (2013). The clinical interview. In K. F. Geisinger, J. F. Carlson, J.-I. C. Hansen, N. R. Kuncel, S. P. Reise, &amp; M. C. Rodriguez (Eds.), APA handbook of testing and assessment in psychology, Vol. 2: Testing and assessment in clinical and counseling psychology (pp. 103–117). American Psychological Association. Smith, G. T., Atkinson, E. A., Davis, H. A., Riley, E. N., &amp; Oltmanns, J. R. (2020). The general factor of psychopathology. Annual Review of Clinical Psychology, 16(1), 75–98. https://doi.org/10.1146/annurev-clinpsy-071119-115848 Sommers-Flanagan, J., &amp; Sommers-Flanagan, R. (2016). Clinical interviewing. Wiley. Sullivan, H. S. (1970). The psychiatric interview. Norton. Summerfeldt, L. J., Kloosterman, P. H., &amp; Antony, M. M. (2010). Structured and semistructured diagnostic interviews. In M. M. Antony &amp; D. H. Barlow (Eds.), Handbook of assessment and treatment planning for psychological disorders (2nd ed., pp. 95–137). Guilford Press. Tiego, J., Martin, E. A., DeYoung, C. G., Hagan, K., Cooper, S. E., Pasion, R., Satchell, L., Shackman, A. J., Bellgrove, M. A., Fornito, A., Abend, R., Goulter, N., Eaton, N. R., Kaczkurkin, A. N., &amp; and, R. N. (2023). Precision behavioral phenotyping as a strategy for uncovering the biological correlates of psychopathology. Nature Mental Health, 1, 304–315. https://doi.org/10.1038/s44220-023-00057-5 Widiger, T. A. (2002). Personality disorders. In M. M. Antony &amp; D. H. Barlow (Eds.), Handbook of assessment and treatment planning for psychological disorders (pp. 453–480). Guilford Publications. "],["objective-personality.html", "Chapter 18 Objective Personality Testing 18.1 Overview of Personality Tests 18.2 Example of an Objective Personality Test: MMPI 18.3 Problems with Objective True/False Measures 18.4 Approaches to Developing Personality Measures 18.5 Measure Development and Item Selection 18.6 Emerging Techniques 18.7 The Flawed Nature of Self-Assessments 18.8 Observational Assessments 18.9 Structure of Personality 18.10 Personality Across the Lifespan 18.11 Conclusion 18.12 Suggested Readings", " Chapter 18 Objective Personality Testing 18.1 Overview of Personality Tests There are two main types of personality tests: objective personality tests and projective personality tests. Of course, no measure is truly “objective” but some measures are more or less so. In a so-called objective personality test (or structured personality test), a stimulus is presented to a respondent, who makes a closed-ended (constrained) response, such as True/False or Likert ratings. Examples of objective personality or symptomatology tests include the Minnesota Multiphasic Personality Inventory (MMPI) and the Beck Depression Inventory (BDI). In a projective personality test, an ambiguous stimulus is presented to a respondent, who is asked to make an open-ended response. Examples of projective personality tests include the Rorschach Inkblot Test and the Thematic Apperception Test (TAT). Projective tests have largely fallen by the wayside now, but it is still helpful to think about their potential advantages. Projective tests are described in further detail in Chapter 19. 18.1.1 Projective Personality Tests In a projective personality test, the client’s response is not limited. Projective personality tests were designed from a psychodynamic perspective, and they are supposed to have limitless variability and therefore a freer access to the client’s internal world. However, Card V of the Rorschach Inkblot Test looks like a moth or a bat, and around 90% of respondents likely give that response (Wiggins, 1973), so projective tests are not completely limitless. Projective personality tests are designed to have ambiguous content. This is, in part, to make them hard to figure out what is being assessed. That is, they tend to have low face validity. However, even so-called objective personality tests can have items that are ambiguous and that function similarly to a projective test. For instance, one of the items on the original MMPI asks respondents whether they like mechanics magazines. But in modern times, many people have never looked at a mechanics magazine. So, it becomes like a Rorschach question because the client starts to think about other factors. Scoring of projective tests does not rely on the client’s insight, so projective tests might get past social desirability of clients’ responses and potential defensiveness. Therefore, projective tests are potentially difficult to fake. Faking good means to present oneself as better (or in a more positive light) than one actually is, whereas faking bad means to present oneself as worse (or in a more negative light) than one actually is. Faking or feigning responses often happens for incentives due to external reasons. For instance, a client may want to fake good on a test if it allows them to get a job or to win custody of a child. By contrast, a client may want to fake bad to receive disability insurance or to be pronounced not guilty by reason of insanity. As of 1995, the TAT and Rorschach were the #5 and #6 most widely used assessments, respectively, by clinical psychologists (Watkins et al., 1995). However, they and other projective techniques have lost considerable ground since then. 18.1.2 Objective Personality Tests In contrast to projective personality tests, objective personality tests, the client’s responses are substantially constrained to the possible answers. Moreover, objective personality tests tend to be cheap and fast to administer, and they can be scored by computers now. In addition, objective tests have more reliable scoring than projective tests. It takes a very long time to score the Rorschach Inkblot Test, and it still has very low reliability. With an objective test, by contrast, scoring reliability approaches perfection. The earliest examples of objective personality tests were the MMPI and the Strong Vocational Interest Blank. The MMPI assesses personality, whereas the Strong Vocational Interest Blank assessed preferences for different jobs or professions. An overview of objective personality tests is provided by Wiggins (1973). 18.2 Example of an Objective Personality Test: MMPI The MMPI is an example of a so-called “objective” personality test. But the MMPI and other objective personality tests are not truly objective. Consider an example of a True/False item from the MMPI: “I hardly ever notice my heart pounding and I am seldom short of breath”. The item is intended to assess somatic symptom disorder, with a response of “false” being more indicative of disorder. But there are other factors that could influence a person’s response to the question besides whether the person has somatic symptom disorder. Figure 18.1 depicts how a person’s response to the question could be influenced by several factors. A comorbidity of somatic symptom disorder could make it more likely that a person answers with a response of “false”, in line with somatic symptom disorder. In addition, a person’s insight ability may influence whether they answer with a response of “false”. Also, people who exercise more may experience a pounding heart and shortness of breath more frequently (i.e., while exercising), and therefore answer with a response of “false”. In sum, the item and the test as a whole is clearly not “objective” because constructs in the questions are not clearly defined. What does a “pounding heart” mean? How much is “hardly ever”? People may define “seldom” differently. Figure 18.1: Various Factors That Could Influence a Respondent’s Answer to the True/False Question: “I hardly ever notice my heart pounding, and I am seldom short of breath”. There are multiple forms of measurement error for a given item. For instance, there are situational sources of measurement error. Consider the item “I hardly ever notice my heart pounding, and I am seldom short of breath”. For example, how many stairs the participant climbed to get to the lab could influence their response to the question. Other situational factors could include whether they are dehydrated, whether they have a cold or are sick, and what they did over the past week. Another form of measurement error could result from the purpose of test taking. It is important to consider the purpose of the assessment. For example, if the assessment is for disability payments or to get a physical job, such as a firefighter, it may change a person’s answers. Response biases, such as social desirability, could also influence a person’s answers. In addition, memory limitations of the person could influence their responses. There have been multiple versions of the MMPI. J. R. Graham et al. (2022) provide an overview of how the MMPI was developed. The original MMPI was developed using the external approach to scale construction. The second version of the MMPI (MMPI-2) placed greater emphasis on content validity of the items, on removing items with outdated or offensive language, and on updating the norms to be more representative. Later, restructured versions of the MMPI-2 were created, which became known as the MMPI-2-Restructured Forms (MMPI-2-RF). Evidence on the MMPI-2-RF is reviewed by Sellbom (2019). The latest version of the MMPI is the MMPI-3. 18.3 Problems with Objective True/False Measures There are a number of problems with objective True/False measures. However, the problems are not necessarily unique to objective personality measures or to True/False measures. One problem is related to the response biases of acquiescence and disacquiescence. Acquiescence occurs when the person agrees in response to items regardless of item content. Acquiescence occurs oftentimes when the participant is just going along because they think the experimenter may want them to have certain characteristics, therefore they often just say “TRUE” a lot. That is, they may want to please the investigators. Disacquiescence, by contrast, is when the person disagrees in response to the items regardless of item content. Disacquiescence is also called opposition bias. Disacquiescence may occur if the person does not think they have the disorder or problem of the characteristics being explored. True/False measures, just like other questionnaire formats, are influenced by multiple sources of variation. In an objective personality test, there is a high demand on respondents. For instance, on the MMPI, respondents must be aware of recall over an unspecified period of time, which can cause confusion. The high demand on respondents can lead to inaccuracy of individuals’ reading of the items. Additional sources of information could influence the respondents’ answers to the item, “I hardly ever notice my heart pounding, and I am seldom short of breath”. Some respondents may be intoxicated while answering questions, which could influence physiological experiences. Some respondents may have comorbordities that influence their responses. Psychological and medical comorbidities could cause difficulty in interpreting the questions. For example, the respondent may wonder whether the investigator is inquiring about heart beating that reflects anxiety versus heart beating that reflects high blood pressure. In addition, there are effort differences that influence people’s responding. There has been a recent resurgence in how to assess respondents’ effort. There are often limits to predicting phenomena due to individuals not putting effort into answering questions. Other potential sources of variation in an item could include psychopathology, stress, physical fitness, age, and gender. For instance, older people are more likely to notice shortness of breath because their lung capacity is less than what it used to be. Moreover, women are more likely to say “FALSE” to this item than men, but the item has less social desirability bias than some other items, so women could be putting in more effort and giving different responses than men. In sum, lots of factors beyond the construct of interest can influence a response on an item. It is valuable to scan through your measures and consider what goes into a person’s answers. It is important to keep the content in mind of all things contributing to scores, and you should consider these potential factors when interpreting results! Another problem with some questions is compound questions, also called double-barreled questions. Double-barreled questions following the structure: \\(\\text{T/F: X} + \\text{Y}\\). An example of a compound question is, “True or False: Cars should be faster and safer”. It is unclear which components of a compound question people are responding to. In conclusion, every objective test is partly projective—that is, the stimuli are interpreted in different ways by different people. 18.4 Approaches to Developing Personality Measures There are three primary approaches to developing personality measures and other scales: An external approach to scale construction, also called an empirical approach or criterion-keyed approach A deductive approach to scale construction, also called a rational, theoretical, or intuitive approach An inductive approach to scale construction, also called an internal or item-metric approach However, these approaches are not mutually exclusive and can be combined. 18.4.1 External Approach to Scale Contruction The external approach to developing a personality measure is also called the empirical approach or the criterion-keyed approach. The external approach relies on an external criterion. In general, a criterion-keyed approach examines scores on items in relation to the criterion, and selects items that are associated with the criterion, regardless of the item content. The MMPI is an example of a scale that was developed using the external approach. For the MMPI, the criterion was a group (i.e., a criterion group approach), or more accurately, multiple groups: patients with different disorders and controls. The original MMPI was not developed based on theory (e.g., theoretical understanding of the construct of depression); instead, it was developed based on items’ empirical associations with a criterion group. Because an external approach relies on an external criterion, if people lose interest in the scale criterion, the scale loses interest. Consider the development of the MMPI as an example of a measure that was developed using the external approach to scale construction. The developers grouped people together based on their criterion status, for example one group of people with schizophrenia and a control group that does not have mental disorders. How did they develop the test? The idea of the external approach is to let nature decide what goes into the test. So, the MMPI developers sampled thousands of questions very broadly from pre-existing questionnaires of personality, symptoms, etc. Then, they had the criterion groups (e.g., schizophrenia) and control groups answer the questions. They examined the item responses to determine which items discriminate between groups (i.e., which items are associated with criterion group status). And if an item(s) is good at discriminating between the criterion and control groups, the items are selected for the measure. It was purely empirical business—i.e., dustbowl empiricism. In sum, using an external approach, item selection depends on the discriminatory power of each item to inform about an external criterion of interest. Such an approach does not require having any theoretical assumptions about item functioning. 18.4.1.1 Pros There are several pros of using the external approach to develop measures: You do not need to know anything—it requires no theory or theoretical knowledge about disorders and their etiology There is likely to be some generalizability of the utility of the measure in the future because there is some carryover of criterion-related validity The measure has some practical utility given its relation to criteria of interest “Subtle items” with poor face validity can be selected solely based on their discriminatory power. Subtle items are items that neither you nor the respondent predicted would show differences between groups. Subtle items provide an advantage that the data are moving beyond our ignorance. It is also an advantage that clients cannot fake subtle items as well as they can more obvious items. 18.4.1.2 Cons There are also cons of using the external approach to develop measures: Measures developed using the external approach have lower content validity and/or face validity because they include subtle items. For example, the original version of the MMPI had low content validity and face validity. Using the external approach, it is possible to construct a scale that makes no sense due to lack of consideration of face validity. The success of the external approach depends highly on the quality of the criterion and control groups—if a criterion falls out of favor as an indicator of the construct (or interest fades), then the utility of the test decreases because it is no longer applicable. Items will not always generalize because the generalizability depends on the representativeness of the sample and the quality of the groups—if you select poor groups, it is not a representative sample, which results in a biased measure. The validity of the scale depends on the representativeness of your groups in regard to the criterion of interest. It is possible that your findings might not be generalizable if your sample is not—this is especially problematic when you rely only on data and not on theory. An example of the importance of the representativeness of the sample comes from the original MMPI. The norms of the original MMPI were based on largely White participants from Scandinavian, German, and Irish descent in the Midwestern U.S., with an average of an 8th grade education. The norms became known as “Minnesota farmers”. Therefore, “Minnesota normals” (i.e., the control group) were pretty “dull” normals. In addition, to be included in the norms, the respondents for the MMPI had to be waiting in the hospital (not everyone does that for family in the hospital) and had to be kind enough to take a 4–5 hour measure for psychologists (not all would do this). For all of these reasons, there were issues with poor generalizability of the original MMPI norms in the broader population. A scale is most likely to be valid when it is used with similar populations and in similar conditions. Shrinkage often occurs when using a measure developed using an external approach. As described in Section 10.4, shrinkage is when variables with stronger predictive power in the original data set tend to show somewhat smaller predictive power (smaller validity coefficients) when applied to new groups. When variables are selected empirically, they tend to show less predictive power (smaller validity coefficients) when applied to new groups during cross-validation. Shrinkage reflects a model over-fitting, because it is somewhat capitalizing on chance in selecting items. Many subtle items may be instances of Type I error (false positives). Shrinkage is especially likely when the original sample is small and/or unrepresentative and the number of variables considered for inclusion is large. Externally developed measures also have a problem of communicability (Burisch, 1984). For something to have meaning to others, it should have connection to constructs, which may not be true for many measures developed using the external approach. It can take a long time to develop measures using the external approach, and they tend to be longer to administer because of having more items. For instance, there are 567 items in the MMPI-2. 18.4.1.3 MMPI Examples Paul Meehl wrote his dissertation to develop the K scale of the MMPI to attempt to detect faking good. In the context of the MMPI, faking good would involve under-reporting of symptoms. He developed the K scale based on all positive qualities that around half of people typically endorse. So, it is not obvious that the items reflect faking good, and if someone is trying to respond in a socially desirable way, they endorse more of these positive qualities. There is also a “faking bad” version of the scale, too. The “faking bad” (F) scale was developed as an attempt to detect malingering (over-reporting of symptoms). However, the MMPI could pathologize normality in some cases. Some psychotic patients may have been identified as “faking bad” because they actually have had a lot of odd experiences. And very healthy people may have gotten higher score on “faking good”, but they just may be very positive and well-adjusted. As examples from the original MMPI from 1940, male teenagers tended to have elevated scores on the psychopathy and mania scales. Graduate students, including female graduate students, tended to have higher “masculine” scores on the Masculinity/Femininity (Mf) scale. This shows that non-clinical samples can still have “clinical” scores. In the 1930s–1940s, there was an emphasis on developing empirically based measures, where the researchers rely on data, not theory as had previously been emphasized. 18.4.2 Deductive Approach to Scale Construction The deductive approach to developing a personality measure is also called a rational, theoretical, or intuitive approach. Using a deductive approach, the choice and definition of constructs precede and govern the formulation of items. Item pools are generated using theoretical considerations, and item selection depends on possessing a rich theoretical knowledge about the construct and selecting which items assess the construct the best. In a deductive approach, the measure developer deduces the content; they do not rely on criterion data to select the content. Deducing the content involves thinking and talking about the construct, and having experts write items that they think would do well in eliciting information about the construct. The measure developer deduces from the construct which items to use. Therefore, the deductive approach completely depends on our ability to understand a given construct and translate this understanding to the generation of item content that will be understood by the examinees in such a way that it elicits accurate ratings for the construct of interest. Most assessments are developed using the deductive approach. 18.4.2.1 Pros There are several pros of using the deductive approach to develop measures: Using the deductive approach is fast, easy, and short. Generating such scales requires few people, and is often fast and accurate. It does not typically take as much time to develop a measure using the deductive approach. Moreover, it allows the possibility of short scales that are quick to administer. By contrast to short scales developed by the deductive approach, the MMPI (developed using the external approach) is very long. Measures developed (well) using the deductive approach are always content valid because of the reliance on theory—items tend to be prototypical of the construct. Measures developed using the deductive approach are usually face valid, more likely than external approach. Face validity is often an advantage, but not always. Face validity can help with disseminability because others may be more likely to adopt it if it appears to assess what it claims to assess. However, face validity is not desirable when trying to prevent faking good or faking bad. Measures developed using the deductive approach tend to have better communicability—i.e., how comprehensible the information communicated is to the examiner based on the responses. 18.4.2.2 Cons There are also cons of using the deductive approach to develop measures: If your theory or understanding is wrong, your scale will be wrong! Additionally, if the construct itself is vague, and there is overlap between constructs, scales may be difficult to differentiate between them, making it difficult to establish discriminant validity. Many theories and constructs overlap. Therefore, measures often overlap—even measures with very different names! For example, consider the Rosenberg Self-Esteem Scale and the Spielberger State–Trait Anxiety Inventory: Items on the Rosenberg Self-Esteem Scale include: “At times I think I am no good at all” “I certainly feel useless at times” “On the whole, I am satisfied with myself” Items on the Spielberger State–Trait Anxiety Inventory (STAI): “I lack self-confidence” “I feel inadequate” “I feel satisfied with myself” 18.4.3 Inductive Approach to Scale Construct The inductive approach to developing a personality measure is also called an internal or item-metric approach. The inductive approach is an empirical, data-driven approach for scale construction, in which scales are derived from the pre-existing internal associations between items. A large pool of items is selected, and scales are generated from the item pools based on the structure of the internal association between items. The inductive approach assumes that universal laws exist for personality structure, that is, that there is a natural structure. The hope is that personality has simple structure: that each item loads onto (i.e., reflects) one and only one factor. It is the hope of the inductive approach that there is simple structure because it makes the natural structure easier to detect using available methods. The inductive approach is empirical because the answers come from within the data. But the inductive approach differs from the external approach. In the inductive approach, the data come from the internal structure of the measure’s items. The empirical approach is also based on empirical data. Science involves prediction and both the inductive and empirical approach use prediction. The empirical approach examines how items predict some external criterion. By contrast, the inductive approach examines how items predict or relate to each other. The goal of the inductive approach is to “describe nature at its joints”. According to the inductive approach, once you understand constructs, you can understand how they are connected to each other. The external approach does not really care about the items themselves. In the inductive approach, you need to use theoretical knowledge to interpret findings. The deductive approach uses theory up front to make the scale. The inductive approach concerns itself with the items and what inferences can be made. The method helps group the large set of items into subscales based on clusters of items that covary most strongly with each other using factor analysis, and it drops items with a low item–total correlation, factor loading, or discrimination parameter. Factor analysis is used for the inductive approach to developing measures. It is used to evaluate the internal structure of a measure, and ideally, the structure of a construct. Factor analysis is considered to be a “pure” data-driven method for structuring data, but as noted in Section 14.1.4, the “truth” that we get depends heavily on the decisions we make regarding the parameters of our factor analysis. In sum, factor analysis is not purely inductive because the result is influenced by many decisions by the investigator. Even though the inductive approach (factor analysis) is empirical, theory and interpretability should also inform decisions. 18.4.3.1 Pros There are several pros of using the inductive approach to develop measures: The inductive approach yields estimates of associations between items and can arrive at estimates of a simple, homogeneous construct. You do not need to know much to use the inductive approach (relative to the deductive approach): just use the items you have and use a data reduction approach. The inductive approach can derive short, homogeneous scales. Then, you can see how constructs relate to other constructs. The inductive approach is a “purer” method of scale construction because it relies on the natural structure of the data, and no theoretical knowledge or validation to a criterion is required on the front end of scale development. The data are allowed to “speak for themselves”. 18.4.3.2 Cons There are also cons of using the inductive approach to develop measures: The inductivists (i.e., users of the inductive approach) hope that a simple structure exists within a set of items and that this structure can meaningfully differentiate between constructs. If there is no simple structure, interpretations of scales that emerge can be difficult. In addition, this approach is not “pure” because the structure we get depends on the analysis decisions we make. The answers you get depend on the decisions you make, and there really is no basis on which to make decisions. This is called indeterminacy. There are a number of decisions in factor analysis, including decisions such as the number of factors and the nature of factors—i.e., how to interpret them. Factor analysis is not straightforward, and depends on decisions made along the way. Therefore, some argue because so much is in the hands of the investigator that factor analysis is really a semi-empirical approach. SPSS likely contributes to the problem because it makes so many decisions for you, and many have no idea what they are doing! SPSS is for ease of use, but it is limiting. In SPSS, you can use principal component analysis (PCA) for item extraction. Investigators often determine how many components/factors to keep based on the criterion of keeping components with eigenvalues greater than 1, often use orthogonal rotation of data, etc. Factor analysis and PCA are described in Chapter 14. 18.4.4 Hybrid Approach The preceding discussion described the three primary approaches to developing objective personality measures. However, the approaches can be mixed. For instance, one could write a large set of items based on theory (i.e., the deductive approach) and then pick items to keep based on their criterion-related validity (i.e., the external approach), and group them into scales based on their internal structure (i.e., the inductive approach). There is not strong evidence for the superiority of any of the approaches compared to the others. 18.5 Measure Development and Item Selection Despite not having strong evidence for the superiority of any of the approaches to scale construction, best practices to measure development include: Start with theory to define the construct and create item pools, using a deductive approach. Be inclusive at this stage. Create more items than you will actually use—even if items are only tangentially related—so you have a broad pool of items. Include items of other constructs to establish the boundaries of the construct, i.e., discriminant validity. Then, test these item pools, and consider their empirical relations to revise and/or drop items. Ideally, you would test the items in large and heterogeneous samples that are representative of the population. Examine the items in relation to external criteria, using an empirical approach. Examine items in relation to each other, using an inductive approach. This likely involves factor analysis and/or item response theory. We want items, collectively, to span the full range of difficulty/severity of the construct in the target range of interest. It is important for the items to have accuracy (i.e., strong discrimination and information) in the target range of interest on the construct (e.g., low, medium, and/or high). The target range of interest depends on the purpose of the assessment, as described in Section 8.1.7.1. For example, items used for diagnosis should focus on higher levels of the construct, whereas items used for screening should identify those with elevated risk but might not need to discriminate at higher levels. For assessing individual differences, you would want items that discriminate across the full range, including at the lower end. Items should show some internal consistency, as evidenced by an inter-item or item–total correlation, but items should not be too highly inter-correlated. If items are too highly correlated, then they are redundant and do not provide unique information. Inter-item correlations should only be moderate, e.g., should range from approximately .15 to .50. But items should be highly correlated with the latent factor representing the target construct. That is, the items should have a high discrimination or a strong factor loading. Then, interpret the results and label the factors based on theory. Evaluate multiple aspects of reliability and validity of the scale. Other ideas in scale development are discussed by Burisch (1984), L. A. Clark &amp; Watson (1995), L. A. Clark &amp; Watson (2019), and Loevinger (1957). 18.5.1 The Response Scale Evidence suggests that there may not benefits of having more than six response options for likert-scale items that assess personality (Simms et al., 2019). 18.6 Emerging Techniques One emerging technique for developing personalized models of personality is the group iterative multiple model estimation (GIMME) model (Wright et al., 2019), as described in Section 23.5.5. 18.7 The Flawed Nature of Self-Assessments It is a common finding that people tend to over-estimate their skill and abilities—most people tend to describe themselves as “above average”, which is statistically impossible. People are over-confident, and they over-estimate the likelihood of achieving desirable outcomes and underestimate how long it will take to complete future projects. Self-report is only weakly associated with people’s actual behavior. There are there prominent ways in which self-assessment has been shown to be flawed: response bias ambiguity of items lack of insight by people to rate themselves 18.7.1 Response Bias One way that self-assessment has been shown to be flawed is in terms of response bias. Bias involves a systematic measurement error. For instance, it is not uncommon for participants to fake good or fake bad on assessments due to situational reasons or due to how questions are worded. When answering questions, many people desire to seem better than they are. This phenomenon is called social desirability bias, which is a form of method bias in which people systematically adjust their responses to reflect more socially desirable attributes. The degree of a person’s faking has been shown to be related to social desirability bias (Bensch et al., 2019). If you want to want to know the true prevalence of a given behavior, you can deal with social desirability bias using a randomized response model, as described in Section 4.10.3.1. There are indicators of response bias that are worth considering (Burchett &amp; Ben-Porath, 2019). 18.7.2 Ambiguity of Items Another way that self-assessment has been shown to be flawed is in the ambiguity of items. People tend to have a difficult time providing an accurate characterization of their skills on tasks that are poorly defined or ambiguous. For example, what does it mean to be a “warm” parent? Beyond this, even the language on self-assessments can be highly ambiguous. For example, how often is “rarely”? Such questions can systematically skew answers to self-assessments. 18.7.3 Lack of Insight by People to Rate Themselves A third way that self-assessment has been shown to be flawed is due to respondents’ lack of insight into the required information to make good self-assessments. People often make poor judgments because they lack the required skills and information necessary to have insight into their actual performance, or they neglect it when it is available. The lack of insight into poor judgments can also be caused by errors of omission—in which they do not know that they have made a mistake because they do not know what the best alternative would have been. Also, people infrequently get feedback from others on the constructs we are attempting to assess—as a result, their self-views are not informed by objective feedback. 18.7.4 Ways to Improve Self-Assessment Ways to improve self-assessment are described by Dunning et al. (2004): Use clear items that are behaviorally specific Provide frequent, timely, objective, and individualized feedback Use self-testing, following some delay of time after studying the material Review one’s past performance Use peer assessment Target the motivational basis of the over-confidence Benchmark—compare one’s performance against others’ performance Introduce “desirable difficulties” to instruction, such as spreading training over several sessions and varying the circumstances of the training. These challenges can harm the speed that students learn but leaves them better able to retain what they learned and to transfer it to new situations in the future. To account for over-confidence, add in safety factors and buffer time. For instance, add 30%–50% extra time to all time-completion estimates for projects. 18.7.5 Satisficing (Versus Optimizing) There have been new developments in gaining insight into the cognitive processes by which respondents generate answers to survey questions (Krosnick, 1999). 18.7.5.1 Optimizing Optimizing involves a respondent responding optimally to a question—i.e., responding in an unbiased and thorough manner. There are four cognitive steps or stages that respondents must complete to answer a question optimally: They interpret the question They search their memory for relevant information They integrate all relevant information into a single judgment They use that judgment to select a response The complexity of the cognitive processes one must engage in when giving an optimal answer requires a lot of cognitive effort. Giving such effort can happen for a variety of reasons: desire of self-expression, being altruistic, desire for gratification, etc. The extent to which such motivations inspire a person to engage in the cognitive requirements to respond to questions in an unbiased and thorough manner is referred to as optimization. Optimizing deals with how hard the person worked and how much they care about giving their best response. 18.7.5.2 Satisficing If, for some reason, a person is not motivated to expend the cognitive effort required to make an optimal response, and instead settle for a satisfactory response, they are said to be satisficing. That is, they are expending less effort that prevents obtaining the optimal answer. There are two types of satisficing: weak satisficing and strong satisficing. In weak satisficing, respondents execute all four cognitive steps, they are just less diligent about doing so, and they settle on selecting a satisfactory answer rather than the optimal answer. Weak satisficing may lead to selecting earlier response options without careful evaluation of later response options, which is susceptible to confirmation bias. In strong satisficing, a respondent skips the retrieval and judgment steps, interprets the question superficially and selects an answer based on what they think will be a reasonable (or “satisfactory”) response. Their answer is not reflective of a person’s actual feelings about the construct of interest. Answers could also be selected arbitrarily. There are several conditions when satisficing is most likely to occur: The greater the task difficulty The lower the respondent’s ability The lower the respondent’s motivation When “No opinion” responses are an option In sum, to reduce the likelihood of satisficing, match the task to the participant’s ability, make sure they have motivation to respond correctly, and do not provide a “no opinion” response option. 18.8 Observational Assessments Given the challenges with self- and informant-report of personality, it can also be worth considering observational assessments. For example, one observational approach to assessing personality involves a “thin-slice” approach, in which observers briefly assess people’s personality via observations across a range of situations or contexts (Tackett, Lang, et al., 2019). Observational ratings of personality can then be combined with self- and informant-rated personality in a multitrait-multimethod matrix (Tackett, Lang, et al., 2019). 18.9 Structure of Personality The most well-supported structure of personality is the five-factor model of personality. The highest-order dimensions of the five-factor model are defined by the Big Five. The Big Five are known by the acronym OCEAN: Openness to experience (versus closed-mindedness), Contientiousness (versus disorganization), Extraversion (versus intraversion), Agreeableness (versus disagreeableness), and Neuroticism (versus emotional stability). 18.10 Personality Across the Lifespan As described by Costa Jr. et al. (2019), individual differences (i.e., rank order) in personality are relatively stable from middle childhood to old age. On average, neuroticism tends to decline, and agreeableness and contientiousness tend to increase with age (Costa Jr. et al., 2019). It is unclear how extraversion and openness to new experiences change across the life span. 18.11 Conclusion In an objective personality test, a stimulus is presented to a respondent, who makes a closed-ended response, such as True/False or Likert ratings. An example of an objective personality test is the Minnesota Multiphasic Personality Inventory (MMPI). There are three primary approaches to developing personality measures and other scales. One is the external approach, in which items are selected based on their association with an external criterion. A second approach is the deductive approach, in which items are deduced based on theory. A third approach is the inductive approach, in which items are selected based on the internal association between items that are intended to assess the same construct. Guidelines for measurement development are provided. Nevertheless, relying on self-report and self-assessment is prone to key weaknesses including response bias, ambiguity of items, lack of insight, and satisficing. There are ways to improve self-assessment, but it can also be helpful to supplement self-assessments with informants’ ratings and with observational assessments. 18.12 Suggested Readings Burisch (1984); Dunning et al. (2004); Krosnick (1999) References Bensch, D., Maaß, U., Greiff, S., Horstmann, K. T., &amp; Ziegler, M. (2019). The nature of faking: A homogeneous and predictable construct? Psychological Assessment, 31(4), 532–544. https://doi.org/10.1037/pas0000619 Burchett, D., &amp; Ben-Porath, Y. S. (2019). Methodological considerations for developing and evaluating response bias indicators. Psychological Assessment, 31(12), 1497–1511. https://doi.org/10.1037/pas0000680 Burisch, M. (1984). Approaches to personality inventory construction: A comparison of merits. American Psychologist, 39, 214–227. https://doi.org/10.1037/0003-066X.39.3.214 Clark, L. A., &amp; Watson, D. (1995). Constructing validity: Basic issues in objective scale development. Psychological Assessment, 7, 309–319. https://doi.org/10.1037/1040-3590.7.3.309 Clark, L. A., &amp; Watson, D. (2019). Constructing validity: New developments in creating objective measuring instruments. Psychological Assessment, 31(12), 1412–1427. https://doi.org/10.1037/pas0000626 Costa Jr., P. T., McCrae, R. R., &amp; Löckenhoff, C. E. (2019). Personality across the life span. Annual Review of Psychology, 70(1), 423–448. https://doi.org/10.1146/annurev-psych-010418-103244 Dunning, D., Heath, C., &amp; Suls, J. M. (2004). Flawed self-assessment: Implications for health, education, and the workplace. Psychological Science in the Public Interest, 5, 69–106. https://doi.org/10.1111/j.1529-1006.2004.00018.x Graham, J. R., Veltri, C. O. C., &amp; Lee, T. T. C. (2022). MMPI instruments: Assessing personality and psychopathology (6th ed.). Oxford University Press. Krosnick, J. A. (1999). Survey research. Annual Review of Psychology, 50, 537–567. https://doi.org/10.1146/annurev.psych.50.1.537 Loevinger, J. (1957). Objective tests as instruments of psychological theory. Psychological Reports, 3(3), 635–694. https://doi.org/10.2466/pr0.1957.3.3.635 Sellbom, M. (2019). The MMPI-2-restructured form (MMPI-2-RF): Assessment of personality and psychopathology in the twenty-first century. Annual Review of Clinical Psychology, 15(1), 149–177. https://doi.org/10.1146/annurev-clinpsy-050718-095701 Simms, L. J., Zelazny, K., Williams, T. F., &amp; Bernstein, L. (2019). Does the number of response options matter? Psychometric perspectives using personality questionnaire data. Psychological Assessment, 31(4), 557–566. https://doi.org/10.1037/pas0000648 Tackett, J. L., Lang, J. W. B., Markon, K. E., &amp; Herzhoff, K. (2019). A correlated traits, correlated methods model for thin-slice child personality assessment. Psychological Assessment, 31(4), 545–556. https://doi.org/10.1037/pas0000635 Watkins, C. E., Campbell, V. L., Nieberding, R., &amp; Hallmark, R. (1995). Contemporary practice of psychological assessment by clinical psychologists. Professional Psychology: Research and Practice, 26(1), 54–60. https://doi.org/10.1037/0735-7028.26.1.54 Wiggins, J. S. (1973). Personality and prediction: Principles of personality assessment. Addison-Wesley. Wright, A. G. C., Gates, K. M., Arizmendi, C., Lane, S. T., Woods, W. C., &amp; Edershile, E. A. (2019). Focusing personality assessment on the person: Modeling general, shared, and person specific processes in personality and psychopathology. Psychological Assessment, 31(4), 502–515. https://doi.org/10.1037/pas0000617 "],["projective.html", "Chapter 19 Projective Personality Testing 19.1 Overview of Projective Personality Testing 19.2 Examples of Projective Measures 19.3 Most Widely Used Assessments for Children 19.4 Evaluating the Scientific Status of Projective Measures 19.5 Conclusion 19.6 Suggested Readings", " Chapter 19 Projective Personality Testing 19.1 Overview of Projective Personality Testing Surveys consistently show projective measures to be among the most commonly used assessment devices in clinical psychology, school psychology, and neuropsychology (see Youngstrom &amp; Van Meter, 2016). There has been some decline in their usage, in part due to managed care and in part due to criticisms, but they are still widely used. Frank (1939) presents a problem to solve—how best to assess personality—and his proposed solution. He sought to develop a predictive model of people’s personality and behavior based on how people process information. The approach of predictive modeling has worked in other areas of psychology. According to their conceptual framework, people exist in multiple spheres: in the common public world of nature (i.e., as humans) as members of their social and cultural groups as individuals in their private worlds of highly idiosyncratic meanings, significances, and feelings (i.e., personality) When psychologists try to assess people’s private worlds (personalities), they are not seeking to assess the cultural and social norms—instead they seek to assess the peculiarities of the individual. Psychodynamic theorists viewed standardized tests as a measure of how much a person conforms to the expectations of a cultural group, not as a measure of the person as an individual. Psychodynamic theorists viewed personality as a dynamic process of how a person organizes their experience according to the unique individual’s private world. They viewed projective measures as an “x-ray of the soul” that reveals its components and organization, including the state, condition, and maturation of the organism. Projective measures are an indirect method used to assess the internal organization and composition of an individual organism. According to the conceptual framework, projective measures allowed examiners to get a glimpse into this dynamic process and their private world by observing the examinee’s response to various “fields.” Fields are ambiguous stimuli consisting of objects, materials, and experiences with relatively little structure and cultural patterning. Psychodynamic theorists hypothesized that an individual’s personality projects onto the field their way of seeing life, their meanings, significances, patterns, and feelings. Projection, according to Freud (1911), is a defense mechanism by which people unconsciously attribute their negative personality traits and impulses to others. According to a psychodynamic framework, psychopathology is hypothesized to be caused by unconscious motives, conflicts, and memories. In this framework, defense mechanisms keep us unaware of the painful contents of the unconscious. However, attributing negative characteristics to others does not appear to be effective in reducing anxiety or keeping one unaware of one’s own negative characteristics. Projective measures were thought to be a way to access a person’s unconscious. When presented with a field (an ambiguous stimulus), the person has to organize the field, interpret the material, and react affectively to it. Therefore, according to the framework, projective measures elicit a projection of the individual person’s private world, that is how they organize their life space. Psychoanalysts seek to assess “what one cannot or will not say” because the person does not know themselves well enough, and is unaware of what they are revealing about themselves through their projections. According to this perspective, the person rarely has an understanding of themselves or awareness of what their activities signify. 19.1.1 Categorization of Responses The psychoanalysis separates projections (i.e., responses to the ambiguous stimuli) into categories, such as: Constitution: forming a holistic entity or “Gestalt” Interpretive: describing what a stimulus-situation means to them Cathartic: discharging affect or feeling upon the stimulus-situation for emotional release Constructive: building with the materials in a way that reflects something meaningful about their life 19.1.2 Projective Hypothesis Psychodynamic theorists are less concerned with psychometrics (reliability and validity) from statistical correlations across many people. Instead, they seek to assess one person in many ways to see how they structure their life space. They would argue that the projective measures are valid if the examinee provides similar response configurations across stimuli and situations. Projective measures are based on the projective hypothesis. According to the projective hypothesis, whatever a person does when exposed to an ambiguous stimulus will reveal important aspects of the person’s personality. An ancillary hypothesis to the projective hypothesis is that indirect responses are more valid than direct responses. Indirect responses are responses to ambiguous stimuli, whereas direct responses are responses to interviews or questionnaires. 19.1.3 Projective Measures Projective measures are measures in which an ambiguous stimulus is presented to the respondent, who is asked to make an open-ended response. There is theoretical strength behind projective measures in that they have good goals, but they are based on poor implementation. The goal of projective measures is to discover things that do not depend on self-knowledge and the person’s ability or willingness to share it. Projective measures aim to bypass defense mechanisms so that they assess characteristics the person has and which the person is not consciously aware of. For example, projective measures aim to assess characteristics the person does not recognize or wants to hide. Examples of projective measures include the Rorschach Inkblot Test, the Thematic Apperception Test (TAT), Draw-A-Person Test, and drawing a family (Kinetic Family Drawing). Most projective techniques do not have: standardized stimuli and testing instructions, systematic algorithms for scoring responses to stimuli, or well-calibrated norms for comparing responses with those of other people There are various types of projective measures, including measures that use: association techniques, often with an ambiguous form—e.g., inkblot, clouds, or word association tests construction techniques, often with artistic media—e.g., human figure drawing or story creation methods like the Thematic Apperception Test (TAT). Thematic perception involves writing or telling stories about a series of pictures. completion techniques—e.g., sentence completion tests arrangement or selection techniques expression techniques, often with movement or play—e.g., projective doll play, puppetry, or hand-writing analysis. Play is often used by psychoanalysts as a projective measure with children because it is thought that children have fewer defenses to hide behind (compared to adults) and are less aware of how much they are revealing through their play. For instance, psychoanalysts believe that children who have been abused will reveal that in their play configurations with dolls. The Rorschach Inkblot Test is a classic example of a projective measure. It was designed by Swiss psychiatrist Hermann Rorschach in 1921. It was initially designed as an instrument for studying perception, but it became to be conceived of as a test of personality pathology. It was conceived by psychoanalysts as an “x-ray of the soul.” The Rorschach Inkblot Test has low face validity, and is therefore potentially difficult to fake. For instance, a person may fake good (present as if they are better than they are) in the context of a custody evaluation or job interview, whereas a person may fake bad (present as if they are worse than they are) in the context of an evaluation for disability claims or of the person’s sanity to stand trial. In 1968, a projectively oriented, classic textbook was published, titled The Interpretation of Psychological Tests. The textbook provides case examples of a patient. Examiners were trying to decide if the patient was psychotic, and they interpret her crystallized intelligence from the Wechsler Adult Instelligence Scale based on Rorschach scores (“insufficiency of ego strength”). The examiners attributed her lack of consistency across items not to the inconsistency in the test (and its stimuli), but to her own inconsistencies. 19.1.4 Why Not Use Projective Measures? Projective measures can be considered examples of performance-based assessments. It can be helpful to include performance-based assessments as part of a broader assessment battery to avoid exclusive reliance on self-report. From this perspective, one might think that it could be beneficial to use projective measures. However, there are many reasons not to use projective measures. First, projective measures can be very time consuming: both for the client and the psychologist, in terms of administration, scoring, and interpretation. Second, unlike other observational and performance-based measures, projective measures provide very little information. You could just ask clients questions on a questionnaire or in an interview. Consider the example of assessing a client for depression. Instead of relying on the client to report relatively color-less items in the Rorschach, why not ask them (and others) how their mood is and observe their energy and affect? Thus, projective measures have an incremental validity problem. The scientific status of projective measures is described in further detail later in the chapter in Section 19.4. 19.2 Examples of Projective Measures In this chapter, two projective measures are described in detail. The first is the Rorschach Inkblot Test and the second is the Thematic Apperception Test (TAT). 19.2.1 Rorschach Inkblot Test The Rorschach Inkblot Test was designed by Hermann Rorschach in 1921 and has ten inkblots, five of which are black and white, and five of which contain color. The measure is composed of three phases, depending on whether one is using the Comprehensive System (CS) (Exner, 1974; Exner &amp; Erdberg, 2005) or the Rorschach Performance Assessment System (R-PAS) (Meyer et al., 2011). The phases include: (1) the Free Association (CS) or Response (R-PAS) Phase, (2) the Inquiry (CS) or Clarification (R-PAS) Phase, and (3) the Follow-Up Phase. In the “Free Association” or “Response” phase, the examinee is handed one inkblot card at a time and is asked what they see. The stimuli were designed with an aim to provide the least amount of intrinsic information; the test developers did not want to impose intrinsic information from concrete stimuli. The inkblots start completely black and white, then they move to grayscale, which was considered to be “easier emotionally” for clients, and then color inkblots are shown, which was considered more challenging emotionally. The clinician intends not to guide the examinee to particular responses, but they also want to hear more than one response. The idea is that eventually, with additional responses, the examinee will get to be less censored. After finishing the response phase, the clinician moves to the “Inquiry” or “Clarification” phase, in which the examiner reminds the examinee of the examinee’s responses and asks where on the card the examinee saw their responses and what about the inkblot made it look like that. The clinician inquires what part of the card includes the image the examinee had described. The examinee then identifies the location in the card, i.e., whether it is the whole card or particular parts of the card, or excluded parts (negative space) of the card. It involves asking the examinee what exactly they saw, and what (determinants) made them see that. It also involves examining whether they use integration to complete the whole picture. Or if they are more defensive, it is hypothesized that the examinee will pick out a little part of the picture when describing what they see. Then, they are asked to flip the card around. The inquiry phase can last a long time, sometimes 1 hour or more. In the open-ended “Follow-Up” phase, the examiner tries to learn what the examinee was thinking and the process that may have led to the response obtained (Choca &amp; Rossini, 2018). The examiner may use a technique called “testing the limits”. This may involve additional questions or observations. For instance, questions might ask why a respondent may not see what many others see. Did the person see two insects or two people? Were they males or females? Observations might include noting that the examinee provided many responses. 19.2.1.1 Variables There are a number of variables that are often scored in a Rorschach administration. Here are some of the ones that can be scored: Latency to respond Let the examinee stew for a while because the first response is the least interesting because it is the most obvious. How are they holding the card? How do they turn the card? How many times do they turn the card? What content do they see and describe in the inkblot? Do they see sexual content? Violent content? Location: did the examinee see the whole blot as a picture or just one particular area of the blot, or excluded parts (negative space) of the card? Determinants: the characteristics of what the image looked like to the respondent Form: form or shape features Movement: hypothesized to reflect intelligence Color: lack of color is hypothesized to reflect depression Shading: e.g., texture Form dimension: e.g., three-dimensionality Pairs and reflections: identical objects 19.2.1.2 Scoring The clinician then can spend several hours scoring, and there are many different potential indices. Whether the content described by the examinee is considered good or bad is based on textbook judgments. This could represent an example of an illusory correlation. An illusory correlation is the tendency to perceive associations between signs and psychopathology even when they do not exist. John Exner (1974) developed standardized rules for administration and scoring, which improved the reliability of scores. The Exner system, also known as the “Comprehensive System” (CS) (Exner, 1974; Exner &amp; Erdberg, 2005) is the most widely used scoring system for the Rorschach. But there are still major problems with the system. A more recent scoring system has been developed, known as the Rorschach Performance Assessment System (R-PAS) (Meyer et al., 2011). R-PAS has limited research on its use by those outside of its developers, but it likely has similar problems with validity and utility. 19.2.2 Thematic Apperception Test (TAT) The Thematic Apperception Test (TAT) is described by Lindzey (1952). The TAT was developed by Christiana Morgan and Henry Murray in 1935 (Morgan &amp; Murray, 1935). The TAT is more about storytelling, and having the examinee tell a story. So, unlike the Rorschach, storytelling is allowed in the TAT. It was developed by scientists, and there has been a lot of research on the TAT. The TAT aims to get a better understanding of underlying motivations (e.g., achievement motivation), and it is thought by psychoanalysts to assess implicit motives, unlike self-attributed motives assessed by self-report instruments. Most stimuli in the TAT involve humans, but the situations are ambiguous. Examinees’ responses on the TAT are strongly predictive of various motivations, but they are no more predictive than just asking people their motivations. Motivations that are intended to be assessed by the TAT include: achievement motivation, need for power, need for affiliation, and object relations. Object relations are people’s mental representations of other people. The administrator picks cards to administer that “pull” at issues of interest related to the respondent’s presenting difficulties. As a result, the TAT is non-standardized. The examiner asks the examinee what is going on in the picture, what the characters are thinking and feeling, what led up to it, and how it ended. The examiner might examine the respondent’s response latency, and writes down all words of the response. They would examine who the examinee is relating to and identifying with. The examiner seeks to identify patterns and commonalities across stories in response to different TAT cards. Towards the end of the story, there is very little information from the card, so the respondent has to fill in the story. Thus, psychoanalysts believe that responses at the end of the story are the most “pure”, and come from the inside (as opposed to external factors). The examiner pays attention for themes that are consistent across cards. However, fantasy behavior is not the same as actual behavior. For instance, telling violent stories is not the same as being violent. The horror author Stephen King is an apt example of how describing violence does not mean that the person is violent. 19.3 Most Widely Used Assessments for Children Projective assessments are widely used. As of 2002, the top 30 most widely used assessments for children (Cashel, 2002) include: Wechsler Intelligence Scale for Children Child Behavior Checklist And then many projective techniques, including: Sentence completion Draw-A-Person test (with psychodynamic oriented scoring) House–Tree–Person Technique Kinetic Family Drawing Rorschach Inkblot Test Thematic Apperception Test (TAT) Children’s Apperception Test (Children’s TAT) In sum, seven of the most widely used assessments for children are projective tests. 19.4 Evaluating the Scientific Status of Projective Measures Given that projective assessments are so widely used, it would be expected that they have a strong scientific status. Lilienfeld et al. (2000) evaluated the scientific status of projective techniques. To evaluate the scientific status of the Rorschach Inkblot Test (or any other projective measure), we would assess across several domains of evidence, as described below. Evaluation of the Rorschach Inkblot Test is provided by Wood and colleagues (1996a, 1996b; 2001). 19.4.1 Utility of Norms One consideration to evaluate is the utility of the norms. This includes assessing if the normative information on which scoring of the test is based sets the cut-off accurately for those with and without maladaptive behavior, across cultural and minority groups. Of note, the Rorschach does not correctly distinguish between those with and without maladaptive behavior, and it does not lead to unbiased estimates for ethnic minorities. In addition, we cannot evaluate the norms because they are not published. In terms of the Exner norms, they showed somewhat higher reliability than the original Rorschach norms, but they did not show strong evidence for validity. The Exner norms are not well-calibrated. The Exner norms result in over-diagnosis and over-pathologizing of normality. And they are not representative and generalizable: there are ethnic and cultural biases. 19.4.2 Reliability of Scores Another important consideration of projective measures is the reliability of scores. This would include evaluating both inter-rater reliability and test–retest reliability. For the Rorschach, the inter-rater reliability depends on the subscale of diagnosis, and has to be much lower than that of objective techniques, especially in field settings. Field reliability refers to reliability in actual practice rather than ideal reliability in optimal conditions of a controlled lab study. The reliability and standardization of administration of the Rorschach is also questionable. The test–retest reliability is also low—but high enough to be passable for research settings—but it should be used with caution in clinical diagnostic settings. 19.4.3 Influence of Measurement Error on Scores Another important consideration is the influence of measurement error on scores: one aspect of the Rorschach in particular that can lead to measurement error is the effect of response frequency on test validity. If people provide more answers, which is correlated with intelligence, these people are more likely to express a deviant answer indicative of maladjustment. 19.4.4 Validity A crucial consideration of the scientific status is their validity. Validity is not a yes/no thing. It is important to ask what the validity is for what purpose? Rorschach responses can be moderately valid indicators of creativity. Projective drawings have moderate validity for drawing ability. But are projective tests valid for their typical purposes in clinical psychology? For instance, do they have diagnostic accuracy? Their accuracy for their typical purposes in clinical psychology is discussed below. 19.4.4.1 Criterion-Related Validity Criterion-related validity would consider whether the diagnosis given on the basis of the Rorschach aligns with an external criterion of an actual diagnosis. For the Rorschach, the answer is: not really. Scores have some relation to psychosis and thought disorders, but they are not strongly related to other aspects of psychopathology. Research designs comparing diagnostic versus control groups often over-estimate predictive validity of the Rorschach compared to what it would be in practice because clinicians are typically interested in low base rate phenomena for which predictive accuracy will be lower. 19.4.4.2 Convergent Validity Convergent validity would consider whether Rorschach findings correspond to the findings of more reliable objective tests such as the Minnesota Multiphasic Personality Inventory? The answer is also no. 19.4.4.3 Incremental Validity In terms of incremental validity, a frequent argument by proponents of projective tests is that, “I don’t use the Rorschach/TAT in isolation but in combination with other measures”. But does the inclusion of the Rorschach lead to improvements in the validity of diagnoses of clinicians? No. In many cases, it has decremental validity—it decreases predictive or diagnostic accuracy (Garb et al., 2005). 19.4.4.4 Treatment Utility Treatment utility considers whether it is worth the vast amount of time spent administering, scoring, and interpreting the measure (2–3 hours)? The answer is also no. You can assess psychopathology more accurately with more reliable and less time-consuming measures, including interviews, questionnaires, and demographic data. Projective measures present actual costs in addition to opportunity costs. Actual costs of projective measures include that they are expensive and time-consuming for training, and are time-consuming to administer, score, and interpret. Opportunity costs of projective measures include that they take time away from other, more useful procedures. 19.4.5 Summary Hence, we would not say the scientific status of the Rorschach is very positive. The evidence does not provide the evidence necessary to call projective measures “tests”. No known projective measures have enough power to justify their lengthy use. Considering the important role that assessments can play in clinical decision-making that influences people’s lives, it seems unethical to use projective techniques, as they are currently designed and used. 19.5 Conclusion Projective measures are measures in which an ambiguous stimulus is presented to the respondent, who is asked to make an open-ended response. Projective measures were developed based on psychodynamic theory and the projective hypothesis that whatever a person does when exposed to an ambiguous stimulus will reveal important aspects of the person’s personality. The goal of projective measures is to discover things that do not depend on self-knowledge and the person’s ability or willingness to share it. Projective measures aim to bypass defense mechanisms so that they assess characteristics the person has and which the person is not consciously aware of. Examples of projective measures include the Rorschach Inkblot Test, the Thematic Apperception Test (TAT), Draw-A-Person Test, and drawing a family (Kinetic Family Drawing). However, there are many problems with projective measures: most projective techniques do not have standardized stimuli and testing instructions, systematic algorithms for scoring responses to stimuli, or well-calibrated norms for comparing responses with those of other people. Their scientific status is not strong in terms of utility of norms, reliability, influence of measurement error, or validity. In addition, they can be very time-consuming, and they provide very little information. Thus, there are many reasons not to use projective measures. 19.6 Suggested Readings Lilienfeld et al. (2000) References Cashel, M. L. (2002). Child and adolescent psychological assessment: Current clinical practices and the impact of managed care. Professional Psychology: Research and Practice, 33(5), 446–453. https://doi.org/10.1037/0735-7028.33.5.446 Choca, J. P., &amp; Rossini, E. D. (2018). Assessment using the Rorschach inkblot test. American Psychological Association. Exner, J. E. (1974). The Rorschach: A comprehensive system. John Wiley &amp; Sons. Exner, J. E., &amp; Erdberg, S. P. (2005). The Rorschach, a comprehensive system: Advanced interpretation (3rd ed., Vol. 2). John Wiley &amp; Sons, Inc. Frank, L. K. (1939). Projective methods for the study of personality. Journal of Psychology, 8, 389–413. https://doi.org/10.1080/00223980.1939.9917671 Freud, S. (1911). Psycho-analytic notes on an autobiographical account of a case of paranoia (dementia paranoides). In J. Strachey (Ed.), The standard edition of the complete psychological works of Sigmund Freud: The case of Schreber, papers on technique and other works, Vol. 12 (1911–1913) (pp. 1–82). Garb, H. N., Wood, J. M., Lilienfeld, S. O., &amp; Nezworski, M. T. (2005). Roots of the Rorschach controversy. Clinical Psychology Review, 25(1), 97–118. https://doi.org/10.1016/j.cpr.2004.09.002 Lilienfeld, S. O., Wood, J. M., &amp; Garb, H. N. (2000). The scientific status of projective techniques. Psychological Science in the Public Interest, 1, 27–66. https://doi.org/10.1111/1529-1006.002 Lindzey, G. (1952). Thematic apperception test: Interpretive assumptions and related empirical evidence. Psychological Bulletin, 49, 1–25. https://doi.org/10.1037/h0062363 Meyer, G. J., Erard, R. E., Erdberg, P., Mihura, J. L., &amp; Viglione, D. J. (2011). Rorschach Performance Assessment System: Administration, coding, interpretation, and technical manual. Rorschach Performance Asessement Systems LLC. Morgan, C. D., &amp; Murray, H. A. (1935). A method for investigating fantasies: The thematic apperception test. Archives of Neurology &amp; Psychiatry, 34(2), 289–306. https://doi.org/10.1001/archneurpsyc.1935.02250200049005 Wood, J. M., Nezworski, M. T., Garb, H. N., &amp; Lilienfeld, S. O. (2001). Problems with the norms of the Comprehensive System for the Rorschach: Methodological and conceptual considerations. Clinical Psychology: Science and Practice, 8(3), 397–402. https://doi.org/10.1093/clipsy.8.3.397 Wood, J. M., Nezworski, M. T., &amp; Stejskal, W. J. (1996a). The Comprehensive System for the Rorschach: A critical examination. Psychological Science, 7(1), 3–10. https://doi.org/10.1111/j.1467-9280.1996.tb00658.x Wood, J. M., Nezworski, M. T., &amp; Stejskal, W. J. (1996b). Thinking critically about the Comprehensive System for the Rorschach: A reply to exner. Psychological Science, 7(1), 14–17. https://doi.org/10.1111/j.1467-9280.1996.tb00660.x Youngstrom, E. A., &amp; Van Meter, A. (2016). Empirically supported assessment of children and adolescents. Clinical Psychology: Science and Practice, 23(4), 327–347. https://doi.org/10.1111/cpsp.12172 "],["psychophysiological.html", "Chapter 20 Psychophysiological and Ambulatory Assessment 20.1 NIMH Research Domain Criteria (RDoC) 20.2 Psychophysiological Measures 20.3 Conclusion 20.4 Suggested Readings", " Chapter 20 Psychophysiological and Ambulatory Assessment There are a number of alternative conceptualizations of psychopathology compared to the conceptualization of psychopathology provided in the Diagnostic and Statistical Manual of Mental Disorders (DSM). One alternative conceptualization of psychopathology is the p-factor (Caspi et al., 2014; Smith et al., 2020). The p-factor is a hierarchical view of psychopathology that accounts for covariation among various kinds of psychopathology. The p-factor is a general psychopathology factor, similar to the general intelligence factor (g), and has three sub-factors: internalizing problems, externalizing problems, and thought-disordered problems. Hierarchical conceptualizations tend to be advanced by lumpers, who group similar forms of psychopathology together, as opposed to splitters, who tend to split forms of psychopathology into separate categories. Another hierarchical view of psychopathology is HiTOP, the Hierarchical Taxonomy of Psychopathology (Kotov et al., 2017, 2021). A third conceptualization of psychopathology is presented by the Research Domain Criteria, also called (RDoC), from the National Institute of Mental Health (NIMH). Recommendations for conducting assessment in a transdiagnostic way that is consistent with HiTOP and RDoC are provided by Stanton et al. (2020). 20.1 NIMH Research Domain Criteria (RDoC) The National Institute of Mental Health (NIMH) has funded research to understand the etiology of mental illnesses and how best to intervene. Historically, NIMH funded considerable work on traditional DSM-based diagnostic categories. However, work studying DSM-defined diagnostic categories has not had as much impact as NIMH would have liked. So, the NIMH led an initiative known as the Research Domain Criteria, also called RDoC, to provide a template for research on psychopathology. An overview of RDoC is provided by Kozak &amp; Cuthbert (2016). There are several ways in which RDoC is different from the DSM. Compared to the DSM, RDoC is dimensional, not categorical. RDoC views psychopathology as existing on continua, not as discrete categories. Another difference is that RDoC works from the ground up, starting with brain–behavior relations, and linking those to clinical symptoms. By contrast, the DSM is top-down, starting with diagnostic categories and determining what fits in those categories based on behavioral symptoms. However, as discussed in Section 17.5.2, diagnoses are fictive categories that have poor diagnostic validity. The DSM-defined categories are based on behavioral presentations. That is, disorders are not things that people “have”; they are things that people “do”. The same behavior can occur for different underlying reasons, a principle known as equifinality. Different neurodevelopmental trajectories may underlie the same behavior for two different people. Behavioral presentations in the same disorder are heterogeneous. The pathophysiology in a given DSM category is not unitary. DSM categories also share symptoms and show lots of co-occurrence, so they are not entirely distinct. The DSM has resulted in the reification of these fictive diagnostic categories. RDoC is a template for psychopathology research that consists of dimensional constructs integrating elements of psychology and biology. It incorporates a wider range of data, and multiple levels of analysis, including genetics, brain structure and function, and physiology. By contrast, the DSM only incorporates symptoms. In RDoC, there is a focus on narrower dimensions, such as cognition, emotion, reward seeking, fear, learning, memory, motivation, and perception, rather than disorders. The focus on narrow dimensions is based on the idea that there is greater potential to relate biological processes to simpler, lower-order, narrower dimensions of psychological constructs compared to disorders. These narrower dimensions of psychological constructs can be related to important clinical dimensions. Psychopathology is thought to represent extremes on the psychobiological distribution, e.g., high fearfulness in phobia versus low fearfulness in psychopathy. Thus, psychophysiological processes are a key emphasis in RDoC (G. A. Miller et al., 2016). 20.1.1 RDoC Matrix The RDoC matrix is a matrix of two dimensions: six domains by eight units of analysis. In the RDoC matrix, domains are constructs that include elements, processes, mechanisms, and responses. The domains include: negative valence systems (i.e., negative affect: fear, anxiety, etc.), positive valence systems (i.e., positive affect: reward processing), cognitive systems (attention, perception, language, cognitive control, working memory), social processes (attachment, social communication, empathy), arousal and regulatory systems (sleep, circadian rhythms), sensorimotor systems (e.g., motor actions). In the RDoC matrix, units of analysis are classes of measurement that are similar to levels of analysis. Units of analysis include: genes, molecules, cells, circuits, physiology, behaviors, self-reports, and paradigms. The RDoC matrix is presented in Figure 20.1. Figure 20.1: National Institute of Mental Health (NIMH) Research Domain Criteria (RDoC) Matrix. In the RDoC framework, there is theoretical neutrality of the units of analysis. No unit of analysis is thought to be more important than any other; they are each thought to be important in their own right. That is why they are referred to as units of analysis rather than levels of analysis. “Levels” connotes order from basic to higher-order, and the developers of RDoC wanted to avoid suggesting that one level underlies another. RDoC dimensional constructs are meant to be integrative rather than reductionistic. It involves moving away from the subjectivist tradition and moving toward a heterophenomenological approach. The subjectivist tradition is one in which subjective experience is deemed the primary measure of a phenomenon (e.g., fear). The heterophenomenological approach combines multiple units of analysis to identify phenomena. The domains and constructs will need to be added and refined with further research. The RDoC matrix is just a tentative and incomplete starting template. The goal is to understand the bridge (i.e., mechanisms) that links the different units of analysis for the same construct, for example, how the biological and psychological bases of fear influence each other. Examples of mechanisms include endophenotypes, that are heritable, “unobservable”, intermediary traits that signify disease liability, and that mediate the association between genotype and phenotypic expressions of psychopathology. An example of an endophenotype is depicted in Figure 20.2. Figure 20.2: Example of an Endophenotype. The idea is that genes influence endophenotypes and that endophenotypes lead to the phenotype. An endophenotype is similar to “intermediate phenotype”, but an intermediate phenotype does not have the requirement of a genetic cause. An example of an intermediate phenotype is depicted in Figure 20.3. Endophenotypes and intermediate phenotypes are different from biomarkers, which are biological indicators (i.e., correlates) that are not necessarily causal. All endophenotypes are biomarkers, but not all biomarkers are endophenotypes. Figure 20.3: Example of an Intermediate Phenotype. One dimension that the RDoC does not include is development. It has been a challenge incorporating development into the RDoC matrix (Conradt et al., 2021; Durbin et al., 2022). When considering the RDoC matrix, it is important to consider the RDoC dimensions—domains of functioning and units of analysis—within the context of environmental and developmental influences (Lupien et al., 2017; Woody &amp; Gibb, 2015), as depicted in Figure 20.4. Figure 20.4: Schematization Representation (From an Original idea from Woody &amp; Gibb (2015)) of the Four-Dimensional Matrix of the RDoC Framework. Each row represents a sub-domain of functioning that could be studied in different studies or in a single study. Each column represents a unit of analysis. The different matrices are represented as a function of the environmental and developmental dimensions. For example, the domain of functioning in figure could represent the ‘Negative Valence System’ where ‘1’ represents acute threat or fear, ‘2’ represents potential threat of anxiety, ‘3’ represents sustained threat, and ‘4’ represents loss. The units of analysis could be represented by ‘A’ Genes; ‘B’ Stress Hormones; ‘C’ EEG; ‘D’ Brain imaging, and ‘E’: Metabolic markers. Each row of this schematized RDoC matrix represents a particular study (e.g., the gray boxes in Row #1 could represent a study measuring acute threat of fear as a function of stress hormones and brain imaging in an adult population) and the results of various studies are represented by the different filled boxes represented in rows and columns. Environmental and developmental factors could be added to all of these studies. (Figure reprinted from Lupien et al. (2017), Figure 1, p. 9. Lupien, S. J., Sasseville, M., François, N., Giguère, C. E., Boissonneault, J., Plusquellec, P., Godbout, R., Xiong, L., Potvin, S., Kouassi, E., &amp; Lesage, A. (2017). The DSM5/RDoC debate on the future of mental health research: implication for studies on human stress and presentation of the signature bank. Stress, 20(1), 2-18. https://doi.org/10.1080/10253890.2017.1286324) 20.2 Psychophysiological Measures Examples of psychophysiological measures include electroencephalography (EEG), event-related potential (ERP), (functional) magnetic resonance imaging (f)MRI, computerized axial tomography (CAT), magnetoencephalography (MEG), functional near infrared spectroscopy (fNIRS), electrocardiography (ECG or EKG), electromyography (EMG), electrooculography (EOG), eyetracking, and actigraphy. G. A. Miller et al. (2007) provide an overview of neuroimaging techniques for clinical assessment. EEG caps can include 256 electrode sensors from millisecond to millisecond, with strong temporal resolution, yielding lots of data. Having more sensors allows better estimates of spatial localization. Psychophysiological measures are relevant because lots of behavior problems have important physiological facets. But it is not always clear whether biological processes cause psychopathology or the reverse, or whether there are third variable confounds that influence both psychophysiology and behavior problems (that explain why they are associated, even though they are non-causally related). Psychophysiological measures are not invulnerable to basic measurement issues, including reliability and validity. Psychophysiological measures tend to be more expensive than questionnaire measures, so people traditionally were reluctant to consider issues of reliability and validity. Galton and Cattell, nearly 200 years ago, were more interested in direct measures, like grip strength, than self-report. Hans Berger made the first EEG recording in 1924: EEG has been around a long time. 20.2.1 Reliability Reliability matters, especially when you are examining individual differences (Dubois &amp; Adolphs, 2016). From a generalizability theory point of view, we could examine the consistency or inconsistency of scores across facets and factors to determine which ones matter and which ones do not matter. Important considerations for evaluating reliability are discussed by Calamia (2019). Internal consistency involves examining the consistency of scores across trials to demonstrate that the measure assesses the same thing across trials. An example of internal consistency that could be applied to psychophysiological data is split-half reliability. Another important form of reliability is test–retest reliability, which involves examining the consistency of scores across time. You could examine the consistency of individual differences across time, i.e., stability, and the consistency of scores within a person across time, i.e., repeatability. An example of repeatability would be examining a Bland-Altman plot. Another important form of reliability is inter-rater reliability, which involves examining the consistency of scores across raters or processing methods. The goal is to generalize across instruments (machines, operators, etc.) to identify the reliability of the general method and not just the reliability of a particular operator or machine. Parallel-forms reliability would involve examining the consistency of scores across slightly different stimuli thought to reflect the same cognitive processes. For instance, one could examine the reliability of scores across trials, time, raters, and stimuli. And if the reliability is low, it would be important to follow up to determine why the reliability is low. It can be beneficial to conduct sensitivity analyses. Sensitivity analyses evaluate the robustness of findings by examining the extent to which findings differ based on changes in methods. For instance, you could examine whether findings differ when using different processing methods. If findings are consistent when using different processing methods, this provides further confidence in your findings because they do not appear to be due to the processing method chosen. However, if the findings differ across processing methods, it tells you that the processing method has an important consequence in relation to your outcome of interest. Sensitivity analysis can help you learn more about the phenomenon of interest. 20.2.2 Lots of Data Psychophysiological measures yield lots of data. Some of the variance in the data reflects garbage (noise), some of the variance in the data reflects signal. So, when collecting lots of information, there’s a possibility of finding an effect merely by chance. There are many researcher degrees of freedom. There are many brain voxels. There are often multiple conditions. And there are many ways to analyze the data. The researcher degrees of freedom can lead to p-hacking. An example of the potential problem with researcher degrees of freedom is the “dead salmon study” (Bennett et al., 2009, 2010). In the study, activation was detected in voxels of a dead salmon during fMRI scanning. The false positive activation detected was due to a failure to account for many multiple comparisons. The point of the dead salmon study was not to invalidate fMRI—it was to show the importance of how rich data should be analyzed to adjust for multiple testing to lower the rate of Type I errors. When dealing with lots of data, over-fitting is a common issue. As described in Section 5.3.1.3.3.3, over-fitting involves explaining noise variance, a finding which would not generalize to a new sample. To avoid over-fitting when examining individual differences, such as with a correlation, it can be helpful to cross-validate the findings in an independent sample or in a hold-out sample. A hold-out sample is when you sub-divide a sample into a training and test data set. You develop the model on the training data set and see how the findings cross-validate on the hold-out (test) data set. Cross-validation requires larger sample (typically \\(N &gt; 100\\)), which is difficult with psychophysiological measures. Most samples with psychophysiological data are relatively small, and as a result, have low power (Button et al., 2013b, 2013a). With lots of data, researcher degrees of freedom, a small sample, and a statistical significance filter, reported findings have inflated estimates of effect sizes and poor replicability (Loken &amp; Gelman, 2017). One way to deal with the large data is to reduce the massive data down, and to use scoring and algorithms. For instance, you can examine the reliability of the scoring system, versus the reliability of the person conducting the scoring system. For data reduction, principal component analysis (PCA) is a useful technique. An emerging technique for integrating information from high-dimensional data is machine learning (Galatzer-Levy &amp; Onnela, 2023). Historically, psychophysiological data were averaged across subjects (group averaging) to improve the signal-to-noise ratio. For example, data were averaged for a clinical group versus controls. However, we think of psychopathology as dimensional and involving individual differences. There have been recent attempts to examine data at the individual differences level. Issues such as reliability and validity are even more crucial when trying to make inferences about individual differences. It is important to remove as much noise as possible without sacrificing signal. Movement artifacts in psychophysiological measurements can lead to poor data quality. It is important to correct for artifacts because they do not occur at random. For example, patients with attention-deficit hyperactivity disorder (ADHD) and schizophrenia will likely show more movement artifacts than controls. One approach is to regress out the artifact (e.g., heart rate), if you have an independent measure of the artifact. If you do not have an independent measure of the artifact, you can use independent component analysis (ICA) to separate independent components of data and to remove artifacts. 20.2.3 Validity Consideration of aspects of validity are crucial for any measure, including psychophysiological measures. Consideration of construct validity, whether the measure actually assesses what it intends to assess, is crucial. For instance, performance on a measure of counting back by 7 is associated with many domains, including psychosis, anxiety, impulsivity, intelligence, etc. It is a measure of many constructs. It is important for our measures to have convergent validity in that they are associated with what they should be associated with, but also discriminant validity, in that they should not be associated with things that we do not expect them to be associated with. If the research identifies activation in a specific part of the brain in association with the task, it is important to consider whether it is the same region across participants. For instance, the researcher should consider whether they are using a single atlas/head model for the whole sample or whether they are using a personalized atlas for each participant. Oftentimes, a given brain region is involved in lots of different cognitive processes and behaviors. This raises construct validity questions. Ask yourself, “What is the evidence to support my interpretation of these effects?” Evidence is strengthened by showing evidence of convergent and discriminant validity. For example, differential deficits can provide stronger evidence of an effect. For instance, people with schizophrenia show deficits, abnormalities, or differences in many different brain processes—but an important question is whether they show relatively greater abnormalities in a given neural process than another neural process. This helps inform the specificity of the process (as opposed to other processes) for explaining differences in the condition of interest. Another important question is: what is considered baseline? For heart rate reactivity and many psychophysiological measures, a comparison is made to a “baseline”, which yields a contrast or difference score. But what should be considered “baseline”? There are many different possibilities for what could be considered “baseline,” and each could yield different results. And, time during a participant’s visit to the lab might not provide a good baseline because people may be anxious about being in a psychology lab. In addition, participants’ brains are not resting during “resting-state”. Another important consideration is the ecological validity of the measures. Measures are often assessed in contexts and with stimuli that are not naturalistic. Psychophysiological measures may not reflect a participant’s typical functioning. A goal would be to use more naturalistic contexts and task paradigms. Another challenge that biological measures have is in relating the biological criteria to actual behavior in the real world. It is not sufficient to show group differences on a biological measure; the researcher needs to show that the biological measure is associated with particular behaviors, that is, an independent measure in the same subjects. When reliability and validity of psychophysiological measures are improved, a goal is to have norms for their use, so you know where a person stands compared to the population on the measure of interest. 20.2.4 Examples 20.2.4.1 Ambulatory Assessment An ambulatory assessment is a measurement strategy designed to acquire minimally disruptive measures of a person engaging in normal activities in their natural environment. Ambulatory biosensors are assessment tools that measure physiological or motor activity (Haynes &amp; Yoshioka, 2007). Ecological momentary assessment (EMA) is an example of an ambulatory assessment. EMA involves repeated sampling of people’s current behaviors and experiences in real time, in their natural environments (Shiffman et al., 2008). EMA typically uses electronic diaries or mobile phones (Trull &amp; Ebner-Priemer, 2013). Stone and colleagues (2023) discuss important considerations for EMA assessment. EMA is a type of ambulatory assessment, but there are also other types of ambulatory assessment, such as physiological monitoring (e.g., sensors or actigraphs), global positioning system (GPS) tracking, and ecological momentary intervention (EMI) (Trull &amp; Ebner-Priemer, 2013). Ambulatory assessment allows rich measurement including, for example, audio, pictures, video, geolocation (GPS), physiological functioning (e.g., heart rate), and physical activity (accelerometer). Ambulatory assessment may provide more precise measurement of phenotypes because they can assess contextual and environmental influences. Ambulatory assessments can collect many measurements passively, without the respondent needing to do anything. Ambulatory assessment minimizes retrospective reporting biases because they can assess people in the moment. They also allow alerts and notifications for compliance reminders so that the participant completes the measurement. And they allow context-based triggers based on GPS. Queries can be random, event-based, time-based, or location-based. Some research suggests that event-based reporting may be preferable compared to signal-based reporting because participants may report that the target behavior occurs more frequently when using event-based reporting compared to signal-based reporting (Himmelstein et al., 2019). Ambulatory assessments can also connect to external sensors via Bluetooth to record, for instance, blood pressure. They can provide data with high temporal resolution; for example, they can provide measurement samples from second to second or even millisecond to millisecond. When dealing with rich data, one important question is the level of granularity you want to analyze the data: seconds, days, weeks, etc. The optimal granularity depends on the research question. Ambulatory assessment might provide opportunities for developing idiographic (personalized) models of the inter-relations among thoughts, feelings, behavior, physiology, and context (Wright &amp; Zimmermann, 2019). In addition, ambulatory assessment might have utility for intervention. For instance, it could help provide a just-in-time intervention when an event or context occurs. Ambulatory assessment can also be used for self-monitoring. And alerts can be used to increase homework compliance. The three most popular versions of ambulatory biosensors are those that monitor cardiovascular activity (e.g., heart rate and blood pressure), physical activity (e.g., actigraph and pedometer), and cortisol levels. However, other ambulatory biosensors assess EEG, respiration, muscle tension, blood glucose, skin conductance, and other processes. An applied example of an ambulatory biosensor is actigraphy. Actigraphy is the tracking of physical movement. Actigraphy uses an actigraph device. An actigraphy is a watch-like device attached to the wrist or ankle that has an accelerometer. Smartphones can also serve as actigraphs because they have accelerometers. Actigraphy is a peripheral measure because it assesses gross motor movements. Actigraphy is also frequently used as a corresponding index of sleep because limb movements occur less frequently during sleep. Actigraphy is considered a better index of sleep than self-/informant-report, but not as good as polysomnography (PSG). However, actigraphy allows for greater ecological validity than a sleep study conducted in the lab, because a person can wear it while sleeping in their own bed at home. In prior work, we took hundreds of sleep variables from actigraphy and using PCA to reduce the data down to key independent dimensions of sleep deficit: activity (sleep quality), (late) timing, (short) duration, and night-to-night variability (Staples et al., 2019). 20.2.4.1.1 Pros Pros of ambulatory assessment include: Ambulatory biosensors do not rely on participants to report their experience; they provide information about constructs that is derived from a different method—and they are useful in a multimethod assessment strategy. Ambulatory biosensors lead to an increase in incremental validity and can improve specificity of decisions. Ambulatory biosensors are highly sensitive to change—they can track rapid changes in the variable of interest, with rapid timecourses (i.e., strong temporal resolution or precision). Ambulatory biosensors are portable, and have potential for greater ecological validity. Biosensor variables collected in naturalistic settings are often only weakly associated with biosensor variables collected in lab settings, possibly due to reactivity in a lab setting. Ambulatory biosensors allow concurrent evaluation of contexts that influence psychophysiological responses. 20.2.4.1.2 Cons Cons of ambulatory assessment include: Ambulatory biosensors are expensive—they are potentially not cost-efficient depending on the use—i.e., they are potentially too expensive for clinical use. Some devices can be intrusive, cumbersome, and technical. There is limited psychometric evaluation conducted with many ambulatory methods. Ambulatory biosensors are potentially unreliable. In addition, ambulatory biosensors are potentially not accurate for certain trait levels. For example, heart rate devices may be less accurate at high levels of physical activity. Ambulatory biosensors may not generalize well across time, devices, labs, and raters. The research investigator has less experimental control. There are practical challenges in conducting assessments outside of the research laboratory, as discussed by Holmlund et al. (2019) and Raugh et al. (2019). 20.2.4.2 Other Examples Other examples of psychophysiological measures include eyetracking, which is used to assess attention, and wearables, which are smart “textiles” or clothing that has embedded biosensors. 20.2.5 Virtual Reality Virtual reality (VR) is an emerging approach to assessment that allows structured assessment in simulated but life-like situations. It provides the investigator greater experimental control compared to ambulatory assessment approaches. Considerations in using virtual reality and other immersive technological approaches to assessment are described by Roberts et al. (2019). 20.2.6 Using Psychophysiological Measures Ideally, we would be able to combine psychophysiological measures as objective measures with other measures, e.g., a performance-based assessment or self-/informant-report, in a multimethod measurement of constructs, as suggested by Patrick et al. (2019). Psychophysiological measures as part of a multimethod assessment approach helps to address the bias of participants and the bias in clinical judgment of clinicians. Ambulatory measures can provide incremental validity and increase the specificity of clinical judgment. Recommendations for using psychophysiological measures include using them in an ecologically valid way, including ambulatory approaches that allow ecological momentary assessment of contexts that influence psychophysiological responses, and conducting recordings over lengthy periods (Raugh et al., 2019). Recommendations for using ambulatory assessment in research on psychopathology are provided by Trull &amp; Ebner-Priemer (2020). Challenges exist in selecting the granularity of analysis—how fine-grained to assess at each level of analysis—and how to combine measures across multiple levels of analysis because each may operate at different timescales. One option is to use multimodal imaging, e.g., simultaneous recordings of EEG and fMRI or EEG and fNIRS, to cancel out weaknesses of each. For instance, EEG has stronger temporal resolution than fMRI—it provides a faster timecourse—but EEG has weaker spatial resolution than fMRI—it provides less accurate estimates of where in the brain the neural activity is occurring. It is worth considering combining multiple measures in a latent variable framework using, for example, factor analysis or structural equation modeling. Given the costs, challenges, and time-consuming nature of psychophysiological data collection, science may be best positioned to address reliability and validity issues by sharing and combining data sets through consortia and open science. Consortia and data-sharing can help ensure large enough sample sizes for adequate statistical power, especially for individual difference analyses. Larger samples would also allow cross-validation of analyses in independent samples using out-of-sample prediction. 20.3 Conclusion The National Institute of Mental Health (NIMH) provided the Research Domain Criteria (RDoC) as an alternative structure of psychopathology. Compared to the DSM, RDoC is dimensional and it works from the ground up, starting with brain–behavior relations, and linking those to clinical symptoms. RDoC spans multiple domains—such as positive and negative valience systems—and units of analysis, including genes, molecules, cells, circuits, physiology, behaviors, self-reports, and paradigms. Psychophysiological measures include electroencephalography (EEG), event-related potential (ERP), (functional) magnetic resonance imaging (f)MRI, computerized axial tomography (CAT), magnetoencephalography (MEG), functional near infrared spectroscopy (fNIRS), electrocardiography (ECG or EKG), electromyography (EMG), electrooculography (EOG), eyetracking, and actigraphy. Psychophysiological measures are not invulnerable to basic measurement issues, including reliability and validity. Another important consideration of psychophysiological measures is how to handle the rich data without increasing the likelihood of Type I errors. Another class of measurement includes ambulatory assessment. Ambulatory assessment is designed to acquire minimally disruptive measures of a person engaging in normal activities in their natural environment. Ambulatory assessment allows rich measurement including, for example, audio, pictures, video, geolocation (global positioning system [GPS]), and physical activity (accelerometer). Ambulatory biosensors are assessment tools that measure physiological or motor activity, such as those that monitor cardiovascular activity (e.g., heart rate and blood pressure), physical activity (e.g., actigraph and pedometer), cortisol levels, EEG, respiration, muscle tension, blood glucose, skin conductance, and other processes. Virtual reality is an emerging approach that allows structured assessment in simulated but life-like situations. 20.4 Suggested Readings G. A. Miller et al. (2007); Haynes &amp; Yoshioka (2007) References Bennett, C. M., Miller, M. B., &amp; Wolford, G. L. (2009). Neural correlates of interspecies perspective taking in the post-mortem Atlantic Salmon: An argument for multiple comparisons correction. NeuroImage, 47, S125. https://doi.org/10.1016/S1053-8119(09)71202-9 Bennett, C. M., Miller, M. B., &amp; Wolford, G. L. (2010). Neural correlates of interspecies perspective taking in the post-mortem Atlantic Salmon: An argument for multiple comparisons correction. Journal of Serendipitous and Unexpected Results, 1, 1–5. https://teenspecies.github.io/pdfs/NeuralCorrelates.pdf Button, K. S., Ioannidis, J. P. A., Mokrysz, C., Nosek, B. A., Flint, J., Robinson, E. S. J., &amp; Munafo, M. R. (2013a). Confidence and precision increase with high statistical power. Nature Reviews Neuroscience, 14(8), 585–585. https://doi.org/10.1038/nrn3475-c4 Button, K. S., Ioannidis, J. P. A., Mokrysz, C., Nosek, B. A., Flint, J., Robinson, E. S. J., &amp; Munafo, M. R. (2013b). Power failure: Why small sample size undermines the reliability of neuroscience. Nature Reviews Neuroscience, 14(5), 365–376. https://doi.org/10.1038/nrn3475 Calamia, M. (2019). Practical considerations for evaluating reliability in ambulatory assessment studies. Psychological Assessment, 31(3), 285–291. https://doi.org/10.1037/pas0000599 Caspi, A., Houts, R. M., Belsky, D. W., Goldman-Mellor, S. J., Harrington, H., Israel, S., Meier, M. H., Ramrakha, S., Shalev, I., Poulton, R., &amp; Moffitt, T. E. (2014). The p factor: One general psychopathology factor in the structure of psychiatric disorders? Clinical Psychological Science, 2(2), 119–137. https://doi.org/10.1177/2167702613497473 Conradt, E., Crowell, S. E., &amp; Cicchetti, D. (2021). Using development and psychopathology principles to inform the research domain criteria (RDoC) framework. Development and Psychopathology, 33(5), 1521–1525. https://doi.org/10.1017/S0954579421000985 Dubois, J., &amp; Adolphs, R. (2016). Building a science of individual differences from fMRI. Trends in Cognitive Sciences, 20(6), 425–443. https://doi.org/10.1016/j.tics.2016.03.014 Durbin, C. E., Wilson, S., &amp; MacDonald, I., Angus W. (2022). Integrating development into the research domain criteria (RDoC) framework: Introduction to the special section. Journal of Psychopathology and Clinical Science, 131(6), 535–541. https://doi.org/10.1037/abn0000767 Galatzer-Levy, I. R., &amp; Onnela, J.-P. (2023). Machine learning and the digital measurement of psychological health. Annual Review of Clinical Psychology, 19, 133–154. https://doi.org/10.1146/annurev-clinpsy-080921-073212 Haynes, S. N., &amp; Yoshioka, D. T. (2007). Clinical assessment applications of ambulatory biosensors. Psychological Assessment, 19(1), 44–57. https://doi.org/10.1037/1040-3590.19.1.44 Himmelstein, P. H., Woods, W. C., &amp; Wright, A. G. C. (2019). A comparison of signal- and event-contingent ambulatory assessment of interpersonal behavior and affect in social situations. Psychological Assessment, 31(7), 952–960. https://doi.org/10.1037/pas0000718 Holmlund, T. B., Foltz, P. W., Cohen, A. S., Johansen, H. D., Sigurdsen, R., Fugelli, P., Bergsager, D., Cheng, J., Bernstein, J., Rosenfeld, E., &amp; Elvevåg, B. (2019). Moving psychological assessment out of the controlled laboratory setting: Practical challenges. Psychological Assessment, 31(3), 292–303. https://doi.org/10.1037/pas0000647 Kotov, R., Krueger, R. F., Watson, D., Achenbach, T. M., Althoff, R. R., Bagby, R. M., Brown, T. A., Carpenter, W. T., Caspi, A., Clark, L. A., Eaton, N. R., Forbes, M. K., Forbush, K. T., Goldberg, D., Hasin, D., Hyman, S. E., Ivanova, M. Y., Lynam, D. R., Markon, K., … Zimmerman, M. (2017). The hierarchical taxonomy of psychopathology (HiTOP): A dimensional alternative to traditional nosologies. Journal of Abnormal Psychology, 126(4), 454–477. https://doi.org/10.1037/abn0000258 Kotov, R., Krueger, R. F., Watson, D., Cicero, D. C., Conway, C. C., DeYoung, C. G., Eaton, N. R., Forbes, M. K., Hallquist, M. N., Latzman, R. D., Mullins-Sweatt, S. N., Ruggero, C. J., Simms, L. J., Waldman, I. D., Waszczuk, M. A., &amp; Wright, A. G. C. (2021). The hierarchical taxonomy of psychopathology (HiTOP): A quantitative nosology based on consensus of evidence. Annual Review of Clinical Psychology, 17(1), 83–108. https://doi.org/10.1146/annurev-clinpsy-081219-093304 Kozak, M. J., &amp; Cuthbert, B. N. (2016). The NIMH research domain criteria initiative: Background, issues, and pragmatics. Psychophysiology, 53(3), 286–297. https://doi.org/10.1111/psyp.12518 Loken, E., &amp; Gelman, A. (2017). Measurement error and the replication crisis. Science, 355(6325), 584–585. https://doi.org/10.1126/science.aal3618 Lupien, S. J., Sasseville, M., François, N., Giguère, C. E., Boissonneault, J., Plusquellec, P., Godbout, R., Xiong, L., Potvin, S., Kouassi, E., &amp; Lesage, A. (2017). The DSM5/RDoC debate on the future of mental health research: Implication for studies on human stress and presentation of the signature bank. Stress, 20(1), 2–18. https://doi.org/10.1080/10253890.2017.1286324 Miller, G. A., Elbert, T., Sutton, B. P., &amp; Heller, W. (2007). Innovative clinical assessment technologies: Challenges and opportunities in neuroimaging. Psychological Assessment, 19(1), 58–73. https://doi.org/10.1037/1040-3590.19.1.58 Miller, G. A., Rockstroh, B. S., Hamilton, H. K., &amp; Yee, C. M. (2016). Psychophysiology as a core strategy in RDoC. Psychophysiology, 53(3), 410–414. https://doi.org/10.1111/psyp.12581 Patrick, C. J., Iacono, W. G., &amp; Venables, N. C. (2019). Incorporating neurophysiological measures into clinical assessments: Fundamental challenges and a strategy for addressing them. Psychological Assessment, 31(7), 952–960. https://doi.org/10.1037/pas0000713 Raugh, I. M., Chapman, H. C., Bartolomeo, L. A., Gonzalez, C., &amp; Strauss, G. P. (2019). A comprehensive review of psychophysiological applications for ecological momentary assessment in psychiatric populations. Psychological Assessment, 31(3), 304–317. https://doi.org/10.1037/pas0000651 Roberts, A. C., Yeap, Y. W., Seah, H. S., Chan, E., Soh, C.-K., &amp; Christopoulos, G. I. (2019). Assessing the suitability of virtual reality for psychological testing. Psychological Assessment, 31(3), 318–328. https://doi.org/10.1037/pas0000663 Shiffman, S., Stone, A. A., &amp; Hufford, M. R. (2008). Ecological momentary assessment. Annual Review of Clinical Psychology, 4, 1–32. https://doi.org/https://doi.org/10.1146/annurev.clinpsy.3.022806.091415 Smith, G. T., Atkinson, E. A., Davis, H. A., Riley, E. N., &amp; Oltmanns, J. R. (2020). The general factor of psychopathology. Annual Review of Clinical Psychology, 16(1), 75–98. https://doi.org/10.1146/annurev-clinpsy-071119-115848 Stanton, K., McDonnell, C. G., Hayden, E. P., &amp; Watson, D. (2020). Transdiagnostic approaches to psychopathology measurement: Recommendations for measure selection, data analysis, and participant recruitment. Journal of Abnormal Psychology, 129(1), 21–28. https://doi.org/10.1037/abn0000464 Staples, A. D., Bates, J. E., Petersen, I. T., McQuillan, M. E., &amp; Hoyniak, C. (2019). Measuring sleep in young children and their mothers: Identifying actigraphic sleep composites. International Journal of Behavioral Development, 43(3), 278–285. https://doi.org/10.1177/0165025419830236 Stone, A. A., Schneider, S., &amp; Smyth, J. M. (2023). Evaluation of pressing issues in ecological momentary assessment. Annual Review of Clinical Psychology, 19(1), 107–131. https://doi.org/10.1146/annurev-clinpsy-080921-083128 Trull, T. J., &amp; Ebner-Priemer, U. (2013). Ambulatory assessment. Annual Review of Clinical Psychology, 9, 151–176. https://doi.org/https://doi.org/10.1146/annurev-clinpsy-050212-185510 Trull, T. J., &amp; Ebner-Priemer, U. W. (2020). Ambulatory assessment in psychopathology research: A review of recommended reporting guidelines and current practices. Journal of Abnormal Psychology, 129(1), 56–63. https://doi.org/10.1037/abn0000473 Woody, M. L., &amp; Gibb, B. E. (2015). Integrating NIMH Research Domain Criteria (RDoC) into depression research. Current Opinion in Psychology, 4, 6–12. https://doi.org/10.1016/j.copsyc.2015.01.004 Wright, A. G. C., &amp; Zimmermann, J. (2019). Applied ambulatory assessment: Integrating idiographic and nomothetic principles of measurement. Psychological Assessment, 31(12), 1467–1480. https://doi.org/10.1037/pas0000685 "],["cat.html", "Chapter 21 Computers and Adaptive Testing 21.1 Computer-Administered/Online Assessment 21.2 Adaptive Testing 21.3 Getting Started 21.4 Example of Unidimensional CAT 21.5 Creating a Computerized Adaptive Test From an Item Response Theory Model 21.6 Conclusion 21.7 Suggested Readings 21.8 Exercises", " Chapter 21 Computers and Adaptive Testing 21.1 Computer-Administered/Online Assessment Computer-administered and online assessments have the potential to be both desirable and dangerous (Buchanan, 2002). The potential is to have a mental health screening instrument online that a person fills out, is automatically scored, and automatic feedback is provided with suggestions to follow a particular course of action. This would save the clinician time and could lead to a more productive therapy session. As an example, I have clients complete a brief measure of behavioral issues on a tablet when they arrive and are waiting in the waiting room. This is an example of measurement-based care, in which the participant’s treatment response is assessed throughout treatment to help determine whether treatment is working or not, and whether to try something different. In addition, emerging techniques in machine learning may be useful for scoring of high-dimensional data in assessment in clinical practice (Galatzer-Levy &amp; Onnela, 2023). 21.1.1 Advantages There are many advantages of computer-administered or online assessment: Computer-administered and online assessment requires less time on the part of the clinician and administrators than traditional assessment methods. They do not need the clinician to administer and score the assessment, and scores can be entered directly into the client’s chart. Computer-administered and online assessments tend to be less costly than traditional assessment methods. Computer-administered and online assessments can increase access to services because some people can complete the assessment who otherwise would not be able to complete it. For example, some people may be restricted by geographic or financial circumstances. You can provide the questionnaire in multiple languages and modalities, including written, spoken, and pictorial. People may disclose more information to a computer than to a person. The degree of one’s perceived anonymity matters. Perceived anonymity may be especially important for information on sensitive yet important issues, such as suicidality, homicidality, abuse, neglect, criminal behavior, etc. Computerized and online assessment has the potential to be more structured and the potential to do a more comprehensive and accurate assessment than an in-person interview, which tends to be less structured and more biased by clinical judgment (Garb, 2007). Computerized and online assessment tends to provide more comprehensive information than is usually collected in practice. Structured approaches tend to ask more questions. Computers can increase the likelihood that the client answers all questions, because it can prevent clients from skipping questions. Computers can apply “branching logic” to adapt which questions are asked based on the participant’s responses (i.e., adaptive testing) to more efficiently gain greater coverage. Clinicians tend not to ask questions about important issues. Clinicians fail to examine disorders other than the hypothesized ones, which is known as diagnostic overshadowing due to the confirmation bias of looking for evidence-confirming information rather than disconfirming evidence. In general, computers give more diagnoses than clinicians. Clinicians often miss co-occurring disorders. Doing a more comprehensive assessment normalizes that people can and do experience the behavioral issues that they are asked about, and can increase self-disclosure. Greater structure leads to higher inter-rater reliability, less bias, and greater validity. Computerized and online assessment lends itself to continuous monitoring during treatment for measurement-based care. Measurement-based care has been shown to be related to better treatment outcomes. Computerized or online assessment may be important in risk assessment that might otherwise go unrecognized. Treatment utility of computerized assessment has been demonstrated for suicide and substance use. 21.1.2 Validity Challenges There are a number of challenges to the validity of computer-administered and online assessment: The impersonality of computers; however, this does not seem to be a strong threat. Lack of control in the testing situation. The possibility that extraneous factors can influence responses. Language and cultural differences; it is difficult to ask follow-up questions to assess understanding unless a clinician is present. Inability to observe the client’s body language or nonverbals. Computers are not a great medium to assess constructs that could be affected by computer-related anxiety. Some constructs show different ratings online versus in-person. Computer-administered and online assessment may require different norms and cutoffs than paper-and-pencil assessment. Different norms could be necessary, for example, due to increased self-disclosure and higher ratings of negative affect in computer-administered assessment compared to in-person assessment. Careless and fraudulent responding can be common when recruiting an online sample to complete a survey (Chandler et al., 2020). Additional steps may need to be taken to ensure high-quality responses such as CAPTCHA (or reCAPTCHA), internet protocol (IP) verification of location, and attention or validity checks. In general, online assessments of personality seem generally comparable to their paper-and-pencil versions, but their psychometric properties are not identical. In general, computer-administered and online assessments yield the same factor structure as in-person assessments, but particular items may not function in the same way across computer versus in-person assessments. 21.1.3 Ethical Challenges There are important ethical challenges of using computer-administered and online assessments: The internet and app stores provide a medium for the dissemination and proliferation of quackery. Businesses sell access to assessment information despite no evidence of the reliability or validity of the measure. When giving potentially distressing or threatening feedback, it is important to provide opportunities for follow-up sessions or counseling. When using online assessment in research, you can state that release of test scores is not possible, and that participants should consult a professional if they are worried or would like a professional assessment. An additional ethical challenge of computer-administered and online assessment deals with test security. The validity of many tests is based on the assumption that the client is not knowledgeable about the test materials. Detailed information on many protected tests is available on the internet (Ruiz et al., 2002), and it could be used by malingerers to fake health or illness. Data security is another potential challenge of computer-administered and online assessments, including how to protect HIPAA compliance in the clinical context. 21.1.4 Best Practices Below are best practices for computer-administered and online assessments: Only use measures with established reliability and validity, unless you are studying them, in which case you should evaluate their psychometrics. Have a trained clinician review the results of the assessment. Combine computer-administered assessment with clinical judgment. Clients can be expected to make errors completing the assessment, so ask necessary follow-up questions after getting results from computer-administered assessments. Asking follow-up questions can help avoid false positive errors in diagnosis. Check whether a client mistakenly reported something; or whether they under-/over-reported psychopathology. Provide opportunities for follow-up sessions or counseling if distressing or threatening feedback is provided. It may be important to correct for a person’s level of experience with computers when using computerized tasks for clinical decision-making (Lee Meeuw Kjoe et al., 2021). 21.2 Adaptive Testing Adaptive testing involves having the respondent complete only those items that are needed to answer an assessment question. It involves changing which items are administered based on responses to previous items, and administering only a subset of the possible items. Adaptive testing is commonly used in intelligence testing, achievement testing, and aptitude testing. Adaptive testing is not yet commonly used for personality or psychopathology assessment. Most approaches to assessment of personality and psychopathology rely on conventional testing approaches using classical test theory. But many structured clinical interviews, use decision rules to skip a diagnosis, module, or section if a decision rule is not met. Adaptive testing has also been used with observational assessments (e.g., Granziol et al., 2022). A goal of adaptive testing is to get the most accurate estimate of a person’s level on a construct with the fewest items possible. Ideally, you would get similar results between adaptive testing and conventional testing that uses all items, when adaptive testing is done well. There are multiple approaches to adaptive testing. Broadly, one class of approaches uses manual administration, whereas another class of approaches uses computerized adaptive testing (CAT) based on item response theory (IRT). 21.2.1 Manual Administration of Adaptive Testing (Adaptive Testing without IRT) Manual administration of adaptive testing involves moving people up and down in the item difficulty according to their responses. Approaches to manual administration of adaptive testing include using skip rules, basal and ceiling criteria, and the countdown method. 21.2.1.1 Skip Rules Many structured clinical interviews, such as the Structured Clinical Interview for DSM Disorders (SCID) and Mini-International Neuropsychiatric Interview (MINI), use decision rules to skip a diagnosis, module, or section if a decision rule is not met. The skip rules allow the interviews to be more efficient and therefore clinically practical. 21.2.1.2 Basal and Ceiling Approach One approach to manual administration of adaptive testing involves establishing a respondent’s basal and ceiling level. The basal level is the difficulty level at which the examinee answers almost all items correctly. The respondent’s ceiling level is the difficulty level at which the examinee answers almost all items incorrectly. The basal and ceiling criteria set the rules for establishing the basal and ceiling level for each respondent and for when to terminate testing. The goal of adaptive testing is to administer the fewest items necessary to get an accurate estimate of the person’s ability, to save time and prevent the respondent from becoming bored from too-easy items or frustrated from too-difficult items. The basal and ceiling approach starts testing at the recommended starting point for a person’s age or ability level. The examiner scores as items are administered. If the person gets too many items wrong in the beginning, the examiner moves to an earlier starting point, i.e., provides easier items, until the respondent gets most items correct, which establishes their basal level. Then, after establishing the respondent’s basal level, the examiner proceeds forward to progressively more difficult items until they reach their ceiling level, i.e., they get too many wrong in some set, which establishes their ceiling level. 21.2.1.3 Countdown Approach The countdown approach of adaptive testing is a variant of the variable termination criterion approach to adaptive testing. It classifies the respondent into one of two groups—elevated or not elevated—based on whether they exceed the cutoff criterion on a given scale. The cutoff criterion is usually the raw score on the scale that corresponds to a clinical elevation. Two countdown approaches include (1) the classification method and (2) the full scores on elevated scales (FSES) method. The countdown approach to adaptive testing is described by Forbey &amp; Ben-Porath (2007). 21.2.1.3.1 Classification Method Using the classification method to the countdown approach of adaptive testing, the examiner stops administering items once elevation is either ruled in or ruled out. The classification method only tells you whether a client produced an elevated score on the scale. It does not tell you their actual score on that scale. 21.2.1.3.2 Full Scores on Elevated Scales Method Using the full scores on elevated scales method to the countdown approach of adaptive testing, the examiner stops administering items only if elevation is ruled out. The approach generates a score on that scale only for people who produced an elevated score. 21.2.1.4 Summary When doing manual administration with clinical scales, you see the greatest item savings when ordering items from least to most frequently endorsed (i.e., from most to least difficulty) so you can rule people out faster. Ordering items in this way results in a 20–30% item savings on the Minnesota Multiphasic Personality Inventory (MMPI), and corresponding time savings. Comparative studies using the MMPI tend to show comparable results (i.e., similar validity) between the countdown adaptive method and the conventional testing method. That is, the reduction in items does not impair the validity of the adaptive scales. However, you can get even greater item savings when using an IRT approach to adaptive testing. Using IRT, you can order the administration of items differently for each participant to administer the next item that will provide the most information (i.e., precision) for a respondent’s ability given their responses on all previous items. 21.2.2 CAT with IRT Computerized adaptive testing (CAT) based on IRT is widely used in educational testing, including the Graduate Record Examination (GRE) and the Graduate Management Admission Test (GMAT), but it is not widespread in mental health measurement for two reasons: (1) it works best with large item banks; large item banks are generally unavailable for mental health constructs; and (2) mental health constructs are multidimensional and CAT using IRT has primarily been restricted to unidimensional constructs, such as math achievement. However, that is changing; there are now multidimensional IRT approaches that allow simultaneously estimating people’s scores on multiple dimensions, as described in Section 8.6. For example, the higher-order construct of externalizing problems includes sub-dimensions including aggression and rule-breaking. A CAT is designed to locate a person’s level on the construct (theta) with as few items as possible. CAT has potential for greater item savings than the countdown approach, but it has more assumptions. IRT can be used to create CATs using information gleaned from the IRT model’s item parameters, including discrimination and difficulty (or severity). The item’s discrimination indicates how strongly the item is related to the construct. For a strong measure, you want highly discriminating items that are strongly related to the construct. In terms of item difficulty/severity, you want items that span the full distribution of difficulty/severity so they are non-redundant. An IRT-based CAT starts with a highly discriminating item at the 50th percentile of the distribution (severity) (or at whatever level is the best estimate of the person’s ability/severity before the CAT is administered). Then, based on the participant’s response, it generates a provisional estimate of the person’s level on the construct and administers the item that will provide the most information. For example, if the respondent gets the first item correct, the CAT administers a highly discriminating item that might be at the 75th percentile of the distribution (severity). And so on, based on the participants’ responses. As described in Section 8.1.6, item information is how much measurement precision for the construct is provided by a particular item. In other words, iitem information indicates how much the item reduces the standard error of measurement; i.e., how much the item reduces uncertainty of our estimate of the respondent’s construct level. An IRT-based CAT continues to generate provisional estimates of the person’s construct level and updates it based on new responses. The CAT tries to find the construct level where the respondent keeps getting items right and wrong (or endorsing versus not endorsing) 50% of the time. This process continues until the uncertainty in the person’s estimated construct level is smaller than a pre-defined threshold—that is, when we are fairly confident (to some threshold) about a person’s construct level. CAT is a promising approach that saves time because it tailors which items are administered to which person and in which order based on their construct level to get the most reliable estimate in the shortest time possible. It administers items that are appropriate to the person’s construct level, and it does not administer items that are way too easy or way too hard for the person, which saves time, boredom, and frustration. It shortens the assessment because not all items are administered, and it typically results in around 50% item savings or more. It also allows for increased measurement precision, because you are drilling down deeper (i.e., asking more items) near the person’s construct level. You can control the measurement precision of the CAT because you can specify the threshold of measurement precision (i.e., standard error of measurement) at which testing is terminated. For example, you could have the CAT terminate when the standard error of measurement of a person’s construct level becomes less than 0.2. However, CATs assume that if you get a more difficult item correct, you would have gotten easier items correct, which might not be true in all contexts (especially for constructs that are not unidimensional). For example, just because a person does not endorse low-severity symptoms does not necessarily mean that they will not endorse higher-severity symptoms, especially when assessing a multidimensional construct. Simpler CATs are built using unidimensional IRT. But many aspects of psychopathology are not unidimensional, and would violate assumptions of unidimensional IRT. For example, the externalizing spectrum includes sub-dimensions including aggression, disinhibition, and substance use. You can use multidimensional IRT to build CATs, though this is more complicated. For instance, you could estimate a bifactor model in which each item (e.g., “hits others”) loads onto the general latent factor (e.g., externalizing problems) and its specific sub-dimension (e.g., aggression versus rule-breaking). Computerized adaptive testing of mental health disorders is reviewed by Gibbons et al. (2016). Well-designed CATs show equivalent reliability and validity to their full-scale counterparts. By contrast, many short forms are not as accurate as their full-scale counterparts. Part of the reason that CATs tend to do better than short forms is that the CATs are adaptive; they determine which items to administer based on the participant’s responses to prior items, unlike short forms. For guidelines on developing and evaluating short forms, see Smith et al. (2000). 21.3 Getting Started 21.3.1 Load Libraries Code library(&quot;petersenlab&quot;) #to install: install.packages(&quot;remotes&quot;); remotes::install_github(&quot;DevPsyLab/petersenlab&quot;) library(&quot;mirtCAT&quot;) library(&quot;here&quot;) library(&quot;tinytex&quot;) 21.4 Example of Unidimensional CAT The computerized adaptive test (CAT) was fit using the mirtCAT package (Chalmers, 2021). The example of a CAT with unidimensional data is adapted from mirtCAT documentation: https://philchalmers.github.io/mirtCAT/html/unidim-exampleGUI.html (archived at https://perma.cc/3ZW4-DAUR) 21.4.1 Define Population IRT Parameters For reproducibility, we set the seed below. Using the same seed will yield the same answer every time. There is nothing special about this particular seed. Code set.seed(52242) nitems &lt;- 100 itemnames &lt;- paste(&quot;Item.&quot;, 1:nitems, sep = &quot;&quot;) a &lt;- matrix(rlnorm(nitems, meanlog = .2, sdlog = .3)) d &lt;- matrix(rnorm(nitems, mean = 0, sd = 1)) pars &lt;- data.frame(a1 = a, d = d, g = 0.2) 21.4.2 Fit IRT Model Code mod &lt;- generate.mirt_object(pars, &quot;3PL&quot;) 21.4.3 Model Summary Code summary(mod) F1 h2 Item.1 0.673 0.453 Item.2 0.369 0.136 Item.3 0.548 0.300 Item.4 0.500 0.250 Item.5 0.783 0.614 Item.6 0.579 0.335 Item.7 0.733 0.538 Item.8 0.377 0.142 Item.9 0.535 0.286 Item.10 0.503 0.253 Item.11 0.546 0.298 Item.12 0.591 0.349 Item.13 0.630 0.397 Item.14 0.597 0.356 Item.15 0.583 0.340 Item.16 0.389 0.151 Item.17 0.570 0.325 Item.18 0.460 0.211 Item.19 0.683 0.466 Item.20 0.507 0.257 Item.21 0.547 0.300 Item.22 0.581 0.337 Item.23 0.784 0.614 Item.24 0.546 0.298 Item.25 0.551 0.303 Item.26 0.615 0.379 Item.27 0.567 0.321 Item.28 0.554 0.307 Item.29 0.625 0.391 Item.30 0.733 0.537 Item.31 0.737 0.544 Item.32 0.458 0.210 Item.33 0.520 0.271 Item.34 0.575 0.331 Item.35 0.532 0.283 Item.36 0.448 0.201 Item.37 0.481 0.231 Item.38 0.485 0.236 Item.39 0.459 0.210 Item.40 0.656 0.430 Item.41 0.682 0.464 Item.42 0.596 0.355 Item.43 0.682 0.465 Item.44 0.564 0.319 Item.45 0.392 0.154 Item.46 0.538 0.290 Item.47 0.490 0.240 Item.48 0.641 0.411 Item.49 0.519 0.269 Item.50 0.505 0.255 Item.51 0.490 0.240 Item.52 0.530 0.281 Item.53 0.319 0.101 Item.54 0.834 0.695 Item.55 0.643 0.414 Item.56 0.489 0.239 Item.57 0.563 0.317 Item.58 0.538 0.289 Item.59 0.627 0.393 Item.60 0.598 0.357 Item.61 0.480 0.230 Item.62 0.523 0.273 Item.63 0.586 0.343 Item.64 0.650 0.422 Item.65 0.652 0.425 Item.66 0.723 0.522 Item.67 0.447 0.199 Item.68 0.524 0.274 Item.69 0.489 0.239 Item.70 0.783 0.613 Item.71 0.680 0.462 Item.72 0.455 0.207 Item.73 0.452 0.204 Item.74 0.614 0.377 Item.75 0.629 0.396 Item.76 0.595 0.354 Item.77 0.651 0.424 Item.78 0.515 0.265 Item.79 0.671 0.450 Item.80 0.464 0.215 Item.81 0.612 0.375 Item.82 0.491 0.241 Item.83 0.489 0.239 Item.84 0.385 0.148 Item.85 0.455 0.207 Item.86 0.402 0.161 Item.87 0.537 0.288 Item.88 0.651 0.424 Item.89 0.662 0.439 Item.90 0.433 0.188 Item.91 0.562 0.315 Item.92 0.752 0.565 Item.93 0.572 0.328 Item.94 0.518 0.269 Item.95 0.559 0.312 Item.96 0.714 0.509 Item.97 0.714 0.510 Item.98 0.591 0.349 Item.99 0.346 0.120 Item.100 0.443 0.196 SS loadings: 32.822 Proportion Var: 0.328 Factor correlations: F1 F1 1 Code coef(mod, simplify = TRUE, IRTpars = TRUE) $items a b g u Item.1 1.549 0.298 0.2 1 Item.2 0.676 0.742 0.2 1 Item.3 1.115 -0.536 0.2 1 Item.4 0.982 -0.448 0.2 1 Item.5 2.145 -0.380 0.2 1 Item.6 1.208 0.409 0.2 1 Item.7 1.836 -0.606 0.2 1 Item.8 0.694 2.000 0.2 1 Item.9 1.077 -0.991 0.2 1 Item.10 0.991 0.570 0.2 1 Item.11 1.109 0.493 0.2 1 Item.12 1.246 0.363 0.2 1 Item.13 1.382 -0.417 0.2 1 Item.14 1.267 0.925 0.2 1 Item.15 1.222 0.405 0.2 1 Item.16 0.718 2.062 0.2 1 Item.17 1.182 -0.233 0.2 1 Item.18 0.881 -0.151 0.2 1 Item.19 1.591 0.647 0.2 1 Item.20 1.000 -0.120 0.2 1 Item.21 1.113 -0.430 0.2 1 Item.22 1.214 -1.437 0.2 1 Item.23 2.146 -0.025 0.2 1 Item.24 1.110 0.204 0.2 1 Item.25 1.123 -0.584 0.2 1 Item.26 1.329 0.127 0.2 1 Item.27 1.171 -0.421 0.2 1 Item.28 1.132 0.136 0.2 1 Item.29 1.363 0.425 0.2 1 Item.30 1.834 0.019 0.2 1 Item.31 1.858 -0.215 0.2 1 Item.32 0.878 1.927 0.2 1 Item.33 1.037 0.396 0.2 1 Item.34 1.197 -0.304 0.2 1 Item.35 1.069 -1.655 0.2 1 Item.36 0.853 0.979 0.2 1 Item.37 0.933 0.799 0.2 1 Item.38 0.945 -0.062 0.2 1 Item.39 0.879 1.319 0.2 1 Item.40 1.477 0.629 0.2 1 Item.41 1.585 0.136 0.2 1 Item.42 1.262 0.913 0.2 1 Item.43 1.587 0.508 0.2 1 Item.44 1.164 -1.134 0.2 1 Item.45 0.725 -0.191 0.2 1 Item.46 1.087 0.724 0.2 1 Item.47 0.957 1.837 0.2 1 Item.48 1.423 0.155 0.2 1 Item.49 1.033 0.892 0.2 1 Item.50 0.996 -0.109 0.2 1 Item.51 0.958 1.628 0.2 1 Item.52 1.063 0.368 0.2 1 Item.53 0.572 -1.014 0.2 1 Item.54 2.568 0.403 0.2 1 Item.55 1.430 -0.931 0.2 1 Item.56 0.955 -0.193 0.2 1 Item.57 1.160 0.829 0.2 1 Item.58 1.086 0.126 0.2 1 Item.59 1.371 -0.150 0.2 1 Item.60 1.269 0.220 0.2 1 Item.61 0.931 0.868 0.2 1 Item.62 1.044 0.597 0.2 1 Item.63 1.231 -0.278 0.2 1 Item.64 1.455 1.303 0.2 1 Item.65 1.464 -0.644 0.2 1 Item.66 1.780 -0.313 0.2 1 Item.67 0.850 1.466 0.2 1 Item.68 1.046 -1.062 0.2 1 Item.69 0.953 0.441 0.2 1 Item.70 2.141 -0.051 0.2 1 Item.71 1.576 -0.743 0.2 1 Item.72 0.870 -0.833 0.2 1 Item.73 0.862 -0.859 0.2 1 Item.74 1.325 -0.981 0.2 1 Item.75 1.378 -0.196 0.2 1 Item.76 1.259 0.496 0.2 1 Item.77 1.459 0.795 0.2 1 Item.78 1.021 0.834 0.2 1 Item.79 1.539 -0.290 0.2 1 Item.80 0.891 0.943 0.2 1 Item.81 1.319 -0.067 0.2 1 Item.82 0.960 1.233 0.2 1 Item.83 0.953 -1.236 0.2 1 Item.84 0.710 -1.120 0.2 1 Item.85 0.870 0.592 0.2 1 Item.86 0.746 -2.303 0.2 1 Item.87 1.082 -0.205 0.2 1 Item.88 1.460 -0.297 0.2 1 Item.89 1.505 0.138 0.2 1 Item.90 0.819 0.273 0.2 1 Item.91 1.155 0.149 0.2 1 Item.92 1.939 0.153 0.2 1 Item.93 1.188 0.856 0.2 1 Item.94 1.032 -1.256 0.2 1 Item.95 1.147 0.432 0.2 1 Item.96 1.734 -0.289 0.2 1 Item.97 1.737 0.262 0.2 1 Item.98 1.247 -0.538 0.2 1 Item.99 0.628 -0.839 0.2 1 Item.100 0.841 -0.259 0.2 1 $means F1 0 $cov F1 F1 1 Code modItemParameters &lt;- coef( mod, simplify = TRUE, IRTpars = TRUE)$items 21.4.4 Model Plots A test characteristic curve of the measure is in Figure 21.1. Code plot(mod) Figure 21.1: Test Characteristic Curve. The test information and standard error of measurement as a function of the person’s construct level (theta; \\(\\theta\\)) is in Figure 21.2. The test appears to measure theta (\\(\\theta\\)) with \\(SE &lt; .4\\) within a range of −1.5 to 2. Code plot(mod, type = &quot;infoSE&quot;, theta_lim = c(-3, 3)) Figure 21.2: Test Information and Standard Error of Measurement. 21.4.5 Item Plots Item characteristic curves are in Figure 21.3. Code plot(mod, type = &quot;trace&quot;, theta_lim = c(-3, 3)) Figure 21.3: Item Characteristic Curves. Item information curves are in Figure 21.4. Code plot(mod, type = &quot;infotrace&quot;, theta_lim = c(-3, 3)) Figure 21.4: Item Information Curves. Item 30 would be a good starting item; it has a difficulty closest to the 50th percentile \\((b = 0.019)\\) and a high discrimination \\((a = 1.834)\\). Item 70 would be a good starting item; it has a difficulty closest to the 50th percentile \\((b = -0.051)\\) and a high discrimination \\((a = 2.141)\\). Item 86 is an easy item \\((b = -2.303)\\). Item 16 is a difficult item \\((b = 2.062)\\). Item 53 has a low discrimination \\((a = 0.572)\\). Item characteristic curves and information curves for these items are in Figures 21.5–21.9. Code itemplot(object = mod, item = &quot;Item.30&quot;, &quot;infotrace&quot;) Figure 21.5: Item Characteristic Curves and Information Curves: Item 30. Code itemplot(object = mod, item = &quot;Item.70&quot;, &quot;infotrace&quot;) Figure 21.6: Item Characteristic Curves and Information Curves: Item 70. Code itemplot(object = mod, item = &quot;Item.86&quot;, &quot;infotrace&quot;) Figure 21.7: Item Characteristic Curves and Information Curves: Item 86. Code itemplot(object = mod, item = &quot;Item.16&quot;, &quot;infotrace&quot;) Figure 21.8: Item Characteristic Curves and Information Curves: Item 16. Code itemplot(object = mod, item = &quot;Item.53&quot;, &quot;infotrace&quot;) Figure 21.9: Item Characteristic Curves and Information Curves: Item 53. 21.4.6 Create Math Items Code questions &lt;- answers &lt;- character(nitems) choices &lt;- matrix(&quot;a&quot;, nitems, 5) spacing &lt;- floor(d - min(d)) + 1 #easier items have more variation for(i in 1:nitems){ n1 &lt;- sample(1:100, 1) n2 &lt;- sample(101:200, 1) ans &lt;- n1 + n2 questions[i] &lt;- paste(n1, &quot; + &quot;, n2, &quot; = ?&quot;, sep = &quot;&quot;) answers[i] &lt;- as.character(ans) ch &lt;- ans + sample(c(-5:-1, 1:5) * spacing[i,], 5) ch[sample(1:5, 1)] &lt;- ans choices[i,] &lt;- as.character(ch) } df &lt;- data.frame( Questions = questions, Answer = answers, Option = choices, Type = &quot;radio&quot;) 21.4.7 Run Computerized Adaptive Test (CAT) Set the minimum standard error of measurement for the latent trait (theta; \\(\\theta\\)) that must be reached before stopping the CAT. You can lengthen the CAT by lowering the minimum standard error of measurement, or you can shorten the CAT by raising the minimum standard error of measurement. Code minimum_SEM &lt;- .3 Run the CAT and stop once the standard error of measurement for the latent trait (theta; \\(\\theta\\)) becomes \\(0.3\\) or lower. Code result &lt;- mirtCAT( df, mod, start_item = &quot;MI&quot;, method = &quot;EAP&quot;, criteria = &quot;MI&quot;, design = list(min_SEM = minimum_SEM)) Code RNGkind(&quot;default&quot;, &quot;default&quot;, &quot;default&quot;) 21.4.8 CAT Results Code print(result) n.items.answered Theta_1 SE.Theta_1 27 0.238911 0.2975387 Code summary(result) $final_estimates Theta_1 Estimates 0.2389110 SEs 0.2975387 $raw_responses [1] &quot;230&quot; &quot;170&quot; &quot;167&quot; &quot;177&quot; &quot;175&quot; &quot;216&quot; &quot;177&quot; &quot;218&quot; &quot;184&quot; &quot;191&quot; &quot;184&quot; &quot;202&quot; [13] &quot;294&quot; &quot;204&quot; &quot;200&quot; &quot;204&quot; &quot;264&quot; &quot;242&quot; &quot;177&quot; &quot;189&quot; &quot;267&quot; &quot;200&quot; &quot;235&quot; &quot;149&quot; [25] &quot;248&quot; &quot;180&quot; &quot;239&quot; $scored_responses [1] 1 1 1 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 1 1 $items_answered [1] 70 54 92 19 43 23 97 30 1 41 40 31 89 5 66 96 48 79 7 88 75 59 13 26 71 [26] 81 29 $thetas_history Theta_1 [1,] 0.0000000 [2,] 0.4056545 [3,] 0.7815060 [4,] 0.9750919 [5,] 1.1561966 [6,] 0.7421218 [7,] 0.8464452 [8,] 0.5304194 [9,] 0.6256858 [10,] 0.7128811 [11,] 0.7809691 [12,] 0.6338431 [13,] 0.6792420 [14,] 0.5228702 [15,] 0.5589624 [16,] 0.3679772 [17,] 0.4110162 [18,] 0.3112852 [19,] 0.3505818 [20,] 0.1782808 [21,] 0.2190867 [22,] 0.2576467 [23,] 0.1698955 [24,] 0.2023291 [25,] 0.1355895 [26,] 0.1602090 [27,] 0.1960021 [28,] 0.2389110 $thetas_SE_history Theta_1 [1,] 1.0000000 [2,] 0.9069127 [3,] 0.8093285 [4,] 0.7266039 [5,] 0.6878798 [6,] 0.6098350 [7,] 0.5592082 [8,] 0.5192925 [9,] 0.4820574 [10,] 0.4630971 [11,] 0.4487481 [12,] 0.4224150 [13,] 0.4079867 [14,] 0.3918681 [15,] 0.3778294 [16,] 0.3693179 [17,] 0.3568027 [18,] 0.3507425 [19,] 0.3408871 [20,] 0.3437896 [21,] 0.3332037 [22,] 0.3252183 [23,] 0.3240032 [24,] 0.3164469 [25,] 0.3147717 [26,] 0.3076080 [27,] 0.3017742 [28,] 0.2975387 $terminated_sucessfully [1] TRUE 21.4.8.1 CAT Standard Errors Standard errors of measurement of a person’s estimated construct level (theta; \\(\\theta\\)) as a function of the item (presented in the order that items were administered as part of the CAT) are in Figure 21.10. Initially, the respondent got the first items correct, which raised the estimate of their construct level. However, as the respondent got more items incorrect, the CAT converged upon the estimate that the person has a construct level around \\(theta = 0.24\\), which means that the person scored slightly above average (i.e., 0.24 standard deviations above the mean). Code plot(result, SE = 1) Figure 21.10: Standard Errors of Measurement Around Theta in a Computerized Adaptive Test. 21.4.8.2 CAT 95% Confidence Interval 95% confidence intervals of a person’s estimated construct level (theta; \\(\\theta\\)) as a function of the item (presented in the order that items were administered as part of the CAT) are in Figure 21.11. Code plot(result, SE = qnorm(.975)) Figure 21.11: 95% Confidence Interval of Theta in a Computerized Adaptive Test. 21.5 Creating a Computerized Adaptive Test From an Item Response Theory Model You can create a computerized adaptive test from any item response theory model. 21.5.1 Create Items For instance, below we create a matrix with the questions and response options from the graded response model that we fit in Section 8.9 in Chapter 8 on item response theory. Code numItemsGRM &lt;- nrow(coef( gradedResponseModel, simplify = TRUE, IRTpars = TRUE)$items) names(Science) [1] &quot;Comfort&quot; &quot;Work&quot; &quot;Future&quot; &quot;Benefit&quot; Code questionsGRM &lt;- c( &quot;Science and technology are making our lives healthier, easier and more comfortable.&quot;, &quot;The application of science and new technology will make work more interesting.&quot;, &quot;Thanks to science and technology, there will be more opportunities for the future generations.&quot;, &quot;The benefits of science are greater than any harmful effect it may have.&quot; ) responseOptionsGRM &lt;- c( &quot;strongly disagree&quot;, &quot;disagree to some extent&quot;, &quot;agree to some extent&quot;, &quot;strongly agree&quot; ) numResponseOptionsGRM &lt;- length(responseOptionsGRM) choicesGRM &lt;- matrix(&quot;a&quot;, numItemsGRM, numResponseOptionsGRM) for(i in 1:numItemsGRM){ choicesGRM[i,] &lt;- responseOptionsGRM } dfGRM &lt;- data.frame( Questions = questionsGRM, Option = choicesGRM, Type = &quot;radio&quot;) 21.5.2 Run Computerized Adaptive Test Then you can create and run the computerized adaptive model: Code resultGRM &lt;- mirtCAT( dfGRM, gradedResponseModel, start_item = &quot;MI&quot;, method = &quot;EAP&quot;, criteria = &quot;MI&quot;, design = list(min_SEM = minimum_SEM)) 21.6 Conclusion Computer-administered and online assessments have the potential to be both desirable and dangerous. They have key advantages; at the same time, they have both validity and ethical challenges. Best practices for computer-administered and online assessments are provided. Adaptive testing involves having the respondent complete only those items that are needed to answer an assessment question, which can save immense time without sacrificing validity (if done well). There are many approaches to adaptive testing, including manual administration—such as skip rules, basal and ceiling criteria, and the countdown approach—and computerized adaptive testing (CAT) using item response theory. A CAT is designed to locate a person’s level on the construct with as few items as possible. A CAT administers the items that will provide the most information based on participants’ previous responses. CAT typically results in the greatest item savings—around 50% item savings or more. 21.7 Suggested Readings Buchanan (2002); Gibbons et al. (2016) 21.8 Exercises 21.8.1 Questions 21.8.2 Answers References Buchanan, T. (2002). Online assessment: Desirable or dangerous? Professional Psychology: Research and Practice, 33(2), 148–154. https://doi.org/10.1037/0735-7028.33.2.148 Chalmers, P. (2021). mirtCAT: Computerized adaptive testing with multidimensional item response theory. https://CRAN.R-project.org/package=mirtCAT Chandler, J., Sisso, I., &amp; Shapiro, D. (2020). Participant carelessness and fraud: Consequences for clinical research and potential solutions. Journal of Abnormal Psychology, 129(1), 49–55. https://doi.org/10.1037/abn0000479 Forbey, J. D., &amp; Ben-Porath, Y. S. (2007). Computerized adaptive personality testing: A review and illustration with the MMPI-2 computerized adaptive version. Psychological Assessment, 19(1), 14–24. https://doi.org/10.1037/1040-3590.19.1.14 Galatzer-Levy, I. R., &amp; Onnela, J.-P. (2023). Machine learning and the digital measurement of psychological health. Annual Review of Clinical Psychology, 19, 133–154. https://doi.org/10.1146/annurev-clinpsy-080921-073212 Garb, H. N. (2007). Computer-administered interviews and rating scales. Psychological Assessment, 19(1), 4–13. https://doi.org/10.1037/1040-3590.19.1.4 Gibbons, R. D., Weiss, D. J., Frank, E., &amp; Kupfer, D. (2016). Computerized adaptive diagnosis and testing of mental health disorders. Annual Review of Clinical Psychology, 12(1), 83–104. https://doi.org/10.1146/annurev-clinpsy-021815-093634 Granziol, U., Brancaccio, A., Pizziconi, G., Spangaro, M., Gentili, F., Bosia, M., Gregori, E., Luperini, C., Pavan, C., Santarelli, V., Cavallaro, R., Cremonese, C., Favaro, A., Rossi, A., Vidotto, G., &amp; Spoto, A. (2022). On the implementation of computerized adaptive observations for psychological assessment. Assessment, 29(2), 225–241. https://doi.org/10.1177/1073191120960215 Lee Meeuw Kjoe, P. R., Agelink van Rentergem, J. A., Vermeulen, I. E., &amp; Schagen, S. B. (2021). How to correct for computer experience in online cognitive testing? Assessment, 28(5), 1247–1255. https://doi.org/10.1177/1073191120911098 Ruiz, M. A., Drake, E. B., Glass, A., Marcotte, D., &amp; Gorp, W. G. van. (2002). Trying to beat the system: Misuse of the internet to assist in avoiding the detection of psychological symptom dissimulation. Professional Psychology: Research and Practice, 33(3), 294–299. https://doi.org/10.1037/0735-7028.33.3.294 Smith, G. T., McCarthy, D. M., &amp; Anderson, K. G. (2000). On the sins of short-form development. Psychological Assessment, 12(1), 102–111. https://doi.org/10.1037/1040-3590.12.1.102 "],["behavioral.html", "Chapter 22 Behavioral Assessment 22.1 Overview of Behavioral Assessment 22.2 Contexts for Observing 22.3 Costs of Behavioral Observation 22.4 Dependent Variable 22.5 Functional Behavioral Assessment/Analysis 22.6 Mental Status Exam 22.7 Reliability 22.8 Validity 22.9 Forms of Measurement 22.10 Analogue (Structured) Observational Assessments 22.11 Self-Monitoring 22.12 Behavior Rating Scales 22.13 Assessment of Therapeutic Process 22.14 Conclusion 22.15 Suggested Readings", " Chapter 22 Behavioral Assessment 22.1 Overview of Behavioral Assessment In other chapters, we discuss using psychophysiological assessment, cognitive assessment, and other assessment techniques. Psychology is often concerned about understanding behavior. To assess behavior, one approach would be to observe people. Observing people allows seeing the dynamic of the behavior unfold in relation to other processes. Observations might also allow getting around examinees’ response biases such as social desirability. Behavioral observation provides a temporal stream of behavior to see which behaviors precede other behaviors. An overview of observational assessment is provided by Girard &amp; Cohn (2016). When conducting observations, it is important to consider the context and the environment. 22.1.1 Approaches Examples of behavioral assessment include naturalistic behavioral observation, analogue (structured) observation, self-monitoring/diary, behavior ratings (e.g., informant report), and self-report. Self-report is essentially retrospective self-monitoring. I published a practice brief for the assessment of externalizing behavior in school-aged children that integrates these different methods of behavioral assessment (Petersen, 2024a). Naturalistic behavioral observation involves observing a person in their natural context or habitat. For instance, it might involve a person in their home, at school, at work, or their everyday interpersonal interactions. The observer is like a “fly on the wall” in that they want to exert as little effect as possible on the person’s behavior. The observation occurs within normal situational contexts and under naturally occurring reinforcements and consequences. Naturalistic observations were the origins of behavioral assessment. Analogue observation, also called structured observation, is the observation of a person in a contrived environment (Haynes, 2001). The goal of analogue observation is for the contrived situation to be analogous to the situations that the client is likely to encounter in their natural environment. For instance, the examiner might set up similar conditions in a lab or therapy room. Analogue observations can differ in the degree to which the observation is structured, but they are typically more structured than naturalistic observation. Analogue observations might involve role-plays, family or marital interaction tasks, and compliance (do/don’t) tasks. As an example, the observer might observe role-played scenarios in which the person engages in one or more simulated social interactions. 22.2 Contexts for Observing There are multiple contexts in which we can observe people. Here are just a few examples: Classrooms Homes In the community; for example, you could take a client out into the world to see how they interact with others Group homes Psychiatric hospitals Token economies for people with schizophrenia were based on behavioral assessment in psychiatric hospitals. A token economy is a contingency management system based on operant conditioning. Operant conditioning occurs when the frequency of behavior is influenced by the outcomes of the behavior. In a token economy, a secondary reinforcer, such as a token or symbol, is given to the client for matching the target behavior, which can be exchanged for back-up reinforcers. For instance, a token may be given to the client for making their bed. The tokens could then be turned in for back-up reinforcers. Back-up reinforcers include material reinforcers such as food, money, and cigarettes; services, such as having your room cleaned; and privileges such as getting to make a phone call. Token economies have been shown to work and to influence people’s behavior, especially the negative symptoms of schizophrenia. However, token economies are not widely used because of political issues. 22.3 Costs of Behavioral Observation Behavioral observations are costly due to people and time. The cost limits the amount of observation time that is possible, which raises questions about how typical and representative the participant’s behavior was during the observation period relative to their everyday life. The limited amount of observation time raises questions about reliability and external validity, that is the generalizability of the person’s behavior in the situation to their typical functioning. A gold standard for naturalistic observations would be to observe someone in their natural context across more than one day, time, situation, and context with more than one rater to get a more robust sampling of the person’s behavior. But it is costly to do this, and is therefore uncommon in practice. 22.4 Dependent Variable After (or during) the observation, we need to turn the observation into information that can be useful. To do that, we create a dependent variable using behavioral coding. Behavioral coding involves observing behavior and quantifying it, that is turning behavior into numbers that can be analyzed to make inferences. During observation, you can measure the frequency of the behavior (how many times a behavior occurs), which is a useful measure of individual differences. But you may want to know the stream of behavior, so you know when the behavior occurred. So, you might make a coding sample at regular intervals (e.g., every 6 seconds). Coding systems can differ in their granularity with which they treat time. In event-based coding, the coder codes whenever a discrete behavior occurs—i.e., the behavior has discrete start and stop points. For example, an event-based coding system may determine when a person is talking. Event-based coding allows assessing the number, duration, and sequence of behaviors with high temporal precision. In interval-based coding, the coder codes during specified spans of time that are called intervals. Intervals can be of any length. Shorter intervals provide more temporal precision but are more burdensome to code. Longer intervals provide less temporal precision but are less burdensome to code. An example of interval-based coding would be coding the extent of positive affect that a person showed during each 10-second interval. Coding can focus on the small units of behavior or what the behavior means. Sign-based coding involves focusing on small units of behavior, such as movements. Message-based coding involves focusing on the meaning of the behavior using theory. For example, message-based coding might attempt to code examinees’ emotional states. Sign-based coding requires a lesser degree of inference, has less subjectivity, and has higher inter-rater reliability than message-based coding. On the other hand, message-based coding is potentially more theoretically interesting than sign-based coding. 22.5 Functional Behavioral Assessment/Analysis One goal of behavioral assessment is to determine the function of the problem behavior. Every behavior has a function, reason, or purpose (or multiple ones). If you can determine the function(s) of the behavior, you may be able to help the client learn more adaptive ways of accomplishing their goals. To determine the function(s) of behavior, functional behavioral assessments (FBAs) are commonly conducted, especially in schools. FBA involves identifying the patterns of sequences that involve the problem behavior, noting the following: The time The context What led up to the problem behavior (antecedents) What problem behavior occurred What happened after the problem behavior (consequences) How the situation resolved An FBA is based on the antecedent-behavior-consequence (A-B-C) model. The idea of the A-B-C model is that every behavior is a function of its antecedents and consequences. The goal of the A-B-C model is to identify common patterns of antecedents and consequences. Examples of antecedents include insufficient sleep, a change in routine, and an ineffective command. Examples of consequences include giving attention, and getting to play without having to pick up the toys. By identifying the patterns of antecedents and consequences, we can generate hypotheses for the function of the behavior. For example, a possible hypothesis for a function of given problem behavior might be to get attention, to get out of doing what they do not want to do (escape or avoidance), to get access to a preferable activity or object, or to engage in sensory stimulation (e.g., rocking). You can design treatment to change the antecedents and consequences to show them more adaptive ways of behaving. One approach to case conceptualization or case formulation is the “5 Ps”: protective factors, predisposing factors, precipitating factors, perpetuating factors, and prognosis. Treatment would focus on addressing the perpetuating (maintaining) factors, and helping the client identify the triggering (precipitating) factors and how to handle those, while building on their strengths (protective factors) and considering their vulnerabilities (predisposing factors), with the intensity of treatment matched to the prognosis. 22.6 Mental Status Exam A mental status exam is a structured assessment of a client’s cognition, affect, etc. A mental status exam is often used in psychiatric hospitals to quickly “size up” a patient. Mental status exams are also useful in outpatient settings. For training clinicians, it can be useful to get in the habit of performing mental status exams. 22.7 Reliability It is important to consider the reliability of behavioral observations. From a generalizability perspective, we aim to generalize behavioral observations across, raters, contexts, and time points. Thus, inter-rater reliability and test–retest reliability are especially relevant. Averaging ratings across multiple raters can yield scores that are more reliable than the scores from the individual raters, assuming both raters have inter-rater reliability. 22.8 Validity Validity is also an important consideration of behavioral observations. In general, little attention has been paid to validity of observational measures, particularly those used in clinical practice. This is a content validity problem. When coding, the examiner makes choices about what to code; they are not “writing down the real thing”. 22.8.1 Reactivity Another concern of observational measures is response biases, such as faking good, faking bad, and impression control. For instance, people may act differently because they are being observed, which is a form of reactivity. People could be behaving better or worse than they typically do, but observation tends to push people to the extremes. 22.8.2 Coding Regarding coders and observers, it is ideal during training to provide them immediate corrective feedback. It is ideal for them to be blind to hypotheses, so they are not biased in favor of a particular outcome. It can be helpful to periodically compare coders’ ratings to criterion ratings that are accepted as the gold-standard ratings to prevent coder drift. Coder drift occurs when there are changes in coders’ ratings over time. You can evaluate intra-observer reliability by having them rate the same (video-recorded) behavior at two different time points. You can have two or more raters code at least a subset of the participants, to establish inter-rater reliability. 22.9 Forms of Measurement There are several forms of measurement that behavioral assessments can take. One form of measurement from observation is notes, which involves qualitative data. For rating scales, the form of measurement is often Likert scales. Likert scales assess the observer’s impression of the examineee’s behavior, which is a simpler way of getting the same information as many approaches to video coding. Rating scales can be completed in the context of, for example, a home observation, parent report, teacher report, or school observation. It can be helpful to examine correlations between these different methods and contexts. Having multiple forms of measurement is great for research but is difficult to do in clinical practice because it comes at a tremendous expense. There is also difficulty in observing what you want to observe. For example, aggression is a relatively low base rate phenomenon, so the client may not engage in aggressive behavior during the observation window. There are also questions of the incremental validity of naturalistic observational assessments because simpler measures (including analogue assessments) could get at the same thing. 22.10 Analogue (Structured) Observational Assessments Analogue or structured assessments use tasks that evoke individual differences in the behavior of interest. For instance, in our lab we have forbidden toys on a shelf, and we see how the children and parents handle that situation when they are in the room together, then when the parent is occupied on the phone, and then when the parent leaves the room and the child is alone in the room. In the clinic, we have a parent tell their child to clean up the toys so we can observe how the parent gives the command and how the child responds. To study marital conflict, researchers may have the couple provide a list of topics they disagree about, and have them have a discussion about one of those topics to evoke an argument in the lab. To study stress and anxiety, or to perform exposure in the clinic, the clinician or researcher may have the client/participant give a public speech in a simulated session. The goal of analogue assessments is that the tasks should have content validity. That is, the tasks should be relevant to the target construct, and they should be representative of the tasks the person is likely to face in their natural environment by sampling all facets that affect the behavior problem in the natural environment. Analogue assessment can lead to greater frequency of target behavior through the use of contrived situations or stimuli, compared to naturalistic observation. For instance, noncompliance might occur more often when doing a compliance task in the lab than when observing a child in their natural environment. Therefore, the frequency or rate of behavior may not generalize to the natural context. But the functional relations of the behavior in relation to other behaviors, such as antecedents and consequences, may generalize. Thus, they can help identify potential intervention targets. Analogue assessment is most useful for assessing behaviors that are difficult or rare to observe in the natural environment but that are frequent in the analogue environment. For example, analogue assessment is particularly useful for assessing socially sensitive behavior problems such as verbal aggression, panic, and oppositional behavior. Analogue assessment can be sensitive to change, and therefore can be a clinically useful measure of treatment gains or failure. A client who cannot demonstrate a skill in the clinic may not be able to demonstrate a skill in the real world. But just because they show a skill in a session does not necessarily mean they will be able to use it in the real world. In terms of reliability and validity, analogue/structured assessments tend to show greater reliability than naturalistic observations. Analogue assessments tend to show greater internal validity than naturalistic observations. That is, analogue assessments are more likely to reflect causal factors because they have more control of when and how the stimuli are presented. However, analogue assessments show weaker ecological validity than naturalistic observations and therefore tend to show weaker external validity. That is, analogue assessments yield lower generalizability than naturalistic observations because analogue assessments may not be as reflective of their functioning in the real world. Nevertheless, analogue assessments tend to show greater clinical utility, in terms of incremental validity and cost-effectiveness than naturalistic observations, when they are done well. 22.11 Self-Monitoring Self-monitoring involves observing and recording one’s own behaviors, thoughts, emotions, bodily sensations, events, etc. Given the expense of conducting behavioral observations, you can train people to be their own observers, which has better portability. Phone apps can now be used to track many different processes, such as mood, sleep, and nutrition. These are examples of electronic diaries. An overview of research on self-monitoring and electronic diaries is provided by Korotitsch &amp; Nelson-Gray (1999) and Piasecki et al. (2007). Self-monitoring provides a great deal of information at a low cost. There are several benefits of self-monitoring. Self-monitoring can help characterize the problem and the presumed causal or maintaining factors. Self-monitoring can also contribute to greater objective self-awareness. In addition, self-monitoring can be used to track treatment progress. Furthermore, self-monitoring can be helpful for assessing covert behaviors that are hidden to outside observers, such as obsessions, paranoid thoughts, and sexual problems. Self-monitoring is a component of most empirically supported treatments. In addition, the extent to which clients complete self-monitoring is predictive of treatment effectiveness. As an example of self-monitoring in therapy, in parent management training, we have parents track (tally) the frequency of their child’s compliance, noncompliance, and aggression each day during the week between sessions as homework. Self-monitoring provides a more objective way to track treatment progress than retrospective reports or satisfaction ratings because clients track the specific behavior the moment it occurs. We modify treatment in response to the information gained from tracking. We also become more aware of the situations and contexts in which the behavior occurs, so it is predictable and the client can better anticipate, prepare for, and control the problem. Based on principles of measurement-based care, assessment does not just occur at the outset of treatment; it should continue throughout treatment. And self-monitoring helps parents become more aware of the actual frequency of the problem behavior. For instance, they might have over- (or under-)estimated how often a behavior occurs. Thus, self-monitoring can lead to a change in people’s perceptions of the behavior. Also, some children will misbehave less at home or school due to reactivity because they know they are being observed, so this can be a way of kick-starting some behavioral improvement. Reactivity is the change in a behavior for the mere reason that the behavior is being observed. There are often changes in a behavior in response to self-monitoring—typically, the client experiences behavioral improvement. Self-monitoring might increase awareness of how often a problem behavior is happening and their perceived predictability and control of the behavior. Self-monitoring is now considered an empirically supported treatment for a number of conditions, including academic/conduct problems, addictive behaviors, anxiety and depression, disordered eating, physical health problems, psychosis, and insomnia. That is, assessment is a form of intervention! However, there are challenges to self-monitoring. One challenge is how to evaluate the reliability of self-monitoring. Another challenge is how to handle if the client fills out the whole tracking sheet retrospectively, all at once, right before session—this is called “backfilling”. In addition, there is a possibility of inaccuracy and distortion with self-monitoring due to the respondent not having been trained on how to observe the phenomenon of interest. To counter this, in parent management training, we explicitly define what counts as an instance of compliance, noncompliance, and aggression. Another challenge with self-monitoring is that it involves respondent burden because it requires compliance by the client. To handle noncompliance with self-monitoring, it can be helpful to “get procedural” and to discuss with the client when they will track, where they will put the tracking sheet, etc. It can also be helpful to perform advance troubleshooting: help the client anticipate obstacles and how they will address them. For instance, if the client feels that they might forget to do the tracking, have them set a timer. Self-monitoring has lower reliability and validity than behavioral observation, but it is still useful and widely used because it is cheap and easy to obtain lots of information that can lead to behavioral improvement. 22.12 Behavior Rating Scales Behavior rating scales include self- or informant ratings on a checklist of behaviors assessing particular construct(s). An example assessment system that uses behavior rating scales is the Achenbach System of Empirically Based Assessment (ASEBA) that includes the Child Behavior Checklist (CBCL), Teacher’s Report Form (TRF), Adult Self-Report, etc. The ASEBA is an empirically based assessment of psychopathology, rather than a DSM-focused assessment. Behavior rating scales are widely used in research and clinical settings. They are relatively cheap and easy to administer and score. Item response theory techniques reveal issues with different levels of Likert scale anchors of rating scales. For example, people assign different meanings to the levels “never”, “sometimes”, and “a lot”. It is often better to have the rater describe or provide the numeric amount. For example: “How many times during the last week did your child physically hit someone else?” Behavior rating scales are typically retrospective where the rater has to reflect back and estimate the frequency of behaviors and people’s responses to the environment. Retrospective ratings can have challenges with validity due to inaccurate memory recall. 22.13 Assessment of Therapeutic Process Other behavioral assessments include assessments of therapeutic process. Example assessments of therapeutic process include having a video recording of a session and then coding it in a naturalistic setting. For a therapist, the naturalistic setting is the therapeutic context. An example of assessment of the therapeutic process could include assessing the clinician’s adherence and competence. Assessing adherence would involve coding whether the therapist is following the treatment protocol and manual, that is, the degree to which the clinician shows treatment fidelity. Assessing the clinician’s competence would involve coding how skilled the therapist is in providing the treatment. Coding clinician competence in most treatments, including cognitive behavioral therapy, would not be a moment-to-moment coding; instead, the coder would examine the sequence of interactions between client and therapist. 22.14 Conclusion Examples of behavioral assessment include naturalistic behavioral observation, analogue (structured) observation, self-monitoring/diary, behavior ratings (e.g., informant report), and self-report. A gold standard for naturalistic observations is to observe people in their natural context across more than one day, time, situation, and context with more than one rater to get a more robust sampling of the person’s behavior. Compared to naturalistic observation, analogue assessments tend to show greater reliability and internal validity but lower external validity. A useful approach to identify the function of a given behavior is to identify patterns of sequences according to the antecedent-behavior-consequence (A-B-C) model as part of a functional behavior assessment. Another useful approach is self-monitoring, which provides a lot of information at low cost and is therefore a component of most empirically supported treatments. 22.15 Suggested Readings Girard &amp; Cohn (2016) References Girard, J. M., &amp; Cohn, J. F. (2016). A primer on observational measurement. Assessment, 23(4), 404–413. https://doi.org/10.1177/1073191116635807 Haynes, S. N. (2001). Clinical applications of analogue behavioral observation: Dimensions of psychometric evaluation. Psychological Assessment, 13(1), 73–85. https://doi.org/10.1037/1040-3590.13.1.73 Korotitsch, W. J., &amp; Nelson-Gray, R. O. (1999). An overview of self-monitoring research in assessment and treatment. Psychological Assessment, 11(4), 415–425. https://doi.org/10.1037/1040-3590.11.4.415 Petersen, I. T. (2024a). Assessing externalizing behaviors in school-aged children: Implications for school and community providers. https://scsmh.education.uiowa.edu/practice-brief/assessing-externalizing-behaviors-in-school-aged-children-implications-for-school-and-community-providers/ Piasecki, T. M., Hufford, M. R., Solhan, M., &amp; Trull, T. J. (2007). Assessing clients in their natural environments with electronic diaries: Rationale, benefits, limitations, and barriers. Psychological Assessment, 19(1), 25–43. https://doi.org/10.1037/1040-3590.19.1.25 "],["repeated-assessments.html", "Chapter 23 Repeated Assessments Across Time 23.1 Overview of Repeated Measurement 23.2 Examples of Repeated Measurement 23.3 Test Revisions 23.4 Change and Stability 23.5 Assessing Change 23.6 Types of Research Designs 23.7 Using Sequential Designs To Make Developmental Inferences 23.8 Stability Versus Continuity 23.9 Heterotypic Continuity 23.10 Conclusion 23.11 Suggested Readings", " Chapter 23 Repeated Assessments Across Time 23.1 Overview of Repeated Measurement Repeated measurement enables examining within-person change in constructs. Relating within-person change in a construct to within-person change in another construct provides a stronger test of causality compared to simple bivariate correlations. Identifying that within-person change in construct X predicts later within-person change in construct Y does not necessarily indicate that X causes Y but it provides stronger evidence consistent with causality than a simple bivariate association between X and Y. As described in Section 5.3.1.3.3.1, there are three primary reasons a correlation between X and Y does not necessarily mean that X causes Y: First, the association could reflect the opposite direction of effect, where Y actually causes X. Second, the association could reflect the influence of a third variable that influences both X and Y (i.e., a confound). Third, the association might be spurious. If you find that within-person changes in sleep predict later within-person changes in mood, that is a stronger test of causality because (a) it demonstrates temporal precedence; it attenuates the possibility that the association is due to the opposite direction of effect; and (b) it accounts for all time-invariant confounds that differ between people (e.g., birth order) because the association is examined within the individual. That is, the association examines whether the person’s mood is better when they get more sleep compared to when that same person gets less sleep. The association could still be due to time-varying confounds, but you can control for specific time-varying confounds if you measure them. Repeated measurement also enables examination of the developmental timing of effects and sensitive periods. Furthermore, repeated measures designs are important for tests of mediation, which are tests of hypothesized causal mechanisms. Repeated measurement involves more complex statistical analysis than cross-sectional measurement because multiple observations come from the same person, which would violate assumptions of traditional analyses that the observations (i.e., the residuals) are independent and uncorrelated. Mixed-effects modeling (i.e., mixed modeling, multilevel modeling, hierarchical linear modeling) and other analyses, such as structural equation modeling, can handle the longitudinal dependency or nesting of repeated measures data. Repeated measurement can encompass many different timescales, such as milliseconds, days, or years. 23.1.1 Comparison of Approaches Research designs can be compared in terms of their internal validity—the extent to which we can be confident about causal inferences. A cross-sectional association is depicted in Figure 23.1: Figure 23.1: Cross-Sectional Association. Among observational/correlational research designs, cross-sectional designs tend to have the weakest internal validity. That is, if we observe a cross-sectional association between \\(X\\) and \\(Y\\), we have little confidence that \\(X\\) causes \\(Y\\). Consider a lagged association, which is a slightly better approach: Figure 23.2: Lagged Association. A lagged association has somewhat better internal validity than a cross-sectional association because we have greater evidence of temporal precedence—that the influence of the predictor precedes the outcome because the predictor was assessed before the outcome and it shows a predictive association. However, part of the association between the predictor with later levels of the outcome could be due to prior levels of the outcome that are stable across time. Thus, consider an even stronger alternative—a lagged association that controls for prior levels of the outcome: Figure 23.3: Lagged Association, Controlling for Prior Levels of the Outcome. A lagged association controlling for prior levels of the outcome has better internal validity than a lagged association that does not control for prior levels of the outcome. When we control for prior levels of the outcome in the prediction, we are essentially predicting (relative) change. Predicting change provides stronger evidence consistent with causality because it uses the individual as their own control and controls for many time-invariant confounds. However, consider an even stronger alternative—lagged associations that control for prior levels of the outcome and that simultaneously test each direction of effect: Figure 23.4: Lagged Association, Controlling for Prior Levels of the Outcome, Simultaneously Testing Both Directions Of Effect. Lagged associations that control for prior levels of the outcome and that simultaneously test each direction of effect provide the strongest internal validity among correlational/observational designs, especially when examining whether within-person changes in the predictor predict later within-person changes in the outcome (and vice versa). Such a design can help better clarify which among the variables is the chicken and the egg—which variable is more likely to be the cause and which is more likely to be the effect. Or, if there are bidirectional, transactional effects, what the magnitude of each direction of effect are. The cross-lagged panel model and dual latent change score model are examples of models that include lagged associations that control for prior levels of the outcome and that simultaneously test each direction of effect. However, consider an even stronger model—lagged associations that control for prior levels of the outcome and for random intercepts and that simultaneously test each direction of effect: Figure 23.5: Lagged Association, Controlling for Prior Levels of the Outcome and Random Intercepts, Simultaneously Testing Both Directions Of Effect. Some researchers have called into question the cross-lagged panel model (Berry &amp; Willoughby, 2017; Hamaker et al., 2015). A stronger alternative may be a cross-lagged model that controls for stable between-person differences (i.e., random intercepts) (Hamaker et al., 2015). This is known as a random intercepts cross-lagged panel model. 23.2 Examples of Repeated Measurement Below are some examples of repeated measurement. 23.2.1 Alcohol Timeline Followback A clinically relevant example of repeated measurement is the Alcohol Timeline Followback, published by Sobell &amp; Sobell (2008). The Alcohol Timeline Followback is a method for retrospectively assessing the quantity of alcohol consumption on a daily basis. It provides retrospective estimates of people’s drinking using a calendar of the number of drinks consumed each day. The time frame can vary from 30 days to 360 days. The participant is given detailed instructions to enhance the accuracy of recall and reporting of their behavior. Moreover, a specific definition is provided of what counts as a drink. The measure is intended to provide a detailed record of patterns of alcohol use to guide treatment and assess treatment outcome. The measure provides many summary statistics and norms are available. The measure has several clinical uses. First, it can provide personalized feedback to the client regarding their drinking behavior compared to the population as part of enhancing their motivation to change, in a motivational interviewing framework. Second, the measure can help identify high- and low-risk periods for a client to help them prevent relapse. Because the measure provides detailed information with norms, it can be more useful than global estimates of drinking derived from less structured clinical interviews. The detailed instructions help respondents remember more accurately, so the clinician or researcher can examine how people’s substance abuse influences other processes like parenting. 23.2.2 Ambulatory Assessment We discussed ambulatory assessment in Section 20.2.4.1. Ambulatory assessment includes an array of methods used to study people in their natural environment. There is a proliferation of smartphone apps for ambulatory assessment. And the ambulatory assessments often, because of their temporal resolution, provide repeated measurements. For instance, Carpenter et al. (2016) depicted the examination, during a drinking episode, of self-reported drinking in relation to physiological indices including skin temperature, heart rate, breathing rate, physical activity, and in relation to self-reported sadness and mood dysregulation. The self-reported indices were assessed on various intervals (e.g., every 15 minutes) and upon various events (e.g., when they take a drink). However, the physiological indices were assessed continuously from moment to moment. These different timescales pose challenges for how to examine them in relation to each other. Matthews et al. (2016) provided an example of how they developed an ambulatory assessment using a smartphone app to assess the dynamic process of bipolar disorder. Regularity of social rhythms is disrupted in bipolar disorder, so the authors developed an app to help assess social rhythms. The authors used a participatory design in which they involved the patients in the development of the app to ensure it worked well for them. The app tracked the occurrence and timing of daily events, such as getting out of bed, starting your day, having dinner, and going to bed. It also passively detected daily events (“social rhythms”) with many sensors on the smartphone using information from the light, accelerometer, and microphone of the smartphone. And the app used a machine learning algorithm to estimate level of physical activity and social interaction. 23.3 Test Revisions An important part of repeated measurements deals with test revisions. Tests are revised over time. Some test revisions are made to be consistent with improvements in the understanding of the construct. Other test revisions are made to make the norms more up-to-date. For instance, the Weschler Adult Intelligence Scale has multiple versions (WAIS-3, WAIS-IV, etc.). There are important considerations when dealing with test revisions. When you assess people at different times, whether the same people at multiple time points (such as in a longitudinal study) or different people at different times, challenges may result in comparing scores across time. There can be benefits of keeping the same measure across ages or across time for comparability of scores. However, it is not necessarily good to use the same measure across ages or at different times merely for the sake of mathematical comparability of scores. If a measure was revised to improve its validity or if a measure is not valid at particular ages, it may make sense to use different measures across ages or across time. You would not want to use a measure at a given age if it is not valid at that age. Likewise, it does not make sense to use an invalid measure at a later time point if a more valid version becomes available, even if it was used at an earlier time point. That is, even if a measure’s scores are mathematically comparable across ages or time, they may not be conceptually comparable across ages or time. If different measures are used across ages or across time, there may be ways to link scores from the different measures to be on the same scale, as described in Section 23.9.4.1.3. One interesting phenomenon of time-related changes in scores is the Flynn effect, in which the population scores on intelligence tests rise around 3 points every decade. That is, intelligence, as measured by tests of cognitive abilities, increases from generation to generation. Test revisions can hide effects like the Flynn effect if we just keep using score transformations instead of raw scores. To observe a person’s (or group’s) growth, it is recommended to use raw scores rather than standardized scores that are age-normed, such as T scores, z scores, standard scores, and percentile ranks (Moeller, 2015). Age-normed scores prevent observing growth at the person level or at the group level (e.g, changes in means or variances over time). T scores (\\(M = 50\\), \\(SD = 10\\)), z scores (\\(M = 0\\), \\(SD = 1\\)), and standard scores (\\(M = 100\\), \\(SD = 15\\)) have a fixed mean and standard deviation. Percentile ranks have a fixed range (0–100). 23.4 Change and Stability A key goal of developmental science is to assess change and stability of people and constructs. Stability is often referred to in a general way. For instance, you may read that intelligence and personality show “stability” over time. However, this is not a useful way of describing the stability of people and/or constructs because stability is not one thing. There are multiple types of stability, and it is important to refer to the types of stability considered. Types of stability include, for instance, stability of level, rank order (individual differences), and structure. Stability of level refers to whether people show the same level on the construct over time. For example, stability of level considers whether people stay the same in their level of neuroticism across childhood to adulthood. Stability of people’s level is evaluated with the coefficient of repeatability. Mean-level stability refers to whether the mean level stays the same across time. For example, mean-level stability considers whether the population average stays the same in their level of neuroticism across childhood to adulthood. Mean-level stability is evaluated by examining mean-level age-related differences; for instance, with a t-test or ANOVA, or by examining whether the construct is associated with age. Rank-order stability refers to whether individual differences stay the same across time. For instance, rank-order stability considers whether people who have higher neuroticism than their peers as children also tend to have higher neuroticism than their peers as adults. Rank-order stabiliy is examined by examining the association of people’s level at T1 with people’s level at T2 (e.g., correlation coefficient or regression coefficent). Stability of structure considers whether the structure of the construct stays the same across ages. For instance, stability of structure considers whether personality is characterized by the same five factors across ages (i.e., openness, conscientiousness, extraversion, agreeableness, and neuroticism). Stability of structure is examined using longitudinal factorial invariance. People and constructs can show stability in some types and not others. For instance, although intelligence and personality show relative stability in structure and rank-order, they do not show absolute stability in level. Even though individual differences in intelligence and personality show relative stability, people tend to change across time in their absolute level of intelligence and various personality factors across ages. For instance, neuroticism decreases in later adulthood. Examining age-normed scores would preclude identifying mean-level change because age-normed scores have a fixed mean. Moreover, examining a strong correlation of intelligence and personality scores across time merely indicates that individual differences show relative stability; it does not indicate whether individuals show absolute stability in level. 23.5 Assessing Change There are many important considerations for assessing change, that is changes in a person’s level on a construct across time. There is a key challenge in assessing change between two time points. If a person’s score differs between two time points, how do you know that the differences across time reflect a person’s true change in their level on the construct rather than measurement error, such as regression to the mean? Regression to the mean occurs if an extreme observation on a measure at time 1 (T1) more closely approximates the person’s mean at later time points, including time 2 (T2). For instance, clients with high levels of symptomatology tend to get better with the mere passage of time, which has been known as the self-righting reflex. 23.5.1 Inferring Change The inference of change is strengthened to the extent that: the magnitude of the difference between the scores at the two time points is large (i.e., a large effect size) the measurement error (unreliability) at both time points is small the measure has the same meaning and is on a comparable scale at each time point (i.e., the measure’s scores show measurement invariance across time) evidence suggests that the differences across time are not likely to be due to potential confounds of change such as practice effects, cohort effects, and time-of-measurement effects. Measurement error can be reduced by combining multiple measures of a construct into a latent variable, such as in structural equation modeling (SEM) or item response theory (IRT). To detect change, it is important to use measures that are sensitive to change. For instance, the measures should not show range restriction owing to ceiling effects or floor effects. 23.5.2 Potential Confounds of Change Potential confounds of change include practice effects, cohort effects, and time-of-measurement effects. 23.5.2.1 Practice Effects It is important to consider the potential for practice or retest effects (Hertzog &amp; Nesselroade, 2003). Practice/retest effects occur to the extent that a person improves on a task assessed at multiple time points for reasons due to the repeated assessment rather than true changes in the construct. That is, a person is likely to improve on a task if they complete the same task at multiple time points, even if they are not changing in their level on the construct. Practice and retest effects are especially likely to be larger the closer in time the retest is to the prior testing. Improvement in scores across time might not reflect people’s true change in the construct; thus, you might not be able to fairly compare people’s scores who have completed the task different numbers of times. In sum, the ages and frequency of assessment should depend on theory. 23.5.2.2 Cohort Effects It is also important to consider the potential for cohort effects (Hertzog &amp; Nesselroade, 2003). Cohorts often refer to birth cohorts—people who were born at the same time (e.g., the same year). Cohorts tend to have some similar experiences at similar ages, including major events (e.g., 9/11 terrorist attacks, assassination of John F. Kennedy, bombing of Pearl Harbor) and economic or socio-political climates (e.g., the Great Depression, coronavirus pandemic, Cold War, World War II). These collective differences in the different experiences between different cohorts may lead to apparent age-related differences between cohorts that are not actually due to age differences per se, but rather to differences in the experiences between the cohorts. 23.5.2.3 Time-of-Measurement Effects It is also important to consider time-of-measurement effects. For instance, people’s general anxiety may be higher, on average, during the coronavirus pandemic. Thus, some apparent age-related differences might be due to the time of measurement (i.e., a pandemic) rather than age. 23.5.3 When and How Frequently to Assess Key questions arise in terms of when (at what ages or times) and how often to assess the construct. If the measurement intervals are too long, you might miss important change in between. If the measurement intervals are too frequent, differences might reflect only measurement error. The Nyquist theorem provides guidance in terms of how frequently to assess a construct. According to the Nyquist theorem, in order to accurately reproduce a signal, you should sample it at twice the highest frequency you wish to detect. For instance, to detect a 15 Hz wave, you should sample it at least at 30 Hz (Hertz means times per second). Thus, the frequency of assessment should be as frequent as, if not more frequent than, the highest frequency of change you seek to observe. 23.5.4 Difference Scores As described in Section 4.5.8, difference scores (also called change scores) can have limitations because they can be less reliable than each of the individual measures that compose the difference score. The reliability of individual differences in change scores depends on the extent to which: (a) the measures used to calculate the change scores are reliable, and (b) the variability of true individual differences in change is large—that is, there are true individual differences in change. 23.5.4.1 Superior Approaches to Difference Scores There are several options that are better than traditional difference scores or change scores. Instead of difference scores, use autoregressive cross-lagged models, latent change score models, or growth curve models. 23.5.4.1.1 Autoregressive Cross-Lagged Models Autoregressive cross-lagged models (aka cross-lagged panel models) can be fit in path analysis or SEM. Cross-lagged panel models examine relative change in a variable, that is, change in individual differences in the variable, but not changes in level on the variable. Such a model allows examining whether one variable predicts relative change in another variable. The cross-lagged panel model tests whether individuals with low levels of X relative to others experience subsequent rank-order increases in Y (Evans &amp; Shaughnessy, in press; Orth et al., 2021). By contrast, the random-intercepts cross-lagged panel model tests whether, when individuals experience lower-than-their-own usual levels of X, they experience subsequent increases in Y relative to their usual level (Evans &amp; Shaughnessy, in press; Orth et al., 2021). As an example of a cross-lagged panel model, I conducted a collaborative study that examined whether language ability predicts later behavior problems controlling for prior levels of behavior problems—that is, whether language ability predicts a person’s change in behavior problems relative to the change among other people in the sample (Petersen et al., 2013; Petersen &amp; LeBeau, 2021). Cross-lagged panel models predict relative (rank-order) change, not absolute change in level. For tests of absolute change, models such as change score models or growth curve models are necessary. 23.5.4.1.2 Latent Change Score Models Latent change score models can be fit in SEM. Unlike autoregressive cross-lagged models, latent change score models examine absolute change—that is, people’s changes in level. With latent change score models, you can examine change in latent variables that reflect the common variance among multiple measures. Latent variables are theoretically error-free (at least of random error), so latent change scores can be perfectly reliable, unlike traditional difference scores, whose reliability is strongly influenced by the measurement unreliability of the specific measures that compose them. Latent change score models are described by R. A. Kievit et al. (2018). 23.5.4.1.3 Growth Curve Models Growth curve models can be fit in mixed-effects modeling or SEM. Growth curve models require three or more time points to be able to account for measurement error in terms of the shape of a linear trajectory. Growth curve models examine absolute change, i.e., changes in level. Growth curve models include an intercept (starting point) and slope (amount of change), but you can model other nonlinear forms of change as well, such as quadratic, logistic, etc. with more time points. Growth curve models correct for measurement error in the shape of the person’s trajectory. In addition, growth curve models can examine risk and protective factors as predictors of the intercepts and slopes. For instance, in collaboration, I examined risk and protective factors that predict the development of externalizing problems (Petersen et al., 2015). 23.5.5 Nomothetic Versus Idiographic The approaches described above (autoregressive cross-lagged models, latent change score models, and growth curve models) tend to be nomothetic in nature. That is, they are interested in examining group-level phenomena and assume that all people come from the same population and can be described by the same parameters of change. That is, they assume a degree of homogeneity, even though they allow each person to differ in their intercept and slope. This nomothetic (population) approach differs from an idiographic (individual) approach. In clinical work, we are often interested in making inferences or predictions at the level of the individual person, consistent with an idiographic approach. But fully ignoring the population estimates is unwise because they tend to be more accurate than individual-level judgments. Further, individual-level predictions may not generalize to the population. Nomothetic modeling approaches can be adapted to examine change for different subgroups if there are theoretically different populations. Approaches that examine different subgroups include group-based trajectory models, latent class growth models, and growth mixture models (Fontaine &amp; Petersen, 2017; van der Nest et al., 2020). For instance, Moffit (1993, 2006b, 2006a) created the developmental taxonomy of antisocial behavior with the support of these approaches. Moffitt identified one subgroup that showed very low levels of antisocial behavior across the lifespan, which she labeled the “low” group. She identified one subgroup that showed antisocial behavior only in childhood, which she labeled the “childhood-limited” group. She hypothesized that the antisocial behavior among the childhood-limited group was influenced primarily by genetic factors and the early environment. She identified another subgroup that showed antisocial behavior only in adolescence, which she labeled the “adolescent-limited”group. She hypothesized that the antisocial behavior among the adolescent-limited group was largely socially mediated. She identified another subgroup that showed antisocial behavior across the lifespan, which she labeled the “life-course persistent” group. She hypothesized that the “life-course persistent” group reflected neuropsychological deficits. Idiographic approaches for personalized modeling of psychopathology are described by Wright &amp; Woods (2020). An example of an approach that combines an idiographic (person-centered) approach with a nomothetic (population) approach to understand change is the group iterative multiple model estimation (GIMME) model, as described by Beltz et al. (2016) and Wright et al. (2019). The GIMME model is a data-driven approach to identifying person-specific time-series models. The approach first identifies the associations among constructs that replicate across most participants. The group-level associations are then used as a starting point for identifying person-specific associations. Thus, the GIMME model provides an approach for developing personalized time-series models. The choices on which model to use depend on the research question and should be guided by theory. 23.5.6 Structural Equation Modeling Structural equation modeling (SEM) allows multiple dependent variables. This provides a key advantage in longitudinal designs, such that two constructs can each be represented by a dependent variable that is predicted by the other. If examining changes in two constructs in the context of SEM, you could examine for instance whether one precedes the other in prediction, to better understand which might cause which and to help tease apart the chicken and the egg. For instance, in collaboration, I found that low language ability predicts later behavior problems stronger than behavior problems predict later language ability, suggesting that, if they are causally related, that language ability is more likely to have a causal effect on behavior problems than the reverse (Petersen et al., 2013). 23.5.7 Attrition and Systematic Missingness Rates of attrition in longitudinal studies can be substantial and missingness is often not completely at random. There is often selective and systematic attrition. For example, attrition often differs as a function of socioeconomic status and other important factors. It is important to use approaches that examine and account for systematic missingness. It can be helpful to use full-information maximum likelihood (FIML) in SEM to make use of all available data. Multiple imputation is another approach that can be helpful for dealing with missingness. With multiple imputation, you can use predictors that might account for missingness, such as socioeconomic status and demographic factors. You can help account for practice effects with planned missingness designs in which participants are randomly assigned to receive or not receive a given measurement occasion. 23.5.8 Measurement Invariance Just like measurement or factorial invariance can be tested and established across genders, races, ethnicities, cultures, etc. to ensure measurement equivalence, it can also be tested across ages or times. The goal is to establish longitudinal measurement invariance, i.e., measurement invariance across ages, so that you know that you are assessing the same construct over time in a comparable way so that you know that differences across time reflect true change. To examine longitudinal measurement invariance, you can use SEM or IRT. In SEM, you could examine whether measures have the same intercepts, factor loadings, and residuals across ages. In IRT, you could examine whether items show differential item functioning, versus whether they show the same difficulty and discrimination across ages. But there are times when we would expect a construct would, theoretically, fail to show longitudinal factorial invariance, as in the case of heterotypic continuity, described in Section 23.9. 23.6 Types of Research Designs Schaie (1965), Baltes (1968), and Schaie &amp; Baltes (1975) proposed various types of research designs, in terms of the various combinations of age, period (i.e., time of measurement), and cohort. Age refers to a person’s chronological age at the time of measurement. Period refers to the time of measurement (e.g., April 12, 2014 at 7:53 AM). Cohort refers to a person’s time (e.g., year) of birth (e.g., 2003). If you know two of these (e.g., age and period), you can determine the third (e.g., cohort). Here are the types of research designs based on combinations of age, period, and cohort: cross-sectional cross-sectional sequences time-lag longitudinal longitudinal sequences time-sequential cross-sequential cohort-sequential These research designs are depicted by age and cohort in Figure 23.6 and by time of measurement and cohort in Figure 23.7. Although the depictions of cohort-sequential and cross-sequential designs differ from the depictions by Schaie (2005) (i.e., they are reversed), they are consistent with contemporary definitions of these designs (Little, 2013; Masche &amp; Dulmen, 2004; Whitbourne, 2019). Figure 23.6: Research Designs by Age and Cohort. Values in the cells are the times (years) of measurement. Dashed line indicates different participants were assessed at each time of measurement. Figure 23.7: Research Designs by Time Of Measurement and Cohort. Values in the cells are ages of the participants. Dashed line indicates different participants were assessed at each time of measurement. 23.6.1 Cross-Sectional Design A cross-sectional design involves multiple participants, often spanning different ages, assessed at one time of measurement. A cross-sectional design is a single factor design, where the researcher is interested in comparing effects of one factor: age. However, in a cross-sectional design, cohort (i.e., birth year) is confounded with age differences. Thus, observed age-related differences could reflect cohort differences rather than true age differences. Additionally, sampling differences at each age could yield non-comparable age groups. Cross-sectional designs are thus limited in their ability to describe developmental processes including change and stability over time. Any age-related differences are confounded with cohort differences and are strongly influenced by between age-group sampling variability. Cross-sectional studies are useful for preliminary data on validity of measures for the age groups, and for examining whether the age differences are in the expected direction. Cross-sectional studies are less costly and time-consuming than longitudinal studies. Therefore, they are widely used and provide a useful starting point. However, findings from cross-sectional studies can differ from findings examining the same people over time, which violates the convergence assumption. In cross-sectional studies, the convergence assumption is the assumption that cross-sectional age differences and longitudinal age changes converge onto a common trajectory. However, it has been shown, for example, that cross-sectional studies over-estimate age-related cognitive declines compared to following the same people over time in a longitudinal study (Ackerman, 2013). 23.6.2 Cross-Sectional Sequences Design A cross-sectional sequences design involves successive cross-sectional studies of different participants at different times of measurement. In a cross-sectional sequences design, cohort and time-of-measurement are confounded with age differences. Thus, observed age-related differences could reflect cohort differences or time-of-measurement differences rather than true age differences. This poses challenges for using cross-sectional sequences to estimate age-related changes due to time-related effects such as the Flynn effect. 23.6.3 Time-Lag Design A time-lag design involves participants from different cohorts assessed at the same age. The time-lag design is a single factor design: the researcher specifies one factor: cohort. However, time of measurement is confounded with cohort. Thus, any cohort differences could reflect different times of measurement. 23.6.4 Longitudinal Design A longitudinal design involves following the same participants over time. The single-cohort longitudinal design is a single factor design: the researcher specifies one factor: age. However, in a single-cohort longitudinal design, cohort and time of measurement are confounded with age. Thus, any observed changes with age could be due to time-of-measurement effects or cohort effects (instead of people’s true change). 23.6.5 Longitudinal Sequences Design A longitudinal sequences design involves following multiple cohorts across time. There are three types of longitudinal sequences designs: time-sequential, cross-sequential, and cohort-sequential designs. The three types of longitudinal sequences are depicted in Figure 23.8 as a function of which two factors are specified by the researcher. Figure 23.8: Types of Longitudinal Sequences as a Function of Which Two Factors are Specified by the Researcher. 23.6.5.1 Time-Sequential Design A time-sequential design is depicted in Figure 23.9. In a time-sequential design, multiple age groups are assessed at multiple times (Whitbourne, 2019). It is a repeated cross-sectional design, with some participants followed longitudinally. Additional age groups are added to a time-lag design, with some of the participants assessed at more than time point. In other words, the age range is kept the same and is repeatedly assessed (with only some participants being repeatedly assessed). The two factors a researcher specifies in a time-sequential design are age and time of measurement. A time-sequential design can be helpful for identifying age differences while controlling for time-of-measurement differences, or for identifying time-of-measurement differences while controlling for age differences. However, cohort effects are confounded with the interaction of age and time of measurement (Whitbourne, 2019). Thus, observed differences as a function of age or time of measurement could reflect cohort effects. Figure 23.9: Time-Sequential Research Design. 23.6.5.2 Cross-Sequential Design A cross-sequential design is depicted in Figure 23.10. In a cross-sequential design, multiple cohorts are assessed at multiple times (Whitbourne, 2019). It is a cross of a cross-sectional and longitudinal design. It starts as a cross-sectional study with participants from multiple cohorts, and then all participants are followed longitudinally (typically across the same duration). It is also called an accelerated longitudinal design. The two factors a researcher specifies in a cross-sequential design are time of measurement and cohort. A cross-sequential design can be helpful for identifying cohort differences while controlling for time-of-measurement differences, or for identifying time-of-measurement differences while controlling for cohort differences. However, age differences are confounded with the interaction between cohort and time of measurement. Thus, observed differences as a function of cohort or time of measurement could reflect age effects. Figure 23.10: Cross-Sequential Research Design. 23.6.5.3 Cohort-Sequential Design A cohort-sequential design is depicted in Figure 23.11. In a cohort-sequential design, multiple cohorts are assessed at multiple ages (Whitbourne, 2019). It starts multiple cohorts at the same age and then follows them longitudinally (typically across the same duration). It is like starting a longitudinal study at the same age over and over again. The two factors a researcher specifies in a cohort-sequential design are age and cohort. A cohort-sequential design can be helpful for identifying age differences while controlling for cohort differences, or for identifying cohort differences while controlling for age differences. However, time-of-measurement effects are confounded with the interaction of age and cohort (Whitbourne, 2019). Thus, observed differences as a function of age or cohort could reflect time-of-measurement effects. Figure 23.11: Cohort-Sequential Research Design. 23.7 Using Sequential Designs To Make Developmental Inferences Which research design you select should depend on which change function you are most interested in: age, cohort, or time of measurement. However, age is not the only metric for development; time after an exposure (e.g., puberty onset) can also be an important time metric. There are many other potential metrics of time for studying development. To have greater confidence that age-related differences reflect true change (development) rather than effects of cohort or time of measurement, we would conduct all three sequential designs. To the extent that the age-related effects in the time-sequential and cohort-sequential designs are stronger than the cohort- and time-of-measurement effects in the cross-sequential design, we have confidence that the observed age-related differences reflect development (Whitbourne, 2019). Moreover, there is an entire set of analytic approaches, known as age-period-cohort analysis (Yang &amp; Land, 2013), whose goal is to disentangle the effects of age, period, and cohort. 23.8 Stability Versus Continuity Continuity addresses the trajectory of development within individuals. By contrast, stability refers to the maintenance of between-individual or group characteristics (Petersen, in press; Schulenberg et al., 2014; Schulenberg &amp; Zarrett, 2006). When describing stability or continuity, it is important to specify which aspects of stability (e.g., of mean, variance, or rank order) and continuity (e.g., of causes, courses, forms, functions, or environments) one is referring to. For example, Figure 23.12 depicts the distinction between mean-level stability and rank-order stability; Figure 23.13 depicts the distinction between continuity in course and rank-order stability. Figure 23.12: Mean-Level Stability Versus Rank-Order Stability. C1, C2, and C3 are three individual children assessed at two timepoints: T1 and T2. The gray arrow depicts the mean (average) trajectory. (Figure reprinted from Petersen (in press), Figure 1, Petersen, I. T. (in press). Reexamining developmental continuity and discontinuity in the 21st century: Better aligning behaviors, functions, and mechanisms. Developmental Psychology. https://doi.org/10.1037/dev0001657 Copyright (c) American Psychological Association. Used with permission.) Figure 23.13: Continuity in Course Versus Rank-Order Stability. C1, C2, and C3 are three individual children assessed at two timepoints: T1 and T2. Continuity in course is sometimes called “stability in level.” (Figure reprinted from Petersen (in press), Figure 2, Petersen, I. T. (in press). Reexamining developmental continuity and discontinuity in the 21st century: Better aligning behaviors, functions, and mechanisms. Developmental Psychology. https://doi.org/10.1037/dev0001657 Copyright (c) American Psychological Association. Used with permission.) The various types of continuity, including homotypic continuity, heterotypic continuity, and phenotypic continuity, in addition to discontinuity are depicted in Figures 23.14 and 23.15. Figure 23.14: The Latin square depicts Kagan’s -Kagan (1969) typology of continuity as a function of behavior form and function. However, it is also important to consider continuity versus discontinuity of the mechanisms underlying a given behavior–function pair. (Figure reprinted from Petersen (in press), Figure 3, Petersen, I. T. (in press). Reexamining developmental continuity and discontinuity in the 21st century: Better aligning behaviors, functions, and mechanisms. Developmental Psychology. https://doi.org/10.1037/dev0001657 Copyright (c) American Psychological Association. Used with permission.) Figure 23.15: Three Types of Continuity in Addition to Discontinuity in the Form of a 2 (Behavioral Manifestation, Underlying Processes) x 2 (Same Versus Different Across Time) Latin Square. ‘T1,’ ‘T2,’ and ‘T3’ reflect time points 1, 2, and 3, respectively. The illustrations above the lines are buildings, representing the surface structure (i.e., behavioral manifestation). The illustrations below the lines depict the underlying processes supporting the buildings at each time point. The squares on the buildings are windows. The black windows represent content facets that are active across all time points (i.e., age-common content). The windows that contain X’s represent content facets that are active at some but not all time points (i.e., age-unique content). The white windows represent content facets that are inactive, and therefore are not part of the construct at that time point. The increasing size of the buildings at later time points reflects growth with development. The top row of the Latin square involves the same underlying processes across time, whereas the bottom row involves different underlying proccesses across time. The left column of the Latin square involves the same behavioral manifestation across time, whereas the right column involves a different behavioral manifestation across time. Homotypic continuity (top left) describes the same behaviorial manifestation with the same underlying process (i.e., construct) across development. Heterotypic continuity (top right) describes the same underlying process with a different behaviorial manifestation across development. Phenotypic continuity (or functional discontinuity) (bottom left) describes the same behavior with different underlying processes across development. Discontinuity (bottom right) describes different behavioral manifestations with different underlying processes across development. Thus, in both homotypic continuity and heterotypic continuity, the active content facets reflect the same construct or underlying process across time, whereas in phenotypic continuity and discontinuity, the active content facets do not reflect the same construct across time. (Figure reprinted from Petersen et al. (2020), Figure 1, p. 2. Petersen, I. T., Choe, D. E., &amp; LeBeau, B. (2020). Studying a moving target in development: The challenge and opportunity of heterotypic continuity. Developmental Review, 58, 100935. https://doi.org/10.1016/j.dr.2020.100935 Copyright (c) Elsevier. Used with permission.) 23.9 Heterotypic Continuity Heterotypic continuity is the persistence of an underlying construct with behavioral manifestations that change across development (Caspi &amp; Shiner, 2006; Cicchetti &amp; Rogosch, 2002; Petersen et al., 2020; Petersen, in press). Patterson (1993) described antisocial behavior like a chimera. A chimera is a mythical creature with a body of a goat that, with development, grows the head of a lion and then a tail of a snake. Patterson (1993) conceptualized antisocial behavior across development as having an underlying essence of oppositionality that is maintained while adding more mature features with age, including the ability to inflict serious damage with aggression and violence. Other analogies of heterotypic continuity include the change from water to ice or steam, and the metamorphosis of a caterpillar to a butterfly—the same construct shows different manifestations across development. 23.9.1 How to Identify Heterotypic Continuity Heterotypic continuity can be identified or quantified as changes in the factor structure (i.e., content) of a construct across time. The content of a measure include the facets assessed by a given measure, such as individual behaviors, questionnaire items, or sub-dimensions of a broader construct. The content of the measure can be compared to content of a construct to evaluate the content validity of the measure. Heterotypic continuity is present to the extent that the content of a construct show cross-time changes in: people’s rank-order stability, the content’s level on the construct, and how strongly the content reflect the construct. Cross-time changes in people’s rank-order stability indicate changes in the degree of stability of individual differences in the construct across time, and are examined with correlation of the content across time. The content’s level on the construct is similar to the inverse of frequency, prevalence, or base rate of the content. The content’s level on the construct is examined with item difficulty (severity) in IRT or with intercepts in SEM. For example, the item “sets fires” has a higher item severity than the item “argues” because setting fires is less frequent than arguing. How strongly the content reflect the construct, conceptually is the degree of a content’s construct validity, and is examined with item discrimination in IRT or with factor loadings in SEM. For example, the item “hits others” has a higher item discrimination than “eats ice cream” for the construct of externalizing problems because hitting others is more strongly related to the construct (compared to eating ice cream). Externalizing problems are an example of a construct that changes in behavioral manifestation with development (F. R. Chen &amp; Jaffee, 2015; J. L. Miller et al., 2009; Moffitt, 1993; Patterson, 1993; Petersen et al., 2015; Petersen &amp; LeBeau, 2022; Wakschlag et al., 2010). The content of externalizing problems show changes in the magnitude of rank-order stability, changes in the content’s level on the construct, and changes in how strongly the construct reflect the construct. For example, inattention shows increases in the magnitude of rank-order stability with age (Arnett et al., 2012). Drinking alcohol becomes more frequent in adolescence compared to childhood, so its item severity decreases. Threatening other people is more strongly associated with externalizing problems in adolescence than in early childhood (Lubke et al., 2018). Moreover, externalizing content, including disobedience and biting, likely change in meaning with developoment. In childhood, disobedience could be an indicator of externalizing problems. In adulthood, disobedience could serve prosocial functions, including protesting against societally unjust actions, and thus shows weaker construct validity for externalizing problems. In general, in early childhood, externalizing problems often manifest in overt forms, such as physical aggression and temper tantrums. Later in development, externalizing problems tends to manifest in covert forms, such as indirect aggression, relational aggression, and substance use (J. L. Miller et al., 2009). In addition to externalizing problems, other constructs also likely show heterotypic continuity across development. For instance, internalizing problems may show heterotypic continuity (Weems, 2008; Weiss &amp; Garber, 2003) such that anxiety precedes depression (Garber &amp; Weersing, 2010). In addition, somatic complaints such as headaches, stomachaches, and heart pounding are more strongly associated with and more common in those with internalizing problems in childhood than adulthood (Petersen et al., 2018). Other constructs that show heterotypic continuity include inhibitory control (Petersen et al., 2016; Petersen, Bates, et al., 2021), substance use (Schulenberg &amp; Maslowsky, 2009), sleep states (Blumberg, 2013), and temperament (Putnam et al., 2008). In addition, constructs that are typically assessed with different measures across ages also likely show heterotypic continuity, including language ability (Petscher et al., 2018), nonverbal ability (McArdle &amp; Grimm, 2011), working memory (McArdle et al., 2009), and academic skills (Tong &amp; Kolen, 2007). In sum, many constructs likely show heterotypic continuity. However, it is important rule out potential alternatives to heterotypic continuity, as depicted in Figure 23.16. Figure 23.16: Heterotypic Continuity Versus Alternative Developmental Processes. Arrows reflect causal influence. Dotted lines reflect persistence (i.e., continuity) of the same behavior or construct across time. “T1” and “T2” reflect time points 1 and 2, respectively. (Figure reprinted from Petersen (in press), Figure 6, Petersen, I. T. (in press). Reexamining developmental continuity and discontinuity in the 21st century: Better aligning behaviors, functions, and mechanisms. Developmental Psychology. https://doi.org/10.1037/dev0001657 Copyright (c) American Psychological Association. Used with permission.) 23.9.2 The Problem Heterotypic continuity poses challenges for longitudinal assessment. According to the maxim, “what we know depends on how we know it”. If a construct changes in manifestation across time, and the measures do not align with the changing manifestation, the scores will not validly assess individual’s growth. Failing to account for heterotypic continuity leads to misidentified trajectories of people’s growth (F. R. Chen &amp; Jaffee, 2015) and to trajectories that are less able to detect people’s change (Petersen et al., 2018; Petersen, LeBeau, et al., 2021). Thus, it is important that our measurement and statistical schemes account for heterotypic continuity when a construct changes in manifestation across time. These measurement and statistical schemes are discussed below. 23.9.3 Ways to Assess a Construct Across Development There are three ways of assessing a construct across development: All possible content Only the common content Only the construct-valid content Assume content A is construct-valid at T1, content B is content-valid at T1 and T2, and content C is content-valid at T2. All possible content would assess: ABC at T1, ABC at T2. Only the common content would assess: B at T1, B at T2. Only the construct-valid content would assess: AB at T1, BC at T2. A depiction of assessing only the construct-valid content is in Figure 23.17. Figure 23.17: Using Only the Construct-Valid Content at Each Age. Content set A corresponds to content that is construct-valid at only T1. Content set B corresponds to content that is construct-valid at both T1 and T2. Content set C corresponds to content that is construct-valid at only T2. The common content (content set B) is highlighted in gray. The three approaches to assessing a construct over time are as follows: (1) using all possible content across all ages: ABC at T1 and T2, (2) using only common content across all ages: B at T1 and T2, or (3) using only construct-valid content at each age: AB at T1 and BC at T2. (Figure reprinted from Petersen, LeBeau, et al. (2021), Figure 1, p. e3. Petersen, I. T., LeBeau, B., &amp; Choe, D.E. (2021). Creating a developmental scale to account for heterotypic continuity in development: A simulation study. Child Development, 92(1), e1–e19. https://doi.org/10.1111/cdev.13433 Copyright (c) John Wiley and Sons. Used with permission.) 23.9.3.1 All Possible Content There are advantages and disadvantages of each. Advantages of using all possible content include that it is comprehensive and allows examining change in each content facet. However, it also has key disadvantages. First, it is inefficient and assesses lots of items. Second, it could assess developmentally inappropriate content. And in the case of heterotypic continuity, it assesses content that lack construct validity, i.e., there are item intrusions. 23.9.3.2 Only the Common Content Advantages of using only the common content include that it is efficient and may exclude developmentally inappropriate content. Moreover, it permits examining consistent content across ages. However, it also has key disadvantages. First, it results in fewer items and a loss of information. In particular, it may involve a systematic loss of content that reflects very low or very high levels on the construct. Third, and due to the loss of information, it may be less sensitive to developmental change. And in the case of heterotypic continuity, it assesses content that lack content validity because all important facets of the construct are not assessed—i.e., there are item gaps. 23.9.3.3 Only the Construct-Valid Content There are disadvantages to using only the construct-valid content. First, it is more time-intensive than assessing only the common content. Second, it uses a different measure across time, which can pose challenges to ensuring scores across time are comparable. Nevertheless, the approach has important advantages. First, it is more efficient than using all possible content. Second, it retains content validity and construct validity. And in the case of heterotypic continuity, it is recommended. Developmental theory requires that we account for changes in the construct’s expression! Many measures are used outside the age range for which they were designed and validated to use. Many measures for children and adolescents are little more than downward extensions of those developed for use with adults (Hunsley &amp; Mash, 2007). Use of the same measure across ages does not ensure measurement equivalence. The problem is that even if the same measure is used across ages, the measure may lack construct validity at some ages. Thus heterotypic continuity may require different measures at different ages to retain construct validity. But then there is the challenge of how to assess change when using different measures across time. 23.9.4 Accounting for Heterotypic Continuity To account for heterotypic continuity, it is recommended to use different measures across time. Specifically, it is recommended to use (only) the construct-valid content at each age when the content is valid for assessing the construct. To ensure scores are comparable across time, despite the differing measures, it is important to take steps to ensure statistical and theoretical equivalence of scores across time. 23.9.4.1 Ensuring Statistical Equivalence Historically, there have been various attempts to handle different measures across time to ensure statistical equivalence of scores—i.e., that measures are on the same mathematical metric. 23.9.4.1.1 Age-norming Age-norming includes standardized scores (e.g., T scores, z scores, standard scores) and percentile ranks. However, as discussed in Section 23.3, standardized scores prevent observing individuals’ or groups’ absolute change over time. Moreover, despite putting scores on the same mathematical metric, they do not ensure scores are on the same conceptual metric. Thus, age-norming is not recommended for putting different measures onto the same score to account for heterotypic continuity and examine individuals’ growth. 23.9.4.1.2 Average or Proportion Scores Average and proportion scores account for the different numbers of items in each measure. However, average and proportion scores assume that measures do not differ in difficulty or discrimination. Thus, average and proportion scores are not recommended to account for heterotypic continuity and examine individuals’ growth. 23.9.4.1.3 Developmental Scaling A third approach to ensure statistical equivalence of measures is developmental scaling, which is also called vertical scaling (see Figure 23.18). In developmental scaling, measures that differ in difficulty and discrimination are placed on the same scale using the common content. The common content set the scale, but all content (including both age-common content and age-differing content) estimate each person’s score on that scale. There are several approaches to developmental scaling, including Thurstone scaling, factor analysis, and IRT. Figure 23.18: Illustrative Example of a Vertical Scaling Design That Uses Common Content to Link the Different Measures at Adjacent Ages to be on the Same Scale. For example, content set B is administered at both kindergarten and 1st grade, and is the common content used to link scores at 1st grade to the same scale as kindergarten scores. Content set A is the unique content at kindergarten; content set C is the unique content at 1st grade (but it is common content with content set C at 2nd grade). The unique content represents age-specific manifestations of the construct. The procedure of linking measures that differ in difficulty to be on the same scale is called vertical scaling (as opposed to horizontal scaling) because the measures, especially in educational settings, tend to increase in difficulty with age (relative to a given level of ability; as depicted above with the upward-trend). Thus, vertical scaling is particularly useful for linking different measures across ages to be on the same scale. Horizontal scaling, by contrast, links different measures that have the same difficulty (commonly at the same age). To better align with the breadth of constructs in developmental psychology, we use the term ‘developmental scaling’ instead of vertical scaling to refer to putting measures across development on the same scale. (Figure reprinted from Petersen et al. (2020), Figure 3, p. 9. Petersen, I. T., Choe, D. E., &amp; LeBeau, B. (2020). Studying a moving target in development: The challenge and opportunity of heterotypic continuity. Developmental Review, 58, 100935. https://doi.org/10.1016/j.dr.2020.100935 Copyright (c) Elsevier. Used with permission.) Thurstone scaling aligns the percentile scores of two measures based on z scores on the common content. It is an observed score approach. An example of Thurstone scaling is provided by Petersen et al. (2018). Factor analysis allows estimation of a latent variable using different content over time. An example of a factor analysis approach to developmental scaling is provided by S. Wang et al. (2013). IRT finds scaling parameters that put people’s scores from the different measures on the same metric, and it links the measures’ scales based on the difficulty and discrimination of the common content. Examples of an IRT approach to developmental scaling are provided by Petersen et al. (2018), Petersen, LeBeau, et al. (2021), Petersen &amp; LeBeau (2022), and Petersen &amp; LeBeau (2021). The IRT approach to developmental scaling is depicted in Figure 23.19. Figure 23.19: Effect of Linking the Latent Externalizing Problems Scores, \\(\\theta\\), Across Ages, Using Mother-Reported Externalizing Problems at Ages 4 and 5 as an Example. The left panel illustrates the test characteristic curves representing the model-implied proportion out of total possible scores across the latent externalizing problems score at age 4 and 5, before the linking process. The right panel illustrates the test characteristic curves after the linking process. The shading between the age 4 and age 5 test characteristic curves represents differences between the two test characteristic curves in terms of discrimination and/or severity, where larger differences reflect scores that are less comparable. Linking minimizes differences between the discrimination and severity of the common items. Discrimination is depicted by the steepness of the slope at the inflection point of the test characteristic curve. Severity is represented by the value on the x-axis at the inflection point of the test characteristic curve. The left panel indicates that the externalizing problem items showed higher severity at age 5 than at age 4. The right panel shows considerably smaller differences between the two test characteristic curves, which provides empirical evidence that the linking successfully placed the latent externalizing problem scores across age on a more comparable scale (i.e., more similar discrimination and severity of the common items). (Figure reprinted from Petersen &amp; LeBeau (2021), Figure 1, p. 7. Petersen, I. T., &amp; LeBeau, B. (2021). Language ability in the development of externalizing behavior problems in childhood. Journal of Educational Psychology, 113(1), 68–85. https://doi.org/10.1037/edu0000461 Copyright (c) American Psychological Association. Used with permission.) 23.9.4.2 Ensuring Theoretical Equivalence A number of pieces of evidence provide support for the theoretical equivalence of measures. The goal is for the measures to have construct validity invariance—the measure should assess the same construct across time. The content that are assessed at each age should be based on theory. The assessed content should reflect the same construct at relevant ages—i.e., the content should show construct validity at each age they are assessed. Second, the content should adequately sample all aspects of construct—i.e., the content should show content validity. Second, assuming the construct is thought to be relatively stable, the measures should show test–retest reliability, at least in the short term. In addition, the measures should show convergent and discriminant validity. Furthermore, the measures assessed should show a similar factor structure across time, based on factor analysis. In addition, the measures should have high internal consistency reliability. 23.9.5 Are There Measures that Can Span the Lifespan? There are measures that attempt to span the lifespan, including the NIH Toolbox (Weintraub et al., 2013) and the Minnesota Executive Function Scale (Carlson &amp; Zelazo, 2014). However, I would argue that a single (non-adaptive) measure would not be able to validly assess the same construct across development if the construct shows heterotypic continuity. The content must change with changes in the construct’s expression in order to remain valid for the construct given its changing behavioral manifestation. Adaptive testing may help to achieve this goal. However, just because something is on the same mathematical metric does not mean it is on the same conceptual metric. The same challenge can be relevant for many different types of data, including questionnaire items, reaction time, event-related potentials (ERPs), electrocardiography (ECG/EKG), and functional magnetic resonance imaging (fMRI). 23.9.6 Why not Just Study Development in a Piecewise Way? If heterotypic continuity poses such challenges, why not just study development in a piecewise way? Although there can be benefits to examining development in a piecewise way, relying on a fixed set of content restricts our ability to see growth over a longer time, and to understand key developmental transitions. The pathways to outcomes can be just as important as the outcomes themselves. Consistent with the principle of multifinality, the same genetic and environmental influences can lead to different outcomes for different people. Conversely, consistent with the principle of equifinality, different genetic and environmental influences can lead to the same outcome for different people. Longitudinal studies allow understanding antecedent-consequent effects, better estimation of change than difference scores derived from two time points, and separating age-related from cohort effects. 23.9.7 Why not Just Study Individual Behaviors? If higher-order constructs show heterotypic continuity, why not just study individual behaviors? Individual behaviors can also show changes in meaning. Moreover, there is utility in examining constructs rather than individual behaviors. Theoretical constructs have better psychometric properties, including better reliability than individual behaviors due to the principle of aggregation (Rushton et al., 1983). Moreover, constructs are theoretically and empirically derived. You can always reduce to a lower-level unit, and this reductionism can result in a loss of utility. 23.9.8 Summary In summary, it is important to pay attention to whether constructs change in expression across development. Use different measures across time to align with changes in the manifestation of the construct. Use developmental scaling approaches to link different measures on the same scale. Use theoretical considerations to ensure the measures reflect the same construct across development. In doing so, we will be better able to study development across the lifespan. 23.10 Conclusion Repeated measurement is a design feature that can provide stronger tests of causality compared to concurrent associations. By repeatedly assessing multiple constructs, we can examine lagged associations that control for prior levels of the outcome and that simultaneously test each direction of effect. There are many important considerations for assessing change. The inference of change is strengthened to the extent that (a) the magnitude of the difference between the scores at the two time points is large, (b) the measurement error at both time points is small, (c) the measure has the same meaning and is on a comparable scale at each time point, (d) the differences across time are not likely to be due to potential confounds of change such as practice effects, cohort effects, and time-of-measurement effects. Difference scores, though widely used, can have limitations because they can be less reliable than each of the individual measures that compose the difference score. Instead of using traditional difference scores to represent change, consider other approaches such as autoregressive cross-lagged models, latent change score models, or growth curve models. There are a variety of types of research designs based on combinations of age, period, and cohort: cross-sectional, cross-sectional sequences, time-lag, longitudinal, and longitudinal sequences, such as time-sequential, cross-sequential, and cohort-sequential designs. Which research design you select should depend on which change function you are most interested in: age, cohort, or time of measurement. However, any single research design has either age, cohort, or time of measurement confounded, so the strongest inferences come from inferences drawn from more than one research design. When considering people’s change, it is important to consider changes in the behavioral manifestation of constructs, known as heterotypic continuity. Heterotypic continuity can be identified as changes in the factor structure (i.e., content) of a construct across time. Use different measures across time to align with changes in the manifestation of the construct. Use developmental scaling to link different measures on the same scale. Use theoretical considerations to ensure the measures reflect the same construct across development. 23.11 Suggested Readings Petersen et al. (2020); Hertzog &amp; Nesselroade (2003) References Ackerman, P. L. (2013). Assessment of intellectual functioning in adults. In K. F. Geisinger, J. F. Carlson, J.-I. C. Hansen, N. R. Kuncel, S. P. Reise, &amp; M. C. Rodriguez (Eds.), APA handbook of testing and assessment in psychology, Vol 2: Testing and assessment in clinical and counseling psychology (pp. 119–132). American Psychological Association. Arnett, A., Pennington, B., Willcutt, E., Dmitrieva, J., Byrne, B., Samuelsson, S., &amp; Olson, R. (2012). A cross-lagged model of the development of ADHD inattention symptoms and rapid naming speed. Journal of Abnormal Child Psychology, 40(8), 1313–1326. https://doi.org/10.1007/s10802-012-9644-5 Baltes, P. B. (1968). Longitudinal and cross-sectional sequences in the study of age and generation effects. Human Development, 11(3), 145–171. http://www.jstor.org/stable/26761719 Beltz, A. M., Wright, A. G. C., Sprague, B. N., &amp; Molenaar, P. C. M. (2016). Bridging the nomothetic and idiographic approaches to the analysis of clinical data. Assessment, 23(4), 447–458. https://doi.org/10.1177/1073191116648209 Berry, D., &amp; Willoughby, M. T. (2017). On the practical interpretability of cross-lagged panel models: Rethinking a developmental workhorse. Child Development, 88(4), 1186–1206. https://doi.org/10.1111/cdev.12660 Blumberg, M. S. (2013). Homology, correspondence, and continuity across development: The case of sleep. Developmental Psychobiology, 55(1), 92–100. https://doi.org/10.1002/dev.21024 Carlson, S. M., &amp; Zelazo, P. D. (2014). Minnesota executive function scale. Test manual. Reflection Sciences, LLC. Carpenter, R. W., Wycoff, A. M., &amp; Trull, T. J. (2016). Ambulatory assessment: New adventures in characterizing dynamic processes. Assessment, 23(4), 414–424. https://doi.org/10.1177/1073191116632341 Caspi, A., &amp; Shiner, R. L. (2006). Personality development. In N. Eisenberg, W. Damon, &amp; R. M. Lerner (Eds.), Handbook of child psychology (6th ed., Vol. 3, pp. 300–365). John Wiley &amp; Sons, Inc. Chen, F. R., &amp; Jaffee, S. R. (2015). The heterogeneity in the development of homotypic and heterotypic antisocial behavior. Journal of Developmental and Life-Course Criminology, 1(3), 269–288. https://doi.org/10.1007/s40865-015-0012-3 Cicchetti, D., &amp; Rogosch, F. A. (2002). A developmental psychopathology perspective on adolescence. Journal of Consulting and Clinical Psychology, 70(1), 6–20. https://doi.org/10.1037/0022-006X.70.1.6 Evans, S. C., &amp; Shaughnessy, S. (in press). Emotion regulation as central to psychopathology across childhood and adolescence: A commentary on Nobakht et al. (2023). Journal of Child Psychology and Psychiatry. https://doi.org/https://doi.org/10.1111/jcpp.13910 Fontaine, N. M. G., &amp; Petersen, I. T. (2017). Developmental trajectories of psychopathology: An overview of approaches and applications. In L. Centifanti &amp; D. Williams (Eds.), The wiley handbook of developmental psychopathology (pp. 5–28). Wiley-Blackwell. Garber, J., &amp; Weersing, V. R. (2010). Comorbidity of anxiety and depression in youth: Implications for treatment and prevention. Clinical Psychology: Science and Practice, 17(4), 293–306. https://doi.org/10.1111/j.1468-2850.2010.01221.x Hamaker, E. L., Kuiper, R. M., &amp; Grasman, R. P. P. P. (2015). A critique of the cross-lagged panel model. Psychological Methods, 20(1), 102–116. https://doi.org/10.1037/a0038889 Hertzog, C., &amp; Nesselroade, J. R. (2003). Assessing psychological change in adulthood: An overview of methodological issues. Psychology and Aging, 18(4), 639–657. https://doi.org/10.1037/0882-7974.18.4.639 Hunsley, J., &amp; Mash, E. J. (2007). Evidence-based assessment. Annual Review of Clinical Psychology, 3, 29–51. https://doi.org/10.1146/annurev.clinpsy.3.022806.091419 Kagan, J. (1969). The three faces of continuity in human development. In D. A. Goslin (Ed.), Handbook of socialization theory and research (pp. 983–1002). Rand McNally. Kievit, R. A., Brandmaier, A. M., Ziegler, G., Harmelen, A.-L. van, Mooij, S. M. M. de, Moutoussis, M., Goodyer, I., Bullmore, E., Jones, P. B., Fonagy, P., Lindenberger, U., &amp; Dolan, R. J. (2018). Developmental cognitive neuroscience using latent change score models: A tutorial and applications. Developmental Cognitive Neuroscience, 33, 99–117. https://doi.org/10.1016/j.dcn.2017.11.007 Little, T. D. (2013). Longitudinal structural equation modeling. The Guilford Press. Lubke, G. H., McArtor, D. B., Boomsma, D. I., &amp; Bartels, M. (2018). Genetic and environmental contributions to the development of childhood aggression. Developmental Psychology, 54(1), 39–50. https://doi.org/10.1037/dev0000403 Masche, J. G., &amp; Dulmen, M. H. M. van. (2004). Advances in disentangling age, cohort, and time effects: No quadrature of the circle, but a help. Developmental Review, 24(3), 322–342. https://doi.org/10.1016/j.dr.2004.04.002 Matthews, M., Abdullah, S., Murnane, E., Voida, S., Choudhury, T., Gay, G., &amp; Frank, E. (2016). Development and evaluation of a smartphone-based measure of social rhythms for bipolar disorder. Assessment, 23(4), 472–483. https://doi.org/10.1177/1073191116656794 McArdle, J. J., &amp; Grimm, K. J. (2011). An empirical example of change analysis by linking longitudinal item response data from multiple tests. In A. A. von Davier (Ed.), Statistical models for test equating, scaling, and linking (pp. 71–88). Springer Science &amp; Business Media. McArdle, J. J., Grimm, K. J., Hamagami, F., Bowles, R. P., &amp; Meredith, W. (2009). Modeling life-span growth curves of cognition using longitudinal data with multiple samples and changing scales of measurement. Psychological Methods, 14(2), 126–149. https://doi.org/10.1037/a0015857 Miller, J. L., Vaillancourt, T., &amp; Boyle, M. H. (2009). Examining the heterotypic continuity of aggression using teacher reports: Results from a national Canadian study. Social Development, 18(1), 164–180. https://doi.org/10.1111/j.1467-9507.2008.00480.x Moeller, J. (2015). A word on standardization in longitudinal studies: don’t. Frontiers in Psychology, 6(1389), 1–4. https://doi.org/10.3389/fpsyg.2015.01389 Moffitt, T. E. (1993). Adolescence-limited and life-course-persistent antisocial behavior: A developmental taxonomy. Psychological Review, 100(4), 674–701. https://doi.org/10.1037/0033-295X.100.4.674 Moffitt, T. E. (2006a). A review of research on the taxonomy of life-course persistent versus adolescence-limited antisocial behavior. Taking Stock: The Status of Criminological Theory, 15, 277–312. Moffitt, T. E. (2006b). Life-course-persistent versus adolescence-limited antisocial behavior. In D. C. D. J. Cohen (Ed.), Developmental psychopathology, vol 3: Risk, disorder, and adaptation (2nd ed.) (pp. 570–598). John Wiley &amp; Sons Inc. Orth, U., Clark, D. A., Donnellan, M. B., &amp; Robins, R. W. (2021). Testing prospective effects in longitudinal research: Comparing seven competing cross-lagged models. Journal of Personality and Social Psychology, 120(4), 1013–1034. https://doi.org/10.1037/pspp0000358 Patterson, G. R. (1993). Orderly change in a stable world: The antisocial trait as a chimera. Journal of Consulting and Clinical Psychology, 61(6), 911–919. https://doi.org/10.1037/0022-006X.61.6.911 Petersen, I. T. (in press). Reexamining developmental continuity and discontinuity in the 21st century: Better aligning behaviors, functions, and mechanisms. Developmental Psychology. https://doi.org/10.1037/dev0001657 Petersen, I. T., Bates, J. E., D’Onofrio, B. M., Coyne, C. A., Lansford, J. E., Dodge, K. A., Pettit, G. S., &amp; Van Hulle, C. A. (2013). Language ability predicts the development of behavior problems in children. Journal of Abnormal Psychology, 122(2), 542–557. https://doi.org/10.1037/a0031963 Petersen, I. T., Bates, J. E., Dodge, K. A., Lansford, J. E., &amp; Pettit, G. S. (2015). Describing and predicting developmental profiles of externalizing problems from childhood to adulthood. Development and Psychopathology, 27(3), 791–818. https://doi.org/10.1017/S0954579414000789 Petersen, I. T., Bates, J. E., McQuillan, M. E., Hoyniak, C. P., Staples, A. D., Rudasill, K. M., Molfese, D. L., &amp; Molfese, V. J. (2021). Heterotypic continuity of inhibitory control in early childhood: Evidence from four widely used measures. Developmental Psychology, 57(11), 1755–1771. https://doi.org/10.1037/dev0001025 Petersen, I. T., Choe, D. E., &amp; LeBeau, B. (2020). Studying a moving target in development: The challenge and opportunity of heterotypic continuity. Developmental Review, 58, 100935. https://doi.org/10.1016/j.dr.2020.100935 Petersen, I. T., Hoyniak, C. P., McQuillan, M. E., Bates, J. E., &amp; Staples, A. D. (2016). Measuring the development of inhibitory control: The challenge of heterotypic continuity. Developmental Review, 40, 25–71. https://doi.org/10.1016/j.dr.2016.02.001 Petersen, I. T., &amp; LeBeau, B. (2021). Language ability in the development of externalizing behavior problems in childhood. Journal of Educational Psychology, 113(1), 68–85. https://doi.org/10.1037/edu0000461 Petersen, I. T., &amp; LeBeau, B. (2022). Creating a developmental scale to chart the development of psychopathology with different informants and measures across time. Journal of Psychopathology and Clinical Science, 131(6), 611–625. https://doi.org/10.1037/abn0000649 Petersen, I. T., LeBeau, B., &amp; Choe, D. E. (2021). Creating a developmental scale to account for heterotypic continuity in development: A simulation study. Child Development, 92(1), e1–e19. https://doi.org/10.1111/cdev.13433 Petersen, I. T., Lindhiem, O., LeBeau, B., Bates, J. E., Pettit, G. S., Lansford, J. E., &amp; Dodge, K. A. (2018). Development of internalizing problems from adolescence to emerging adulthood: Accounting for heterotypic continuity with vertical scaling. Developmental Psychology, 54(3), 586–599. https://doi.org/10.1037/dev0000449 Petscher, Y., Justice, L. M., &amp; Hogan, T. (2018). Modeling the early language trajectory of language development when the measures change and its relation to poor reading comprehension. Child Development, 89(6), 2136–2156. https://doi.org/10.1111/cdev.12880 Putnam, S. P., Rothbart, M. K., &amp; Gartstein, M. A. (2008). Homotypic and heterotypic continuity of fine-grained temperament during infancy, toddlerhood, and early childhood. Infant &amp; Child Development, 17(4), 387–405. https://doi.org/10.1002/ICD.582 Rushton, J. P., Brainerd, C. J., &amp; Pressley, M. (1983). Behavioral development and construct validity: The principle of aggregation. Psychological Bulletin, 94(1), 18–38. https://doi.org/10.1037/0033-2909.94.1.18 Schaie, K. W. (1965). A general model for the study of developmental problems. Psychological Bulletin, 64(2), 92–107. https://doi.org/10.1037/h0022371 Schaie, K. W. (2005). Developmental influences on adult intelligence: The Seattle longitudinal study. Oxford University Press. Schaie, K. W., &amp; Baltes, P. B. (1975). On sequential strategies in developmental research. Human Development, 18(5), 384–390. https://doi.org/10.1159/000271498 Schulenberg, J. E., &amp; Maslowsky, J. (2009). Taking substance use and development seriously: Developmentally distal and proximal influences on adolescence drug use. Monographs of the Society for Research in Child Development, 74(3), 121–130. https://doi.org/10.1111/j.1540-5834.2009.00544.x Schulenberg, J. E., Patrick, M. E., Maslowsky, J., &amp; Maggs, J. L. (2014). The epidemiology and etiology of adolescent substance use in developmental perspective. In M. Lewis &amp; K. D. Rudolph (Eds.), Handbook of developmental psychopathology (pp. 601–620). Springer US. Schulenberg, J. E., &amp; Zarrett, N. R. (2006). Mental health during emerging adulthood: Continuity and discontinuity in courses, causes, and functions. In Emerging adults in america: Coming of age in the 21st century. (pp. 135–172). American Psychological Association. Sobell, L. C., &amp; Sobell, M. B. (2008). Timeline followback (TLFB). In A. J. Rush Jr., M. B. First, &amp; D. Blacker (Eds.), Handbook of psychiatric measures (2nd ed., pp. 466–468). American Psychiatric Publishing. Tong, Y., &amp; Kolen, M. J. (2007). Comparisons of methodologies and results in vertical scaling for educational achievement tests. Applied Measurement in Education, 20(2), 227–253. https://doi.org/10.1080/08957340701301207 van der Nest, G., Lima Passos, V., Candel, M. J. J. M., &amp; van Breukelen, G. J. P. (2020). An overview of mixture modelling for latent evolutions in longitudinal data: Modelling approaches, fit statistics and software. Advances in Life Course Research, 43, 100323. https://doi.org/10.1016/j.alcr.2019.100323 Wakschlag, L. S., Tolan, P. H., &amp; Leventhal, B. L. (2010). Research review: “Ain’t misbehavin”: Towards a developmentally-specified nosology for preschool disruptive behavior. Journal of Child Psychology and Psychiatry, 51(1), 3–22. https://doi.org/10.1111/j.1469-7610.2009.02184.x Wang, S., Jiao, H., &amp; Zhang, L. (2013). Validation of longitudinal achievement constructs of vertically scaled computerised adaptive tests: A multiple-indicator, latent-growth modelling approach. International Journal of Quantitative Research in Education, 1(4), 383–407. https://doi.org/10.1504/IJQRE.2013.058307 Weems, C. F. (2008). Developmental trajectories of childhood anxiety: Identifying continuity and change in anxious emotion. Developmental Review, 28(4), 488–502. https://doi.org/10.1016/j.dr.2008.01.001 Weintraub, S., Bauer, P. J., Zelazo, P. D., Wallner-Allen, K., Dikmen, S. S., Heaton, R. K., Tulsky, D. S., Slotkin, J., Blitz, D. L., Carlozzi, N. E., Havlik, R. J., Beaumont, J. L., Mungas, D., Manly, J. J., Borosh, B. G., Nowinski, C. J., &amp; Gershon, R. C. (2013). I. NIH toolbox cognition battery (CB): Introduction and pediatric data. Monographs of the Society for Research in Child Development, 78(4), 1–15. https://doi.org/10.1111/mono.12031 Weiss, B., &amp; Garber, J. (2003). Developmental differences in the phenomenology of depression. Development and Psychopathology, 15(2), 403–430. https://doi.org/10.1017/S0954579403000221 Whitbourne, S. K. (2019). Longitudinal, cross-sectional, and sequential designs in lifespan developmental psychology. Oxford University Press. Wright, A. G. C., Gates, K. M., Arizmendi, C., Lane, S. T., Woods, W. C., &amp; Edershile, E. A. (2019). Focusing personality assessment on the person: Modeling general, shared, and person specific processes in personality and psychopathology. Psychological Assessment, 31(4), 502–515. https://doi.org/10.1037/pas0000617 Wright, A. G. C., &amp; Woods, W. C. (2020). Personalized models of psychopathology. Annual Review of Clinical Psychology, 16(1), 49–74. https://doi.org/10.1146/annurev-clinpsy-102419-125032 Yang, Y., &amp; Land, K. C. (2013). Age-period-cohort analysis: New models, methods, and empirical applications. Taylor &amp; Francis. "],["cognition.html", "Chapter 24 Assessment of Cognition 24.1 Overview of Cognition 24.2 Aspects of Cognition Assessed 24.3 Approaches to Assessing Cognition 24.4 Conclusion 24.5 Suggested Readings", " Chapter 24 Assessment of Cognition 24.1 Overview of Cognition Cognitive-behavioral therapy (CBT) is the most effective treatment we have for many mental health conditions, especially depression and anxiety. The cognitive therapy component of CBT assumes that thoughts mediate the influences of the environment on emotions and behavioral responses. In general, cognitions and cognitive processes play a key role in many psychological theories. Therefore, cognitions and distorted cognitions are assumed to be a key construct to assess. However, there are many challenges to cognitive assessment. Some consider cognitions to be fictitious entities or epiphenomenal to people’s experience—i.e., that is, they are not a causal process, but rather they are a by-product of neural and biological processes. 24.2 Aspects of Cognition Assessed Multiple aspects of cognition can be assessed, including cognitive products, processes, and structures or organization. Cognitive products include a person’s conscious thoughts or imaginal images. Cognitive processes include how a person transforms the environmental input and how they take meaning from it. Cognitive structures and organization include the hypothesized structures that a person has to guide information processing. Consider an example of a person with an anxiety disorder. Cognitive products for the person may include having the thought, “I’m going to make a fool of myself, and everyone will think I’m stupid”. The person’s cognitive processes may involve the over-estimation of personal risk. The person’s cognitive structures may include “danger” schemas that, when activated, lead to attention bias toward threat stimuli. Though, this finding is under question because of challenges to the reliability of the dot-probe task, which has been frequently used to examine attentional bias. 24.3 Approaches to Assessing Cognition The general approaches to assessing cognition include endorsement methods and production methods. Endorsement methods for assessing cognition contain a predetermined set of thoughts that participants identify or rate, such as a checklist. Using production methods, participants generate or recall their thoughts, i.e., the method involves free response. The psychometric status is currently stronger for endorsement methods; however, increasing attention is being given to production methods. Approaches to assessing cognition are reviewed by Dunkley et al. (2019). 24.3.1 Self-Reports of Cognition Self-reports of cognition can either be in written or interview format and comprise most of the cognitive assessment techniques that are regularly used in clinical practice. Few of the other approaches to assessing cognition are used routinely in clinical practice. In a self-report of cognition, the participant must reflect on and report about their own cognitive style. Thus, self-report of cognition involves introspection. Self-reports of cognition are still used in CBT, to learn about people’s cognitive styles, their attributions to life events or a specific negative event, and their expectancies. Expectancies include, for instance, what the person anticipates happening. 24.3.1.1 Pros Pros of self-reports of cognition include: They are cheap, easy to score and administer, and are able to be useful in clinical practice. They are sensitive to treatment effects. Cognitive therapy advocates would argue that changes in cognitions mediate treatment changes (i.e., they are thought to be a causal mechanism that explains how therapy works). They are standardized, normed, and have been psychometrically validated. 24.3.1.2 Cons Despite the fact that these techniques are so popular, there are still some major drawbacks to these approaches. Cons of self-reports of cognition include: They require a participant to have insight into their cognitions that they can report on—such insight is not always typical. Nisbett &amp; Wilson (1977) published a classic study titled, “Telling more than we can know: Verbal reports on mental processes”. The study was based on the idea that people believe they were thinking particular things; however, they reported thoughts on things that would not have been possible for them to know. Findings such as these challenge the validity of self-reported cognition. They require a person to reflect upon their cognitions, retrospectively recalling what they were thinking in various situations. At least in the written format, which is often used as an endorsement method, participants are limited to responding to the experimenter-generated choice options—which can lead people into having certain thoughts and make people endorse thoughts that they did not actually have. 24.3.2 Think-Aloud Approaches (Thought Listing) In think-aloud approaches, the participant is asked to verbalize their cognitions in the form of a continuous monologue while performing some task, and responses are recorded for later evaluation. In this way, thoughts are assessed concurrently with their occurrence. In some approaches, such as Articulated Thoughts in Simulated Situations (ATSS), thoughts are assessed in simulated situations that are geared towards eliciting certain emotions—the “situations” can be presented in video, audiotape, or virtual reality format. In think-aloud approach, the participant lists their thoughts, so the approach is also called “thought listing”. The purpose of think-aloud approaches is to avoid retrospection, and to create a situation in the lab that resembles a real-world situation to see what thoughts or what stream of thoughts are generated. Think-aloud approaches are reviewed by Davison et al. (1997). 24.3.2.1 Examples Examples of think-aloud assessments of cognition include video replay and private speech. 24.3.2.1.1 Video Replay Video replay is also called videotape thought reconstruction. In videotape thought reconstruction, the examiner video records the interaction, then replays the video for the examinee, and asks the examinee what they were thinking during that moment. It is more proximal than retrospective report. The act of re-watching a video of oneself can be strange, and there is commonly reactivity to that. 24.3.2.1.2 Private Speech Another example of think-aloud cognition is children’s private speech. Young children sometimes talk out loud to themselves to guide their behavior, and the speech is not serving a communicative function. This is known as private or self-directed speech. Private speech is most common among 2- to 7-year-olds, and it provides insight into their cognition, often while performing a challenging task. 24.3.2.2 Pros Pros of think-aloud approaches include: Participants are asked to report on cognitions at the same time or just after the thoughts are occurring, so they are less sensitive to retrospective recall bias. Responses are not limited to experimenter-selected choices—all cognitions can be assessed. The constraints are made in terms of how the thoughts are coded and analyzed by the experimenter, not in terms of the unstructured and open-ended thoughts that the participant can provide. Specific situations of interest to the research team can be explored. For example, the examiner can examine the respondent in situations that are thought to evoke particular cognitive processes, including situations that might be impractical due to low frequency or that are unethical to assess in their natural context. For example, an examiner could examine a person’s self-reported thoughts in response to social criticism for someone with social anxiety. 24.3.2.3 Cons Cons of think-aloud approaches include: Performance is susceptible to observer effects. Observer effects are changes in the person’s response because they are being watched or recorded. That is, response biases may be present because the person’s responses are not anonymous. Social desirability bias: people may censor negative or disturbing thoughts and may describe their thoughts in a way that portrays themselves in a more positive light. It can be difficult to generate the thoughts. There are individual differences in people’s awareness of their cognitions. Think-aloud approaches can interrupt the thought flow—we think faster than we can speak, so verbalizing thoughts can alter the thoughts. That is, talking about thoughts interferes with the task itself. It can be difficult to get a whole narrative of thoughts from think-aloud approaches. It can be difficult to code the qualitative thoughts and reduce them down to meaningful data. And coders may attribute different meaning to a thought than was intended by the respondent. Think-aloud approaches tend to be lower in ecological validity because they typically occur in the lab rather than in a naturalistic context. 24.3.3 Random Thought Sampling The purpose of thought sampling is to quantify characteristics or aspects of thinking in an ecologically valid context. By ecologically valid, we mean that the situation closely represents the specific situations under which the person usually exists. In a random thought sampling procedure, participants are given a beeper or some other device that beeps randomly. When the person hears the beep, they must record what they are doing and what they are thinking—either quantitatively or qualitatively. The person hears a beep, and jots down the thought they were having when the beep occurred, so it is retrospective back a second. In the experience sampling method (ESM), the examiner can also ask participants to report the context in which the experience occurred. This can be helpful for those with schizophrenia who are hallucinating to help link the hallucinations to particular situations or contexts. Thought sampling and ESM are described by Hurlburt (1997). 24.3.3.1 Pros Pros of random thought sampling include: The delay of recall is instantaneous—hence participants are not forced to recall what they were thinking. The situations are ecologically valid in that they are occurring in situations that the person actually experiences, rather than a lab. Random thought sampling gets at what people are actually thinking about rather than what they think they are thinking about. Random thought sampling is better able to detect fluctuations in thoughts than retrospective approaches, and it is better able to detect co-variation with other processes, such as mood. Random thought sampling involves less interference with the interaction. Presumably, the “flow” of a situation is not interrupted…although it very well might be. A client’s reactivity could show therapeutic benefit. 24.3.3.2 Cons Cons of random thought sampling include: It assesses narrow slices of time. Because it assesses narrow slices of time, it is hard to capture the whole sequence of events. It is odd to be interrupted by a beeper. Random thought sampling could elicit reactivity such that thinking about one’s thoughts could change thoughts or create thoughts. Random thought sampling only accesses the contents of the consciousness, like the approaches discussed earlier. That is, the respondent has to be aware of the thoughts in order to write them down. There are likely many cognitive processes that nobody is aware of. The written-down thoughts have to be coded. Because random thought sampling typically occurs in an ecologically valid context, the experimenter has less experimental control of the situation, context, and stimuli. Consequently, the experimenter is less likely to discover relations between rare but theoretically important processes. 24.3.4 Cognitive Science Approaches Cognitive science approaches to assessing cognition include a range of non-mutually exclusive approaches, including performance-based measures, cognitive modeling, and cognitive neuroscience approaches. These cognitive science approaches are different from the approaches described earlier in that they do not rely on respondents reporting their thoughts. Cognitive science approaches try to assess a person’s cognitions based on behavior or neural functioning. For instance, using performance-based measures and cognitive modeling, the approaches are based on behavioral performance and then use a model to try to estimate what is happening cognitively—the participants just have to engage and process information. 24.3.4.1 Performance-Based Measures Performance-based measures are used to assess many domains and constructs, including attention, executive functions, inhibitory control, memory, decision-making, categorization, etc. Examples of performance-based measures include the Stroop test, Iowa Gambling Task, and implicit cognition tasks such as the Implicit Association Test. In many performance-based tasks, respondents’ accuracy and reaction times are evaluated when there are competing demands. 24.3.4.2 Cognitive Modeling Cognitive modeling attempts to decompose behavioral performance into different indices that reflect different sub-processes, such as working memory and processing speed. Behavior is complex and is influenced by many different processes, such as cognitive, motivational, and response processes. If you observe that a person shows behavioral deficits, it can be helpful to know what specific process is responsible for the behavioral deficits. It can be difficult to use multiple behavioral tasks to decompose basic processes because any task taps multiple cognitive processes, including likely overlapping processes, and each is affected by measurement error. Cognitive modeling seeks to identify the “hidden processes” that underlie behavioral performance in complex tasks. Cognitive modeling generally requires a lot of data—for example, many trials—but it can be used with small sample sizes. Busemeyer &amp; Stout (2002) provide an example of cognitive modeling of the Iowa Gambling Task to identify the sub-processes that account for behavioral performance deficits in Huntington’s Disease. The Iowa Gambling Task (IGT) attempts to simulate real-life decision-making based on learning from decisions that have different probabilities of rewards and punishments. In the Iowa Gambling Task, participants are presented with four virtual decks on a computer screen. They pick a card each turn, and have to select from one of the four decks. Some decks reward the player more often and some decks penalize the player more often. The player has to figure out which decks to pick from based on statistical probabilities. Busemeyer &amp; Stout (2002) compared and tested competing models for the task based on different theoretical possibilities. With cognitive models, the aim is to achieve a combination of accuracy and parsimony (simplicity). Busemeyer &amp; Stout (2002) selected a model with three parameters: a cognitive process, a motivational process, and a response mechanism. A cognitive process was examined with an updating rate parameter, which describes a person’s memory for past sequences produced by each deck. A motivational process was examined with the attention weight parameter, which describes the amount of attention a person allocated to gains versus losses. A response mechanism, such as recklessness and/or impulsivity, was assessed with threshold parameter, which describes the sensitivity of the choice mechanisms to the expectancies. Busemeyer &amp; Stout (2002) found that people with Huntington’s Disease showed deficits in the updating rate and threshold parameters but not in the attention weight parameter. In particular, the people with Huntington’s Disease were more reactive to recent information and forgot old information more rapidly, and they became less sensitive with training and produced more random behavior. But people with Huntington’s Disease did not show deficits in how much attention they allocated to losses. Other examples of clinically relevant cognitive model include cognitive modeling approaches to better understand cognitive processes in sexual aggression and eating disorders (Treat et al., 2007). 24.3.4.3 Challenges Despite the advantages of not relying on participants’ recall of their thoughts, there are challenges to using cognitive science approaches to assess cognition. 24.3.4.3.1 Reliability of Difference Scores One challenge related to the use of performance-based assessment of cognition is that many such tasks involve difference scores. A difference score involves the subtraction of one score from another score. In performance-based assessments of cognition, many tasks subtract scores, especially accuracy or reactive time, in one condition from scores in another condition. For example, the most-often used dependent variable in the Flanker task, Stroop task, stop-signal task, and dot-probe task is a difference score. For instance, in the Flanker task, the reaction time to incongruent (interference) stimuli is subtracted from the reaction time to congruent stimuli. The problem is that difference scores tend to be lower in reliability than other scores because differences depend on the reliability of both indices in the subtraction, as described in Section 4.5.8. Difference scores tend to be lower in reliability than each of the indices that compose it, especially when the two indexes are correlated. Therefore, scores on these tasks tend to be less reliable than scores on other tasks that do not involve difference scores. To be reliable, difference scores require high reliability of the individual indices compared to the correlation between them. Otherwise said, the more two things are the same thing, the more likely that subtracting one from the other leaves measurement error rather than construct variance. Because of the reliance of difference scores in the dot-probe task, Rodebaugh et al. (2016) challenged whether attention bias toward threat exists or whether it is stable across time for those with anxiety. 24.3.4.3.2 Reliability Paradox Another challenge with performance-based assessments of cognition is known as the reliability paradox (Hedge et al., 2018). Many basic cognitive paradigms, such as the Flanker task, were designed for detecting normative cognitive effects. However, a downside of that is that not all of them are good for assessing individual differences. That is, their scores are not reliable for a particular person. Low reliability precludes making strong inferences and decisions about individuals and calls into question the validity of inferences regarding individual differences, or people’s change, in scores on those tasks. The reliability paradox is that despite some robust cognitive tasks showing well-established experimental effects, many of these tasks do not produce reliable individual differences, including the Flanker task, Stroop task, and stop-signal task, etc. Experimental effects become well-established, and therefore those tasks become widely used, when between-subject variability is low. However, low between-subject variability leads to low reliability of individual differences. The stability coefficient, i.e., test–retest reliability, an index of reliability of individual differences relies on a Pearson correlation. As described in Section 4.5.1.1.1, correlation requires variability: restricted range leads to weaker associations. When a measure has low between-subject variability, it prevents us from detecting consistent rank ordering of people across time. That is, the very reason such tasks produce robust and easily replicable experimental effects due to low between-person variability makes their use as correlational tools problematic. Moreover, the poor test–retest reliability of many performance-based assessments is exacerbated by the use of difference scores. The implications of the reliability paradox are that many well-established approaches in experimental, cognitive, and neuropsychology may not translate well to the study of individual differences, even though the measures can be useful for making group-level inferences. It is important to know the reliability of your measures, especially when dealing with issues of trying to understand where a particular person stands on the construct relative to other people. Unreliability and measurement error are threats to science and knowledge. Unreliability leads to false inferences and failures to replicate findings. As described in Section 5.6, and formalized with the attenuation formula (Equation (5.2)), associations with other variables are weakened to the extent that measurement error exists. If you know the degree of unreliability, you can account for it using the disattenuation formula (Equation (5.3)), to get more accurate estimates of the true associations with other variables. It is important to test the reliability of your measures and report the reliability in papers. Continually work to improve the reliability of your measures. Ways to improve the reliability of measures are described in Section 4.14. Another way to improve reliability of scores is to aggregate multiple measures using a multimethod approach in structural equation modeling. 24.3.5 Cognitive Neuroscience Approaches Another branch of cognitive science approaches includes cognitive neuroscience approaches. For example, cognitive neuroscience approaches connect cognitive processes to measures of brain functioning, such as electroencephalography (EEG) or functional magnetic resonance imaging (fMRI). An example of linking cognitive processes to measures of brain functioning might be examining people’s digit span in relation to fMRI measures. Cognitive neuroscience techniques are discussed in Chapter 20. Cognitive neuroscience is a high priority of the National Institute of Mental Health with their emphasis on the Research Domain Criteria (RDoC). RDoC seeks to assess the underlying substrates of illness across multiple levels of analysis and the neurodevelopmental trajectories rather than just behavioral symptoms, which are heterogeneous. 24.4 Conclusion Multiple aspects of cognition can be assessed, including cognitive products, processes, and structures or organization. The general approaches to assessing cognition include endorsement methods and production methods. Cognitive assessments include self-report, think-aloud approaches, random thought sampling, performance-based measures, cognitive modeling, and cognitive neuroscience approaches. One challenge related to the use of performance-based assessment of cognition is that many such tasks involve difference scores, which tend to be lower in reliability than each of the indices that compose it. Another challenge with performance-based assessments of cognition is known as the reliability paradox that despite some robust cognitive tasks showing well-established experimental effects, many of these tasks do not produce reliable individual differences due to low between-subject variability and the use of difference scores. 24.5 Suggested Readings Dunkley et al. (2019); Treat et al. (2007) References Busemeyer, J. R., &amp; Stout, J. C. (2002). A contribution of cognitive decision models to clinical assessment: Decomposing performance on the Bechara gambling task. Psychological Assessment, 14(3), 253–262. https://doi.org/10.1037/1040-3590.14.3.253 Davison, G. C., Vogel, R. S., &amp; Coffman, S. G. (1997). Think-aloud approaches to cognitive assessment and the articulated thoughts in simulated situations paradigm. Journal of Consulting and Clinical Psychology, 65(6), 950–958. https://doi.org/10.1037/0022-006X.65.6.950 Dunkley, D. M., Segal, Z. V., &amp; Blankstein, K. R. (2019). Cognitive assessment: Issues and methods. In K. S. Dobson &amp; D. J. A. Dozois (Eds.), Handbook of cognitive-behavioral therapies (4th ed., pp. 85–119). Guilford Press. Hedge, C., Powell, G., &amp; Sumner, P. (2018). The reliability paradox: Why robust cognitive tasks do not produce reliable individual differences. Behavior Research Methods, 50(3), 1166–1186. https://doi.org/10.3758/s13428-017-0935-1 Hurlburt, R. T. (1997). Randomly sampling thinking in the natural environment. Journal of Consulting and Clinical Psychology, 65(6), 941–949. https://doi.org/10.1037/0022-006X.65.6.941 Nisbett, R. E., &amp; Wilson, T. D. (1977). Telling more than we can know: Verbal reports on mental processes. Psychological Review, 84(3), 231–259. https://doi.org/10.1037/0033-295x.84.3.231 Rodebaugh, T. L., Scullin, R. B., Langer, J. K., Dixon, D. J., Huppert, J. D., Bernstein, A., Zvielli, A., &amp; Lenze, E. J. (2016). Unreliability as a threat to understanding psychopathology: The cautionary tale of attentional bias. Journal of Abnormal Psychology, 125(6), 840–851. https://doi.org/10.1037/abn0000184 Treat, T. A., McFall, R. M., Viken, R. J., Kruschke, J. K., Nosofsky, R. M., &amp; Wang, S. S. (2007). Clinical cognitive science: Applying quantitative models of cognitive processing to examine cognitive aspects of psychopathology. In R. W. J. Neufeld (Ed.), Advances in clinical cognitive science: Formal modeling of processes and symptoms (pp. 179–205). American Psychological Association. "],["diversity.html", "Chapter 25 Cultural and Individual Diversity 25.1 Terminology 25.2 Assessing Cultural and Individual Diversity: Multicultural Assessment Frameworks 25.3 Assessments with Ethnic, Linguistic, and Culturally Diverse Populations 25.4 Conclusion 25.5 Suggested Readings", " Chapter 25 Cultural and Individual Diversity Like ethics, considerations of cultural and individual diversity are relevant to all of our domains as psychologists, including (but not limited to) research, teaching, assessment, intervention, and supervision. As a result, we have discussed important considerations about diversity throughout the book, including the chapters on validity, evidence-based assessment, and test bias. At the same time, given the importance of cultural and individual diversity, we also provide this section, which is devoted more fully to these issues within the assessment context. Diverse samples provide stronger tests of theories and better generalizability of findings (external validity). On the assessment side, we need measures that provide us with the best information, and that are reliable and valid for all people we use them with. Diversity is a broad concept, and it spans many dimensions of differences among people. 25.1 Terminology 25.1.1 Diversity, Equity, and Inclusion (DEI) 25.1.1.1 Diversity Diversity refers to all aspects of differences between people. Diversity is a characteristic of a group, not a characteristic of an individual. Thus, a person is not diverse. There are many aspects of human difference, including but not limited to race, ethnicity, creed, color, sex, gender, gender identity, sexual orientation, socioeconomic status, language, culture, national origin, religion/spirituality, age, (dis)ability, military/veteran status, personality, political perspective, and associational preferences. Having a diverse group (e.g., of participants, clients, patients, etc.) is insufficient. It is also important to make sure that we take steps for all groups to feel included (inclusion) and to thrive (equity). 25.1.1.2 Equity Equity refers to fair and just practices and policies that ensure everyone can thrive. Equity is different from equality. Equality involves treating everyone equally (i.e., treating everyone the same). However, due to structural inequities—historic and current (e.g., structural racism and patriarchy)—some people are disadvantaged and marginalized more so than others. Structural racism refers to “the normalization and legitimization of an array of dynamics—historical, cultural, institutional, and interpersonal—that routinely advantage non-Latinx White persons while producing cumulative and chronic adverse outcomes for Black and other people with minoritized identities in housing, education, employment, health care, criminal justice, and psychology.” (Byrd et al., 2021, p. 279). Patriarchal societies reflect society’s long history of wage gaps, gender discrimination, violence against women, etc. Treating people equally (equality) does not allow everyone to thrive given these different experiences and unequal access to opportunities. Thus, equity seeks to help everyone thrive, including those who are marginalized with fewer access to opportunities. 25.1.1.3 Inclusion Inclusion refers to providing a community where all members are and feel respected, have a sense of belonging, and are able to participate and achieve their potential. 25.1.2 Aspects of Difference There are many aspects of difference. However, identifying labels to capture these differences can be challenging. “Asian” is an overly broad term. There are more than 4 billion people from Asian countries, and considerable heterogeneity. “Hispanic” is based on language (i.e., a person from a Spanish speaking country), whereas “Latino/a/x” and “African American” are based on geography (i.e., a person from a Latin American country). Traditional classifications of race/ethnicity by the National Institutes of Health miss groups that may be important to distinguish, including Arab, Middle Eastern, or North African (AMENA) populations. Labels by the 2020 U.S. Census attempt to be more precise than traditional classifications of race and ethnicity by the National Institutes of Health. For the classification of “Hispanic, Latino, or Spanish origin” in the 2020 U.S. Census, see Figure 25.1. Notice how the question distinguishes between identities of people from (or whose ancestors were from) various countries, including Mexico, Puerto Rico, Cuba, etc. Figure 25.1: Question Asking About One’s Hispanic Origin in the 2020 U.S. Census. (Figure reprinted from the American Community Survey (2020): https://censusreporter.org/topics/race-hispanic/ [archived at https://perma.cc/LRW6-4JAJ]) For the classification of “Race” in the 2020 U.S. Census, see Figure 25.2. Notice how the question distinguishes between identities of people from (or whose ancestors were from) various countries, including India, China, Philippines, etc. Figure 25.2: Question Asking About One’s Race in the 2020 U.S. Census. (Figure reprinted from the American Community Survey (2020): https://censusreporter.org/topics/race-hispanic/ [archived at https://perma.cc/LRW6-4JAJ]) Society treats people differently based on factors that they are born into and are outside of their control, including their race and their family’s socioeconomic status. Socioeconomic status refers to the social standing or class of an individual, and it is often assessed as a combination of education, income, occupation, access to resources, privilege, power, and control (Suzuki et al., 2013). There is also growing attention to neighborhood- and community-level factors associated with deprivation. There are also many other important aspects of difference to assess when thinking about individual differences in behavior. For instance, one important aspect of difference is family structure. Family structure includes lots of components, such as whether the parent is a single parent or whether there are two parents, whether the parents are working parents or whether the family has a stay-at-home parent, and whether there are other family members (e.g., grandparents) in the household. Another important aspect of diversity includes one’s sex, gender identity, and gender expression. Sex is the person’s gender that was assigned at birth, and includes male, female, and intersex. Gender identity is how one self-identifies in terms of their gender. Gender identity is a spectrum in terms of the degree to which a person identifies as a female/woman/girl, a male/man/boy, or other genders, in terms of personality, likes/dislikes, jobs/hobbies, and roles and expectations. The person’s gender identity may be non-binary or genderqueer if they do not identify as a man or woman (e.g., genderfluid). When the person’s sex assigned at birth and their gender identity are aligned, the person is cis-gender. When the person’s sex assigned at birth and their gender identity are not aligned, the person is transgender. Gender expression deals with how the person’s gender is expressed to others, including their appearance. Another dimension of diversity is whether someone is from a rural versus urban area. People from rural areas may have less access to resources. Another aspect of diversity is refugee or immigrant status. Refugee and immigrant groups may have less access to resources and may have language barriers, in addition to difficulty finding work, and fear of deportation. It is also important to consider diversity in terms of sexual orientation and sexual behaviors. There is a distinction between one’s sexual orientation (e.g., lesbian, gay, bisexual, heterosexual, pansexual, asexual)—that is, who one is sexually or romantically attracted to—versus whom one has sex with (i.e., sexual behavior). There are also differences in terms of the degree of people’s sexual risk taking and sexual aggression. Ability and disability are other important facets of diversity, including learning disabilities. Additionally, there are important brain-related differences that lead people to interact and experience the world in different ways, as reflected in the term, neurodiversity. Neurodiversity can include differences related to autism and other conditions such as attention-deficit hyperactivity disorder. Age is another important dimension of diversity. Consider the issue of heterotypic continuity. For example, externalizing problems look different at different ages and should be assessed in different ways with different measures at different ages (F. R. Chen &amp; Jaffee, 2015; J. L. Miller et al., 2009; Moffitt, 1993; Patterson, 1993; Petersen et al., 2015; Petersen &amp; LeBeau, 2022; Wakschlag et al., 2010). Another important aspect of diversity is culture, which includes belief systems, value orientations, psychological processes, worldview, learned and transmitted beliefs, and practices (Suzuki et al., 2013). A person’s culture has many facets, including but not limited to experiences related to geographic boundaries, language, religious belief, social class, gender, sexual orientation, and ability status (Suzuki et al., 2013). However, culture and one’s related identities are dynamic and changing (Suzuki et al., 2013), which makes them challenging to assess. Culture tends to be ignored in psychological research relative to other facets of diversity, but culture is arguably the most important in terms of influencing behavior. Additional dimensions of difference include religion and spirituality. Another aspect of diversity is intersectionality, the intersecting of multiple identities. Intersectionality deals with how the combination of various identities can combine to influence behavior and bias. There are many other important aspects of difference, too. This was not an exhaustive list! 25.2 Assessing Cultural and Individual Diversity: Multicultural Assessment Frameworks A review of frameworks for multicultural assessment is provided by L. M. Edwards et al. (2017). 25.2.1 The ADDRESSING Model The ADDRESSING model (Hays, 2016) provides a framework to consider some of the commonly examined differences between people within a clinical context. ADDRESSING is an acronym that stands for: Age and generational differences Developmental or other Disability Religion and spiritual orientation Ethnic and racial identity Socioeconomic status Sexual orientation Indigenous heritage National origin Gender These are important aspects of differences between people that can be helpful to understand when working with a client and how they self-identify. For instance, the therapist could ask the client how they self-identify in each of these domains and how their identity is meaningful to them. In addition to the aspects of diversity identified in ADDRESSING, there are other aspects of diversity and difference that may be worth considering, including but not limited to personality, culture, and political beliefs. 25.2.2 DSM-5 Cultural Formulation Interview The Diagnostic and Statistical Manual of Mental Disorders (DSM) provides a framework for clinicians to organize cultural information about a client using the Cultural Formulation Interview (Lewis-Fernández et al., 2014). The Cultural Formulation Interview is a semi-structured interview and is available here: https://www.psychiatry.org/File%20Library/Psychiatrists/Practice/DSM/APA_DSM5_Cultural-Formulation-Interview.pdf (archived at https://perma.cc/3EWR-LGEN) 25.2.3 Multicultural Assessment Procedure The Multicultural Assessment Procedure was developed by Ridley et al. (1998). The framework includes four phases: Gather clinical data, including cultural data, through history taking and multiple methods for the purpose of formulating a case conceptualization Interpret clinical and cultural data to formulate a hypothesis, while keeping in mind to: differentiate the cultural data as being either idiosyncratic (i.e., unique to the individual client and would not necessarily be expected of other members of the client’s culture) or culture-specific consider base rates differentiate dispositional from environmental stressors differentiate clinically significant data from data that are not clinically significant Incorporate cultural data with other clinical information to test the hypotheses by ruling out medical explanations and using appropriate assessments and testing Arrive at a sound assessment decision (case conceptualization) The procedure also includes debiasing strategies to minimize the likelihood of clinical judgment errors. Ridley et al. (2001) expanded on this framework in consideration of ethical issues in multicultural assessment. 25.2.4 Multicultural Assessment–Intervention Process The Multicultural Assessment–Intervention Process was developed by R. H. Dana (1998). The process provides a flowchart for clinicians to conduct assessment by asking relevant questions about cultural orientation, type of assessment instrument (e.g., etic or emic), cultural formulation for diagnosis, and intervention approach (e.g., universal, culture-general, culture-specific, identity-specific, etc.). Etic assessments are culture-general, whereas emic assessments are culture-specific. 25.3 Assessments with Ethnic, Linguistic, and Culturally Diverse Populations 25.3.1 Methodological and Conceptual Challenges in Research Methodological and conceptual challenges for conducting research with ethnic minorities are described by Okazaki &amp; Sue (1995). Guidelines for conducting research with diverse groups are described by Burlew et al. (2019). 25.3.1.1 Use of Terms: Race, Ethnicity, Culture Use of terminology with respect to race, ethnicity, and culture is a key challenge in assessment of diverse samples. Many people refer to race, ethnicity, and culture interchangeably, which reflects conceptual confusion. There is no definition of race, ethnicity, and culture that is universally agreed on (Okazaki &amp; Sue, 1995). As described by Okazaki &amp; Sue (1995), “the use of the term ‘race’ appears to imply biological factors, as races are typically defined by observable physiognomic features such as skin color, hair type and color, eye color, stature, facial features, and so forth” (p. 367). By contrast, ethnicity (ethnic status) has been defined as “an easily identifiable characteristic that implies a common cultural history with others possessing the same characteristic” (Eaton, 1980, p. 160). Ethnic identifiers for evaluating whether people share a “common cultural history” include racial, national, tribal, religious, linguistic, or cultural origin or background. Thus, race and ethnicity, though conceptually distinct, are related as race can be an ethnic identifier. Researchers have argued that race designations are arbitrary (Helms et al., 2005), that race is a social, cultural, and/or political construct rather than a biological construct (Sternberg et al., 2005; Yudell et al., 2016), that race has no genetic basis (Yudell et al., 2016), that within-race differences are greater than between-race differences (Zuckerman, 1990), and that racial categories lack conceptual meaning (Helms et al., 2005). According to the American Anthropological Association (1998, p. 712), “physical variations in the human species have no meaning except the social ones that humans put on them”. However, even though race does not appear to have a genetic or biological basis and therefore appears to be a social construct rather than a biological construct, racism as a social problem is real and is a major issue (Smedley &amp; Smedley, 2005). People are born into different social reactions based on their race. Thus, race is important to consider from a perspective of racialized experiences and discrimination even if the racial categories do not represent underlying biological differences. In terms of determining what to assess and analyze, it is important for the researcher to determine whether they are interested in evaluating race as a biological variable, ethnicity as a demographic variable, or an aspect of cultural experience as a psychological variable (Okazaki &amp; Sue, 1995). Researchers often use race and/or ethnicity as proxy variables for psychological variables such as cultural values, self-concept, minority status, etc. However, race and ethnicity are distal to the variables that may be of most psychological interest. Some people identify their primary culture as being different from their self-identified ethnicity. It is thus important to assess the psychological processes of interest, rather than merely relying on racial or ethnic categories as proxies. Culture refers to a psychological variable: i.e., the social context. Researchers often group people together based on race or ethnicity with the implicit assumption that they share some cultural experience. According to Okazaki and Sue (1995), assumptions underlying the use of race or ethnicity in a study should be made explicit. Research should directly assess the underlying psychological variables associated with culture, such as different spiritual practices, that are hypothesized to produce the racial/ethnic/cultural differences. That is, it is generally best not to rely solely on race or ethnicity, which are imprecise, to classify people unless the goal is to classify people based on racialized reactions and experiences. Thus, the researcher can classify people according to race/ethnicity, but can also examine the more proximal cultural variables that are thought to drive group-related differences. 25.3.1.2 Whether to Examine Individual Differences or Group Characteristics One question the researcher should consider is whether they are interested in understanding individual differences or group characteristics. Both can be valuable and important questions to examine. However, researchers should not under-estimate within-group heterogeneity (Okazaki &amp; Sue, 1995). The greater the within-group heterogeneity, the less accurate predictions tend to be. Moreover, inferences at the group-level cannot necessarily be applied to the individual level; the confounding of an individual with the individual’s culture leads to stereotyping (Okazaki &amp; Sue, 1995), which should be avoided. 25.3.1.3 Selecting Participants One question is which groups to include in the research design and in what proportion. For group-related comparisons, the researcher would need a large enough sample size for every group examined to have adequate power to identify group-related differences. If the researcher is interested in studying a particular group, a question is whether to use a comparison group. A comparison group can provide a basis of comparison to better interpret the findings in the group of interest. However, inclusion of a comparison group can add time, cost, and as a result may lead to achieving a smaller sample in the group of interest. Moreover, the group comparison approach has been criticized for reinforcing racial stereotypes, reinforcing Whites as the standard group and non-White behavior as deviant, and overlooking within-group variation (Okazaki &amp; Sue, 1995). Thus, decisions about which groups to include, in what proportion, and whether to include a comparison group should be guided by the questions and purpose of the study. Practical considerations can come into play, and having a comparison group should not be considered the necessary default for all studies. Moreover, White groups should not be considered the default. 25.3.1.4 Ensuring Fair Group Comparisons Another issue is how to achieve fair group comparisons. One approach is to match groups by selecting participants a priori to be similar on relevant, secondary characteristics such as demographic characteristics (e.g., age, sex, socioeconomic status) or other abilities (e.g., intelligence). The goal of matching is for participants to be as similar as possible in the relevant characteristics apart from their group classification. However, there are challenges to matching groups on all relevant characteristics. Another approach is to control for these secondary characteristics post hoc in the statistical analysis. Which variables to include as control variables is an important question that depends on the goal. A key question is which variables to match participants on. However, there is not an agreed-upon list of matching variables. Generally, researchers recommend controlling for social and demographic characteristics such as educational attainment, income level, and language fluency, when group-related differences exist on those variables and the researcher believes that such differences may moderate the associations of interest (Okazaki &amp; Sue, 1995). When comparing Black and White participants, it is important to control for group differences in socioeconomic status, especially in the U.S. In addition to main effects of group status, it may also be important to consider interaction effects of group status, consistent with an intersectionality approach. However, power to detect interactions tends to be weaker than power to detect main effects, both because of smaller effect sizes (archived at https://perma.cc/E3KA-SGB7) and because of smaller sample sizes of the intersecting groups. 25.3.1.5 Sampling Researchers face important questions with how to identify, sample, and recruit ethnic minority samples. Groups, such as ethnic groups, may be classified at a broad level (e.g., Latinx or Hispanic) or at a more specific level (e.g., Puerto Rican, Mexican American, etc.); however, these approaches do not need to be mutually exclusive. Researchers must also determine how to classify people of mixed racial or ethnic backgrounds. It is also important to note that it is an incorrect assumption that once people are identified as belonging to a particular ethnic-cultural group, that they share a common understanding of their own ethnicity or culture and identify with the ethnic-cultural group (Okazaki &amp; Sue, 1995). There are challenges of recruiting a sizeable sample size with specific ethnic-cultural groups (e.g., Japanese Americans). This is partly a challenge of small overall population size (e.g., American Indians). This is also partly a challenge of difficult-to-reach populations who are distrustful of science and researchers. Research has exploited under-privileged populations (e.g., the Tuskegee syphilis study in Black men), which has led to changes in the consent process to ensure that participation is voluntary. There may also be selection effects, such that the people who are willing to participate in research may differ in important ways from those less willing to participate in research, which is a challenge to generalizability. To address sample size challenges, researchers often combine data from multiple ethnic-cultural groups with some common origin. For instance, researchers might combine Chinese Americans, Japanese Americans, and Korean Americans into one group. Or, researchers might combine people from multiple tribal groups among American Indians. However, broadening the ethnic grouping increases the heterogeneity of the groups, so the researcher must decide which are the sources of variability that can and cannot be overlooked (Okazaki &amp; Sue, 1995). Much of the literature in general and on diversity, in particular, is from research on college students. There are limitations of conducting research on college students. College students are not representative of the broader population. Research using college students under-estimates the population heterogeneity in terms of demographic and psychosocial diversity. One goal of research is to learn findings that apply to the broader population, with the goal of having a representative sample, where the mean of the sample is approximately the same as the mean of the population. However, having a population-representative sample is not as useful for comparing differences between groups. To do this, we would need large samples of each group—e.g., equally weighted samples. Most research is on participants from WEIRD countries (Henrich et al., 2010): Western, educated, industrialized, rich, and democratic (WEIRD) societies. This further limits the potential generalizability of findings to different cultures. Provide a thorough description of the sample and sampling methodology used. For instance, it can be helpful to describe participants on additional dimensions to race and ethnicity, including generational status, acculturation, self-identification, ethnic and cultural composition of the neighborhoods or communities, etc. (Okazaki &amp; Sue, 1995). Acculturation deals with the change that individuals undergo due to contact with members of different cultures, and it includes strategies such as assimilation, separation, marginalization, and integration (Suzuki et al., 2013). 25.3.1.6 Establishing Equivalence of Measures Across Groups Five key areas need to be empirically evaluated in using or adapting instruments across cultures or groups to reduce or eliminate cultural bias (Hunsley &amp; Mash, 2007): Conceptual equivalence Linguistic (or translation) equivalence Psychological equivalence of the items Functional equivalence Metric and scalar equivalence (i.e., measurement invariance) Conceptual equivalence refers to establishing that the construct has the same meaning across groups. Linguistic (or translation) equivalence refers to the measure having the same linguistic meaning in each language to which it has been translated. Best practices for establishing translation equivalence include having experts translate and back-translate the measure, comparing both the original and back-translated versions, and revising the translation accordingly (Okazaki &amp; Sue, 1995). Psychological equivalence of the items deals with establishing that the psychological effect each item has for each group or in the different versions. Functional equivalence deals with establishing similar predictive and concurrent criterion-related validity to prevent test bias (i.e., similar regression intercepts and slopes between measure and criterion). Functional equivalence can be difficult to establish when both the test and the external criteria of interest are both rooted in the same dominant universalist cultural epistemology or worldview; instead, it may be advantageous to validate tests against external criteria that are grounded in the nondominant group’s culture (Suzuki et al., 2013). Metric and scalar equivalence (i.e., measurement invariance) deal with establishing that the construct is measured on the same metric across groups, that is, a particular score reflects the same level on the construct for people in each group. Metric equivalence/invariance deals with establishing that items’ discrimination parameters (or factor loadings) are the same across groups. Scalar equivalence/invariance deals with establishing that both items’ discrimination parameters (or factor loadings) and difficulty parameters (or intercepts) are the same across groups. 25.3.1.7 Methods of Assessment It is debatable whether some assessment methods may be more likely to result in cultural or ethnic bias compared to other assessment methods (Okazaki &amp; Sue, 1995). Certain types of assessment techniques might be more likely to bias or obscure differences between cultural groups. This especially depends on who is doing the observing and their own beliefs. There may be selection biases regarding the types of people who are willing to participate in in-depth studies that involve observational or psychophysiological assessment. It may be worth considering supplementing quantitative measurement approaches with qualitative measurement approaches, such as interviews and life histories, which may be better able to capture cultural factors. In general, it is advantageous to assess people using multiple measures and methods to establish convergent validity of cultural constructs (Okazaki &amp; Sue, 1995). 25.3.1.8 Interpretation of Data Another challenge deals with how to interpret data from group comparison designs. Historically, group differences in scores were most commonly interpreted from a deficit-based narrative rather than a strengths-based narrative (Byrd et al., 2021). 25.3.2 APA Guidelines There are many disparities in psychological services for under-served populations, including health disparities, diagnostic disparities, and disparities in the use and availability of psychological services (Rivera Mindt et al., 2010). To help address these disparities, the American Psychological Association (APA) Office of Ethnic Minority Affairs published guidelines for providers of psychological services to ethnic, linguistic, and culturally diverse populations (American Psychological Association Office of Ethnic Minority Affairs, 1993). Their guidelines included suggestions such as: Whenever possible, provide information in writing along with oral explanations. Whenever possible, provide the information and interact in the language that is understandable to the client. If that is not feasible, make an appropriate referral. If this is not possible, provide a translator with cultural knowledge and an appropriate professional background. When no translator is available, then use a trained paraprofessional from the client’s culture as a translator. If you do not possess knowledge and training about an ethnic group, seek consultation with, and/or make referrals to, appropriate experts as necessary. Consider the validity of a given instrument or procedure and interpret the resulting data, while keeping in mind the cultural and linguistic characteristics of the client. Be aware of the test’s reference population and possible limitations of using the instrument with other populations. Seek to help clients determine whether a “problem” stems from racism or bias in others so that the client does not inappropriately personalize problems. The APA has also published guidelines for working with particular groups, including: Groups based on race and ethnicity (archived at https://perma.cc/4LM9-X6JN) People with low income (archived at https://perma.cc/PZQ5-8YFG) People with disabilities (archived at https://perma.cc/5YZ8-FNJT; PDF: https://perma.cc/KYY4-CQFQ) People who are transgender or gender nonconforming (archived at https://perma.cc/E9VH-QG3W) Sexual minorities (archived at https://perma.cc/4BJD-QZCF) Girls and women (archived at https://perma.cc/AYA4-KB5W) Boys and men (archived at https://perma.cc/6HWK-W9R6) Older adults (archived at https://perma.cc/EQ95-Y78Y) 25.3.3 Seek Training and Consultation Research has shown that relatively few practitioners receive extensive training in multicultural assessment and few use frameworks for multicultural assessment (Brickman et al., 2006; L. M. Edwards et al., 2017). Researchers have encouraged increasing multicultural education, training, awareness, and knowledge, increasing multicultural psychological research, and increasing the provision of culturally competent psychological services to ethnic minorities (Rivera Mindt et al., 2010). It can also be worthwhile to consult with informal support systems, including churches, ethnic clubs, family associations, and community leaders (Leong &amp; Kalibatseva, 2016). 25.3.4 Develop Awareness of Cultural Differences One goal should be to develop cultural competence with the groups, communities, and individuals with which one works or interacts. It can be helpful to be aware of cultural differences, including cultural differences in communication style and use of language (Leong &amp; Kalibatseva, 2016). 25.3.5 Show Cultural Humility Another goal should be to show cultural humility. Tervalon &amp; Murray-Garcia (1998) describe three key aspects of cultural humility: (1) a life-long commitment to self-evaluation and self-critique, (2) fixing power imbalances, and (3) developing mutually beneficial and nonpaternalistic partnerships with communities who advocate for others. For instance, it is important to be aware of one’s biases and stereotypical beliefs. Cultural humility involves treating the client as the expert on their experience. Therefore, cultural humility involves collaboration between the clinician and client, because each brings important knowledge. The client brings their knowledge of their situation, and their preferences. The clinician brings their scientific knowledge about strengths and difficulties, and about behavior change. 25.3.6 Do Not Assume or Stereotype It is important to consider cultural factors; at the same time, groups are not monolithic and every person is unique. Do not assume about someone based on their demographics, which is stereotyping. Just because a person comes from a particular group does not mean the person shares all beliefs, behaviors, etc. with the group (Okazaki &amp; Sue, 1995). Thus, it is important to be aware of within-group differences, as well (Leong &amp; Kalibatseva, 2016; Okazaki &amp; Sue, 1995), which are often as large or larger than between-group differences (Zuckerman, 1990). Show genuine interest and curiosity to get to know a person as an individual. 25.3.7 Ensure Adequate Representation of Under-Represented Groups in Research There are important issues of generalizability of research findings. Participant samples historically have been largely White and middle or upper middle class. This reduces the potential generalizability of findings, and limits effectiveness of treatments to under-represented populations, many of whom have experienced discrimination and social disadvantage. Many groups, including people who are Black or Latinx, and gender and sexual minorities have not been adequately represented in research studies. As a result, the knowledge gained from unrepresentative studies may not generalize to other under-represented groups. Science plays an important role in advancing knowledge that can lead to the development of new treatments. However, groups who are under-represented in research are less likely to benefit from the new treatments because the knowledge upon which the treatments were developed and tested may not generalize to the under-represented groups. Moreover, the lack of inclusion of under-represented groups in studies leads to inappropriate normative data. Thus, it is crucial to have better representation of historically marginalized and minoritized groups in science, including people who are Black or Latinx, and gender and sexual minorities. People from Finland and Sweden are often studied to examine health-related questions. Because they have a socialized health care system, the government keeps medical records on all citizens, so researchers can examine millions of participants, and can study rare conditions and behaviors with a low base rate, like suicide (Lysell et al., 2018). However, there are questions of generalizability from the Scandinavian population and health care system to those of other countries, including the United States. One could hypothetically use propensity score matching for matching their sample to other populations, but one cannot really do that because they have too many empty cells for propensity score matching due to too few people of various under-represented racial/ethnic groups. 25.3.8 Use Appropriate Measurement 25.3.8.1 Do Not Use Offensive Content or Procedures It is important to use stimuli that are valid for the populations being tested. It is important to avoid using content or procedures that are culturally or racially insensitive, or that are racist, sexist, heteronormative, and/or ableist (Byrd et al., 2021). When developing assessments, it may be important to evaluate stimuli in focus groups to ensure they are not offensive. One of the most widely used standardized assessments in psychological assessment is the Boston Naming Test, a measure of visual confrontation naming that is used to assess cognitive impairment, aphasia, and dementia. The Boston Naming Test includes a noose as one of the items. Historically, lynchings with a noose were used in the United States as a form of racial terror against Black people. Thus, the use of a noose as an item in the Boston Naming Test is racially insensitive and inappropriate for use because it is deeply offensive and has a harmful nature. It can have a strong negative impact on Black respondents in terms of distress, performance disparities, and health disparities (Byrd et al., 2021). Culturally offensive stimuli can lead to stereotype threat and compromise the validity of the assessment. Stereotype threat occurs when people are or feel at risk of conforming themselves to stereotypes about their social group, thus leading them to show poorer performance in ways that are consistent with the stereotype. Moreover, the use of offensive procedures likely contributes to greater distrust of the scientific and medical fields among under-served communities, which acts as a barrier to health care and to health disparities. Thus, removing and replacing offensive stimuli may be necessary from a moral and ethical perspective to do no harm and to ensure valid assessment, even if it risks modifying the standardized procedure from which the norms were developed (Byrd et al., 2021). When deviating from standardized procedures, it is important to describe such deviations in resulting papers or reports. 25.3.8.2 Use Culturally Valid Measures Most measures originated from Western societies (Suzuki et al., 2013) and have been validated only among non-Latinx, English-speaking Whites (Manly, 2005). It is important to use measures that have been validated for the populations with whom they will be used. Many measures have been used for different populations than the populations with which the instruments were developed and validated, such as with immigrants, refugees, or when exporting measures to different countries or cultural contexts. Exporting measures for use with people from different cultures assumes universality (i.e., cross-cultural validity). More work should examine adaptation and validation of measures for other cultures, countries, languages, etc. (Leong &amp; Kalibatseva, 2016). However, some measures may not be able to be applied validly across cultures. Using valid measures for the target population may in some cases mean using culturally specific (etic) instruments rather than universal (emic) instruments. If using measures whose validity and measurement equivalence has not been established, it is important to interpret results with caution and to provide disclaimers in any resulting papers or reports (Leong &amp; Kalibatseva, 2016). As described in Section 5.3.1.16, it is also important for measures to have cultural validity, which refers to “the effectiveness of a measure or the accuracy of a clinical diagnosis to address the existence and importance of essential cultural factors.” (Leong &amp; Kalibatseva, 2016, p. 58). Essential cultural factors may include values, beliefs, experiences, communication patterns, and approaches to knowledge (epistemologies). Threats to cultural validity of assessments includes: pathoplasticity of psychological disorders, bias in clinical judgment, language capability of the client, biased measures, or inappropriate use or interpretation of measures (Leong &amp; Kalibatseva, 2016). 25.3.8.3 Use Linguistically Appropriate Assessment It is important to use assessment approaches that are linguistically appropriate for the examinee (Leong &amp; Kalibatseva, 2016). First, the measure should be in a language that is understood by the examinee. Second, the measure should be consistent with the examinee’s language ability. Sometimes interpreters or translators may be necessary. However, use of interpreters has been shown to lead to under-estimations of patients’ emotion suffering and despair (Leong &amp; Kalibatseva, 2016). In addition, clients and therapists can attach different meanings to the same words. It is thus important not to assume that you know what the client means by various terms and not to assume that the client knows what you mean by various terms. 25.3.8.4 Use Follow-Up As Necessary As described by Suzuki et al. (2013), it may be necessary to use follow-up procedures to clarify people’s responses to items or to test the limits by readministering items with additional supports (e.g., modifying instructions to involve more or fewer cues, adjusting the pace of information presented, adjusting memory demands, changing the response format) (Sattler &amp; Hoge, 2006). When deviating from standardized procedures, it is important to describe such deviations in resulting papers or reports. 25.3.8.5 Examine and Correct for Test Bias As described in Chapter 16, there are two types of test bias: predictive test bias and test structure bias. It is important to evaluate the possibility of both when using assessments across groups. It is important to establish measurement invariance across groups to ensure that scores have the same meaning for each group (Burlew et al., 2019). If measures are non-invariant, the means (i.e., level) and or associations between constructs could appear to (artifactually) differ across groups, when in reality they do not (F. F. Chen, 2008). Measurement invariance can be examined in terms of item intercepts, item factor loadings, and item residuals. Establishing measurement invariance of just intercepts or factor loadings (or vice versa) is not sufficient to establish measurement invariance because tests can show strong bias in one dimension (e.g., intercepts) even if the test does not show bias in other dimensions (e.g., factor loadings) (Wicherts &amp; Dolan, 2010). Approaches for evaluating measurement invariance across groups are described by Han et al. (2019). To help minimize the potential for test bias, reseachers are encouraged to develop the scales simultaneously across the different groups or cultures (Fernández &amp; Abe, 2018). It is also important to determine whether the measure’s scores show similar relations to meaningful external criteria (Burlew et al., 2019). Fernández &amp; Abe (2018) provide strategies to address cross-cultural bias, such as excluding words or concepts that are specific to one language or culture. Use of North American neuropsychological tests in other countries has shown to lead to considerable false positives, and the extent of misdiagnosis differs by country (Daugherty et al., 2017). It is important to examine factors that moderate the validity of assessment, e.g., groups for whom the assessment is biased. Ways to investigate and detect bias are described in Section 16.2. The number of potential moderating variables is so large, that it is not realistic to develop an assessment base that encompasses all of these factors and their interactions. Evidence should be used regarding which factors seem to matter for a given assessment purpose—that is, the factors that have a medium-to-large effect size. It is valuable to examine the effect size of moderators; if the effects are small, it might not be sufficient to scrap the instrument for the population. Gender and cultural differences have shown a number of statistically significant effects for a number of different assessment purposes, but many of the observed effects are quite small and likely trivial, and they do not present compelling reasons to change the assessment (Youngstrom &amp; Van Meter, 2016). Nevertheless, if you do find evidence of bias, it is important to correct for it. Ways to correct for bias using score adjustment and other approaches are discussed in Section 16.5. 25.3.9 Consider Pathoplasticity of Constructs Constructs can manifest differently across cultures. Pathoplasticity of mental disorders refers to the variability in symptoms, course, outcome, and distribution of mental disorders among various cultural groups (Leong &amp; Kalibatseva, 2016). Some syndromes are considered culture-bound—that is, they exist in some cultures but not in other cultures. Hwabyung is a Korean culture-bound syndrome in which suppressed negative emotions manifest themselves in bodily ways such as chest pains and difficulty breathing. In addition, features associated with a disorder, including the content, severity, or frequency of symptoms, may vary across cultures. For instance, Asian Americans tend to report a greater degree of psychological disturbance than Whites, which could be due to several reasons (Leong &amp; Kalibatseva, 2016). It is possible that Asian Americans (a) show higher rates of mental health problems than other groups, (b) under-utilize mental health services, and/or (c) are more likely than other groups to be misdiagnosed due to miscommunication, use of invalid instruments for Asian Americans, and/or lack of cultural knowledge on the part of therapists. In addition, how clients act in a diagnostic interview may depend on cultural factors, and it thus may be important to sample the clients more broadly than a diagnostic interview (Leong &amp; Kalibatseva, 2016). For a case example of pathoplasticity, there is a disorder among the traditional Navajo called “Moth Madness” (American Psychological Association Office of Ethnic Minority Affairs, 1993). Symptoms of Moth Madness include seizure-like behaviors. This disorder is believed by the Navajo to be the supernatural result of incestuous thoughts or behaviors. Both differential diagnosis and intervention should take into consideration the traditional values of Moth Madness. 25.3.10 Formulate a Culturally Informed Case Conceptualization It can be helpful to discuss your diagnosis and case conceptualization with the client. For instance, you may learn that some behaviors are normative in their culture and may not be distressing or concerning to them. It can thus be helpful to consider client preferences in terms of what to address and how to address it, while adhering to evidence-based approaches. 25.3.11 Bias in Clinical Judgment As reviewed by Garb (1997), there is evidence of race bias, social class bias, and gender bias in clinical judgment. Race bias, social class bias, and gender bias in clinical judgment occurs when the accuracy of judgments varies as a function of client race, social class, or gender. That is, clinical judgment bias occurs when the accuracy of judgments varies as function of other variables (e.g., race, social class, gender). Bias is not when the judgments themselves vary as a function of other factors, but when the accuracy of those judgments varies. Examining how judgments differ across groups does not control for differences in clinical severity between the groups. If a clinician rates more people in one group as having a disorder than another group, this is not necessarily evidence of bias because the true prevalence of the condition could differ across groups. Thus, it is important to compare groups with similar levels of symptomatology or to control for level of symptomatology. If clinical judgments are more accurate for White clients than Black clients, we would say that race bias exists. Accuracy of judgments could vary by the client’s race, social class, or gender for several reasons (Garb, 1997). One way in which judgments could be biased is if the diagnostic criteria are biased—for instance, if the diagnoses are more valid for one group (e.g., White clients) than for another group (e.g., Black clients). Another reason that accuracy of judgments could vary as a function of client race, social class, or gender is if the judgments are made based on a biased instrument. Another way that accuracy of judgments could vary as a function of client race, social class, or gender is related to confirmation bias or biases introduced during the usage of a measure. This could occur, for instance if clinicians do not consider alternative hypotheses, and based on their hypotheses, they choose the data to collect that support their hypotheses rather than collecting data that could refute their hypotheses. Bias could also occur in the way that data are integrated from multiple measures or methods. Another form of bias could arise from how test results are used, in terms of inequitable social consequences (Suzuki et al., 2013). Many studies do not find bias in clinical judgments as a function of race, social class, or gender, but some studies do find evidence of bias (Garb, 1997). 25.3.11.1 Race Bias One pattern of race bias is that Black and Latinx patients with a psychotic affective disorder are more likely than White patients to be misdiagnosed as having schizophrenia (Garb, 1997). That is, Black and Latinx patients are less likely than White patients to be diagnosed with psychotic affective disorder (with the same symptoms) and are more likely to be diagnosed as having schizophrenia, even when the measures do not indicate that a diagnosis of schizophrenia is justified. Another pattern of race bias is that the risk of violence is over-estimated for Black patients but not for White patients (Garb, 1997). In addition, cases of child abuse are more likely to be reported when children are White than when they are Black (even when comparing similar behavior) (Garb, 1997). Race bias has also been observed with respect to antipsychotic medication. Antipsychotic medications are more often prescribed for Black patients than for other patients even for similar levels of psychosis (Garb, 1997). Moreover, affective symptoms in patients who are severely ill are more often under-treated for Black or Latinx patients than for White patients (Garb, 1997). Another potential pattern of race bias is that, compared to non-Hispanic White children, racial/ethnic minorities appear to be more likely to be diagnosed with a disruptive behavior disorder rather than attention-deficit/hyperactivity disorder (ADHD) (Fadus et al., 2020); however, it is not yet known whether this holds when comparing individuals with the same symptom presentation. 25.3.11.2 Social Class Bias One pattern of social class bias is that child abuse is more likely to be reported if a child is from a lower-class background rather than a middle- or upper-class background (even when comparing similar behavior) (Garb, 1997). In addition, middle-class children are more likely to be referred to special education programs compared to lower-class children (Garb, 1997). There is also social class bias in clinicians’ decisions about psychotherapy. Referrals for psychotherapy are more often made for middle-class clients than for lower-class clients (Garb, 1997). Clinicians are more likely to recommend psychotherapy and expect a client to do well in psychotherapy when the client is from a middle- or upper-class background compared to when the client is from a lower-class background, leading to a tendency for clinicians to refer clients to different types of treatments based on the client’s social class (Garb, 1997). Specifically, lower-class clients are more likely to be referred for supportive psychotherapy, whereas middle-class clients are more likely to be referred for insight-oriented psychotherapy. 25.3.11.3 Gender Bias One pattern of gender bias is that women are more likely than men to be diagnosed with a histrionic personality disorder, whereas men are more likely than women to be diagnosed with antisocial personality disorder, even when female and male clients do not differ in symptomatology (Garb, 1997). Another pattern is that clinicians tend to over-estimate likelihood of violent behavior for male clients and under-estimate likelihood of violent behavior for female clients (Garb, 1997; McNiel &amp; Binder, 1995). Although males tend to show more violence than females, studies have shown that false positives tend to be more common for male patients than female patients, whereas false negatives tend to be more common for female patients than male patients (McNiel &amp; Binder, 1995). In terms of autism, some conceptualizations of autism consider it to be a manifestation of “extreme male brain” (Baron-Cohen, 2002, 2010; Greenberg et al., 2018). Girls diagnosed with autism tend to be more severely affected than boys diagnosed with autism. People are less likely to notice autism in girls. This could possibly be due to girls with autism having fewer restricted interests and lower levels of repetitive behavior compared to boys. Research has tried to develop assessments to equate boys and girls, but girls and boys appear to be unequal in their underlying autism process—that is, boys and girls appear to show sex-specific autism phenotypes (Frazier et al., 2014). 25.3.11.4 Recommendations for Reducing Bias in Clinical Judgment Below are recommendations for reducing bias in clinical judgment (Garb, 1997), including: Be aware of biases that have been reported in the clinical judgment literature. Give the clinician timely and specific feedback on their judgments. Use standardized and structured assessment approaches and attend strictly to diagnostic criteria when making diagnoses. Use actuarial approaches and statistical prediction rules. 25.3.12 Use Appropriate Normative Data When group-referenced judgments will be used, it is important to consider what the appropriate norm is. The question of whether separate norms should be used for various racial/ethnic minority groups is of considerable controversy (Burlew et al., 2019). Pros and cons of using group-specific norms are discussed in Section 16.5.2.2.2 and reviewed by Manly (2005). The question about which norms to use is complex, and psychologists should evaluate the cost and benefit of each norm, and use the norm with the greatest benefit and the least cost for the client (Manly &amp; Echemendia, 2007). Burlew et al. (2019) recommend limiting the use of measures that require the use of norms to interpret, and to note in the limitations of a paper or report if a norm based on one group is used to evaluate a person from a different group. 25.3.13 Contextualize Group-Related Differences When conducting research and identifying different scores between groups, it is important to be cautious not to assume that differences in the scores between groups are caused by the group membership. That is, correlation does not necessarily mean causation! For instance, if the researcher identifies higher rates of suicide in transgender than cisgender individuals (as has been observed, Toomey et al., 2018), it is important not to automatically assume the higher suicide rate is because of the person’s transgender status, per se. Rather, there are likely many aspects of discrimination and victimization that transgender individuals experience that, unfortunately, may increase their suicide rate relative to cisgender individuals. It is important to contextualize test score differences between groups within sociocultural and historical inequities rather than interpreting group-related differences in tests scores from a deficit-based narrative (Byrd et al., 2021). It is important to avoid interpretive bias and to pay attention not to over- or under-pathologize (Okazaki &amp; Sue, 1995). Over-pathologizing can lead to stigmatization and institutionalization. Under-estimating psychopathology can deprive patients of treatments or services that they would benefit from. For instance, one should be aware of the danger of under-estimating psychopathology for clients from different cultures (from that of the therapist) through over-attributing bizarre behavior or thought patterns to the client’s culture (Okazaki &amp; Sue, 1995). 25.3.14 Dissemination and Implementation Dissemination and implementation deals not just with finding differences between groups, but what to do about these differences. Aspirationally, we would successfully be able to custom-tailor treatments to individuals based on their individual characteristics. The goal is to identify the best treatment for each group, or ideally, each individual. 25.3.15 Use Inclusive and Bias-Free Language It is important to use bias-free language, as discussed in the Publication manual of the American Psychological Association [APA; American Psychological Association (2020)]. Guidelines for bias-free language include the following: Describe at the appropriate level of specificity Focus on relevant characteristics Acknowledge relevant differences that exist Be appropriately specific Be sensitive to labels Acknowledge people’s humanity Provide operational definitions and labels Avoid false hierarchies When writing reports, manuscripts, or in conversation, it can be helpful to use inclusive and person-first language. The aim of people-first language is to reduce stigma because it treats everyone as a human first. People-first language puts the person before the disability and describes the difficulties a person has rather than who a person is. For instance, say “a child with intellectual disability” instead of “an intellectually disabled child”. That said, some individuals may prefer identity-first labels (e.g., autistic person) rather than person-first labels for their own identities. Thus, it is important to use the labels preferred by the particular client. The APA guidelines for inclusive language are located here: https://www.apa.org/about/apa/equity-diversity-inclusion/language-guide.pdf (archived at https://perma.cc/82QH-ED96). 25.4 Conclusion Diverse samples provide stronger tests of theories and better generalizability of findings (external validity). We need measures that are reliable and valid for all people we use them with. There are various multicultural frameworks to assessment. Guidelines for assessment of diverse populations are discussed. Recommendations include developing and using appropriate and valid measures for the target population(s), ensuring fair group comparisons, seeking training and consultation, developing awareness of cultural differences, showing cultural humility, avoiding assuming or stereotyping, ensuring adequate representation of under-represented minorities, considering pathoplasticity of constructs, formulating a culturally informed case conceptualization, using appropriate normative data, contextualizing group-related differences, and using inclusive language. 25.5 Suggested Readings Burlew et al. (2019); Byrd et al. (2021); Okazaki &amp; Sue (1995) References American Psychological Association. (2020). Publication manual of the American Psychological Association (7th ed.). American Psychological Association Office of Ethnic Minority Affairs. (1993). Guidelines for providers of psychological services to ethnic, linguistic, and culturally diverse populations. American Psychologist, 48(1), 45–48. https://doi.org/10.1037/0003-066X.48.1.45 Baron-Cohen, S. (2002). The extreme male brain theory of autism. Trends in Cognitive Sciences, 6(6), 248–254. https://doi.org/10.1016/S1364-6613(02)01904-6 Baron-Cohen, S. (2010). Empathizing, systemizing, and the extreme male brain theory of autism. In I. Savic (Ed.), Progress in brain research (Vol. 186, pp. 167–175). Elsevier. Brickman, A. M., Cabo, R., &amp; Manly, J. J. (2006). Ethical issues in cross-cultural neuropsychology. Applied Neuropsychology, 13(2), 91–100. https://doi.org/10.1207/s15324826an1302_4 Burlew, A. K., Peteet, B. J., McCuistian, C., &amp; Miller-Roenigk, B. D. (2019). Best practices for researching diverse groups. American Journal of Orthopsychiatry, 89(3), 354–368. https://doi.org/10.1037/ort0000350 Byrd, D. A., Rivera Mindt, M. M., Clark, U. S., Clarke, Y., Thames, A. D., Gammada, E. Z., &amp; Manly, J. J. (2021). Creating an antiracist psychology by addressing professional complicity in psychological assessment. Psychological Assessment, 33(3), 279–285. https://doi.org/10.1037/pas0000993 Chen, F. F. (2008). What happens if we compare chopsticks with forks? The impact of making inappropriate comparisons in cross-cultural research. Journal of Personality and Social Psychology, 95(5), 1005–1018. https://doi.org/10.1037/a0013193 Chen, F. R., &amp; Jaffee, S. R. (2015). The heterogeneity in the development of homotypic and heterotypic antisocial behavior. Journal of Developmental and Life-Course Criminology, 1(3), 269–288. https://doi.org/10.1007/s40865-015-0012-3 Dana, R. H. (1998). Multicultural assessment of personality and psychopathology in the United States: Still art, not yet science, and controversial. European Journal of Psychological Assessment, 14(1), 62–70. https://doi.org/10.1027/1015-5759.14.1.62 Daugherty, J. C., Puente, A. E., Fasfous, A. F., Hidalgo-Ruzzante, N., &amp; Pérez-Garcia, M. (2017). Diagnostic mistakes of culturally diverse individuals when using North American neuropsychological tests. Applied Neuropsychology: Adult, 24(1), 16–22. https://doi.org/10.1080/23279095.2015.1036992 Eaton, W. W. (1980). The sociology of mental disorders. Praeger. Edwards, L. M., Burkard, A. W., Adams, H. A., &amp; Newcomb, S. A. (2017). A mixed-method study of psychologists’ use of multicultural assessment. Professional Psychology: Research and Practice, 48(2), 131–138. https://doi.org/10.1037/pro0000095 Executive Board of the American Anthropological Association. (1998). AAA statement on race. American Anthropologist, 100(3), 712–713. https://doi.org/10.1525/aa.1998.100.3.712 Fadus, M. C., Ginsburg, K. R., Sobowale, K., Halliday-Boykins, C. A., Bryant, B. E., Gray, K. M., &amp; Squeglia, L. M. (2020). Unconscious bias and the diagnosis of disruptive behavior disorders and ADHD in african american and hispanic youth. Academic Psychiatry, 44(1), 95–102. https://doi.org/10.1007/s40596-019-01127-6 Fernández, A. L., &amp; Abe, J. (2018). Bias in cross-cultural neuropsychological testing: Problems and possible solutions. Culture and Brain, 6(1), 1–35. https://doi.org/10.1007/s40167-017-0050-2 Frazier, T. W., Georgiades, S., Bishop, S. L., &amp; Hardan, A. Y. (2014). Behavioral and cognitive characteristics of females and males with autism in the simons simplex collection. Journal of the American Academy of Child &amp; Adolescent Psychiatry, 53(3), 329–340.e3. https://doi.org/10.1016/j.jaac.2013.12.004 Garb, H. N. (1997). Race bias, social class bias, and gender bias in clinical judgment. Clinical Psychology: Science and Practice, 4(2), 99–120. https://doi.org/10.1111/j.1468-2850.1997.tb00104.x Greenberg, D. M., Warrier, V., Allison, C., &amp; Baron-Cohen, S. (2018). Testing the empathizing–systemizing theory of sex differences and the extreme male brain theory of autism in half a million people. Proceedings of the National Academy of Sciences, 115(48), 12152–12157. https://doi.org/10.1073/pnas.1811032115 Han, K., Colarelli, S. M., &amp; Weed, N. C. (2019). Methodological and statistical advances in the consideration of cultural diversity in assessment: A critical review of group classification and measurement invariance testing. Psychological Assessment, 31(12), 1481–1496. https://doi.org/10.1037/pas0000731 Hays, P. A. (2016). Addressing cultural complexities in practice: Assessment, diagnosis, and therapy. American Psychological Association. Helms, J. E., Jernigan, M., &amp; Mascher, J. (2005). The meaning of race in psychology and how to change it: A methodological perspective. American Psychologist, 60(1), 27–36. https://doi.org/10.1037/0003-066X.60.1.27 Henrich, J., Heine, S. J., &amp; Norenzayan, A. (2010). Most people are not WEIRD. Nature, 466(7302), 29–29. https://doi.org/10.1038/466029a Hunsley, J., &amp; Mash, E. J. (2007). Evidence-based assessment. Annual Review of Clinical Psychology, 3, 29–51. https://doi.org/10.1146/annurev.clinpsy.3.022806.091419 Leong, F. T. L., &amp; Kalibatseva, Z. (2016). Threats to cultural validity in clinical diagnosis and assessment: Illustrated with the case of Asian Americans. In N. Zane, G. Bernal, &amp; F. T. L. Leong (Eds.), Evidence-based psychological practice with ethnic minorities: Culturally informed research and clinical strategies (pp. 57–74). American Psychological Association. Lewis-Fernández, R., Aggarwal, N. K., Bäärnhielm, S., Rohlof, H., Kirmayer, L. J., Weiss, M. G., Jadhav, S., Hinton, L., Alarcón, R. D., Bhugra, D., Groen, S., Dijk, R. van, Qureshi, A., Collazos, F., Rousseau, C., Caballero, L., Ramos, M., &amp; Lu, F. (2014). Culture and psychiatric evaluation: Operationalizing cultural formulation for DSM-5. Psychiatry: Interpersonal and Biological Processes, 77(2), 130–154. https://doi.org/10.1521/psyc.2014.77.2.130 Lysell, H., Dahlin, M., Viktorin, A., Ljungberg, E., D’Onofrio, B. M., Dickman, P., &amp; Runeson, B. (2018). Maternal suicide – register based study of all suicides occurring after delivery in sweden 1974–2009. PLOS ONE, 13(1), e0190133. https://doi.org/10.1371/journal.pone.0190133 Manly, J. J. (2005). Advantages and disadvantages of separate norms for African Americans. The Clinical Neuropsychologist, 19(2), 270–275. https://doi.org/10.1080/13854040590945346 Manly, J. J., &amp; Echemendia, R. J. (2007). Race-specific norms: Using the model of hypertension to understand issues of race, culture, and education in neuropsychology. Archives of Clinical Neuropsychology, 22(3), 319–325. https://doi.org/10.1016/j.acn.2007.01.006 McNiel, D. E., &amp; Binder, R. L. (1995). Correlates of accuracy in the assessment of psychiatric inpatients’ risk of violence. American Journal of Psychiatry, 152(6), 901–906. https://doi.org/10.1176/ajp.152.6.901 Miller, J. L., Vaillancourt, T., &amp; Boyle, M. H. (2009). Examining the heterotypic continuity of aggression using teacher reports: Results from a national Canadian study. Social Development, 18(1), 164–180. https://doi.org/10.1111/j.1467-9507.2008.00480.x Moffitt, T. E. (1993). Adolescence-limited and life-course-persistent antisocial behavior: A developmental taxonomy. Psychological Review, 100(4), 674–701. https://doi.org/10.1037/0033-295X.100.4.674 Okazaki, S., &amp; Sue, S. (1995). Methodological issues in assessment research with ethnic minorities. Psychological Assessment, 7(3), 367–375. https://doi.org/10.1037/1040-3590.7.3.367 Patterson, G. R. (1993). Orderly change in a stable world: The antisocial trait as a chimera. Journal of Consulting and Clinical Psychology, 61(6), 911–919. https://doi.org/10.1037/0022-006X.61.6.911 Petersen, I. T., Bates, J. E., Dodge, K. A., Lansford, J. E., &amp; Pettit, G. S. (2015). Describing and predicting developmental profiles of externalizing problems from childhood to adulthood. Development and Psychopathology, 27(3), 791–818. https://doi.org/10.1017/S0954579414000789 Petersen, I. T., &amp; LeBeau, B. (2022). Creating a developmental scale to chart the development of psychopathology with different informants and measures across time. Journal of Psychopathology and Clinical Science, 131(6), 611–625. https://doi.org/10.1037/abn0000649 Ridley, C. R., Hill, C. L., &amp; Wiese, D. L. (2001). Ethics in multicultural assessment a model of reasoned application. In D. L. Wiese (Ed.), Handbook of multicultural assessment: Clinical, psychological, and educational applications (p. 29). Ridley, C. R., Li, L. C., &amp; Hill, C. L. (1998). Multicultural assessment: Reexamination, reconceptualization, and practical application. The Counseling Psychologist, 26(6), 827–910. https://doi.org/10.1177/0011000098266001 Rivera Mindt, M., Byrd, D., Saez, P., &amp; Manly, J. (2010). Increasing culturally competent neuropsychological services for ethnic minority populations: A call to action. The Clinical Neuropsychologist, 24(3), 429–453. https://doi.org/10.1080/13854040903058960 Sattler, J. M., &amp; Hoge, R. D. (2006). Assessment of children: Behavioral, social, and clinical foundations (5th ed.). Jerome M. Sattler, Publisher, Inc. Smedley, A., &amp; Smedley, B. D. (2005). Race as biology is fiction, racism as a social problem is real: Anthropological and historical perspectives on the social construction of race. American Psychologist, 60(1), 16–26. https://doi.org/10.1037/0003-066X.60.1.16 Sternberg, R. J., Grigorenko, E. L., &amp; Kidd, K. K. (2005). Intelligence, race, and genetics. American Psychologist, 60(1), 46–59. https://doi.org/10.1037/0003-066x.60.1.46 Suzuki, L. A., Onoue, M. A., &amp; Hill, J. S. (2013). Clinical assessment: A multicultural perspective. In K. F. Geisinger, J. F. Carlson, J.-I. C. Hansen, N. R. Kuncel, S. P. Reise, &amp; M. C. Rodriguez (Eds.), APA handbook of testing and assessment in psychology, Vol. 2: Testing and assessment in clinical and counseling psychology (pp. 193–212). American Psychological Association. Tervalon, M., &amp; Murray-Garcia, J. (1998). Cultural humility versus cultural competence: A critical distinction in defining physician training outcomes in multicultural education. Journal of Health Care for the Poor and Underserved, 9(2), 117–125. Toomey, R. B., Syvertsen, A. K., &amp; Shramko, M. (2018). Transgender adolescent suicide behavior. Pediatrics, 142(4). https://doi.org/10.1542/peds.2017-4218 Wakschlag, L. S., Tolan, P. H., &amp; Leventhal, B. L. (2010). Research review: “Ain’t misbehavin”: Towards a developmentally-specified nosology for preschool disruptive behavior. Journal of Child Psychology and Psychiatry, 51(1), 3–22. https://doi.org/10.1111/j.1469-7610.2009.02184.x Wicherts, J. M., &amp; Dolan, C. V. (2010). Measurement invariance in confirmatory factor analysis: An illustration using IQ test performance of minorities. Educational Measurement: Issues and Practice, 29(3), 39–47. https://doi.org/10.1111/j.1745-3992.2010.00182.x Youngstrom, E. A., &amp; Van Meter, A. (2016). Empirically supported assessment of children and adolescents. Clinical Psychology: Science and Practice, 23(4), 327–347. https://doi.org/10.1111/cpsp.12172 Yudell, M., Roberts, D., DeSalle, R., &amp; Tishkoff, S. (2016). Taking race out of human genetics. Science, 351(6273), 564–565. https://doi.org/10.1126/science.aac4951 Zuckerman, M. (1990). Some dubious premises in research and theory on racial differences: Scientific, social, and ethical issues. American Psychologist, 45(12), 1297–1303. https://doi.org/10.1037/0003-066X.45.12.1297 "],["reproducibility.html", "Chapter 26 Reproducibility 26.1 Session Info", " Chapter 26 Reproducibility It is important for work to be reproducible by others. Reproducibility and replicability are key goals of the open science movement (Gandrud, 2020; Open Science Collaboration, 2015; Tackett, Brandes, King, et al., 2019; Tackett, Brandes, &amp; Reardon, 2019). To that aim, I provide information below on my setup that was used to generate the results found in this book. If you run (all of) my code in the same order with the exact same setup, you should get the same results—but let me know if not! 26.1 Session Info Code sessionInfo() R version 4.4.0 (2024-04-24) Platform: x86_64-pc-linux-gnu Running under: Ubuntu 22.04.4 LTS Matrix products: default BLAS: /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so; LAPACK version 3.10.0 locale: [1] LC_CTYPE=C.UTF-8 LC_NUMERIC=C LC_TIME=C.UTF-8 [4] LC_COLLATE=C.UTF-8 LC_MONETARY=C.UTF-8 LC_MESSAGES=C.UTF-8 [7] LC_PAPER=C.UTF-8 LC_NAME=C LC_ADDRESS=C [10] LC_TELEPHONE=C LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C time zone: UTC tzcode source: system (glibc) attached base packages: [1] grid stats4 splines stats graphics grDevices utils [8] datasets methods base other attached packages: [1] mirtCAT_1.13 shiny_1.8.1.1 strucchange_1.5-3 [4] sandwich_3.1-0 zoo_1.8-12 dmacs_0.1.0.9002 [7] dagitty_0.3-4 nFactors_2.4.1.1 corrplot_0.92 [10] glmnet_4.1-8 LiblineaR_2.10-23 elasticnet_1.3 [13] lars_1.3 ordinalForest_2.4-3 ranger_0.16.0 [16] e1071_1.7-14 randomForest_4.7-1.1 caret_6.0-94 [19] ggrepel_0.9.5 car_3.1-2 carData_3.0-5 [22] msir_1.3.3 ggpubr_0.6.0 gridExtra_2.3 [25] uroc_0.1.0 PredictABEL_1.2-4 ResourceSelection_0.3-6 [28] rms_6.8-0 Hmisc_5.1-2 ROCR_1.0-11 [31] pROC_1.18.5 magrittr_2.0.3 mirt_1.41 [34] lattice_0.22-6 drc_3.0-1 semTools_0.5-6 [37] nonnest2_0.5-6 quantreg_5.97 SparseM_1.81 [40] mice_3.16.0 snow_0.4-4 simsem_0.5-16 [43] DT_0.33 simstandard_0.6.3 MASS_7.3-60.2 [46] kableExtra_1.4.0 rockchalk_1.8.157 semPlot_1.1.6 [49] here_1.0.1 MOTE_1.0.2 performance_0.11.0 [52] gtheory_0.1.2 lme4_1.1-35.3 Matrix_1.7-0 [55] irrNA_0.2.3 irrICC_1.0 irrCAC_1.0 [58] psychmeta_2.6.5 lavaan_0.6-17 MBESS_4.9.3 [61] blandr_0.5.1 psych_2.4.3 viridis_0.6.5 [64] viridisLite_0.4.2 bookdown_0.39 rmarkdown_2.26 [67] knitr_1.46 tinytex_0.50 lubridate_1.9.3 [70] forcats_1.0.0 stringr_1.5.1 purrr_1.0.2 [73] readr_2.1.5 tidyr_1.3.1 tibble_3.2.1 [76] ggplot2_3.5.1 tidyverse_2.0.0 dplyr_1.1.4 [79] petersenlab_1.0.0 loaded via a namespace (and not attached): [1] RColorBrewer_1.1-3 insight_0.19.10 numDeriv_2016.8-1.1 [4] tools_4.4.0 backports_1.4.1 utf8_1.2.4 [7] R6_2.5.1 vegan_2.6-4 mgcv_1.9-1 [10] jomo_2.7-6 permute_0.9-7 withr_3.0.0 [13] prettyunits_1.2.0 fdrtool_1.2.17 qgraph_1.9.8 [16] cli_3.6.2 mix_1.0-11 labeling_0.4.3 [19] sass_0.4.9 mvtnorm_1.2-4 polspline_1.1.24 [22] proxy_0.4-27 pbapply_1.7-2 pbivnorm_0.6.0 [25] systemfonts_1.0.6 foreign_0.8-86 svglite_2.1.3 [28] parallelly_1.37.1 lisrelToR_0.3 plotrix_3.8-4 [31] rstudioapi_0.16.0 generics_0.1.3 shape_1.4.6.1 [34] combinat_0.0-8 gtools_3.9.5 vroom_1.6.5 [37] zip_2.3.1 OpenMx_2.21.11 interp_1.1-6 [40] fansi_1.0.6 abind_1.4-5 lifecycle_1.0.4 [43] multcomp_1.4-25 yaml_2.3.8 CompQuadForm_1.4.3 [46] recipes_1.0.10 promises_1.3.0 crayon_1.5.2 [49] mitml_0.4-5 pillar_1.9.0 tcltk_4.4.0 [52] boot_1.3-30 corpcor_1.6.10 lpSolve_5.6.20 [55] future.apply_1.11.2 codetools_0.2-20 pan_1.9 [58] glue_1.7.0 V8_4.4.2 data.table_1.15.4 [61] vctrs_0.6.5 png_0.1-8 gtable_0.3.5 [64] cachem_1.0.8 gower_1.0.1 xfun_0.43 [67] openxlsx_4.2.5.2 mime_0.12 prodlim_2023.08.28 [70] coda_0.19-4.1 survival_3.5-8 timeDate_4032.109 [73] iterators_1.0.14 hardhat_1.3.1 lava_1.8.0 [76] TH.data_1.1-2 ipred_0.9-14 nlme_3.1-164 [79] bit64_4.0.5 progress_1.2.3 rprojroot_2.0.4 [82] mi_1.1 bslib_0.7.0 Deriv_4.1.3 [85] rpart_4.1.23 colorspace_2.1-0 DBI_1.2.2 [88] nnet_7.3-19 mnormt_2.1.1 tidyselect_1.2.1 [91] bit_4.0.5 compiler_4.4.0 curl_5.2.1 [94] htmlTable_2.4.2 animation_2.7 xml2_1.3.6 [97] checkmate_2.3.1 scales_1.3.0 quadprog_1.5-8 [100] sem_3.1-15 digest_0.6.35 minqa_1.2.6 [103] htmltools_0.5.8.1 pkgconfig_2.0.3 jpeg_0.1-10 [106] base64enc_0.1-3 highr_0.10 fastmap_1.1.1 [109] rlang_1.1.3 htmlwidgets_1.6.4 jmvcore_2.4.7 [112] farver_2.1.1 jquerylib_0.1.4 jsonlite_1.8.8 [115] mclust_6.1 dcurver_0.9.2 ModelMetrics_1.2.2.2 [118] polynom_1.4-1 Formula_1.2-5 munsell_0.5.1 [121] Rcpp_1.0.12 stringi_1.8.3 plyr_1.8.9 [124] listenv_0.9.1 parallel_4.4.0 deldir_2.0-4 [127] kutils_1.73 hms_1.1.3 PBSmodelling_2.69.3 [130] igraph_2.0.3 ggsignif_0.6.4 reshape2_1.4.4 [133] GPArotation_2024.3-1 XML_3.99-0.16.1 evaluate_0.23 [136] latticeExtra_0.6-30 mitools_2.4 RcppParallel_5.1.7 [139] httpuv_1.6.15 nloptr_2.0.3 tzdb_0.4.0 [142] foreach_1.5.2 MatrixModels_0.5-3 future_1.33.2 [145] reshape_0.8.9 broom_1.0.5 xtable_1.8-4 [148] later_1.3.2 rstatix_0.7.2 class_7.3-22 [151] glasso_1.11 ez_4.4-0 arm_1.14-4 [154] cluster_2.1.6 globals_0.16.3 timechange_0.3.0 References Gandrud, C. (2020). Reproducible research with R and R studio (3rd ed.). CRC Press. https://www.routledge.com/Reproducible-Research-with-R-and-RStudio/Gandrud/p/book/9780367143985 Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251). https://doi.org/10.1126/science.aac4716 Tackett, J. L., Brandes, C. M., King, K. M., &amp; Markon, K. E. (2019). Psychology’s replication crisis and clinical psychological science. Annual Review of Clinical Psychology, 15(1), 579–604. https://doi.org/10.1146/annurev-clinpsy-050718-095710 Tackett, J. L., Brandes, C. M., &amp; Reardon, K. W. (2019). Leveraging the open science framework in clinical psychological assessment research. Psychological Assessment, 31(12), 1386–1394. https://doi.org/10.1037/pas0000583 "],["references.html", "References", " References Achenbach, T. M. (2001). What are norms and why do we need valid ones? Clinical Psychology: Science and Practice, 8(4), 446–450. https://doi.org/10.1093/clipsy.8.4.446 Ackerman, P. L. (2013). Assessment of intellectual functioning in adults. In K. F. Geisinger, J. F. Carlson, J.-I. C. Hansen, N. R. Kuncel, S. P. Reise, &amp; M. C. Rodriguez (Eds.), APA handbook of testing and assessment in psychology, Vol 2: Testing and assessment in clinical and counseling psychology (pp. 119–132). American Psychological Association. Ægisdóttir, S., White, M. J., Spengler, P. M., Maugherman, A. S., Anderson, L. A., Cook, R. S., Nichols, C. N., Lampropoulos, G. K., Walker, B. S., Cohen, G., &amp; Rush, J. D. (2006). The meta-analysis of clinical judgment project: Fifty-six years of accumulated research on clinical versus statistical prediction. The Counseling Psychologist, 34(3), 341–382. https://doi.org/10.1177/0011000005285875 Aguinis, H., Culpepper, S. A., &amp; Pierce, C. A. (2010). Revival of test bias research in preemployment testing. Journal of Applied Psychology, 95(4), 648–680. https://doi.org/10.1037/a0018714 Allaire, J., Xie, Y., McPherson, J., Luraschi, J., Ushey, K., Atkins, A., Wickham, H., Cheng, J., Chang, W., &amp; Iannone, R. (2022). rmarkdown: Dynamic documents for R. https://CRAN.R-project.org/package=rmarkdown American Educational Research Association, American Psychological Association, &amp; National Council on Measurement in Education. (2014). Standards for educational and psychological testing. American Educational Research Association. American Psychological Association. (2017). Ethical principles of psychologists and code of conduct. American Psychological Association. (2020). Publication manual of the American Psychological Association (7th ed.). American Psychological Association Office of Ethnic Minority Affairs. (1993). Guidelines for providers of psychological services to ethnic, linguistic, and culturally diverse populations. American Psychologist, 48(1), 45–48. https://doi.org/10.1037/0003-066X.48.1.45 Antony, M. M., &amp; Rowa, K. (2005). Evidence-based assessment of anxiety disorders in adults. Psychological Assessment, 17(3), 256–266. https://doi.org/10.1037/1040-3590.17.3.256 Arnett, A., Pennington, B., Willcutt, E., Dmitrieva, J., Byrne, B., Samuelsson, S., &amp; Olson, R. (2012). A cross-lagged model of the development of ADHD inattention symptoms and rapid naming speed. Journal of Abnormal Child Psychology, 40(8), 1313–1326. https://doi.org/10.1007/s10802-012-9644-5 Arvey, R. D., Bouchard, T. J., Carroll, J. B., Cattell, R. B., Cohen, D. B., Dawis, R. V., Detterman, D. K., Dunnette, M., Eysenck, H., Feldman, J. M., Fleishman, E. A., Gilmore, G. C., Gordon, R. A., Gottfredson, L. S., Greene, R. L., Haier, R. J., Hardin, G., Hogan, R., Horn, J. M., … Willerman, L. (1994). Mainstream science on intelligence. Wall Street Journal, 13(1), 18–25. Atanasov, P., Witkowski, J., Ungar, L., Mellers, B., &amp; Tetlock, P. (2020). Small steps to accuracy: Incremental belief updaters are better forecasters. Organizational Behavior and Human Decision Processes, 160, 19–35. https://doi.org/10.1016/j.obhdp.2020.02.001 Austin, P. C., &amp; Steyerberg, E. W. (2014). Graphical assessment of internal and external calibration of logistic regression models by using loess smoothers. Statistics in Medicine, 33(3), 517–535. https://doi.org/10.1002/sim.5941 Avugos, S., Köppen, J., Czienskowski, U., Raab, M., &amp; Bar-Eli, M. (2013). The “hot hand” reconsidered: A meta-analytic approach. Psychology of Sport and Exercise, 14(1), 21–27. https://doi.org/10.1016/j.psychsport.2012.07.005 Baird, C., &amp; Wagner, D. (2000). The relative validity of actuarial- and consensus-based risk assessment systems. Children and Youth Services Review, 22(11), 839–871. https://doi.org/10.1016/S0190-7409(00)00122-5 Bakeman, R., &amp; Goodman, S. H. (2020). Interobserver reliability in clinical research: Current issues and discussion of how to establish best practices. Journal of Abnormal Psychology, 129(1), 5–13. https://doi.org/10.1037/abn0000487 Baker, F. B., &amp; Kim, S.-H. (2017). The basics of item response theory using R. Springer. Ballesteros-Pérez, P., González-Cruz, M. C., &amp; Mora-Melià, D. (2018). Explaining the Bayes’ theorem graphically. Proceedings of the International Technology, Education and Development Conference. Baltes, P. B. (1968). Longitudinal and cross-sectional sequences in the study of age and generation effects. Human Development, 11(3), 145–171. http://www.jstor.org/stable/26761719 Bandalos, D. L. (2018). Measurement theory and applications for the social sciences. Guilford Publications. Bar-Eli, M., Avugos, S., &amp; Raab, M. (2006). Twenty years of “hot hand” research: Review and critique. Psychology of Sport and Exercise, 7(6), 525–553. https://doi.org/10.1016/j.psychsport.2006.03.001 Baron-Cohen, S. (2002). The extreme male brain theory of autism. Trends in Cognitive Sciences, 6(6), 248–254. https://doi.org/10.1016/S1364-6613(02)01904-6 Baron-Cohen, S. (2010). Empathizing, systemizing, and the extreme male brain theory of autism. In I. Savic (Ed.), Progress in brain research (Vol. 186, pp. 167–175). Elsevier. Barrash, J., Stillman, A., Anderson, S. W., Uc, E. Y., Dawson, J. D., &amp; Rizzo, M. (2010). Prediction of driving ability with neuropsychological tests: Demographic adjustments diminish accuracy. Journal of the International Neuropsychological Society, 16(4), 679–686. https://doi.org/10.1017/S1355617710000470 Bates, D., Maechler, M., Bolker, B., &amp; Walker, S. (2022). lme4: Linear mixed-effects models using Eigen and S4. https://github.com/lme4/lme4/ Bauer, D. J., Belzak, W. C. M., &amp; Cole, V. T. (2020). Simplifying the assessment of measurement invariance over multiple background variables: Using regularized moderated nonlinear factor analysis to detect differential item functioning. Structural Equation Modeling: A Multidisciplinary Journal, 27(1), 43–55. https://doi.org/10.1080/10705511.2019.1642754 Beaujean, A. A. (2014). Latent variable modeling using R: A step-by-step guide. Routledge. Beltz, A. M., Wright, A. G. C., Sprague, B. N., &amp; Molenaar, P. C. M. (2016). Bridging the nomothetic and idiographic approaches to the analysis of clinical data. Assessment, 23(4), 447–458. https://doi.org/10.1177/1073191116648209 Belzak, W. C. M., &amp; Bauer, D. J. (2020). Improving the assessment of measurement invariance: Using regularization to select anchor items and identify differential item functioning. Psychological Methods, 25(6), 673–690. https://doi.org/10.1037/met0000253 Benjamin, L. T. (2005). A history of clinical psychology as a profession in America (and a glimpse of its future). Annual Review of Clinical Psychology, 1, 1–30. https://doi.org/10.1146/annurev.clinpsy.1.102803.143758 Bennett, C. M., Miller, M. B., &amp; Wolford, G. L. (2009). Neural correlates of interspecies perspective taking in the post-mortem Atlantic Salmon: An argument for multiple comparisons correction. NeuroImage, 47, S125. https://doi.org/10.1016/S1053-8119(09)71202-9 Bennett, C. M., Miller, M. B., &amp; Wolford, G. L. (2010). Neural correlates of interspecies perspective taking in the post-mortem Atlantic Salmon: An argument for multiple comparisons correction. Journal of Serendipitous and Unexpected Results, 1, 1–5. https://teenspecies.github.io/pdfs/NeuralCorrelates.pdf Benning, S. D., Bachrach, R. L., Smith, E. A., Freeman, A. J., &amp; Wright, A. G. C. (2019). The registration continuum in clinical science: A guide toward transparent practices. Journal of Abnormal Psychology, 128(6), 528–540. https://doi.org/10.1037/abn0000451 Bensch, D., Maaß, U., Greiff, S., Horstmann, K. T., &amp; Ziegler, M. (2019). The nature of faking: A homogeneous and predictable construct? Psychological Assessment, 31(4), 532–544. https://doi.org/10.1037/pas0000619 Berry, D., &amp; Willoughby, M. T. (2017). On the practical interpretability of cross-lagged panel models: Rethinking a developmental workhorse. Child Development, 88(4), 1186–1206. https://doi.org/10.1111/cdev.12660 Bersoff, D. N., DeMatteo, D., &amp; Foster, E. E. (2012). Assessment and testing. In S. J. Knapp (Ed.), APA handbook of ethics in psychology, Vol 2: Practice, teaching, and research (pp. 45–74). American Psychological Association. Bickel, J. E., &amp; Kim, S. D. (2008). Verification of The Weather Channel probability of precipitation forecasts. Monthly Weather Review, 136(12), 4867–4881. https://doi.org/10.1175/2008MWR2547.1 Bland, J. M., &amp; Altman, D. G. (1986). Statistical methods for assessing agreement between two methods of clinical measurement. The Lancet, 327(8476), 307–310. https://doi.org/10.1016/S0140-6736(86)90837-8 Bland, J. M., &amp; Altman, D. G. (1999). Measuring agreement in method comparison studies. Statistical Methods in Medical Research, 8(2), 135–160. https://doi.org/10.1177/096228029900800204 Blashfield, R. K., Keeley, J. W., Flanagan, E. H., &amp; Miles, S. R. (2014). The cycle of classification: DSM-I through DSM-5. Annual Review of Clinical Psychology, 10(1), 25–51. https://doi.org/10.1146/annurev-clinpsy-032813-153639 Blumberg, M. S. (2013). Homology, correspondence, and continuity across development: The case of sleep. Developmental Psychobiology, 55(1), 92–100. https://doi.org/10.1002/dev.21024 Bocskocsky, A., Ezekowitz, J., &amp; Stein, C. (2014). The hot hand: A new approach to an old “fallacy.” MIT Sloan Sports Analytics Conference. Bolger, F., &amp; Önkal-Atay, D. (2004). The effects of feedback on judgmental interval predictions. International Journal of Forecasting, 20(1), 29–39. https://doi.org/10.1016/S0169-2070(03)00009-8 Bollen, K. A. (1989). Structural equations with latent variables. John Wiley &amp; Sons. Bollen, K. A. (2002). Latent variables in psychology and the social sciences. Annual Review of Psychology, 53(1), 605–634. https://doi.org/10.1146/annurev.psych.53.100901.135239 Bollen, K. A., &amp; Bauldry, S. (2011). Three Cs in measurement models: Causal indicators, composite indicators, and covariates. Psychological Methods, 16(3), 265–284. https://doi.org/10.1037/a0024448 Bollen, K. A., &amp; Diamantopoulos, A. (2017). In defense of causal-formative indicators: A minority report. Psychological Methods, 22(3), 581–596. https://doi.org/10.1037/met0000056 Bollen, K. A., &amp; Lennox, R. D. (1991). Conventional wisdom on measurement: A structural equation perspective. Psychological Bulletin, 110(2), 305–314. https://doi.org/10.1037/0033-2909.110.2.305 Boring, E. G. (1923). Intelligence as the tests test it. New Republic, 36, 35–37. Bornstein, R. F. (2011). Toward a process-focused model of test score validity: Improving psychological assessment in science and practice. Psychological Assessment, 23(2), 532–544. https://doi.org/10.1037/a0022402 Borsboom, D. (2003). Conceptual issues in psychological measurement. Universiteit van Amsterdam. Box, G. E. P. (1979). Robustness in the strategy of scientific model building. In R. L. Launer &amp; G. N. Wilkinson (Eds.), Robustness in statistics. Academic Press. Brennan, R. L. (1992). Generalizability theory. Educational Measurement: Issues and Practice, 11(4), 27–34. https://doi.org/10.1111/j.1745-3992.1992.tb00260.x Brennan, R. L. (2001). Generalizability theory. Springer New York. https://books.google.com/books?id=nbHbBwAAQBAJ Brickman, A. M., Cabo, R., &amp; Manly, J. J. (2006). Ethical issues in cross-cultural neuropsychology. Applied Neuropsychology, 13(2), 91–100. https://doi.org/10.1207/s15324826an1302_4 Brown, R. T., Reynolds, C. R., &amp; Whitaker, J. S. (1999). Bias in mental testing since bias in mental testing. School Psychology Quarterly, 14(3), 208–238. https://doi.org/10.1037/h0089007 Buchanan, T. (2002). Online assessment: Desirable or dangerous? Professional Psychology: Research and Practice, 33(2), 148–154. https://doi.org/10.1037/0735-7028.33.2.148 Burchett, D., &amp; Ben-Porath, Y. S. (2019). Methodological considerations for developing and evaluating response bias indicators. Psychological Assessment, 31(12), 1497–1511. https://doi.org/10.1037/pas0000680 Burisch, M. (1984). Approaches to personality inventory construction: A comparison of merits. American Psychologist, 39, 214–227. https://doi.org/10.1037/0003-066X.39.3.214 Bürkner, P.-C. (2021). Bayesian item response modeling in R with brms and Stan. Journal of Statistical Software, 100(5), 1–54. https://doi.org/10.18637/jss.v100.i05 Burlew, A. K., Peteet, B. J., McCuistian, C., &amp; Miller-Roenigk, B. D. (2019). Best practices for researching diverse groups. American Journal of Orthopsychiatry, 89(3), 354–368. https://doi.org/10.1037/ort0000350 Buros Center for Testing. (2021). The twenty-first mental measurements yearbook. Buros Center for Testing. Busemeyer, J. R., &amp; Stout, J. C. (2002). A contribution of cognitive decision models to clinical assessment: Decomposing performance on the Bechara gambling task. Psychological Assessment, 14(3), 253–262. https://doi.org/10.1037/1040-3590.14.3.253 Button, K. S., Ioannidis, J. P. A., Mokrysz, C., Nosek, B. A., Flint, J., Robinson, E. S. J., &amp; Munafo, M. R. (2013a). Confidence and precision increase with high statistical power. Nature Reviews Neuroscience, 14(8), 585–585. https://doi.org/10.1038/nrn3475-c4 Button, K. S., Ioannidis, J. P. A., Mokrysz, C., Nosek, B. A., Flint, J., Robinson, E. S. J., &amp; Munafo, M. R. (2013b). Power failure: Why small sample size undermines the reliability of neuroscience. Nature Reviews Neuroscience, 14(5), 365–376. https://doi.org/10.1038/nrn3475 Byrd, D. A., Rivera Mindt, M. M., Clark, U. S., Clarke, Y., Thames, A. D., Gammada, E. Z., &amp; Manly, J. J. (2021). Creating an antiracist psychology by addressing professional complicity in psychological assessment. Psychological Assessment, 33(3), 279–285. https://doi.org/10.1037/pas0000993 Calamia, M. (2019). Practical considerations for evaluating reliability in ambulatory assessment studies. Psychological Assessment, 31(3), 285–291. https://doi.org/10.1037/pas0000599 Camilli, G. (2013). Ongoing issues in test fairness. Educational Research and Evaluation, 19(2–3), 104–120. https://doi.org/10.1080/13803611.2013.767602 Campbell, D. T., &amp; Fiske, D. W. (1959). Convergent and discriminant validation by the multitrait-multimethod matrix. Psychological Bulletin, 56(2), 81–105. https://doi.org/10.1037/h0046016 Campbell, L., Vasquez, M., Behnke, S., &amp; Kinscherff, R. (2010). APA ethics code commentary and case illustrations (pp. v, 392–v, 392). American Psychological Association. Carlson, S. M., &amp; Zelazo, P. D. (2014). Minnesota executive function scale. Test manual. Reflection Sciences, LLC. Carpenter, R. W., Wycoff, A. M., &amp; Trull, T. J. (2016). Ambulatory assessment: New adventures in characterizing dynamic processes. Assessment, 23(4), 414–424. https://doi.org/10.1177/1073191116632341 Cashel, M. L. (2002). Child and adolescent psychological assessment: Current clinical practices and the impact of managed care. Professional Psychology: Research and Practice, 33(5), 446–453. https://doi.org/10.1037/0735-7028.33.5.446 Caspi, A., Houts, R. M., Ambler, A., Danese, A., Elliott, M. L., Hariri, A., Harrington, H., Hogan, S., Poulton, R., Ramrakha, S., Rasmussen, L. J. H., Reuben, A., Richmond-Rakerd, L., Sugden, K., Wertz, J., Williams, B. S., &amp; Moffitt, T. E. (2020). Longitudinal assessment of mental health disorders and comorbidities across 4 decades among participants in the Dunedin Birth Cohort Study. JAMA Network Open, 3(4), e203221–e203221. https://doi.org/10.1001/jamanetworkopen.2020.3221 Caspi, A., Houts, R. M., Belsky, D. W., Goldman-Mellor, S. J., Harrington, H., Israel, S., Meier, M. H., Ramrakha, S., Shalev, I., Poulton, R., &amp; Moffitt, T. E. (2014). The p factor: One general psychopathology factor in the structure of psychiatric disorders? Clinical Psychological Science, 2(2), 119–137. https://doi.org/10.1177/2167702613497473 Caspi, A., &amp; Shiner, R. L. (2006). Personality development. In N. Eisenberg, W. Damon, &amp; R. M. Lerner (Eds.), Handbook of child psychology (6th ed., Vol. 3, pp. 300–365). John Wiley &amp; Sons, Inc. Chalmers, P. (2020). mirt: Multidimensional item response theory. https://CRAN.R-project.org/package=mirt Chalmers, P. (2021). mirtCAT: Computerized adaptive testing with multidimensional item response theory. https://CRAN.R-project.org/package=mirtCAT Chandler, J., Sisso, I., &amp; Shapiro, D. (2020). Participant carelessness and fraud: Consequences for clinical research and potential solutions. Journal of Abnormal Psychology, 129(1), 49–55. https://doi.org/10.1037/abn0000479 Charba, J. P., &amp; Klein, W. H. (1980). Skill in precipitation forecasting in the National Weather Service. Bulletin of the American Meteorological Society, 61(12), 1546–1555. https://doi.org/10.1175/1520-0477(1980)061&lt;1546:SIPFIT&gt;2.0.CO;2 Chen, F. F. (2007). Sensitivity of goodness of fit indexes to lack of measurement invariance. Structural Equation Modeling: A Multidisciplinary Journal, 14(3), 464–504. https://doi.org/10.1080/10705510701301834 Chen, F. F. (2008). What happens if we compare chopsticks with forks? The impact of making inappropriate comparisons in cross-cultural research. Journal of Personality and Social Psychology, 95(5), 1005–1018. https://doi.org/10.1037/a0013193 Chen, F. R., &amp; Jaffee, S. R. (2015). The heterogeneity in the development of homotypic and heterotypic antisocial behavior. Journal of Developmental and Life-Course Criminology, 1(3), 269–288. https://doi.org/10.1007/s40865-015-0012-3 Chen, Y., Prudêncio, R. B. C., Diethe, T., &amp; Flach, P. (2019). \\(\\beta\\)3-IRT: A new item response model and its applications. arXiv:1903.04016. https://arxiv.org/abs/1903.04016 Cheng, Y., Shao, C., &amp; Lathrop, Q. N. (2016). The mediated MIMIC model for understanding the underlying mechanism of DIF. Educational and Psychological Measurement, 76(1), 43–63. https://doi.org/10.1177/0013164415576187 Cheung, G. W., &amp; Rensvold, R. B. (2002). Evaluating goodness-of-fit indexes for testing measurement invariance. Structural Equation Modeling: A Multidisciplinary Journal, 9(2), 233–255. https://doi.org/10.1207/s15328007sem0902_5 Childs, D. Z., Hindle, B. J., &amp; Warren, P. H. (2021). APS 240: Data analysis and statistics with R. https://dzchilds.github.io/stats-for-bio/ Choca, J. P., &amp; Rossini, E. D. (2018). Assessment using the Rorschach inkblot test. American Psychological Association. Cicchetti, D., &amp; Rogosch, F. A. (2002). A developmental psychopathology perspective on adolescence. Journal of Consulting and Clinical Psychology, 70(1), 6–20. https://doi.org/10.1037/0022-006X.70.1.6 Civelek, M. E. (2018). Essentials of structural equation modeling. Zea E-Books. Clark, L. A., &amp; Watson, D. (1995). Constructing validity: Basic issues in objective scale development. Psychological Assessment, 7, 309–319. https://doi.org/10.1037/1040-3590.7.3.309 Clark, L. A., &amp; Watson, D. (2019). Constructing validity: New developments in creating objective measuring instruments. Psychological Assessment, 31(12), 1412–1427. https://doi.org/10.1037/pas0000626 Clark, M. J., &amp; Grandy, J. (1984). Sex differences in the academic performance of Scholastic Aptitude Test takers: College board report no. 84-8. College Board Publications. Clark, S. J., &amp; Desharnais, R. A. (1998). Honest answers to embarrassing questions: Detecting cheating in the randomized response model. Psychological Methods, 3(2), 160–168. https://doi.org/10.1037/1082-989X.3.2.160 Cohen, Z. D., &amp; DeRubeis, R. J. (2018). Treatment selection in depression. Annual Review of Clinical Psychology, 14(1), 209–236. https://doi.org/10.1146/annurev-clinpsy-050817-084746 Cole, N. S. (1981). Bias in testing. American Psychologist, 36(10), 1067–1077. https://doi.org/10.1037/0003-066X.36.10.1067 Cole, V., Gottfredson, N., &amp; Giordano, M. (2018). aMNLFA: Automated fitting of moderated nonlinear factor analysis through the Mplus program. https://CRAN.R-project.org/package=aMNLFA Committee on the General Aptitude Test Battery, Commission on Behavioral and Social Sciences and Education, &amp; National Research Council. (1989). Fairness in employment testing: Validity generalization, minority issues, and the general aptitude test battery. National Academies Press. Conradt, E., Crowell, S. E., &amp; Cicchetti, D. (2021). Using development and psychopathology principles to inform the research domain criteria (RDoC) framework. Development and Psychopathology, 33(5), 1521–1525. https://doi.org/10.1017/S0954579421000985 Cooper, L. D., &amp; Balsis, S. (2009). When less is more: How fewer diagnostic criteria can indicate greater severity. Psychological Assessment, 21(3), 285–293. https://doi.org/10.1037/a0016698 Cortina, J. M. (1993). What is coefficient alpha? An examination of theory and applications. Journal of Applied Psychology, 78, 98–104. https://doi.org/10.1037/0021-9010.78.1.98 Costa Jr., P. T., McCrae, R. R., &amp; Löckenhoff, C. E. (2019). Personality across the life span. Annual Review of Psychology, 70(1), 423–448. https://doi.org/10.1146/annurev-psych-010418-103244 Counsell, A., Cribbie, R. A., &amp; Flora, D. B. (2020). Evaluating equivalence testing methods for measurement invariance. Multivariate Behavioral Research, 55(2), 312–328. https://doi.org/10.1080/00273171.2019.1633617 Cronbach, L. J., &amp; Meehl, P. E. (1955). Construct validity in psychological tests. Psychological Bulletin, 52(4), 281–302. https://doi.org/10.1037/h0040957 Curran, P. J., Howard, A. L., Bainter, S. A., Lane, S. T., &amp; McGinley, J. S. (2014). The separation of between-person and within-person components of individual change over time: A latent curve model with structured residuals. Journal of Consulting and Clinical Psychology, 82, 8–94. https://doi.org/10.1037/a0035297 Dana, J., &amp; Thomas, R. (2006). In defense of clinical judgment … and mechanical prediction. Journal of Behavioral Decision Making, 19(5), 413–428. https://doi.org/10.1002/bdm.537 Dana, R. H. (1998). Multicultural assessment of personality and psychopathology in the United States: Still art, not yet science, and controversial. European Journal of Psychological Assessment, 14(1), 62–70. https://doi.org/10.1027/1015-5759.14.1.62 Datta, D. (2018). blandr: Bland-Altman method comparison. https://github.com/deepankardatta/blandr/ Daugherty, J. C., Puente, A. E., Fasfous, A. F., Hidalgo-Ruzzante, N., &amp; Pérez-Garcia, M. (2017). Diagnostic mistakes of culturally diverse individuals when using North American neuropsychological tests. Applied Neuropsychology: Adult, 24(1), 16–22. https://doi.org/10.1080/23279095.2015.1036992 Davison, G. C., Vogel, R. S., &amp; Coffman, S. G. (1997). Think-aloud approaches to cognitive assessment and the articulated thoughts in simulated situations paradigm. Journal of Consulting and Clinical Psychology, 65(6), 950–958. https://doi.org/10.1037/0022-006X.65.6.950 Dawes, R. M. (1986). Representative thinking in clinical judgment. Clinical Psychology Review, 6, 425–441. https://doi.org/10.1016/0272-7358(86)90030-9 Dawes, R. M., Faust, D., &amp; Meehl, P. E. (1989). Clinical versus actuarial judgment. Science, 243(4899), 1668–1674. https://doi.org/10.1126/science.2648573 DeRubeis, R. J., Cohen, Z. D., Forand, N. R., Fournier, J. C., Gelfand, L. A., &amp; Lorenzo-Luaces, L. (2014). The personalized advantage index: Translating research on prediction into individualized treatment recommendations. A demonstration. PLoS ONE, 9(1), e83875. https://doi.org/10.1371/journal.pone.0083875 Diamantopoulos, A., Riefler, P., &amp; Roth, K. P. (2008). Advancing formative measurement models. Journal of Business Research, 61(12), 1203–1218. https://doi.org/10.1016/j.jbusres.2008.01.009 Dien, J. (2012). Applying principal components analysis to event-related potentials: A tutorial. Developmental Neuropsychology, 37(6), 497–517. https://doi.org/10.1080/87565641.2012.697503 Digitale, J. C., Martin, J. N., &amp; Glymour, M. M. (2022). Tutorial on directed acyclic graphs. Journal of Clinical Epidemiology, 142, 264–267. https://doi.org/10.1016/j.jclinepi.2021.08.001 Dinno, A. (2014). Gently clarifying the application of Horn’s parallel analysis to principal component analysis versus factor analysis. http://archives.pdx.edu/ds/psu/10527 Dombrowski, S. C., McGill, R. J., &amp; Morgan, G. B. (2021). Monte Carlo modeling of contemporary intelligence test (IQ) factor structure: Implications for IQ assessment, interpretation, and theory. Assessment, 28(3), 977–993. https://doi.org/10.1177/1073191119869828 Dorans, N. J. (2017). Contributions to the quantitative assessment of item, test, and score fairness. In R. E. Bennett &amp; M. von Davier (Eds.), Advancing human assessment (pp. 201–230). Springer, Cham. Dubois, J., &amp; Adolphs, R. (2016). Building a science of individual differences from fMRI. Trends in Cognitive Sciences, 20(6), 425–443. https://doi.org/10.1016/j.tics.2016.03.014 Dueber, D. (2019). dmacs: Measurement nonequivalence effect size calculator. https://github.com/ddueber/dmacs Duncan, G. J., Engel, M., Claessens, A., &amp; Dowsett, C. J. (2014). Replication and robustness in developmental research. Developmental Psychology, 50(11), 2417–2425. https://doi.org/10.1037/a0037996 Dunkley, D. M., Segal, Z. V., &amp; Blankstein, K. R. (2019). Cognitive assessment: Issues and methods. In K. S. Dobson &amp; D. J. A. Dozois (Eds.), Handbook of cognitive-behavioral therapies (4th ed., pp. 85–119). Guilford Press. Dunn, T. J., Baguley, T., &amp; Brunsden, V. (2014). From alpha to omega: A practical solution to the pervasive problem of internal consistency estimation. British Journal of Psychology, 105(3), 399–412. https://doi.org/10.1111/bjop.12046 Dunning, D., Heath, C., &amp; Suls, J. M. (2004). Flawed self-assessment: Implications for health, education, and the workplace. Psychological Science in the Public Interest, 5, 69–106. https://doi.org/10.1111/j.1529-1006.2004.00018.x Durbin, C. E., Wilson, S., &amp; MacDonald, I., Angus W. (2022). Integrating development into the research domain criteria (RDoC) framework: Introduction to the special section. Journal of Psychopathology and Clinical Science, 131(6), 535–541. https://doi.org/10.1037/abn0000767 Dwyer, D. B., Falkai, P., &amp; Koutsouleris, N. (2018). Machine learning approaches for clinical psychology and psychiatry. Annual Review of Clinical Psychology, 14(1), 91–118. https://doi.org/10.1146/annurev-clinpsy-032816-045037 Eaton, W. W. (1980). The sociology of mental disorders. Praeger. Eddy, D. M. (1982). Probabilistic reasoning in clinical medicine: Problems and opportunities. In D. Kahneman, P. Slovic, &amp; A. Tversky (Eds.), Judgment under uncertainty: Heuristics and biases (pp. 249–267). Cambridge University Press. Edwards, J. R. (2011). The fallacy of formative measurement. Organizational Research Methods, 14(2), 370–388. https://doi.org/10.1177/1094428110378369 Edwards, J. R., &amp; Bagozzi, R. P. (2000). On the nature and direction of relationships between constructs and measures. Psychological Methods, 5(2), 155–174. https://doi.org/10.1037/1082-989X.5.2.155 Edwards, L. M., Burkard, A. W., Adams, H. A., &amp; Newcomb, S. A. (2017). A mixed-method study of psychologists’ use of multicultural assessment. Professional Psychology: Research and Practice, 48(2), 131–138. https://doi.org/10.1037/pro0000095 Ellard, K. K., Fairholme, C. P., Boisseau, C. L., Farchione, T. J., &amp; Barlow, D. H. (2010). Unified protocol for the transdiagnostic treatment of emotional disorders: Protocol development and initial outcome data. Cognitive and Behavioral Practice, 17(1), 88–101. https://doi.org/10.1016/j.cbpra.2009.06.002 Embretson, S. E. (1996). The new rules of measurement. Psychological Assessment, 8, 341–349. https://doi.org/10.1037/1040-3590.8.4.341 Embretson, S. E., &amp; Reise, S. P. (2000). Item response theory for psychologists (Vol. 4). Lawrence Erlbaum Associates. Epskamp, S. (2022). semPlot: Path diagrams and visual analysis of various SEM packages’ output. https://github.com/SachaEpskamp/semPlot Evans, S. C., &amp; Shaughnessy, S. (in press). Emotion regulation as central to psychopathology across childhood and adolescence: A commentary on Nobakht et al. (2023). Journal of Child Psychology and Psychiatry. https://doi.org/https://doi.org/10.1111/jcpp.13910 Executive Board of the American Anthropological Association. (1998). AAA statement on race. American Anthropologist, 100(3), 712–713. https://doi.org/10.1525/aa.1998.100.3.712 Exner, J. E. (1974). The Rorschach: A comprehensive system. John Wiley &amp; Sons. Exner, J. E., &amp; Erdberg, S. P. (2005). The Rorschach, a comprehensive system: Advanced interpretation (3rd ed., Vol. 2). John Wiley &amp; Sons, Inc. Fadus, M. C., Ginsburg, K. R., Sobowale, K., Halliday-Boykins, C. A., Bryant, B. E., Gray, K. M., &amp; Squeglia, L. M. (2020). Unconscious bias and the diagnosis of disruptive behavior disorders and ADHD in african american and hispanic youth. Academic Psychiatry, 44(1), 95–102. https://doi.org/10.1007/s40596-019-01127-6 Falotico, R., &amp; Quatto, P. (2010). On avoiding paradoxes in assessing inter-rater agreement. Italian Journal of Applied Statistics, 22, 151–160. Faraone, S. V., &amp; Tsuang, M. T. (1994). Measuring diagnostic accuracy in the absence of a “gold standard.” American Journal of Psychiatry, 151, 650–657. https://doi.org/10.1176/ajp.151.5.650 Farrington, D. P., &amp; Loeber, R. (1989). Relative improvement over chance (RIOC) and phi as measures of predictive efficiency and strength of association in 2×2 tables. Journal of Quantitative Criminology, 5(3), 201–213. https://doi.org/10.1007/BF01062737 Farris, C., Treat, T. A., Viken, R. J., &amp; McFall, R. M. (2008). Perceptual mechanisms that characterize gender differences in decoding women’s sexual intent. Psychological Science, 19(4), 348–354. https://doi.org/10.1111/j.1467-9280.2008.02092.x Farris, C., Viken, R. J., Treat, T. A., &amp; McFall, R. M. (2006). Heterosocial perceptual organization: Application of the choice model to sexual coercion. Psychological Science (0956-7976), 17(10), 869–875. https://doi.org/10.1111/j.1467-9280.2006.01796.x Faul, F., Erdfelder, E., Buchner, A., &amp; Lang, A.-G. (2009). Statistical power analyses using g*power 3.1: Tests for correlation and regression analyses. Behavior Research Methods, 41(4), 1149–1160. https://doi.org/10.3758/brm.41.4.1149 Fernández, A. L., &amp; Abe, J. (2018). Bias in cross-cultural neuropsychological testing: Problems and possible solutions. Culture and Brain, 6(1), 1–35. https://doi.org/10.1007/s40167-017-0050-2 Fiske, D. W., &amp; Campbell, D. T. (1992). Citations do not solve problems. Psychological Bulletin, 112(3), 393–395. https://doi.org/10.1037/0033-2909.112.3.393 Fletcher, R. R., Nakeshimana, A., &amp; Olubeko, O. (2021). Addressing fairness, bias, and appropriate use of artificial intelligence and machine learning in global health. Frontiers in Artificial Intelligence, 3(116). https://doi.org/10.3389/frai.2020.561802 Flora, D. B. (2020). Your coefficient alpha is probably wrong, but which coefficient omega is right? A tutorial on using R to obtain better reliability estimates. Advances in Methods and Practices in Psychological Science, 3(4), 484–501. https://doi.org/10.1177/2515245920951747 Floyd, F. J., &amp; Widaman, K. F. (1995). Factor analysis in the development and refinement of clinical assessment instruments. Psychological Assessment, 7, 286–299. https://doi.org/10.1037/1040-3590.7.3.286 Fontaine, N. M. G., &amp; Petersen, I. T. (2017). Developmental trajectories of psychopathology: An overview of approaches and applications. In L. Centifanti &amp; D. Williams (Eds.), The wiley handbook of developmental psychopathology (pp. 5–28). Wiley-Blackwell. Forbey, J. D., &amp; Ben-Porath, Y. S. (2007). Computerized adaptive personality testing: A review and illustration with the MMPI-2 computerized adaptive version. Psychological Assessment, 19(1), 14–24. https://doi.org/10.1037/1040-3590.19.1.14 Fornell, C., &amp; Larcker, D. F. (1981). Evaluating structural equation models with unobservable variables and measurement error. Journal of Marketing Research, 18(1), 39–50. https://doi.org/10.2307/3151312 Fox, J., Weisberg, S., &amp; Price, B. (2022). Car: Companion to applied regression. https://CRAN.R-project.org/package=car Frank, L. K. (1939). Projective methods for the study of personality. Journal of Psychology, 8, 389–413. https://doi.org/10.1080/00223980.1939.9917671 Frazier, T. W., Georgiades, S., Bishop, S. L., &amp; Hardan, A. Y. (2014). Behavioral and cognitive characteristics of females and males with autism in the simons simplex collection. Journal of the American Academy of Child &amp; Adolescent Psychiatry, 53(3), 329–340.e3. https://doi.org/10.1016/j.jaac.2013.12.004 Freese, J., &amp; Peterson, D. (2017). Replication in social science. Annual Review of Sociology. Freud, S. (1911). Psycho-analytic notes on an autobiographical account of a case of paranoia (dementia paranoides). In J. Strachey (Ed.), The standard edition of the complete psychological works of Sigmund Freud: The case of Schreber, papers on technique and other works, Vol. 12 (1911–1913) (pp. 1–82). Fried, E. I. (2022). Studying mental health problems as systems, not syndromes. Current Directions in Psychological Science, 31(6), 500–508. https://doi.org/10.1177/09637214221114089 Furr, R. M. (2017). Psychometrics: An introduction. SAGE publications. Furr, R. M., &amp; Heuckeroth, S. (2019). The “quantifying construct validity” procedure: Its role, value, interpretations, and computation. Assessment, 26(4), 555–566. https://doi.org/10.1177/1073191118820638 Galatzer-Levy, I. R., &amp; Bryant, R. A. (2013). 636,120 ways to have posttraumatic stress disorder. Perspectives on Psychological Science, 8(6), 651–662. https://doi.org/10.1177/1745691613504115 Galatzer-Levy, I. R., &amp; Onnela, J.-P. (2023). Machine learning and the digital measurement of psychological health. Annual Review of Clinical Psychology, 19, 133–154. https://doi.org/10.1146/annurev-clinpsy-080921-073212 Gambrill, E. (2014). The diagnostic and statistical manual of mental disorders as a major form of dehumanization in the modern world. Research on Social Work Practice, 24(1), 13–36. https://doi.org/10.1177/1049731513499411 Gandrud, C. (2020). Reproducible research with R and R studio (3rd ed.). CRC Press. https://www.routledge.com/Reproducible-Research-with-R-and-RStudio/Gandrud/p/book/9780367143985 Garb, H. N. (1997). Race bias, social class bias, and gender bias in clinical judgment. Clinical Psychology: Science and Practice, 4(2), 99–120. https://doi.org/10.1111/j.1468-2850.1997.tb00104.x Garb, H. N. (2005). Clinical judgment and decision making. Annual Review of Clinical Psychology, 1, 67–89. https://doi.org/10.1146/annurev.clinpsy.1.102803.143810 Garb, H. N. (2007). Computer-administered interviews and rating scales. Psychological Assessment, 19(1), 4–13. https://doi.org/10.1037/1040-3590.19.1.4 Garb, H. N., &amp; Wood, J. M. (2019). Methodological advances in statistical prediction. Psychological Assessment, 31(12), 1456–1466. https://doi.org/10.1037/pas0000673 Garb, H. N., Wood, J. M., Lilienfeld, S. O., &amp; Nezworski, M. T. (2005). Roots of the Rorschach controversy. Clinical Psychology Review, 25(1), 97–118. https://doi.org/10.1016/j.cpr.2004.09.002 Garber, J., &amp; Weersing, V. R. (2010). Comorbidity of anxiety and depression in youth: Implications for treatment and prevention. Clinical Psychology: Science and Practice, 17(4), 293–306. https://doi.org/10.1111/j.1468-2850.2010.01221.x Gelman, A., &amp; Loken, E. (2013). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time. Department of Statistics, Columbia University. Gibbons, R. D., Weiss, D. J., Frank, E., &amp; Kupfer, D. (2016). Computerized adaptive diagnosis and testing of mental health disorders. Annual Review of Clinical Psychology, 12(1), 83–104. https://doi.org/10.1146/annurev-clinpsy-021815-093634 Gilovich, T., Vallone, R., &amp; Tversky, A. (1985). The hot hand in basketball: On the misperception of random sequences. Cognitive Psychology, 17(3), 295–314. https://doi.org/10.1016/0010-0285(85)90010-6 Gipps, C., &amp; Stobart, G. (2009). Fairness in assessment. In C. Wyatt-Smith &amp; J. J. Cumming (Eds.), Educational assessment in the 21st century: Connecting theory and practice (pp. 105–118). Springer Netherlands. https://doi.org/10.1007/978-1-4020-9964-9_6 Girard, J. M., &amp; Cohn, J. F. (2016). A primer on observational measurement. Assessment, 23(4), 404–413. https://doi.org/10.1177/1073191116635807 Gneiting, T., &amp; Walz, E.-M. (2021). Receiver operating characteristic (ROC) movies, universal ROC (UROC) curves, and coefficient of predictive ability (CPA). Machine Learning. https://doi.org/10.1007/s10994-021-06114-3 Gonzalez, O., &amp; Pelham, W. E. (2021). When does differential item functioning matter for screening? A method for empirical evaluation. Assessment, 28(2), 446–456. https://doi.org/10.1177/1073191120913618 Goodwin, L. D., &amp; Leech, N. L. (2006). Understanding correlation: Factors that affect the size of r. The Journal of Experimental Education, 74(3), 249–266. https://doi.org/10.3200/JEXE.74.3.249-266 Gottfredson, L. S. (1994). The science and politics of race-norming. American Psychologist, 49(11), 955–963. https://doi.org/10.1037/0003-066X.49.11.955 Gottfredson, L. S. (1997). Mainstream science on intelligence: An editorial with 52 signatories, history, and bibliography. Intelligence, 24(1), 13–23. Gottfredson, N. C., Cole, V. T., Giordano, M. L., Bauer, D. J., Hussong, A. M., &amp; Ennett, S. T. (2019). Simplifying the implementation of modern scale scoring methods with an automated R package: Automated moderated nonlinear factor analysis (aMNLFA). Addictive Behaviors, 94, 65–73. https://doi.org/10.1016/j.addbeh.2018.10.031 Graham, J. M. (2006). Congeneric and (essentially) tau-equivalent estimates of score reliability: What they are and how to use them. Educational and Psychological Measurement, 66(6), 930–944. https://doi.org/10.1177/0013164406288165 Graham, J. R., Veltri, C. O. C., &amp; Lee, T. T. C. (2022). MMPI instruments: Assessing personality and psychopathology (6th ed.). Oxford University Press. Graham, J., Olchowski, A., &amp; Gilreath, T. (2007). How many imputations are really needed? Some practical clarifications of multiple imputation theory. Prevention Science, 8(3), 206–213. https://doi.org/10.1007/s11121-007-0070-9 Granziol, U., Brancaccio, A., Pizziconi, G., Spangaro, M., Gentili, F., Bosia, M., Gregori, E., Luperini, C., Pavan, C., Santarelli, V., Cavallaro, R., Cremonese, C., Favaro, A., Rossi, A., Vidotto, G., &amp; Spoto, A. (2022). On the implementation of computerized adaptive observations for psychological assessment. Assessment, 29(2), 225–241. https://doi.org/10.1177/1073191120960215 Green, S. B., &amp; Yang, Y. (2015). Evaluation of dimensionality in the assessment of internal consistency reliability: Coefficient alpha and omega coefficients. Educational Measurement: Issues and Practice, 34(4), 14–20. https://doi.org/10.1111/emip.12100 Greenberg, D. M., Warrier, V., Allison, C., &amp; Baron-Cohen, S. (2018). Testing the empathizing–systemizing theory of sex differences and the extreme male brain theory of autism in half a million people. Proceedings of the National Academy of Sciences, 115(48), 12152–12157. https://doi.org/10.1073/pnas.1811032115 Grove, W. M., &amp; Meehl, P. E. (1996). Comparative efficiency of informal (subjective, impressionistic) and formal (mechanical, algorithmic) prediction procedures: The clinical–statistical controversy. Psychology, Public Policy, and Law, 2(2), 293–323. https://doi.org/10.1037/1076-8971.2.2.293 Grove, W. M., Zald, D. H., Lebow, B. S., Snitz, B. E., &amp; Nelson, C. (2000). Clinical versus mechanical prediction: A meta-analysis. Psychological Assessment, 12(1), 19–30. https://doi.org/10.1037/1040-3590.12.1.19 Gunn, H. J., Grimm, K. J., &amp; Edwards, M. C. (2020). Evaluation of six effect size measures of measurement non-invariance for continuous outcomes. Structural Equation Modeling: A Multidisciplinary Journal, 27(4), 503–514. https://doi.org/10.1080/10705511.2019.1689507 Gwet, K. L. (2008). Computing inter-rater reliability and its variance in the presence of high agreement. British Journal of Mathematical and Statistical Psychology, 61(1), 29–48. https://doi.org/10.1348/000711006X126600 Gwet, K. L. (2021a). Handbook of inter-rater reliability: The definitive guide to measuring the extent of agreement among raters, Vol. 1: Analysis of categorical ratings (5th ed.). AgreeStat Analytics. Gwet, K. L. (2021b). Handbook of inter-rater reliability: The definitive guide to measuring the extent of agreement among raters, Vol. 2: Analysis of quantitative ratings (5th ed.). AgreeStat Analytics. Hagquist, C. (2019). Explaining differential item functioning focusing on the crucial role of external information – an example from the measurement of adolescent mental health. BMC Medical Research Methodology, 19(1), 185. https://doi.org/10.1186/s12874-019-0828-3 Hagquist, C., &amp; Andrich, D. (2017). Recent advances in analysis of differential item functioning in health research using the Rasch model. Health and Quality of Life Outcomes, 15(1), 181. https://doi.org/10.1186/s12955-017-0755-0 Hall, G. C. N., Bansal, A., &amp; Lopez, I. R. (1999). Ethnicity and psychopathology: A meta-analytic review of 31 years of comparative MMPI/MMPI-2 research. Psychological Assessment, 11(2), 186–197. https://doi.org/10.1037/1040-3590.11.2.186 Hamaker, E. L., Kuiper, R. M., &amp; Grasman, R. P. P. P. (2015). A critique of the cross-lagged panel model. Psychological Methods, 20(1), 102–116. https://doi.org/10.1037/a0038889 Han, K., Colarelli, S. M., &amp; Weed, N. C. (2019). Methodological and statistical advances in the consideration of cultural diversity in assessment: A critical review of group classification and measurement invariance testing. Psychological Assessment, 31(12), 1481–1496. https://doi.org/10.1037/pas0000731 Hancock, G. R., &amp; French, B. F. (2013). Power analysis in structural equation modeling. In Structural equation modeling: A second course, 2nd ed. (pp. 117–159). IAP Information Age Publishing. Hardin, A. M., Chang, J. C.-J., Fuller, M. A., &amp; Torkzadeh, G. (2011). Formative measurement and academic research: In search of measurement theory. Educational and Psychological Measurement, 71(2), 281–305. https://doi.org/10.1177/0013164410370208 Harrell, F. (2015). Regression modeling strategies: With applications to linear models, logistic and ordinal regression, and survival analysis. Springer. Harrell, Jr., F. E. (2021). rms: Regression modeling strategies. https://CRAN.R-project.org/package=rms Hayes, A. F., &amp; Coutts, J. J. (2020). Use omega rather than cronbach’s alpha for estimating reliability. but…. Communication Methods and Measures, 14(1), 1–24. https://doi.org/10.1080/19312458.2020.1718629 Hayes, S. C., Nelson, R. O., &amp; Jarrett, R. B. (1987). The treatment utility of assessment: A functional approach to evaluating assessment quality. American Psychologist, 42, 963–974. https://doi.org/10.1037/0003-066X.42.11.963 Haynes, S. N. (2001). Clinical applications of analogue behavioral observation: Dimensions of psychometric evaluation. Psychological Assessment, 13(1), 73–85. https://doi.org/10.1037/1040-3590.13.1.73 Haynes, S. N., &amp; Yoshioka, D. T. (2007). Clinical assessment applications of ambulatory biosensors. Psychological Assessment, 19(1), 44–57. https://doi.org/10.1037/1040-3590.19.1.44 Hays, P. A. (2016). Addressing cultural complexities in practice: Assessment, diagnosis, and therapy. American Psychological Association. Hedge, C., Powell, G., &amp; Sumner, P. (2018). The reliability paradox: Why robust cognitive tasks do not produce reliable individual differences. Behavior Research Methods, 50(3), 1166–1186. https://doi.org/10.3758/s13428-017-0935-1 Helms, J. E. (2006). Fairness is not validity or cultural bias in racial-group assessment: A quantitative perspective. American Psychologist, 61(8), 845–859. https://doi.org/10.1037/0003-066X.61.8.845 Helms, J. E., Jernigan, M., &amp; Mascher, J. (2005). The meaning of race in psychology and how to change it: A methodological perspective. American Psychologist, 60(1), 27–36. https://doi.org/10.1037/0003-066X.60.1.27 Henrich, J., Heine, S. J., &amp; Norenzayan, A. (2010). Most people are not WEIRD. Nature, 466(7302), 29–29. https://doi.org/10.1038/466029a Henseler, J., Ringle, C. M., &amp; Sarstedt, M. (2015). A new criterion for assessing discriminant validity in variance-based structural equation modeling. Journal of the Academy of Marketing Science, 43(1), 115–135. https://doi.org/10.1007/s11747-014-0403-8 Hertzog, C., &amp; Nesselroade, J. R. (2003). Assessing psychological change in adulthood: An overview of methodological issues. Psychology and Aging, 18(4), 639–657. https://doi.org/10.1037/0882-7974.18.4.639 Himmelstein, P. H., Woods, W. C., &amp; Wright, A. G. C. (2019). A comparison of signal- and event-contingent ambulatory assessment of interpersonal behavior and affect in social situations. Psychological Assessment, 31(7), 952–960. https://doi.org/10.1037/pas0000718 Hinshaw, S. P., &amp; Nigg, J. T. (1999). Behavior rating scales in the assessment of disruptive behavior problems in childhood. In D. Shaffer, C. P. Lucas, &amp; J. E. Richters (Eds.), Diagnostic assessment in child and adolescent psychopathology. (pp. 91–126). The Guilford Press. Hoch, S. J. (1985). Counterfactual reasoning and accuracy in predicting personal events. Journal of Experimental Psychology: Learning, Memory, and Cognition, 11(4), 719–731. https://doi.org/10.1037/0278-7393.11.1-4.719 Holmlund, T. B., Foltz, P. W., Cohen, A. S., Johansen, H. D., Sigurdsen, R., Fugelli, P., Bergsager, D., Cheng, J., Bernstein, J., Rosenfeld, E., &amp; Elvevåg, B. (2019). Moving psychological assessment out of the controlled laboratory setting: Practical challenges. Psychological Assessment, 31(3), 292–303. https://doi.org/10.1037/pas0000647 Hough, S. E. (2016). Predicting the unpredictable: The tumultuous science of earthquake prediction. Princeton University Press. Howell, R. D., Breivik, E., &amp; Wilcox, J. B. (2007). Reconsidering formative measurement. Psychological Methods, 12(2), 205–218. https://doi.org/10.1037/1082-989X.12.2.205 Huebner, A., &amp; Lucht, M. (2019). Generalizability theory in R. Practical Assessment, Research &amp; Evaluation, 24(5), 2. https://doi.org/10.7275/5065-gc10 Hunsley, J., Lee, C. M., Wood, J. M., &amp; Taylor, W. (2015). Controversial and questionable assessment techniques. In S. O. Lilienfeld, S. J. Lynn, &amp; J. M. Lohr (Eds.), Science and pseudoscience in clinical psychology (2nd ed., pp. 42–82). The Guilford Press. Hunsley, J., &amp; Mash, E. J. (2007). Evidence-based assessment. Annual Review of Clinical Psychology, 3, 29–51. https://doi.org/10.1146/annurev.clinpsy.3.022806.091419 Hurlburt, R. T. (1997). Randomly sampling thinking in the natural environment. Journal of Consulting and Clinical Psychology, 65(6), 941–949. https://doi.org/10.1037/0022-006X.65.6.941 Hussong, A. M., Bauer, D. J., Giordano, M. L., &amp; Curran, P. J. (2020). Harmonizing altered measures in integrative data analysis: A methods analogue study. Behavior Research Methods. https://doi.org/10.3758/s13428-020-01472-7 Hussong, A. M., Curran, P. J., &amp; Bauer, D. J. (2013). Integrative data analysis in clinical psychology research. Annual Review of Clinical Psychology, 9(1), 61–89. https://doi.org/10.1146/annurev-clinpsy-050212-185522 Hyndman, R. J., &amp; Athanasopoulos, G. (2018). Forecasting: Principles and practice (2nd ed.). OTexts. Jensen, A. R. (1980). Précis of bias in mental testing. Behavioral and Brain Sciences, 3(3), 325–333. https://doi.org/10.1017/S0140525X00005161 Jiang, Z. (2018). Using the linear mixed-effect model framework to estimate generalizability variance components in R. Methodology, 14(3), 133–142. https://doi.org/10.1027/1614-2241/a000149 John, L. K., Loewenstein, G., &amp; Prelec, D. (2012). Measuring the prevalence of questionable research practices with incentives for truth telling. Psychological Science, 23(5), 524–532. https://doi.org/10.1177/0956797611430953 Johnson, J. E. V., &amp; Bruce, A. C. (2001). Calibration of subjective probability judgments in a naturalistic setting. Organizational Behavior and Human Decision Processes, 85(2), 265–290. https://doi.org/10.1006/obhd.2000.2949 Johnson, P. E. (2022). rockchalk: Regression estimation and presentation. https://CRAN.R-project.org/package=rockchalk Jonson, J. L., &amp; Geisinger, K. F. (2022). Fairness in educational and psychological testing: Examining theoretical, research, practice, and policy implications of the 2014 standards. American Educational Research Association,. Jorgensen, T. D., Kite, B. A., Chen, P.-Y., &amp; Short, S. D. (2018). Permutation randomization methods for testing measurement equivalence and detecting differential item functioning in multiple-group confirmatory factor analysis. Psychological Methods, 23(4), 708–728. https://doi.org/10.1037/met0000152 Jorgensen, T. D., Pornprasertmanit, S., Schoemann, A. M., &amp; Rosseel, Y. (2021). semTools: Useful tools for structural equation modeling. https://github.com/simsem/semTools/wiki Kagan, J. (1969). The three faces of continuity in human development. In D. A. Goslin (Ed.), Handbook of socialization theory and research (pp. 983–1002). Rand McNally. Kazdin, A. E. (1995). Preparing and evaluating research reports. Psychological Assessment, 7(3), 228–237. https://doi.org/10.1037/1040-3590.7.3.228 Kelley, K. (2020). MBESS: The MBESS R package. http://nd.edu/~kkelley/site/MBESS.html Kelley, K., &amp; Pornprasertmanit, S. (2016). Confidence intervals for population reliability coefficients: Evaluation of methods, recommendations, and software for composite measures. Psychological Methods, 21(1), 69–92. https://doi.org/10.1037/a0040086 Keren, G. (1987). Facing uncertainty in the game of bridge: A calibration study. Organizational Behavior and Human Decision Processes, 39(1), 98–114. https://doi.org/10.1016/0749-5978(87)90047-1 Kessler, R. C., Bossarte, R. M., Luedtke, A., Zaslavsky, A. M., &amp; Zubizarreta, J. R. (2020). Suicide prediction models: A critical review of recent research with recommendations for the way forward. Molecular Psychiatry, 25(1), 168–179. https://doi.org/10.1038/s41380-019-0531-0 Kievit, R. A., Brandmaier, A. M., Ziegler, G., Harmelen, A.-L. van, Mooij, S. M. M. de, Moutoussis, M., Goodyer, I., Bullmore, E., Jones, P. B., Fonagy, P., Lindenberger, U., &amp; Dolan, R. J. (2018). Developmental cognitive neuroscience using latent change score models: A tutorial and applications. Developmental Cognitive Neuroscience, 33, 99–117. https://doi.org/10.1016/j.dcn.2017.11.007 Kievit, R., Frankenhuis, W., Waldorp, L., &amp; Borsboom, D. (2013). Simpson’s paradox in psychological science: A practical guide. Frontiers in Psychology, 4(513). https://doi.org/10.3389/fpsyg.2013.00513 Klein, D. F., &amp; Cleary, T. A. (1969). Platonic true scores: Further comment. Psychological Bulletin, 71(4), 278–280. https://doi.org/10.1037/h0026852 Kline, R. B. (2023). Principles and practice of structural equation modeling (5th ed.). Guilford Publications. Koehler, D. J., Brenner, L., &amp; Griffin, D. (2002). The calibration of expert judgment: Heuristics and biases beyond the laboratory. In T. Gilovich, D. Griffin, &amp; D. Kahneman (Eds.), Heuristics and biases: The psychology of intuitive judgment. Cambridge University Press. Koriat, A., Lichtenstein, S., &amp; Fischhoff, B. (1980). Reasons for confidence. Journal of Experimental Psychology: Human Learning and Memory, 6(2), 107–118. https://doi.org/10.1037/0278-7393.6.2.107 Korotitsch, W. J., &amp; Nelson-Gray, R. O. (1999). An overview of self-monitoring research in assessment and treatment. Psychological Assessment, 11(4), 415–425. https://doi.org/10.1037/1040-3590.11.4.415 Kotov, R., Krueger, R. F., Watson, D., Achenbach, T. M., Althoff, R. R., Bagby, R. M., Brown, T. A., Carpenter, W. T., Caspi, A., Clark, L. A., Eaton, N. R., Forbes, M. K., Forbush, K. T., Goldberg, D., Hasin, D., Hyman, S. E., Ivanova, M. Y., Lynam, D. R., Markon, K., … Zimmerman, M. (2017). The hierarchical taxonomy of psychopathology (HiTOP): A dimensional alternative to traditional nosologies. Journal of Abnormal Psychology, 126(4), 454–477. https://doi.org/10.1037/abn0000258 Kotov, R., Krueger, R. F., Watson, D., Cicero, D. C., Conway, C. C., DeYoung, C. G., Eaton, N. R., Forbes, M. K., Hallquist, M. N., Latzman, R. D., Mullins-Sweatt, S. N., Ruggero, C. J., Simms, L. J., Waldman, I. D., Waszczuk, M. A., &amp; Wright, A. G. C. (2021). The hierarchical taxonomy of psychopathology (HiTOP): A quantitative nosology based on consensus of evidence. Annual Review of Clinical Psychology, 17(1), 83–108. https://doi.org/10.1146/annurev-clinpsy-081219-093304 Kozak, M. J., &amp; Cuthbert, B. N. (2016). The NIMH research domain criteria initiative: Background, issues, and pragmatics. Psychophysiology, 53(3), 286–297. https://doi.org/10.1111/psyp.12518 Kriegman, L. S., &amp; Kriegman, G. (1965). The PaTE report: A new psychodynamic and therapeutic evaluative procedure. The Psychiatric Quarterly, 39(1), 646–674. https://doi.org/10.1007/BF01569493 Krosnick, J. A. (1999). Survey research. Annual Review of Psychology, 50, 537–567. https://doi.org/10.1146/annurev.psych.50.1.537 Krueger, R. F., Nichol, P. E., Hicks, B. M., Markon, K. E., Patrick, C. J., lacono, W. G., &amp; McGue, M. (2004). Using latent trait modeling to conceptualize an alcohol problems continuum. Psychological Assessment, 16(2), 107–119. https://doi.org/10.1037/1040-3590.16.2.107 Kuhn, M. (2022). caret: Classification and regression training. https://github.com/topepo/caret/ Kuncel, N. R., &amp; Hezlett, S. A. (2010). Fact and fiction in cognitive ability testing for admissions and hiring decisions. Current Directions in Psychological Science, 19(6), 339–345. https://doi.org/10.1177/0963721410389459 Kundu, S., Aulchenko, Y. S., &amp; Janssens, A. C. J. W. (2020). PredictABEL: Assessment of risk prediction models. https://CRAN.R-project.org/package=PredictABEL Lai, M. H. C. (2021). Adjusting for measurement noninvariance with alignment in growth modeling. Multivariate Behavioral Research, 1–18. https://doi.org/10.1080/00273171.2021.1941730 Larson, M. J., &amp; Carbine, K. A. (2017). Sample size calculations in human electrophysiology (EEG and ERP) studies: A systematic review and recommendations for increased rigor. International Journal of Psychophysiology, 111, 33–41. https://doi.org/10.1016/j.ijpsycho.2016.06.015 Lee, K., Bull, R., &amp; Ho, R. M. H. (2013). Developmental changes in executive functioning. Child Development, 84(6), 1933–1953. https://doi.org/10.1111/cdev.12096 Lee Meeuw Kjoe, P. R., Agelink van Rentergem, J. A., Vermeulen, I. E., &amp; Schagen, S. B. (2021). How to correct for computer experience in online cognitive testing? Assessment, 28(5), 1247–1255. https://doi.org/10.1177/1073191120911098 Lek, K. M., &amp; Van De Schoot, R. (2018). A comparison of the single, conditional and person-specific standard error of measurement: What do they measure and when to use them? Frontiers in Applied Mathematics and Statistics, 4(40). https://doi.org/10.3389/fams.2018.00040 Lele, S. R., Keim, J. L., &amp; Solymos, P. (2019). ResourceSelection: Resource selection (probability) functions for use-availability data. https://github.com/psolymos/ResourceSelection Leong, F. T. L., &amp; Kalibatseva, Z. (2016). Threats to cultural validity in clinical diagnosis and assessment: Illustrated with the case of Asian Americans. In N. Zane, G. Bernal, &amp; F. T. L. Leong (Eds.), Evidence-based psychological practice with ethnic minorities: Culturally informed research and clinical strategies (pp. 57–74). American Psychological Association. Lewis-Fernández, R., Aggarwal, N. K., Bäärnhielm, S., Rohlof, H., Kirmayer, L. J., Weiss, M. G., Jadhav, S., Hinton, L., Alarcón, R. D., Bhugra, D., Groen, S., Dijk, R. van, Qureshi, A., Collazos, F., Rousseau, C., Caballero, L., Ramos, M., &amp; Lu, F. (2014). Culture and psychiatric evaluation: Operationalizing cultural formulation for DSM-5. Psychiatry: Interpersonal and Biological Processes, 77(2), 130–154. https://doi.org/10.1521/psyc.2014.77.2.130 Lilienfeld, S. O. (2007). Psychological treatments that cause harm. Perspectives on Psychological Science, 2(1), 53–70. https://doi.org/10.1111/j.1745-6916.2007.00029.x Lilienfeld, S. O. (2017). Psychology’s replication crisis and the grant culture: Righting the ship. Perspectives on Psychological Science, 12(4), 660–664. https://doi.org/10.1177/1745691616687745 Lilienfeld, S. O., Sauvigne, K., Lynn, S. J., Latzman, R. D., Cautin, R., &amp; Waldman, I. D. (2015). Fifty psychological and psychiatric terms to avoid: A list of inaccurate, misleading, misused, ambiguous, and logically confused words and phrases. Frontiers in Psychology, 6. https://doi.org/10.3389/fpsyg.2015.01100 Lilienfeld, S. O., Wood, J. M., &amp; Garb, H. N. (2000). The scientific status of projective techniques. Psychological Science in the Public Interest, 1, 27–66. https://doi.org/10.1111/1529-1006.002 Lindhiem, O., Petersen, I. T., Mentch, L. K., &amp; Youngstrom, E. A. (2020). The importance of calibration in clinical psychology. Assessment, 27(4), 840–854. https://doi.org/10.1177/1073191117752055 Lindhiem, O., Yu, L., Grasso, D. J., Kolko, D. J., &amp; Youngstrom, E. A. (2015). Adapting the posterior probability of diagnosis index to enhance evidence-based screening: An application to ADHD in primary care. Assessment, 22(2), 198–207. https://doi.org/10.1177/1073191114540748 Lindzey, G. (1952). Thematic apperception test: Interpretive assumptions and related empirical evidence. Psychological Bulletin, 49, 1–25. https://doi.org/10.1037/h0062363 Little, T. D. (2013). Longitudinal structural equation modeling. The Guilford Press. Little, T. D., Preacher, K. J., Selig, J. P., &amp; Card, N. A. (2007). New developments in latent variable panel analyses of longitudinal data. International Journal of Behavioral Development, 31(4), 357–365. https://doi.org/10.1177/0165025407077757 Little, T. D., Slegers, D. W., &amp; Card, N. A. (2006). A non-arbitrary method of identifying and scaling latent variables in SEM and MACS models. Structural Equation Modeling, 13(1), 59–72. https://doi.org/10.1207/s15328007sem1301_3 Liu, Y., Millsap, R. E., West, S. G., Tein, J.-Y., Tanaka, R., &amp; Grimm, K. J. (2017). Testing measurement invariance in longitudinal data with ordered-categorical measures. Psychological Methods, 22(3), 486–506. https://doi.org/10.1037/met0000075 Lobbestael, J., Leurgans, M., &amp; Arntz, A. (2011). Inter-rater reliability of the Structured Clinical Interview for DSM-IV Axis I Disorders (SCID I) and Axis II Disorders (SCID II). Clinical Psychology &amp; Psychotherapy, 18(1), 75–79. https://doi.org/10.1002/cpp.693 Loevinger, J. (1957). Objective tests as instruments of psychological theory. Psychological Reports, 3(3), 635–694. https://doi.org/10.2466/pr0.1957.3.3.635 Loken, E., &amp; Gelman, A. (2017). Measurement error and the replication crisis. Science, 355(6325), 584–585. https://doi.org/10.1126/science.aal3618 Lubke, G. H., McArtor, D. B., Boomsma, D. I., &amp; Bartels, M. (2018). Genetic and environmental contributions to the development of childhood aggression. Developmental Psychology, 54(1), 39–50. https://doi.org/10.1037/dev0000403 Lüdecke, D., Ben-Shachar, M. S., Patil, I., Waggoner, P., &amp; Makowski, D. (2021). performance: An R package for assessment, comparison and testing of statistical models. Journal of Open Source Software, 6(60), 3139. https://doi.org/10.21105/joss.03139 Lupien, S. J., Sasseville, M., François, N., Giguère, C. E., Boissonneault, J., Plusquellec, P., Godbout, R., Xiong, L., Potvin, S., Kouassi, E., &amp; Lesage, A. (2017). The DSM5/RDoC debate on the future of mental health research: Implication for studies on human stress and presentation of the signature bank. Stress, 20(1), 2–18. https://doi.org/10.1080/10253890.2017.1286324 Lutz, W., Schwartz, B., &amp; Delgadillo, J. (2022). Measurement-based and data-informed psychological therapy. Annual Review of Clinical Psychology, 18(1), 71–98. https://doi.org/10.1146/annurev-clinpsy-071720-014821 Lysell, H., Dahlin, M., Viktorin, A., Ljungberg, E., D’Onofrio, B. M., Dickman, P., &amp; Runeson, B. (2018). Maternal suicide – register based study of all suicides occurring after delivery in sweden 1974–2009. PLOS ONE, 13(1), e0190133. https://doi.org/10.1371/journal.pone.0190133 MacCallum, R. C., &amp; Austin, J. T. (2000). Applications of structural equation modeling in psychological research. Annual Review of Psychology, 51(1), 201–226. https://doi.org/10.1146/annurev.psych.51.1.201 Magis, D. (2013). A note on the item information function of the four-parameter logistic model. Applied Psychological Measurement, 37(4), 304–315. https://doi.org/10.1177/0146621613475471 Makridakis, S., Hogarth, R. M., &amp; Gaba, A. (2009). Forecasting and uncertainty in the economic and business world. International Journal of Forecasting, 25(4), 794–812. https://doi.org/10.1016/j.ijforecast.2009.05.012 Manly, J. J. (2005). Advantages and disadvantages of separate norms for African Americans. The Clinical Neuropsychologist, 19(2), 270–275. https://doi.org/10.1080/13854040590945346 Manly, J. J., &amp; Echemendia, R. J. (2007). Race-specific norms: Using the model of hypertension to understand issues of race, culture, and education in neuropsychology. Archives of Clinical Neuropsychology, 22(3), 319–325. https://doi.org/10.1016/j.acn.2007.01.006 Markon, K. E. (2019). Bifactor and hierarchical models: Specification, inference, and interpretation. Annual Review of Clinical Psychology, 15(1), 51–69. https://doi.org/10.1146/annurev-clinpsy-050718-095522 Markon, K. E., Chmielewski, M., &amp; Miller, C. J. (2011). The reliability and validity of discrete and continuous measures of psychopathology: A quantitative review. Psychological Bulletin, 137(5), 856–879. https://doi.org/10.1037/a0023678 Markus, K. A. (2018). Three conceptual impediments to developing scale theory for formative scales. Methodology, 14(4), 156–164. https://doi.org/10.1027/1614-2241/a000154 Marsh, H. W., Morin, A. J. S., Parker, P. D., &amp; Kaur, G. (2014). Exploratory structural equation modeling: An integration of the best features of exploratory and confirmatory factor analysis. Annual Review of Clinical Psychology, 10(1), 85–110. https://doi.org/10.1146/annurev-clinpsy-032813-153700 Masche, J. G., &amp; Dulmen, M. H. M. van. (2004). Advances in disentangling age, cohort, and time effects: No quadrature of the circle, but a help. Developmental Review, 24(3), 322–342. https://doi.org/10.1016/j.dr.2004.04.002 Matthews, M., Abdullah, S., Murnane, E., Voida, S., Choudhury, T., Gay, G., &amp; Frank, E. (2016). Development and evaluation of a smartphone-based measure of social rhythms for bipolar disorder. Assessment, 23(4), 472–483. https://doi.org/10.1177/1073191116656794 McArdle, J. J., &amp; Grimm, K. J. (2011). An empirical example of change analysis by linking longitudinal item response data from multiple tests. In A. A. von Davier (Ed.), Statistical models for test equating, scaling, and linking (pp. 71–88). Springer Science &amp; Business Media. McArdle, J. J., Grimm, K. J., Hamagami, F., Bowles, R. P., &amp; Meredith, W. (2009). Modeling life-span growth curves of cognition using longitudinal data with multiple samples and changing scales of measurement. Psychological Methods, 14(2), 126–149. https://doi.org/10.1037/a0015857 McClelland, D. C. (1973). Testing for competence rather than for “intelligence.” American Psychologist, 28, 1–14. https://doi.org/10.1037/h0034092 McClelland, D. C. (1994). The knowledge-testing-educational complex strikes back. American Psychologist, 49(1), 66–69. https://doi.org/10.1037/0003-066X.49.1.66 McFall, R. M. (1991). Manifesto for a science of clinical psychology. The Clinical Psychologist, 44(6), 75–91. McFall, R. M. (2000). Elaborate reflections on a simple manifesto. Applied &amp; Preventive Psychology, 9(1), 5–21. https://doi.org/10.1016/s0962-1849(05)80035-6 McNally, R. J. (2021). Network analysis of psychopathology: Controversies and challenges. Annual Review of Clinical Psychology, 17(1), 31–53. https://doi.org/10.1146/annurev-clinpsy-081219-092850 McNeish, D. (2018). Thanks coefficient alpha, we’ll take it from here. Psychological Methods, 23(3), 412–433. https://doi.org/10.1037/met0000144 McNeish, D., &amp; Wolf, M. G. (2023). Dynamic fit index cutoffs for confirmatory factor analysis models. Psychological Methods, 28(1), 61–88. https://doi.org/10.1037/met0000425 McNiel, D. E., &amp; Binder, R. L. (1995). Correlates of accuracy in the assessment of psychiatric inpatients’ risk of violence. American Journal of Psychiatry, 152(6), 901–906. https://doi.org/10.1176/ajp.152.6.901 Meade, A. W. (2010). A taxonomy of effect size measures for the differential functioning of items and scales. Journal of Applied Psychology, 95(4), 728–743. https://doi.org/10.1037/a0018966 Meehl, P. E. (1957). When shall we use our heads instead of the formula? Journal of Counseling Psychology, 4(4), 268–273. https://doi.org/10.1037/h0047554 Meehl, P. E. (1978). Theoretical risks and tabular asterisks: Sir Karl, Sir Ronald, and the slow progress of soft psychology. Journal of Consulting and Clinical Psychology, 46(4), 806–834. https://doi.org/10.1037/0022-006x.46.4.806 Meehl, P. E. (1986). Causes and effects of my disturbing little book. Journal of Personality Assessment, 50(3), 370–375. https://doi.org/10.1207/s15327752jpa5003_6 Meehl, P. E., &amp; Rosen, A. (1955). Antecedent probability and the efficiency of psychometric signs, patterns, or cutting scores. Psychological Bulletin, 52(3), 194–216. https://doi.org/10.1037/h0048070 Melikyan, Z. A., Agranovich, A. V., &amp; Puente, A. E. (2019). Fairness in psychological testing. In G. Goldstein, D. N. Allen, &amp; J. DeLuca (Eds.), Handbook of psychological assessment (fourth edition) (pp. 551–572). Academic Press. https://doi.org/10.1016/B978-0-12-802203-0.00018-3 Metz, C. E., Goodenough, D. J., &amp; Rossmann, K. (1973). Evaluation of receiver operating characteristic curve data in terms of information theory, with applications in radiography. Radiology, 109(2), 297–303. https://doi.org/10.1148/109.2.297 Meyer, G. J., Erard, R. E., Erdberg, P., Mihura, J. L., &amp; Viglione, D. J. (2011). Rorschach Performance Assessment System: Administration, coding, interpretation, and technical manual. Rorschach Performance Asessement Systems LLC. Miller, G. A., Elbert, T., Sutton, B. P., &amp; Heller, W. (2007). Innovative clinical assessment technologies: Challenges and opportunities in neuroimaging. Psychological Assessment, 19(1), 58–73. https://doi.org/10.1037/1040-3590.19.1.58 Miller, G. A., Rockstroh, B. S., Hamilton, H. K., &amp; Yee, C. M. (2016). Psychophysiology as a core strategy in RDoC. Psychophysiology, 53(3), 410–414. https://doi.org/10.1111/psyp.12581 Miller, J. B., &amp; Sanjurjo, A. (2014). A cold shower for the hot hand fallacy. Innocenzo Gasparini Institute for Economic Research. https://repec.unibocconi.it/igier/igi/wp/2014/518.pdf Miller, J. L., Vaillancourt, T., &amp; Boyle, M. H. (2009). Examining the heterotypic continuity of aggression using teacher reports: Results from a national Canadian study. Social Development, 18(1), 164–180. https://doi.org/10.1111/j.1467-9507.2008.00480.x Millsap, R. E. (2011). Statistical approaches to measurement invariance. Taylor &amp; Francis. Moeller, J. (2015). A word on standardization in longitudinal studies: don’t. Frontiers in Psychology, 6(1389), 1–4. https://doi.org/10.3389/fpsyg.2015.01389 Moffitt, T. E. (1993). Adolescence-limited and life-course-persistent antisocial behavior: A developmental taxonomy. Psychological Review, 100(4), 674–701. https://doi.org/10.1037/0033-295X.100.4.674 Moffitt, T. E. (2006a). A review of research on the taxonomy of life-course persistent versus adolescence-limited antisocial behavior. Taking Stock: The Status of Criminological Theory, 15, 277–312. Moffitt, T. E. (2006b). Life-course-persistent versus adolescence-limited antisocial behavior. In D. C. D. J. Cohen (Ed.), Developmental psychopathology, vol 3: Risk, disorder, and adaptation (2nd ed.) (pp. 570–598). John Wiley &amp; Sons Inc. Moore, C. T. (2016). gtheory: Apply generalizability theory with R. http://EvaluationDashboard.com Morgan, C. D., &amp; Murray, H. A. (1935). A method for investigating fantasies: The thematic apperception test. Archives of Neurology &amp; Psychiatry, 34(2), 289–306. https://doi.org/10.1001/archneurpsyc.1935.02250200049005 Morley, S. K., Brito, T. V., &amp; Welling, D. T. (2018). Measures of model performance based on the log accuracy ratio. Space Weather, 16(1), 69–88. https://doi.org/10.1002/2017SW001669 Mullins-Sweatt, S. N., &amp; Widiger, T. A. (2009). Clinical utility and DSM-V. Psychological Assessment, 21(3), 302–312. https://doi.org/10.1037/a0016607 Murphy, A. H., &amp; Winkler, R. L. (1984). Probability forecasting in meterology. Journal of the American Statistical Association, 79(387), 489–500. https://doi.org/10.2307/2288395 Muthén, L. K., &amp; Muthén, B. O. (2002). How to use a Monte Carlo study to decide on sample size and determine power. Structural Equation Modeling: A Multidisciplinary Journal, 9(4), 599–620. https://doi.org/10.1207/s15328007sem0904_8 Muthén, L. K., &amp; Muthén, B. O. (2019). Mplus version 8.4. Muthén &amp; Muthén. Nagy, T. F. (2011). Essential ethics for psychologists: A primer for understanding and mastering core issues (pp. x, 252–x, 252). American Psychological Association. Nelson-Gray, R. O. (2003). Treatment utility of psychological assessment. Psychological Assessment, 15(4), 521–531. https://doi.org/10.1037/1040-3590.15.4.521 Nisbett, R. E., &amp; Wilson, T. D. (1977). Telling more than we can know: Verbal reports on mental processes. Psychological Review, 84(3), 231–259. https://doi.org/10.1037/0033-295x.84.3.231 Nunnally, J. C., &amp; Bernstein, I. H. (1994). Psychometric theory (3rd ed.). McGraw-Hill. Nye, C. D., Bradburn, J., Olenick, J., Bialko, C., &amp; Drasgow, F. (2019). How big are my effects? Examining the magnitude of effect sizes in studies of measurement equivalence. Organizational Research Methods, 22(3), 678–709. https://doi.org/10.1177/1094428118761122 Oberski, D. L. (2014). Evaluating sensitivity of parameters of interest to measurement invariance in latent variable models. Political Analysis, 22(1), 45–60. https://doi.org/10.1093/pan/mpt014 Oberski, D. L., Vermunt, J. K., &amp; Moors, G. B. D. (2015). Evaluating measurement invariance in categorical data latent variable models with the EPC-interest. Political Analysis, 23(4), 550–563. https://doi.org/10.1093/pan/mpv020 Okazaki, S., &amp; Sue, S. (1995). Methodological issues in assessment research with ethnic minorities. Psychological Assessment, 7(3), 367–375. https://doi.org/10.1037/1040-3590.7.3.367 Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251). https://doi.org/10.1126/science.aac4716 Orth, U., Clark, D. A., Donnellan, M. B., &amp; Robins, R. W. (2021). Testing prospective effects in longitudinal research: Comparing seven competing cross-lagged models. Journal of Personality and Social Psychology, 120(4), 1013–1034. https://doi.org/10.1037/pspp0000358 Oskamp, S. (1965). Overconfidence in case-study judgments. Journal of Consulting Psychology, 29(3), 261–265. https://doi.org/10.1037/h0022125 Park, D. C., &amp; Bischof, G. N. (2013). The aging mind: Neuroplasticity in response to cognitive training. Dialogues in Clinical Neuroscience, 15(1), 109–119. https://doi.org/10.31887/DCNS.2013.15.1/dpark Patrick, C. J., Iacono, W. G., &amp; Venables, N. C. (2019). Incorporating neurophysiological measures into clinical assessments: Fundamental challenges and a strategy for addressing them. Psychological Assessment, 31(7), 952–960. https://doi.org/10.1037/pas0000713 Patterson, G. R. (1993). Orderly change in a stable world: The antisocial trait as a chimera. Journal of Consulting and Clinical Psychology, 61(6), 911–919. https://doi.org/10.1037/0022-006X.61.6.911 Paulus, J. K., &amp; Kent, D. M. (2020). Predictably unequal: Understanding and addressing concerns that algorithmic clinical prediction may increase health disparities. Npj Digital Medicine, 3(1), 99. https://doi.org/10.1038/s41746-020-0304-9 Pearl, J. (2013). Linear models: A useful “microscope\" for causal analysis. Journal of Causal Inference, 1(1), 155–170. https://doi.org/10.1515/jci-2013-0003 Peters, G.-J. (2014). The alpha and the omega of scale reliability and validity: Why and how to abandon Cronbach’s alpha and the route towards more comprehensive assessment of scale quality. European Health Psychologist, 16(2), 56–69. Petersen, I. T. (2024a). Assessing externalizing behaviors in school-aged children: Implications for school and community providers. https://scsmh.education.uiowa.edu/practice-brief/assessing-externalizing-behaviors-in-school-aged-children-implications-for-school-and-community-providers/ Petersen, I. T. (2024b). petersenlab: Package of R functions for the Petersen Lab. https://doi.org/10.5281/zenodo.7602890 Petersen, I. T. (in press). Reexamining developmental continuity and discontinuity in the 21st century: Better aligning behaviors, functions, and mechanisms. Developmental Psychology. https://doi.org/10.1037/dev0001657 Petersen, I. T., Apfelbaum, K. S., &amp; McMurray, B. (2024). Adapting open science and pre-registration to longitudinal research. Infant and Child Development, 33(1), e2315. https://doi.org/10.1002/icd.2315 Petersen, I. T., Bates, J. E., D’Onofrio, B. M., Coyne, C. A., Lansford, J. E., Dodge, K. A., Pettit, G. S., &amp; Van Hulle, C. A. (2013). Language ability predicts the development of behavior problems in children. Journal of Abnormal Psychology, 122(2), 542–557. https://doi.org/10.1037/a0031963 Petersen, I. T., Bates, J. E., Dodge, K. A., Lansford, J. E., &amp; Pettit, G. S. (2015). Describing and predicting developmental profiles of externalizing problems from childhood to adulthood. Development and Psychopathology, 27(3), 791–818. https://doi.org/10.1017/S0954579414000789 Petersen, I. T., Bates, J. E., McQuillan, M. E., Hoyniak, C. P., Staples, A. D., Rudasill, K. M., Molfese, D. L., &amp; Molfese, V. J. (2021). Heterotypic continuity of inhibitory control in early childhood: Evidence from four widely used measures. Developmental Psychology, 57(11), 1755–1771. https://doi.org/10.1037/dev0001025 Petersen, I. T., Choe, D. E., &amp; LeBeau, B. (2020). Studying a moving target in development: The challenge and opportunity of heterotypic continuity. Developmental Review, 58, 100935. https://doi.org/10.1016/j.dr.2020.100935 Petersen, I. T., Hoyniak, C. P., McQuillan, M. E., Bates, J. E., &amp; Staples, A. D. (2016). Measuring the development of inhibitory control: The challenge of heterotypic continuity. Developmental Review, 40, 25–71. https://doi.org/10.1016/j.dr.2016.02.001 Petersen, I. T., &amp; LeBeau, B. (2021). Language ability in the development of externalizing behavior problems in childhood. Journal of Educational Psychology, 113(1), 68–85. https://doi.org/10.1037/edu0000461 Petersen, I. T., &amp; LeBeau, B. (2022). Creating a developmental scale to chart the development of psychopathology with different informants and measures across time. Journal of Psychopathology and Clinical Science, 131(6), 611–625. https://doi.org/10.1037/abn0000649 Petersen, I. T., LeBeau, B., &amp; Choe, D. E. (2021). Creating a developmental scale to account for heterotypic continuity in development: A simulation study. Child Development, 92(1), e1–e19. https://doi.org/10.1111/cdev.13433 Petersen, I. T., Lindhiem, O., LeBeau, B., Bates, J. E., Pettit, G. S., Lansford, J. E., &amp; Dodge, K. A. (2018). Development of internalizing problems from adolescence to emerging adulthood: Accounting for heterotypic continuity with vertical scaling. Developmental Psychology, 54(3), 586–599. https://doi.org/10.1037/dev0000449 Petscher, Y., Justice, L. M., &amp; Hogan, T. (2018). Modeling the early language trajectory of language development when the measures change and its relation to poor reading comprehension. Child Development, 89(6), 2136–2156. https://doi.org/10.1111/cdev.12880 Piasecki, T. M., Hufford, M. R., Solhan, M., &amp; Trull, T. J. (2007). Assessing clients in their natural environments with electronic diaries: Rationale, benefits, limitations, and barriers. Psychological Assessment, 19(1), 25–43. https://doi.org/10.1037/1040-3590.19.1.25 Podsakoff, P. M., MacKenzie, S. B., &amp; Podsakoff, N. P. (2012). Sources of method bias in social science research and recommendations on how to control it. Annual Review of Psychology, 63(1), 539–569. https://doi.org/10.1146/annurev-psych-120710-100452 Pornprasertmanit, S., Miller, P., Schoemann, A., &amp; Jorgensen, T. D. (2021). simsem: SIMulated structural equation modeling. http://www.simsem.org Putnam, S. P., Rothbart, M. K., &amp; Gartstein, M. A. (2008). Homotypic and heterotypic continuity of fine-grained temperament during infancy, toddlerhood, and early childhood. Infant &amp; Child Development, 17(4), 387–405. https://doi.org/10.1002/ICD.582 Putnick, D. L., &amp; Bornstein, M. H. (2016). Measurement invariance conventions and reporting: The state of the art and future directions for psychological research. Developmental Review, 41, 71–90. https://doi.org/10.1016/j.dr.2016.06.004 R Core Team. (2022). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/ Raiche, G., &amp; Magis, D. (2020). nFactors: Parallel analysis and other non graphical solutions to the Cattell scree test. https://CRAN.R-project.org/package=nFactors Raugh, I. M., Chapman, H. C., Bartolomeo, L. A., Gonzalez, C., &amp; Strauss, G. P. (2019). A comprehensive review of psychophysiological applications for ecological momentary assessment in psychiatric populations. Psychological Assessment, 31(3), 304–317. https://doi.org/10.1037/pas0000651 Raykov, T. (2001). Bias of coefficient \\(\\alpha\\) for fixed congeneric measures with correlated errors. 25(1), 69–76. https://doi.org/10.1177/01466216010251005 Raykov, T., &amp; Marcoulides, G. A. (2001). Can there be infinitely many models equivalent to a given covariance structure model? Structural Equation Modeling: A Multidisciplinary Journal, 8(1), 142–149. https://doi.org/10.1207/S15328007SEM0801_8 Raykov, T., &amp; Marcoulides, G. A. (2019). Thanks coefficient alpha, we still need you! Educational and Psychological Measurement, 79(1), 200–210. https://doi.org/10.1177/0013164417725127 Raykov, T., Marcoulides, G. A., Harrison, M., &amp; Zhang, M. (2020). On the dependability of a popular procedure for studying measurement invariance: A cause for concern? Structural Equation Modeling: A Multidisciplinary Journal, 27(4), 649–656. https://doi.org/10.1080/10705511.2019.1610409 Reise, S. P., &amp; Waller, N. G. (2009). Item response theory and clinical measurement. Annual Review of Clinical Psychology, 5(1), 27–48. https://doi.org/10.1146/annurev.clinpsy.032408.153553 Revelle, W. (2022). psych: Procedures for psychological, psychometric, and personality research. https://personality-project.org/r/psych/ Revelle, W., &amp; Condon, D. M. (2019). Reliability from \\(\\alpha\\) to \\(\\omega\\): A tutorial. Psychological Assessment, 31(12), 1395–1411. https://doi.org/10.1037/pas0000754 Revelle, W., &amp; Rocklin, T. (1979). Very simple structure: An alternative procedure for estimating the optimal number of interpretable factors. Multivariate Behavioral Research, 14(4), 403–414. https://doi.org/10.1207/s15327906mbr1404_2 Reynolds, C. R., Altmann, R. A., &amp; Allen, D. N. (2021). The problem of bias in psychological assessment. In C. R. Reynolds, R. A. Altmann, &amp; D. N. Allen (Eds.), Mastering modern psychological testing: Theory and methods (pp. 573–613). Springer International Publishing. https://doi.org/10.1007/978-3-030-59455-8_15 Reynolds, C. R., &amp; Suzuki, L. A. (2012). Bias in psychological assessment: An empirical review and recommendations. In I. B. Weiner, J. R. Graham, &amp; J. A. Naglieri (Eds.), Handbook of psychology, Vol. 10: Assessment psychology, Part 1: Assessment issues (2nd ed., pp. 82–113). Rice, M. E., Harris, G. T., &amp; Lang, C. (2013). Validation of and revision to the VRAG and SORAG: The Violence Risk Appraisal Guide—Revised (VRAG-R). Psychological Assessment, 25(3), 951–965. https://doi.org/10.1037/a0032878 Ridley, C. R., Hill, C. L., &amp; Wiese, D. L. (2001). Ethics in multicultural assessment a model of reasoned application. In D. L. Wiese (Ed.), Handbook of multicultural assessment: Clinical, psychological, and educational applications (p. 29). Ridley, C. R., Li, L. C., &amp; Hill, C. L. (1998). Multicultural assessment: Reexamination, reconceptualization, and practical application. The Counseling Psychologist, 26(6), 827–910. https://doi.org/10.1177/0011000098266001 Rigdon, E. E. (2010). Polychoric correlation coefficient. In N. J. Salkind (Ed.), Encyclopedia of research design. SAGE Publications. https://doi.org/10.4135/9781412961288 Rivera Mindt, M., Byrd, D., Saez, P., &amp; Manly, J. (2010). Increasing culturally competent neuropsychological services for ethnic minority populations: A call to action. The Clinical Neuropsychologist, 24(3), 429–453. https://doi.org/10.1080/13854040903058960 Roberts, A. C., Yeap, Y. W., Seah, H. S., Chan, E., Soh, C.-K., &amp; Christopoulos, G. I. (2019). Assessing the suitability of virtual reality for psychological testing. Psychological Assessment, 31(3), 318–328. https://doi.org/10.1037/pas0000663 Robin, X., Turck, N., Hainard, A., Tiberti, N., Lisacek, F., Sanchez, J.-C., &amp; Müller, M. (2021). pROC: Display and analyze ROC curves. http://expasy.org/tools/pROC/ Robitzsch, A. (2019). mnlfa: Moderated nonlinear factor analysis. https://CRAN.R-project.org/package=mnlfa Rodebaugh, T. L., Scullin, R. B., Langer, J. K., Dixon, D. J., Huppert, J. D., Bernstein, A., Zvielli, A., &amp; Lenze, E. J. (2016). Unreliability as a threat to understanding psychopathology: The cautionary tale of attentional bias. Journal of Abnormal Psychology, 125(6), 840–851. https://doi.org/10.1037/abn0000184 Roemer, E., Schuberth, F., &amp; Henseler, J. (2021). HTMT2–an improved criterion for assessing discriminant validity in structural equation modeling. Industrial Management &amp; Data Systems, 121(12), 2637–2650. https://doi.org/10.1108/IMDS-02-2021-0082 Rönkkö, M., &amp; Cho, E. (2020). An updated guideline for assessing discriminant validity. Organizational Research Methods, 1094428120968614. https://doi.org/10.1177/1094428120968614 Rosseel, Y., Jorgensen, T. D., &amp; Rockwood, N. (2022). lavaan: Latent variable analysis. https://lavaan.ugent.be Royal, K. (2016). “Face validity” is not a legitimate type of validity evidence! The American Journal of Surgery, 212(5), 1026–1027. https://doi.org/10.1016/j.amjsurg.2016.02.018 Ruiz, M. A., Drake, E. B., Glass, A., Marcotte, D., &amp; Gorp, W. G. van. (2002). Trying to beat the system: Misuse of the internet to assist in avoiding the detection of psychological symptom dissimulation. Professional Psychology: Research and Practice, 33(3), 294–299. https://doi.org/10.1037/0735-7028.33.3.294 Ruscio, J., &amp; Roche, B. (2012). Determining the number of factors to retain in an exploratory factor analysis using comparison data of known factorial structure. Psychological Assessment, 24(2), 282–292. https://doi.org/10.1037/a0025697 Rush, A. J., First, M. B., &amp; Blacker, D. (2009). Handbook of psychiatric measures. American Psychiatric Publishing. Rushton, J. P., Brainerd, C. J., &amp; Pressley, M. (1983). Behavioral development and construct validity: The principle of aggregation. Psychological Bulletin, 94(1), 18–38. https://doi.org/10.1037/0033-2909.94.1.18 Russo, J. E., &amp; Schoemaker, P. J. (1992). Managing overconfidence. Sloan Management Review, 33(2), 7. Sackett, P. R., Borneman, M. J., &amp; Connelly, B. S. (2008). High stakes testing in higher education and employment: Appraising the evidence for validity and fairness. American Psychologist, 63, 215–227. https://doi.org/10.1037/0003-066X.63.4.215 Sackett, P. R., Schmitt, N., Ellingson, J. E., &amp; Kabin, M. B. (2001). High-stakes testing in employment, credentialing, and higher education. American Psychologist, 56, 301–318. https://doi.org/10.1037/0003-066X.56.4.302 Sackett, P. R., &amp; Wilk, S. L. (1994). Within-group norming and other forms of score adjustment in preemployment testing. American Psychologist, 49(11), 929–954. https://doi.org/10.1037/0003-066X.49.11.929 Sattler, J. M., &amp; Hoge, R. D. (2006). Assessment of children: Behavioral, social, and clinical foundations (5th ed.). Jerome M. Sattler, Publisher, Inc. Schaefer, J. D., Caspi, A., Belsky, D. W., Harrington, H., Houts, R., Horwood, L. J., Hussong, A., Ramrakha, S., Poulton, R., &amp; Moffitt, T. E. (2017). Enduring mental health: Prevalence and prediction. Journal of Abnormal Psychology, 126(2), 212–224. https://doi.org/10.1037/abn0000232 Schaie, K. W. (1965). A general model for the study of developmental problems. Psychological Bulletin, 64(2), 92–107. https://doi.org/10.1037/h0022371 Schaie, K. W. (2005). Developmental influences on adult intelligence: The Seattle longitudinal study. Oxford University Press. Schaie, K. W., &amp; Baltes, P. B. (1975). On sequential strategies in developmental research. Human Development, 18(5), 384–390. https://doi.org/10.1159/000271498 Schmidt, F. L., &amp; Hunter, J. E. (1981). Employment testing: Old theories and new research findings. American Psychologist, 36(10), 1128–1137. https://doi.org/10.1037/0003-066X.36.10.1128 Schmidt, F. L., &amp; Hunter, J. E. (1996). Measurement error in psychological research: Lessons from 26 research scenarios. Psychological Methods, 1(2), 199–223. https://doi.org/10.1037/1082-989X.1.2.199 Schneider, W. J. (2021). simstandard: Generate standardized data. https://github.com/wjschne/simstandard Schreiber, J. B., Nora, A., Stage, F. K., Barlow, E. A., &amp; King, J. (2006). Reporting structural equation modeling and confirmatory factor analysis results: A review. Journal of Educational Research, 99(6), 323–337. https://doi.org/10.3200/JOER.99.6.323-338 Schuberth, F. (2023). The Henseler-Ogasawara specification of composites in structural equation modeling: A tutorial. Psychological Methods, 28(4), 843–859. https://doi.org/10.1037/met0000432 Schulenberg, J. E., &amp; Maslowsky, J. (2009). Taking substance use and development seriously: Developmentally distal and proximal influences on adolescence drug use. Monographs of the Society for Research in Child Development, 74(3), 121–130. https://doi.org/10.1111/j.1540-5834.2009.00544.x Schulenberg, J. E., Patrick, M. E., Maslowsky, J., &amp; Maggs, J. L. (2014). The epidemiology and etiology of adolescent substance use in developmental perspective. In M. Lewis &amp; K. D. Rudolph (Eds.), Handbook of developmental psychopathology (pp. 601–620). Springer US. Schulenberg, J. E., &amp; Zarrett, N. R. (2006). Mental health during emerging adulthood: Continuity and discontinuity in courses, causes, and functions. In Emerging adults in america: Coming of age in the 21st century. (pp. 135–172). American Psychological Association. Sechrest, L. (1963). Incremental validity: A recommendation. Educational and Psychological Measurement, 23, 153–158. https://doi.org/10.1177/001316446302300113 Sechrest, L., Stickle, T. R., &amp; Stewart, M. (1998). The role of assessment in clinical psychology. In A. Bellack, M. Hersen, &amp; C. R. Reynolds (Eds.), Comprehensive clinical psychology, Vol. 4: Assessment. Pergamon. Sellbom, M. (2019). The MMPI-2-restructured form (MMPI-2-RF): Assessment of personality and psychopathology in the twenty-first century. Annual Review of Clinical Psychology, 15(1), 149–177. https://doi.org/10.1146/annurev-clinpsy-050718-095701 Sellbom, M., &amp; Tellegen, A. (2019). Factor analysis in psychological assessment research: Common pitfalls and recommendations. Psychological Assessment, 31(12), 1428–1441. https://doi.org/10.1037/pas0000623 Sharp, K. L., Williams, A. J., Rhyner, K. T., &amp; Ilardi, S. S. (2013). The clinical interview. In K. F. Geisinger, J. F. Carlson, J.-I. C. Hansen, N. R. Kuncel, S. P. Reise, &amp; M. C. Rodriguez (Eds.), APA handbook of testing and assessment in psychology, Vol. 2: Testing and assessment in clinical and counseling psychology (pp. 103–117). American Psychological Association. Shavelson, R. J., Webb, N. M., &amp; Rawley, R. L. (1989). Generalizability theory. American Psychologist, 44, 922–932. https://doi.org/10.1037/0003-066X.44.6.922 Shiffman, S., Stone, A. A., &amp; Hufford, M. R. (2008). Ecological momentary assessment. Annual Review of Clinical Psychology, 4, 1–32. https://doi.org/https://doi.org/10.1146/annurev.clinpsy.3.022806.091415 Shrout, P. E., &amp; Rodgers, J. L. (2018). Psychology, science, and knowledge construction: Broadening perspectives from the replication crisis. Annual Review of Psychology, 69(1), 487–510. https://doi.org/10.1146/annurev-psych-122216-011845 Sijtsma, K. (2008). On the use, the misuse, and the very limited usefulness of cronbach’s alpha. Psychometrika, 74(1), 107. https://doi.org/10.1007/s11336-008-9101-0 Silver, N. (2012). The signal and the noise: Why so many predictions fail–but some don’t. Penguin. Silverberg, N. D., &amp; Millis, S. R. (2009). Impairment versus deficiency in neuropsychological assessment: Implications for ecological validity. Journal of the International Neuropsychological Society, 15(1), 94–102. https://doi.org/10.1017/S1355617708090139 Simms, L. J., Zelazny, K., Williams, T. F., &amp; Bernstein, L. (2019). Does the number of response options matter? Psychometric perspectives using personality questionnaire data. Psychological Assessment, 31(4), 557–566. https://doi.org/10.1037/pas0000648 Skala, D. (2008). Overconfidence in psychology and finance–an interdisciplinary literature review. Bank i Kredyt, 4, 33–50. Slack, M. K., &amp; Draugalis, J., Jolaine R. (2001). Establishing the internal and external validity of experimental studies. American Journal of Health-System Pharmacy, 58(22), 2173–2181. https://doi.org/10.1093/ajhp/58.22.2173 Smedley, A., &amp; Smedley, B. D. (2005). Race as biology is fiction, racism as a social problem is real: Anthropological and historical perspectives on the social construction of race. American Psychologist, 60(1), 16–26. https://doi.org/10.1037/0003-066X.60.1.16 Smith, G. T., Atkinson, E. A., Davis, H. A., Riley, E. N., &amp; Oltmanns, J. R. (2020). The general factor of psychopathology. Annual Review of Clinical Psychology, 16(1), 75–98. https://doi.org/10.1146/annurev-clinpsy-071119-115848 Smith, G. T., McCarthy, D. M., &amp; Anderson, K. G. (2000). On the sins of short-form development. Psychological Assessment, 12(1), 102–111. https://doi.org/10.1037/1040-3590.12.1.102 Sobell, L. C., &amp; Sobell, M. B. (2008). Timeline followback (TLFB). In A. J. Rush Jr., M. B. First, &amp; D. Blacker (Eds.), Handbook of psychiatric measures (2nd ed., pp. 466–468). American Psychiatric Publishing. Sommers-Flanagan, J., &amp; Sommers-Flanagan, R. (2016). Clinical interviewing. Wiley. Somoza, E., Soutullo-Esperon, L., &amp; Mossman, D. (1989). Evaluation and optimization of diagnostic tests using receiver operating characteristic analysis and information theory. International Journal of Bio-Medical Computing, 24(3), 153–189. https://doi.org/10.1016/0020-7101(89)90029-9 Stanislaw, H., &amp; Todorov, N. (1999). Calculation of signal detection theory measures. Behavior Research Methods, Instruments, &amp; Computers, 31(1), 137–149. https://doi.org/10.3758/bf03207704 Stanton, K., McDonnell, C. G., Hayden, E. P., &amp; Watson, D. (2020). Transdiagnostic approaches to psychopathology measurement: Recommendations for measure selection, data analysis, and participant recruitment. Journal of Abnormal Psychology, 129(1), 21–28. https://doi.org/10.1037/abn0000464 Staples, A. D., Bates, J. E., Petersen, I. T., McQuillan, M. E., &amp; Hoyniak, C. (2019). Measuring sleep in young children and their mothers: Identifying actigraphic sleep composites. International Journal of Behavioral Development, 43(3), 278–285. https://doi.org/10.1177/0165025419830236 Sternberg, R. J., Grigorenko, E. L., &amp; Kidd, K. K. (2005). Intelligence, race, and genetics. American Psychologist, 60(1), 46–59. https://doi.org/10.1037/0003-066x.60.1.46 Stevens, R. J., &amp; Poppe, K. K. (2020). Validation of clinical prediction models: What does the “calibration slope” really measure? Journal of Clinical Epidemiology, 118, 93–99. https://doi.org/10.1016/j.jclinepi.2019.09.016 Steyerberg, E. W., &amp; Vergouwe, Y. (2014). Towards better clinical prediction models: Seven steps for development and an ABCD for validation. European Heart Journal, 35(29), 1925–1931. https://doi.org/10.1093/eurheartj/ehu207 Steyerberg, E. W., Vickers, A. J., Cook, N. R., Gerds, T., Gonen, M., Obuchowski, N., Pencina, M. J., &amp; Kattan, M. W. (2010). Assessing the performance of prediction models: A framework for traditional and novel measures. Epidemiology, 21(1), 128–138. https://doi.org/10.1097/EDE.0b013e3181c30fb2 Stone, A. A., Schneider, S., &amp; Smyth, J. M. (2023). Evaluation of pressing issues in ecological momentary assessment. Annual Review of Clinical Psychology, 19(1), 107–131. https://doi.org/10.1146/annurev-clinpsy-080921-083128 Strauss, M. E., &amp; Smith, G. T. (2009). Construct validity: Advances in theory and methodology. Annual Review of Clinical Psychology, 5(1), 1–25. https://doi.org/10.1146/annurev.clinpsy.032408.153639 Sullivan, H. S. (1970). The psychiatric interview. Norton. Summerfeldt, L. J., Kloosterman, P. H., &amp; Antony, M. M. (2010). Structured and semistructured diagnostic interviews. In M. M. Antony &amp; D. H. Barlow (Eds.), Handbook of assessment and treatment planning for psychological disorders (2nd ed., pp. 95–137). Guilford Press. Suzuki, L. A., Onoue, M. A., &amp; Hill, J. S. (2013). Clinical assessment: A multicultural perspective. In K. F. Geisinger, J. F. Carlson, J.-I. C. Hansen, N. R. Kuncel, S. P. Reise, &amp; M. C. Rodriguez (Eds.), APA handbook of testing and assessment in psychology, Vol. 2: Testing and assessment in clinical and counseling psychology (pp. 193–212). American Psychological Association. Swets, J. A., Dawes, R. M., &amp; Monahan, J. (2000). Psychological science can improve diagnostic decisions. Psychological Science in the Public Interest, 1, 1–26. https://doi.org/10.1111/1529-1006.001 Tackett, J. L., Brandes, C. M., King, K. M., &amp; Markon, K. E. (2019). Psychology’s replication crisis and clinical psychological science. Annual Review of Clinical Psychology, 15(1), 579–604. https://doi.org/10.1146/annurev-clinpsy-050718-095710 Tackett, J. L., Brandes, C. M., &amp; Reardon, K. W. (2019). Leveraging the open science framework in clinical psychological assessment research. Psychological Assessment, 31(12), 1386–1394. https://doi.org/10.1037/pas0000583 Tackett, J. L., Lang, J. W. B., Markon, K. E., &amp; Herzhoff, K. (2019). A correlated traits, correlated methods model for thin-slice child personality assessment. Psychological Assessment, 31(4), 545–556. https://doi.org/10.1037/pas0000635 Tervalon, M., &amp; Murray-Garcia, J. (1998). Cultural humility versus cultural competence: A critical distinction in defining physician training outcomes in multicultural education. Journal of Health Care for the Poor and Underserved, 9(2), 117–125. Tetlock, P. E. (2017). Expert political judgment: How good is it? How can we know? - New edition. Princeton University Press. Textor, J., van der Zander, B., &amp; Ankan, A. (2021). dagitty: Graphical analysis of structural causal models. https://CRAN.R-project.org/package=dagitty Textor, J., Zander, B. van der, Gilthorpe, M. S., Liśkiewicz, M., &amp; Ellison, G. T. (2017). Robust causal inference using directed acyclic graphs: The R package “dagitty”. International Journal of Epidemiology, 45(6), 1887–1894. https://doi.org/10.1093/ije/dyw341 Thomas, M. L. (2019). Advances in applications of item response theory to clinical assessment. Psychological Assessment, 31(12), 1442–1455. https://doi.org/10.1037/pas0000597 Thorndike, R. L. (1971). Concepts of culture-fairness. Journal of Educational Measurement, 8(2), 63–70. https://doi.org/10.1111/j.1745-3984.1971.tb00907.x Tiego, J., Martin, E. A., DeYoung, C. G., Hagan, K., Cooper, S. E., Pasion, R., Satchell, L., Shackman, A. J., Bellgrove, M. A., Fornito, A., Abend, R., Goulter, N., Eaton, N. R., Kaczkurkin, A. N., &amp; and, R. N. (2023). Precision behavioral phenotyping as a strategy for uncovering the biological correlates of psychopathology. Nature Mental Health, 1, 304–315. https://doi.org/10.1038/s44220-023-00057-5 Tofallis, C. (2015). A better measure of relative prediction accuracy for model selection and model estimation. Journal of the Operational Research Society, 66(8), 1352–1362. https://doi.org/10.1057/jors.2014.103 Tong, Y., &amp; Kolen, M. J. (2007). Comparisons of methodologies and results in vertical scaling for educational achievement tests. Applied Measurement in Education, 20(2), 227–253. https://doi.org/10.1080/08957340701301207 Toomey, R. B., Syvertsen, A. K., &amp; Shramko, M. (2018). Transgender adolescent suicide behavior. Pediatrics, 142(4). https://doi.org/10.1542/peds.2017-4218 Trafimow, D. (2015). A defense against the alleged unreliability of difference scores. Cogent Mathematics, 2(1), 1064626. https://doi.org/10.1080/23311835.2015.1064626 Treat, T. A., McFall, R. M., Viken, R. J., Kruschke, J. K., Nosofsky, R. M., &amp; Wang, S. S. (2007). Clinical cognitive science: Applying quantitative models of cognitive processing to examine cognitive aspects of psychopathology. In R. W. J. Neufeld (Ed.), Advances in clinical cognitive science: Formal modeling of processes and symptoms (pp. 179–205). American Psychological Association. Treat, T. A., &amp; Viken, R. J. (2023). Measuring test performance with signal detection theory techniques. In H. Cooper, M. N. Coutanche, L. M. McMullen, A. T. Panter, D. Rindskopf, &amp; K. J. Sher (Eds.), APA handbook of research methods in psychology: Foundations, planning, measures, and psychometrics (2nd ed., Vol. 1, pp. 837–858). American Psychological Association. Treiblmaier, H., Bentler, P. M., &amp; Mair, P. (2011). Formative constructs implemented via common factors. Structural Equation Modeling: A Multidisciplinary Journal, 18(1), 1–17. https://doi.org/10.1080/10705511.2011.532693 Trull, T. J., &amp; Ebner-Priemer, U. (2013). Ambulatory assessment. Annual Review of Clinical Psychology, 9, 151–176. https://doi.org/https://doi.org/10.1146/annurev-clinpsy-050212-185510 Trull, T. J., &amp; Ebner-Priemer, U. W. (2020). Ambulatory assessment in psychopathology research: A review of recommended reporting guidelines and current practices. Journal of Abnormal Psychology, 129(1), 56–63. https://doi.org/10.1037/abn0000473 Tversky, A., &amp; Kahneman, D. (1974). Judgment under uncertainty: Heuristics and biases. Science, 185(4157), 1124–1131. https://doi.org/10.1126/science.185.4157.1124 Ursenbach, J., O’Connell, M. E., Neiser, J., Tierney, M. C., Morgan, D., Kosteniuk, J., &amp; Spiteri, R. J. (2019). Scoring algorithms for a computer-based cognitive screening tool: An illustrative example of overfitting machine learning approaches and the impact on estimates of classification accuracy. Psychological Assessment, 31(11), 1377–1382. https://doi.org/10.1037/pas0000764 Van De Schoot, R., Kluytmans, A., Tummers, L., Lugtig, P., Hox, J., &amp; Muthen, B. (2013). Facing off with scylla and charybdis: A comparison of scalar, partial, and the novel possibility of approximate measurement invariance. Frontiers in Psychology, 4(770). https://doi.org/10.3389/fpsyg.2013.00770 Van De Schoot, R., Schmidt, P., De Beuckelaer, A., Lek, K., &amp; Zondervan-Zwijnenburg, M. (2015). Editorial: Measurement invariance. Frontiers in Psychology, 6(1064). https://doi.org/10.3389/fpsyg.2015.01064 van der Nest, G., Lima Passos, V., Candel, M. J. J. M., &amp; van Breukelen, G. J. P. (2020). An overview of mixture modelling for latent evolutions in longitudinal data: Modelling approaches, fit statistics and software. Advances in Life Course Research, 43, 100323. https://doi.org/10.1016/j.alcr.2019.100323 Vaz, S., Falkmer, T., Passmore, A. E., Parsons, R., &amp; Andreou, P. (2013). The case for using the repeatability coefficient when calculating test–retest reliability. PLOS ONE, 8(9), e73990. https://doi.org/10.1371/journal.pone.0073990 Vispoel, W. P., Hong, H., &amp; Lee, H. (2023). Benefits of doing generalizability theory analyses within structural equation modeling frameworks: Illustrations using the Rosenberg self-esteem scale. Structural Equation Modeling: A Multidisciplinary Journal, 1–17. https://doi.org/10.1080/10705511.2023.2187734 Vispoel, W. P., Lee, H., Xu, G., &amp; Hong, H. (2022). Integrating bifactor models into a generalizability theory based structural equation modeling framework. The Journal of Experimental Education, 1–21. https://doi.org/10.1080/00220973.2022.2092833 Vispoel, W. P., Morris, C. A., &amp; Kilinc, M. (2018). Applications of generalizability theory and their relations to classical test theory and structural equation modeling. Psychological Methods, 23(1), 1–26. https://doi.org/10.1037/met0000107 Vispoel, W. P., Morris, C. A., &amp; Kilinc, M. (2019). Using generalizability theory with continuous latent response variables. Psychological Methods, 24(2), 153–178. https://doi.org/10.1037/met0000177 Voorhees, C. M., Brady, M. K., Calantone, R., &amp; Ramirez, E. (2016). Discriminant validity testing in marketing: An analysis, causes for concern, and proposed remedies. Journal of the Academy of Marketing Science, 44(1), 119–134. https://doi.org/10.1007/s11747-015-0455-4 Wakschlag, L. S., Tolan, P. H., &amp; Leventhal, B. L. (2010). Research review: “Ain’t misbehavin”: Towards a developmentally-specified nosology for preschool disruptive behavior. Journal of Child Psychology and Psychiatry, 51(1), 3–22. https://doi.org/10.1111/j.1469-7610.2009.02184.x Wang, S., Jiao, H., &amp; Zhang, L. (2013). Validation of longitudinal achievement constructs of vertically scaled computerised adaptive tests: A multiple-indicator, latent-growth modelling approach. International Journal of Quantitative Research in Education, 1(4), 383–407. https://doi.org/10.1504/IJQRE.2013.058307 Wang, T., Merkle, E. C., &amp; Zeileis, A. (2014). Score-based tests of measurement invariance: Use in practice. Frontiers in Psychology, 5. https://doi.org/10.3389/fpsyg.2014.00438 Wang, W.-C., Shih, C.-L., &amp; Yang, C.-C. (2009). The MIMIC method with scale purification for detecting differential item functioning. Educational and Psychological Measurement, 69(5), 713–731. https://doi.org/10.1177/0013164409332228 Wang, Y. A., &amp; Rhemtulla, M. (2021). Power analysis for parameter estimation in structural equation modeling: A discussion and tutorial. Advances in Methods and Practices in Psychological Science, 4(1), 1–17. Watkins, C. E., Campbell, V. L., Nieberding, R., &amp; Hallmark, R. (1995). Contemporary practice of psychological assessment by clinical psychologists. Professional Psychology: Research and Practice, 26(1), 54–60. https://doi.org/10.1037/0735-7028.26.1.54 Webb, N. M., &amp; Shavelson, R. J. (2005). Generalizability theory: overview. In B. S. Everitt &amp; D. C. Howell (Eds.), Encyclopedia of statistics in behavioral science (Vol. 2, pp. 717–719). John Wiley &amp; Sons, Ltd. Weems, C. F. (2008). Developmental trajectories of childhood anxiety: Identifying continuity and change in anxious emotion. Developmental Review, 28(4), 488–502. https://doi.org/10.1016/j.dr.2008.01.001 Wei, T., &amp; Simko, V. (2021). R package “corrplot\": Visualization of a correlation matrix. https://github.com/taiyun/corrplot Weintraub, S., Bauer, P. J., Zelazo, P. D., Wallner-Allen, K., Dikmen, S. S., Heaton, R. K., Tulsky, D. S., Slotkin, J., Blitz, D. L., Carlozzi, N. E., Havlik, R. J., Beaumont, J. L., Mungas, D., Manly, J. J., Borosh, B. G., Nowinski, C. J., &amp; Gershon, R. C. (2013). I. NIH toolbox cognition battery (CB): Introduction and pediatric data. Monographs of the Society for Research in Child Development, 78(4), 1–15. https://doi.org/10.1111/mono.12031 Weiss, B., &amp; Garber, J. (2003). Developmental differences in the phenomenology of depression. Development and Psychopathology, 15(2), 403–430. https://doi.org/10.1017/S0954579403000221 Whitbourne, S. K. (2019). Longitudinal, cross-sectional, and sequential designs in lifespan developmental psychology. Oxford University Press. Wicherts, J. M., &amp; Dolan, C. V. (2010). Measurement invariance in confirmatory factor analysis: An illustration using IQ test performance of minorities. Educational Measurement: Issues and Practice, 29(3), 39–47. https://doi.org/10.1111/j.1745-3992.2010.00182.x Wickham, H. (2021). tidyverse: Easily install and load the tidyverse. https://CRAN.R-project.org/package=tidyverse Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686. https://doi.org/10.21105/joss.01686 Widiger, T. A. (2002). Personality disorders. In M. M. Antony &amp; D. H. Barlow (Eds.), Handbook of assessment and treatment planning for psychological disorders (pp. 453–480). Guilford Publications. Wiggins, J. S. (1973). Personality and prediction: Principles of personality assessment. Addison-Wesley. Willett, W. (2012). Correction for the effects of measurement error. In W. Willett (Ed.), Nutritional epidemiology (3rd ed., pp. 287–304). Oxford University Press. Williams, A. J., Botanov, Y., Kilshaw, R. E., Wong, R. E., &amp; Sakaluk, J. K. (2021). Potentially harmful therapies: A meta-scientific review of evidential value. Clinical Psychology: Science and Practice, 28(1), 5–18. https://doi.org/10.1111/cpsp.12331 Wood, J. M., Garb, H. N., Lilienfeld, S. O., &amp; Nezworski, M. T. (2002). Clinical assessment. Annual Review of Psychology, 53(1), 519. https://doi.org/10.1146/annurev.psych.53.100901.135136 Wood, J. M., Nezworski, M. T., Garb, H. N., &amp; Lilienfeld, S. O. (2001). Problems with the norms of the Comprehensive System for the Rorschach: Methodological and conceptual considerations. Clinical Psychology: Science and Practice, 8(3), 397–402. https://doi.org/10.1093/clipsy.8.3.397 Wood, J. M., Nezworski, M. T., &amp; Stejskal, W. J. (1996a). The Comprehensive System for the Rorschach: A critical examination. Psychological Science, 7(1), 3–10. https://doi.org/10.1111/j.1467-9280.1996.tb00658.x Wood, J. M., Nezworski, M. T., &amp; Stejskal, W. J. (1996b). Thinking critically about the Comprehensive System for the Rorschach: A reply to exner. Psychological Science, 7(1), 14–17. https://doi.org/10.1111/j.1467-9280.1996.tb00660.x Wood, J. M., Teresa, P. M., Garb, H. N., &amp; Lilienfeld, S. O. (2001). The misperception of psychopathology: Problems with the norms of the Comprehensive System for the Rorschach. Clinical Psychology: Science and Practice, 8(3), 350–373. https://doi.org/10.1093/clipsy.8.3.350 Woody, M. L., &amp; Gibb, B. E. (2015). Integrating NIMH Research Domain Criteria (RDoC) into depression research. Current Opinion in Psychology, 4, 6–12. https://doi.org/10.1016/j.copsyc.2015.01.004 Wright, A. G. C., Gates, K. M., Arizmendi, C., Lane, S. T., Woods, W. C., &amp; Edershile, E. A. (2019). Focusing personality assessment on the person: Modeling general, shared, and person specific processes in personality and psychopathology. Psychological Assessment, 31(4), 502–515. https://doi.org/10.1037/pas0000617 Wright, A. G. C., &amp; Woods, W. C. (2020). Personalized models of psychopathology. Annual Review of Clinical Psychology, 16(1), 49–74. https://doi.org/10.1146/annurev-clinpsy-102419-125032 Wright, A. G. C., &amp; Zimmermann, J. (2019). Applied ambulatory assessment: Integrating idiographic and nomothetic principles of measurement. Psychological Assessment, 31(12), 1467–1480. https://doi.org/10.1037/pas0000685 Xie, Y. (2015). Dynamic documents with R and knitr (2nd ed.). Chapman; Hall/CRC. Xie, Y. (2022a). bookdown: Authoring books and technical documents with R Markdown. https://CRAN.R-project.org/package=bookdown Xie, Y. (2022b). knitr: A general-purpose package for dynamic report generation in R. https://yihui.org/knitr/ Yang, Y., &amp; Land, K. C. (2013). Age-period-cohort analysis: New models, methods, and empirical applications. Taylor &amp; Francis. Youngstrom, E. A., Halverson, T. F., Youngstrom, J. K., Lindhiem, O., &amp; Findling, R. L. (2018). Evidence-based assessment from simple clinical judgments to statistical learning: Evaluating a range of options using pediatric bipolar disorder as a diagnostic challenge. Clinical Psychological Science, 6(2), 243–265. https://doi.org/10.1177/2167702617741845 Youngstrom, E. A., &amp; Van Meter, A. (2016). Empirically supported assessment of children and adolescents. Clinical Psychology: Science and Practice, 23(4), 327–347. https://doi.org/10.1111/cpsp.12172 Youngstrom, E. A., Van Meter, A., Frazier, T. W., Hunsley, J., Prinstein, M. J., Ong, M.-L., &amp; Youngstrom, J. K. (2017). Evidence-based assessment as an integrative model for applying psychological science to guide the voyage of treatment. Clinical Psychology: Science and Practice, 24(4), 331–363. https://doi.org/10.1111/cpsp.12207 Yu, X., Schuberth, F., &amp; Henseler, J. (2023). Specifying composites in structural equation modeling: A refinement of the Henseler-Ogasawara specification. Statistical Analysis and Data Mining: The ASA Data Science Journal, 16(4), 348–357. https://doi.org/https://doi.org/10.1002/sam.11608 Yudell, M., Roberts, D., DeSalle, R., &amp; Tishkoff, S. (2016). Taking race out of human genetics. Science, 351(6273), 564–565. https://doi.org/10.1126/science.aac4951 Zhang, J., &amp; Mueller, S. T. (2005). A note on ROC analysis and non-parametric estimate of sensitivity. Psychometrika, 70(1), 203–212. https://doi.org/10.1007/s11336-003-1119-8 Zieky, M. J. (2006). Fairness review in assessment. In S. M. Downing &amp; T. M. Haladyna (Eds.), Handbook of test development (pp. 359–376). Routledge. https://doi.org/10.4324/9780203874776.ch16 Zieky, M. J. (2013). Fairness review in assessment. In K. F. Geisinger, B. A. Bracken, J. F. Carlson, J.-I. C. Hansen, N. R. Kuncel, S. P. Reise, &amp; M. C. Rodriguez (Eds.), APA handbook of testing and assessment in psychology, Vol. 1: Test theory and testing and assessment in industrial and organizational psychology (pp. 293–302). American Psychological Association. https://doi.org/10.1037/14047-017 Zuckerman, M. (1990). Some dubious premises in research and theory on racial differences: Scientific, social, and ethical issues. American Psychologist, 45(12), 1297–1303. https://doi.org/10.1037/0003-066X.45.12.1297 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
