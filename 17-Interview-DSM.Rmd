# The Interview and the DSM {#interview}

[**Articles on pfactor: Caspi; Smith et al, 2020**]

[**Articles on network analysis: McNally, 2021**]

## Overview of Clinical Interviews

Clinical interviews are a form of conversation with a client, in which the conversation has explicit clinical goals including information gathering on the client. Interviews are the most widely used assessment technique in clinical psychology, yet there is very little done to explore their validity or ways to improve their validity. There is very little data on the validity of interviews. It is likely that there has been little research on the validity of interviews because the field has been riding the coattails of biological psychiatry, in which the tradition has been that the clinical interview is just assumed to be the "gold standard." However, this is based on theory, not data.

The problem is that we do not have a "gold standard" [@Lilienfeld2015]. We do not have any robust measure to compare interviews against to verify their accuracy—i.e., we do not have true knowledge about the pathophysiology as a strong indicator of disease status. Diagnoses are not directly observable concepts. Comparisons from one interviewer to another interviewer establishes inter-rater reliability (i.e., diagnostic agreement), but it does not establish not diagnostic validity—i.e., the correspondence to truth about illness status.

To estimate diagnostic accuracy in the absence of a gold standard, one can use latent class models, where you identify latent (unmeasured) class membership among participants using multiple observed variables. For example, you can identify a latent class membership based on multiple symptoms, raters or observers, and/or at multiple time points. High scores across symptoms, raters, and time points is more likely to reflect true illness. This is consistent with making diagnostic decisions using the LEAD standard: LEAD is an acronym for **l**ongitudinal, **e**xpert, **a**ll **d**ata. So, the best approach may be to use LEAD data to evaluate the diagnostic validity.

The latent class would be the illness status—i.e., whether the client "has" the disorder or not. Illness status is not directly observable, so we use observables to infer latent classes. Each person is assigned to a latent class with different probabilities to account for uncertainty. For example, Person A may be assigned a 75% probability of "having" depression, whereas the probability may be 13% for Person B. Then, we could treat the latent class estimate of illness status as the latent gold standard. Diagnoses based on LEAD data are not 100% valid; thus, they are a LEAD standard, not a gold standard. However, LEAD approaches are expensive and time-consuming, and they are not widely used.

## Two Traditions: Unstructured and Structured Interviews

There are two primary traditions to the clinical interview: the unstructured interview and the structured interview.

### Unstructured Interview {#unstructuredInterview}

The unstructured interview is the oldest tradition of the clinical interview. It was the most widely used interview method until the late 1970s. The unstructured clinical interview came from a psychoanalytic theoretical orientation. In an unstructured interview, the clinician keeps the framework "in the head" of the information they want to extract from the client, including presenting problems, past treatments obtained, comorbid symptoms, etc.

An *unstructured interview* is an open-ended, free flowing conversation between the clinician and client. According to psychoanalysis, the nondirective interview was believed to serve as a catalyst for the client's expression of their unconscious, for example by means of transference and free associations. Transference is when the client treats the therapist as if the therapist was an important figure in the client's past—for example, if they client treats the therapist as if the therapist were the client's parent. Notes were frowned upon because they were viewed as disrespectful to the client and a disruption of the flow of session.

A problem of the unstructured interview is that taking a different "conversation path" (i.e., asking different questions) or having a different clinician led to different information extracted from the client and to low [inter-rater](#interrater-reliability) and [test–retest reliability](#testRetest-reliability). The unstructured interview is susceptible to multiple forms of bias, such as halo effects when dealing with a polite client. Another important form of bias is confirmation bias, in which clinicians look for evidence that confirms their hypothesis about the client, which lead to the the tendency to stop the interview once the first diagnosis is identified, and to fewer diagnoses. This is known as *diagnostic overshadowing*. The unstructured interview is also susceptible to race, gender, class bias, etc.

In general, unstructured interviews show low reliability. The low reliability of unstructured interviews is a result of several factors. One factor is information variance: different information is extracted from the client in each interview. Another factor that contributes to the low reliability of unstructured interviews is criterion variance, where different criteria are used to determine the presence or absence of a condition. Criterion variance was inadequate once criterion-based diagnosis came along. A third factor that contributes to the low reliability of unstructured interviews is varying levels of skill and knowledge of interviewers.

A potential advantage of the unstructured interview is that it can be good for non-specific aspects of interview. For instance, an unstructured interview can potentially be helpful for building rapport because the client feels the clinician's first goal is to understand them and their situation in order to provide help. Classic examples of an unstructured interview are provided in "The Psychiatric Interview" by Harry Stack Sullivan, who is a psychodynamically oriented clinician. Sullivan's book provides a guide to understanding the unstructured interview. He described processes such as "reciprocal emotion", which he described as how the interviewer and interviewee influence each other and responses of the other through one's emotional tone.

### Structured Interview {#structuredInterview}

An important movement in the development of interviews was behavioral interviews that begin in the 1960s. Behavioral interviews came from a cognitive-behavioral theoretical orientation. They started out as unstructured. However, behavioral interviews had a different set of goals from traditional unstructured interviews. Behavioral interviews sought to quantify aspects of behavior—for example, how often the symptoms occur. Behavioral interviews applied the A-B-C (antecedent-behavior-consequence) model. The A-B-C model tries to identify patterns of sequences that include the problem behavior. Specifically, the A-B-C model identifies things that came before the behavior (antecedents), and things the came after the behavior (consequences). This helps to identify precipitating and maintaining factors for the problem behavior, as well as strengths to bolster in treatment. It provides some good ways to generate information and helps with case conceptualization. The behavior interviews eventually led to more structured versions, called structured interviews.

An example of a structured clinical interview is the Structured Clinical Interview of Mental Disorders (SCID). Structured interviews were developed to solve the problem of reliability. In a *structured interview*, a pre-determined domain of questions are asked in a pre-defined way. A fully structured interview provides a script to use to ask each question (to standardize wording), and in a particular order, with a decision tree and branching logic to choose subsequent questions based on the responses given.

For a structured diagnostic interview, the diagnostic criteria for a disorder are codified, and the exact language is provided to ask about each symptom. Then, the responses are scored and integrated in a systematic way, and the diagnosis is determined in a systematic way.

Structured interviews show higher reliability compared to unstructured interviews due to: less information variance because the clinician extracts similar information from a given client, and less criterion variance because the clinician makes diagnostic decisions using the same criteria. Interviews differ in their continuum or degree of structure, from semi-structured to structured. In general, convergent validity improves as the degree of structure for an interview increases [@Widiger2002].

When very structured, even a lay person could perform the interview. Taken to the extreme, a human interviewer becomes unnecessary; the entire interview can be recorded and done by a computer. In these cases, [inter-rater reliability](#interrater-reliability) approaches perfection. However, this removes the role of [clinical judgment](#actuarial), which can lead to errors if the client misinterprets the computer-administered question or describes a behavior that is not actually clinically concerning. Therefore, structured interviews reduce the amount of training necessary for a clinician to administer, but they lose the potential positive qualities of unstructured interview: e.g., the capacity for better rapport; clients may not feel understood, cared about, or that they are working with an expert. Therefore, semi-structured interviews were developed.

#### Semi-Structured Interview {#semiStructuredInterview}

Semi-structured interviews provide some leeway in how the clinician asks questions and the pace of the questions, so the clinician can ask questions in their own words and ask follow-up questions or restate questions using the client's own words. Semi-structured interviews are still structured in that they are still asking about the same range of problems as a structured interview, to avoid confirmation bias, and they provide a structured way to score diagnostic criteria and assign diagnoses, to ensure consistency across therapists. A well-designed semi-structured interview balances improvements in validity—due to greater clinical judgment and probing—with the tradeoff in lower inter-rater reliability due to idiosyncratic decisions. A semi-structured interview also allows clarifying the meaning of questions and response choices.

Conducting semi-structured interviews requires more training and there is a constant discussion of their problems, and it takes a long time to become really skilled. This can become an issue with highly trained individuals because they sometimes "screw around" with the interview (i.e., do things differently because they believe their approach is superior) and bypass the structure, leading to an unstructured interview.

In addition to degree of structure, interviews also differ in their degree of specialization. Some interviews are general/broad, including the SCID. Other interviews are specific to particular domains, such as the Semi-Structured Assessment for the Genetics of Alcoholism (SSAGA), which assesses alcohol use disorder. As you get further away from the construct, you lose valued information and the interview becomes less and less helpful. Specialized interviews provide the maximum amount of information in the area of interest, and they take less time, which is why they are popular in research.

### Structured vs. Unstructured Interviews

Structured interviews show higher reliability than unstructured interviews, in terms of both [inter-rater](#interrater-reliability) and [test–retest reliability](#testRetest-reliability). Inter-rater reliability of a clinical interview is typically estimated based on the degree of agreement between an interviewer and an observer of the interview. That is, most often inter-rater reliability of interviews is estimated based on two raters of the same interview content. However, this does not necessarily capture different styles of different interviewers. Therefore, the typical estimates of inter-rater reliability of interviews do not capture all parts of the reliability of the measure.

[Test–retest reliability](#testRetest-reliability) of a clinical interview, is usually evaluated by conducting interviews of the same client separated (typically) by 2 weeks, often by a different interviewer. However, different ratings might be made at each time point if the diagnosis (e.g., depression) fluctuates over time or especially if disorder is due to variation of context. Therefore, changes in symptoms can contribute to differences in reliability over time—or, alternatively, differences could be due to differences in interviewers. Because of conflating differences in raters and time, [test–retest reliability](#testRetest-reliability) tends to be lower than [inter-rater reliability](#interrater-reliability) in evaluating the reliability of interviews.

Fully structured diagnostic interviews typically deliver [inter-rater reliability](#interrater-reliability) Cohen's kappa ($\kappa$) estimates of .80 or higher. By contrast, inter-rater agreement between clinicians conducting unstructured interviews usually are near zero, which indicates that their agreement is no better than chance. Cohen's kappa ranges from $-1$ (perfect disagreement) to $+1$ (perfect agreement). A kappa greater than .75 would be considered good inter-rater reliability.

Despite the increased reliability of using structured and semi-structured interviews compared to unstructured interviews, many clinicians are reluctant to use structured and semi-structured interviews. Clinicians often feel that structured interviews impinge upon their professional autonomy and believe that structured interviews will damage rapport, even though semi-structured interviews have been shown to increase rapport. Clients prefer structured approaches, feeling that the clinician has a more comprehensive understanding of the client's needs. Clinicians prefer to use their expertise, insight, and judgment.	However, their [judgment](#actuarial) is what results in lower reliability and validity. Unstructured interviews are much more widely used in practice despite the advantages of structured interviews in terms of reliability and validity, because clinicians tend to prefer them.

Clinicians using unstructured approaches tend to diagnose fewer conditions than structured interviews detect. This likely reflects confirmation bias and diagnostic overshadowing.

## Other Findings Regarding Interviews

There are other notable patterns regarding clinical interviews. [Inter-rater reliability](#interrater-reliability) of Diagnostic and Statistical Manual (DSM) diagnoses is better for some conditions than others, but in general it is not very good. This provides further evidence that DSM-defined diagnostic categories are not "real" in a phenomenological sense. Moreover, there is considerable evidence that these diagnostic categories are better conceptualized as dimensional than categorical [**CITATION**]. Therefore, a better understanding of the structure of psychopathology may lead to a more valid diagnostic system.

Low [inter-rater reliability](#interrater-reliability) of interviews could be a result of either low [sensitivity](#sensitivity) or low [specificity](#specificity). Even if one is high but the other is low, you will get low inter-rater reliability.

It is worth noting that the clinical interview is not merely a tool for assessment. A clinical interview can be used as a form of intervention (i.e., therapeutic interview). For instance, motivational interviewing is a therapeutic interview that is often used as a treatment for substance use.

## Best Practice for Diagnostic Assessment

There are several best practice approaches for conducting diagnostic assessment. As discussed earlier, when possible, it is best to follow the LEAD standard: **l**ongitudinal, **e**xpert, **a**ll **d**ata. Have the assessment conducted by an expert or multiple experts in the domain(s) of interest. Have multiple people who are experts on the client (e.g., parent, teacher, friend, sibling, self) report on the person's behavior. Use rating scales to generate the contending hypotheses, and then pick structured or semi-structured interview modules to use for follow-up to examine and rule out hypotheses. Incorporate multiple forms of information, e.g., interviews, rating scales, observations, objective/performance-based measures, and school/work/legal records. If possible, include an observational measure and an objective measure of diagnostic-relevant behavior. Examples of observational measures include parent–child play in the clinic paired with a cleanup command to see how parents give commands, how children respond to the parent's commands, and how parents respond to their child's noncompliance. Examples of more objective measures could include biological assessments, such as polysomnography, or performance-based assessments, such as intelligence tests and academic achievement tests.

There are many important skills when conducting a clinical interview, including:

- Taking a nonjudgmental stance
- Active listening
- Empathy
- Authenticity
- Effective use of silence
- Paraphrasing
- Summarizing
- Providing a sense of hope

## The DSM and ICD {#dsm}

The Diagnostic and Statistical Manual of Mental Disorders (DSM) provides the list of mental disorders and the diagnostic criteria for mental health treatment providers. It is published by the American Psychiatric Association and is used in the United States. The equivalent list of mental disorders and diagnostic criteria for mental health providers outside of the United States is the International Classification of Diseases (ICD). However, the ICD also includes diseases that are not considered mental disorders. Collectively, the DSM and ICD represent the diagnostic system for mental disorders. A goal of DSM and ICD is to define different "mental disorders" to make sure people get the services they need.

### Strengths

There are several strengths of the DSM and ICD. First, they can facilitates communication about disorders with other mental health providers; it provides a common language to describe complex behavioral presentations. Second, ideally, the DSM and ICD also guide treatment selection, especially if people are homogeneous within a disorder in terms of their symptoms, course, etiology, and treatment response. Third, diagnosis is used to justify payment for services from third party payers (e.g., insurance companies, government). Fourth, diagnosis can be normalizing and empowering to some clients. It may help people learn that other people experiencing similar challenges, and it may give them hope that that there is something that can be done to address it. Fifth, the DSM/ICD promotes research in psychopathology, in terms of epidemiology (the disorder distribution in the population), etiology (causes of the disorder), course (how the disorder plays out over time), and treatment (including development and evaluation of interventions).

### Concerns {#dsmConcerns}

However, there are also key concerns with the DSM and ICD. One potential concern with the DSM and ICD is stigmatization. The goal of the DSM/ICD is not to label people. Labeling people can be an unfortunate consequence, however, and it can be stigmatizing because mental disorders often carry a stigma. A second concern is that we may be pathologizing normality. Mental disorders are not infrequent. Estimates of lifetime prevalence of mental disorders are around 75% [**CITATION**]. That is, three out of four people will experience a mental disorder at some point in their life. Thus, abnormality as defined by the DSM and ICD are *normal*.

Ideally, a diagnostic system should have [validity](#validity) and [utility](#treatmentUtility). For instance, the diagnoses in a diagnostic system should have [construct validity](#constructValidity). That is, the diagnostic categories should reflect truth, reality, or the existence of the construct (i.e., diagnostic validity). Diagnostic validity is the extent to which the diagnostic category accurately captures the abnormal phenomenon of interest. Typical indicators of diagnostic validity are homogeneity (similarity) across people receiving a given diagnosis in terms of etiology, course, treatment response, etc. Utility means that the diagnostic system should help clinical decision making, in terms of ease of use (e.g., saving time), facilitating communication with others, and in treatment planning. However, diagnostic systems (DSM and ICD) have serious concerns with both validity and utility. The diagnostic system has serious concerns with [validity](#validity), in terms of whether the diagnostic categories defined by the DSM/ICD are actually real. There is great heterogeneity (variability) between people with the same diagnosis, in terms of symptom profiles, etiologies, course, and treatment outcomes. For instance, two children diagnosed with attention-deficit hyperactivity disorder (ADHD) can share zero symptoms. Subtypes attempts to address this problem, but there is little support for the validity of these subtypes. Many disorder subtypes and features are not based on empirical data—instead, they are based on expert judgment, consensus, and politics. There is also strong comorbidity (co-occurence) across disorders. For instance, some disorders often co-occur, including major depressive disorder and generalized anxiety disorder, which suggests that they may share an underlying liability and do not reflect distinct categories. This complicates treatment and challenges the validity of the diagnostic system.

Another challenge is regarding the coverage of the diagnostic system, including the number of categories and which are represented. How many disorders/categories should there be? Do we have too many categories? There are hundreds of possible diagnoses, including 541 disorders in the DSM-5 [**Blashfield et al., 2014 citation**]. Do we have too few diagnoses? Clinicians often use "Not Otherwise Specified" or "Unspecified" for diagnoses, which suggests that the current diagnostic categories do not cover the variability of clients' presentations. How thinly should we "split" the categories? Some people are "splitters" and tend to split categories into as many categories as possible. Other people are "lumpers" and tend to lump categories together so there are fewer categories. 

The diagnostic systems also has serious concerns with [utility](#treatmentUtility), in terms of lengthy criterion sets that are time-consuming and difficult to assess in practice. One possible way to address the time-consuming nature of using lengthy criterion sets is to use prototypal matching instead of criterion sets. Prototypal matching involves having the clinician rate the similarity of the client to a narrative description of a prototypical person with the disorder. However, prototypal matching could result in lower [inter-rater reliability](#interrater-reliability) (compared to using criterion sets) because of idiosyncratic judgments. There is a tradeoff between assessment time and validity. The clinician's time can be saved by using computerized interviews or computerized tests. Another way to save the clinician's time is to use self-report inventories as a screening device followed by interview that is specific to the most relevant conditions identified by the screen.

The diagnostic system include arbitrary thresholds that do not facilitate social and clinical decision making. Current diagnostic categories do not provide much utility in terms of determining the appropriate treatment.

No single biological substrate has been identified for any psychological disorder that has high enough sensitivity and specificity to be useful for diagnosis or for predicting response to treatment. This is because the DSM/ICD-based psychological disorders are fictive categories. The DSM/ICD-defined categories do not exist in nature. This does not dismiss the importance of what people are experiencing, which is very real. A person's experience of depression is real, but the construct of major depressive disorder—as defined by the DSM/ICD—is not real. Mental disorders are not a concrete thing. Psychological disorders are social constructs. Mental disorders are not something that people "have"; rather, it's something that people "do" or "experience". The mental disorders in the DSM/ICD are merely a description of behaviors. They are fuzzy concepts with vague boundaries. They are subjective, often without a clear "yes" or "no" as to whether someone meets criteria for a particular disorder. Mental disorders are defined in relation to cultural, social, and familial norms and values. That is, they are socially defined not biologically! The boundaries between normality and abnormality vary across cultures. Thus, the diagnostic categories in the DSM/ICD may not involve "biological pathology"—they do not carve nature at its joints.

Researchers have examined biology-related differences people with versus without mental disorders, and they have found some differences, but these biology-related differences are not able to classify perfectly because the categories are defined socially in terms of behaviors, and not in terms of biology.

Another concern with the DSM and ICD is their conceptualization of psychopathology as binary. The DSM and ICD treat most disorders as categorical phenomena that are binary in nature: either you have a disorder or you do not. There is no gray in this conceptualization of psychopathology. However, research has shown that the difference between "normal" and "abnormal" behavior frequently is one of degree rather than kind [**CITATION**]. Thus, a dimensional approach to classification may provide more valid portrayal of many clinical phenomena than the categorical approach used by the DSM and ICD.

There is also potential bias in the diagnostic system or its application, especially against people with particular disorders. For instance, some therapists will not treat people with particular disorders, especially certain types of personality disorders (e.g., borderline personality disorder, antisocial personality disorder), because of perceived challenges. The diagnostic system also might overestimate abnormality in women (e.g., premenstrual dysphoric disorder) or racial/ethnic minorities.

In general, reliability of diagnoses is low. The [inter-rater reliability](#interrater-reliability) of diagnoses tends to be low [**Lobbestael et al. (2010)**]. Moreover, [test–retest reliability](#testRetest-reliability) is also relatively low. A client might cycle back and forth between diagnosis and no diagnosis based on one or a few symptoms. For instance, major depressive disorder is given a diagnosis with 5 or more symptoms, but not diagnosis is given with 4 or few symptoms. The fluctuation of symptoms and disorder status across time suggests that a binary approach to diagnosis is inaccurate.

It will thus be important to improve our diagnostic system. For instance, it will be important for the diagnostic system to account for the dimensional nature of constructs so that the diagnostic system better carves nature at its joints (better validity) and is flexible to allow different optimal thresholds for different social and clinical decisions (better utility). One argument against dimensional diagnostic systems are that treatment decisions are often binary (e.g., whether or not to provide treatment). However, decisions in practice can be dimensional too, and can reflect different levels of treatment, including, for example, the dosage of medication, the frequency and intensity of therapy, and the degree of hospitalization.

## Conclusion
