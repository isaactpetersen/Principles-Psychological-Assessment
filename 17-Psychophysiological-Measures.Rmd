# Psychophysiological Assessment {#psychophysiological}

[**Calamia, 2019 citation**; **Holmlund et al, 2019 citation**, **Raugh et al, 2019 citation**, **Roberts et al, 2019, Psych Assessment citation on VR**: https://psycnet.apa.org/PsycARTICLES/journal/pas/31/3]

[**Himmelstein et al, 2019 citation**; **Patrick et al, 2019 citation**; **Stanton et al, 2020 citation**; **Trull & Ebner-Priemer citation**]

There are a number of alternative conceptualizations of psychopathology compared to the conceputalization of psychopathology provided in the Diagnostic and Statistical Manual of Mental Disorders (DSM). One alternative conceptualization of psychopathology is the *p*-factor. The *p*-factor is a hierarchical view of psychopathology that accounts for covariation among various kinds of psychopathology [**CITATION**]. The *p*-factor is a general psychopathology factor, similar to the general intelligence factor (*g*), and has three sub-factors: internalizing problems, externalizing problems, and thought-disordered problems. Hierarchical conceptualizations tend to be advanced by lumpers, who group similar forms of psychopathology together, as opposed to splitters, who tend to split forms of psychopathology into separate categories.

Another hierarchical view of psychopathology is HiTOP, the Hierarchical Taxonomy of Psychopathology [**CITATION**].

A third conceptualization of psychopathology is presented by the Research Domain Criteria, also called (RDoC) from the National Institute of Mental Health (NIMH).

## NIMH Research Domain Criteria (RDoC)

The National Institute of Mental Health (NIMH) has funded research to understand the etiology of mental illnesses and how best to intervene. Historically, NIMH funded considerable work on traditional DSM-based diagnostic categories. However, work studying DSM-defined diagnostic categories has not had as much impact as NIMH would have liked. So, the NIMH led an initiative known as the Research Domain Criteria, also called RDoC, to provide a template for research on psychopathology.

There are several ways in which RDoC is different from the DSM. Compared to the DSM, RDoC is dimensional, not categorical. RDoC views psychopathology as existing on continua, not as discrete categories. Another difference is that RDoC works from the ground up, starting with brain–behavior relations, and linking those to clinical symptoms. By contrast, the DSM is top-down, starting with diagnostic categories and determining what fits in those categories based on behavioral symptoms. However, as discussed in Section \@ref(dsmConcerns), diagnoses are fictive categories that have poor diagnostic validity. The DSM-defined categories are based on behavioral presentations. That is, disorders are not things that people "have"; they are things that people "do."

The same behavior can occur for different underlying reasons, a principle known as equifinality. Different neurodevelopmental trajectories may underlie the same behavior for two different people. Behavioral presentations in the same disorder are heterogeneous. The pathophysiology in a given DSM category is not unitary.
DSM categories also share symptoms and show lots of co-occurrence, so they are not entirely distinct. The DSM has resulted in the reification of these fictive diagnostic categories.

RDoC is a template for psychopathology research that consists of dimensional constructs integrating elements of psychology and biology. It incorporates a wider range of data, and multiple levels of analysis, including genetics, brain structure and function, and physiology. By contrast, the DSM only incorporates symptoms. In RDoC, there is a focus on narrower dimensions, such as cognition, emotion, reward seeking, fear, learning, memory, motivation, and perception, rather than disorders. The focus on narrow dimensions is based on the idea that there is greater potential to relate biological processes to simpler, lower-order, narrower dimensions of psychological constructs compared to disorders. These narrower dimensions of psychological constructs can be related to important clinical dimensions. Psychopathology is thought to represent extremes on the psychobiological distribution: e.g., high fearfulness in phobia versus low fearfulness in psychopathy.

### RDoC Matrix

The RDoC matrix is a matrix of two dimensions: six domains by eight units of analysis. In the RDoC matrix, domains are constructs that include elements, processes, mechanisms, and responses. The domains include: negative valence systems (i.e., negative affect: fear, anxiety, etc.), positive valence systems (i.e., positive affect: reward processing), cognitive systems (attention, perception, language, cognitive control, working memory), social processes (attachment, social communication, empathy), arousal and regulatory systems (sleep, circadian rhythms), sensorimotor systems (e.g., motor actions).

In the RDoC matrix, units of analysis are are classes of measurement that are similar to levels of analysis. Units of analysis include: genes, molecules, cells, circuits, physiology, behaviors, self-reports, and paradigms.

The RDoC matrix is presented in Figure \@ref(fig:RDoC).

```{r RDoC, out.width = "100%", fig.align = "center", fig.cap = "National Institute of Mental Health (NIMH) Research Domain Criteria (RDoC) matrix.", echo = FALSE}
knitr::include_graphics("./Images/RDoC.png")
```

In the RDoC framework, there is theoretical neutrality of the units of analysis. No unit of analysis is thought to be more important than any other; they are each thought to be important in their own right. That is why they are referred to as units of analysis rather than levels of analysis. "Levels" connotes order from basic to higher-order, and the developers of RDoC wanted to avoid suggesting that one level underlies another.

RDoC dimensional constructs are meant to be integrative rather than reductionistic. It involves moving away from the subjectivist tradition and moving toward a heterophenomenological approach. The subjectivist tradition is one in which subjective experience is deemed the primary measure of a phenomenon (e.g., fear). The heterophenomenological approach combines multiple units of analysis to identify phenomena.

The domains and constructs will need to be added and refined with further research. The RDoC matrix is just a tentative and incomplete starting template. The goal is to understand the bridge (i.e., mechanisms) that link the different units of analysis for the same construct, for example, how the biological and psychological bases of fear influence each other. Examples of mechanisms include endophenotypes, that are heritable, "unobservable," intermediary traits that signify disease liability, and that mediate the association between genotype and phenotypic expressions of psychopathology.

The idea is that genes influence endophotypes and that endophenotypes lead to the phenotype. An endophenotype is similar to "intermediate phenotype", but without the requirement of a genetic cause. Endophenotypes and intermediate phenotypes are different from biomarkers, which are biological indicators that are not necessarily causal. All endophenotypes are biomarkers, but not all biomarkers are endophenotypes.

One dimension that the RDoC does not include is development. It has been a challenge incorporating development into the RDoC matrix. [**CITATION**]

## Psychophysiological Measures

Examples of psychophysiological measures include electroencephalography (EEG), event-related potential (ERP), (functional) magnetic resonance imaging (f)MRI, computerized axial tomography (CAT), magnetoencephalography (MEG), functional near infrared spectroscopy (fNIRS), electrocardiography (ECG or EKG), electromyography (EMG), electrooculography (EOG), eyetracking, and actigraphy.

EEG caps can include 256 electrode sensors from millisecond to millisecond, with strong temporal resolution, yielding lots of data. Having more sensors allows better estimates of spatial localization. Psychophysiological measures are relevant because lots of behavior problems have important physiological facets. But it is not always clear whether biological processes cause psychopathology or the reverse, or whether there are third variable confounds that influence both psychophysiology and behavior problems (that explain why they are associated, even though they are non-causally related).

Psychophysiological measures are not invulnerable to basic measurement issues, including [reliability](#reliability) and [validity](#validity). Psychophysiological measures tend to be more expensive than questionnaire measures, so people traditionally were reluctant to consider issues of [reliability](#reliability) and [validity](#validity).

Galton and Cattell, nearly 200 years ago, were more interested in direct measures, like grip strength, than self-report. Hans Berger made the first EEG recording in 1924: EEG has been around a long time.

### Reliability

[Reliability](#reliability) matters, especially when you are examining individual differences. From a [generalizability theory](#gTheoryReliability) point of view, we could examine the consistency or inconsistency of scores across facets and factors to determine which ones matter and which ones do not matter.

[Internal consistency](#internalConsistency-reliability) involves examining the consistency of scores across trials to demonstrate that the measure assesses the same thing across trials. An example of [internal consistency](#internalConsistency-reliability) that could be applied to psychophysiological data is [split-half reliability](#splitHalf-reliability).

Another important form of [reliability](#reliability) is [test–retest reliability](#testRetest-reliability), which involves examining the consistency of scores across time. You could examine the consistency of individual differences across time, i.e., [stability](#stability), and the consistency of scores within a person across time, i.e., [repeatability](repeatability). An example of [repeatability](repeatability) would be examining a [Bland-Altman plot](#blandAltmanPlot).

Another important form of reliability is [inter-rater reliability](#interrater-reliability), which involves examining the consistency of scores across raters or processing methods. The goal is to generalize across instruments (machines, operators, etc.) to identify the reliability of the general method and not just the reliability of a particular operator or machine.

[Parallel-forms reliability](#parallelForms-reliability) would involve examining the consistency of scores across slightly different stimuli thought to reflect the same cognitive processes. For instance, one could examine the reliability of scores across trials, time, raters, and stimuli. And if the reliability is low, it would be important to follow up to determine why the reliability is low.

It can be beneficial to conduct sensitivity analyses. Sensitivity analysis evaluate the robustness of findings by examining the extent to which findings differ based on changes in methods. For instance, you could examine whether findings differ when using different processing methods. If findings are consistent when using different processing methods, this provides further confidence in your findings because they do not appear to be due to the processing method chosen. However, if the findings differ across processing methods, it tell you that the processing method has an important consequence in relation to your outcome of interest. Sensitivity analysis can help you learn more about the phenomenon of interest.

### Lots of Data

Psychophysiological measures yield lots of data. Some of the variance in the data reflects garbage (noise), some of the variance in the data reflects signal. So, when collecting lots of information, there's a possibility of finding an effect merely by chance. There are so many researcher degrees of freedom. There are so many ways to process the data. There are so many brain voxels. There are often multiple conditions. And there are so many ways to analyze the data. The research degrees of freedom can lead to *p*-hacking.

An example of the potential problem with researcher degrees of freedom is the "dead salmon study" (**Bennett et al., 2009 and 2010 citations**). In the study, activation was detected in voxels of a dead salmon during fMRI scanning. The false positive activation detected was due to a failure to account for many multiple comparisons. The point of the dead salmon study was not to invalidate fMRI, it was to show the importance of how rich data should be analyzed to adjust for multiple testing to lower the rate of Type I errors.

When dealing with lots of data, over-fitting is a common issue. As described in Section \@ref(#overfitting), over-fitting involves explaining noise variance, a finding which would not generalize to a new sample. To avoid over-fitting when examining individual differences, such as with a correlation, it can be helpful to cross-validate the findings in an independent sample or in a hold-out sample. A hold-out sample is when you sub-divide a sample into a training and test data set. You develop the model on the training data set and see how the findings cross-validate on the hold-out (test) data set. Cross-validation requires larger sample (typically $N > 100$), which is difficult with psychophysiological measures. Most samples with psychophysiological data are relatively small, and as a result, have low power [**Button citation**]. With lots of data, researcher degrees of freedom, a small sample, and a statistical significance filter, reported findings have inflated estimates of effect sizes and poor replicability [@Loken2017].

One way to deal with the large data is to reduce the massive data down, and to use scoring and algorithms. For instance, you can examine the reliability of the scoring system, versus the reliability of the person conducting the scoring system. For data reduction, [principal component analysis](#pca) (PCA) is a useful technique.

Historically, psychophysiological data were averaged across subjects (group averaging) to improve the signal-to-noise ratio. For example, data were averaged for a clinical group versus controls. However, we think of psychopathology as dimensional and involving individual differences. There have been recent attempts to examine data at the individual differences level.

Issues such as [reliability](#reliability) and [validity](#validity) are even more crucial when trying to make inferences about individual differences. It is important to remove as much noise as possible without sacrificing signal. Movement artifacts in psychophysiological measurements can lead to poor data quality. It is important to correct for artifacts because they do not occur at random. For example, patients with attention-deficit hyperactivity disorder (ADHD) and schizophrenia will likely show more movement artifacts than controls. One approach is to regress out the artifact (e.g., heart rate), if you have an independent measure of the artifact. If you do not have an independent measure of the artifact, you can use independent component analysis (ICA) to separate independent components of data and to remove artifacts.

### Validity

Consideration of aspects of [validity](#validity) are crucial for any measure, including psychophysiological measures.

Consideration of [construct validity](#constructValidity), whether the measure actually assesses what it intends to assess, is crucial. For instance, performance on a measure of counting back by 7 is associated with many domains, including psychosis, anxiety, impulsivity, intelligence, etc. It is a measure of many constructs. It is important for our measures to have [convergent validity](#convergentValidity) in that they associated with what they should be associated with, but also [discriminant validity](#discriminantValidity), in that they should not be associated with things that we do not expect them to be associated with.

If the research identifies activation in a specific part of the brain in association with the task, it is important to consider whether it is the same region across participants. For instance, the researcher should consider whether they are using a single atlas/head model for the whole sample or whether they are using a personalized atlas for each participant. Oftentimes, a given brain region is involved in lots of different cognitive processes and behaviors. This raises construct validity questions.

Ask yourself, "What is the evidence to support my interpretation of these effects?" Evidence is strengthened by showing evidence of [convergent](#convergentValidity) and [discriminant](#discriminantValidity) validity. For example, differential deficits can provide stronger evidence of an effect. For instance, schizophrenics show deficits, abnormalities, or differences in many different brain processes—but an important question is whether they show relatively greater abnormalities in a given neural process than another neural process. This helps inform the specificity of the process (as opposed to other processes) for explaining differences in the condition of interest.

Another important question is: what is considered baseline? For heart rate reactivity and many psychophysiological measures, a comparison is made to a  "baseline" (e.g., a contrast or difference score). But what should be considered "baseline"? There are many different possibilities for what could be considered "baseline", and each could yield different results; participants' brains are not resting during "resting-state".

Another important consideration is the [ecological validity](#ecologicalValidity) of the measures. Measures are often assessed in contexts and with stimuli that are not naturalistic. Psychophysiological measures may not reflect a participant's typical functioning. A goal would be to use more naturalistic contexts and task paradigms. Another challenge that biological measures have is in relating the biological criteria to actual behavior in the real world. It is not sufficient to show group differences on a biological measure; the researcher needs to show that the biological measure is associated with particular behaviors, that is, an independent measure in the same subjects.

When [reliability](#reliability) and [validity](#validity) of psychophysiological measures are improved, a goal is to have norms for their use, so you know where a person stands compared to the population on the measure of interest.

### Examples

#### Ambulatory Biosensors

An ambulatory assessment is a measurement strategy designed to acquire minimally disruptive measures of a person engaging in normal activities in their natural environment. Ambulatory biosensors are assessment tools that measure physiological or motor activity. Ecological momentary assessment is an example of an ambulatory assessment.

The three most popular versions of ambulatory biosensors are those that monitor cardiovascular activity (e.g., heart rate and blood pressure), physical activity (e.g., actigraph and pedometer), and cortisol levels. However, other ambulatory biosensors assess EEG, respiration, muscle tension, blood glucose, skin conductance, and other processes.

An applied example of an ambulatory biosensor is actigraphy. Actigraphy is the tracking of physical movement. Actigraphy uses an actigraph device. An actigraphy is a watch-like device attached to wrist or ankle that has an accelerometer. Smartphones can also serve as actigraphs because they have accelerometers. Actigraphy is a peripheral measure because it assesses gross motor movements. Actigraphy is also frequently used as a corresponding index of sleep because limb movements occur less frequently during sleep. Actigraphy is considered a better index of sleep than self-/informant-report, but not as good as polysomnography (PSG). However, actigraphy allows for greater ecological validity than a sleep study conducted in the lab, because a person can wear it while sleeping in their own bed at home.

In prior work, we took hundreds of sleep variables from actigraphy and using PCA to reduce the data down to key independent dimensions of sleep deficit: activity (sleep quality), (late) timing, (short) duration, and night-to-night variability [@Staples2019].

##### Pros

Pros of ambulatory biosensors include:

- Ambulatory biosensors do not rely on participants to report their experience; they provide information about constructs that is derived from a different method—and they are useful in a multi-method assessment strategy. Ambulatory biosensors lead to an increase in incremental validity and can improve specificity of decisions.
- Ambulatory biosensors are highly sensitive to change—they can track rapid changes in the variable of interest, with rapid timecourses.
- Ambulatory biosensors are portable, and have potential for greater ecological validity. Biosensor variables collected in naturalistic settings are often only weakly associated with biosensor variables collected in lab settings, possibly due to reactivity in a lab setting.

##### Cons

Cons of ambulatory biosensors include:

- Ambulatory biosensors are expensive—potentially not cost-efficient depending on the use—i.e., they potentially too expensive for clinical use
- Some devices can be intrusive, cumbersome, and technical
- There is limited psychometric evaluation conducted with many ambulatory methods. Ambulatory biosensors are potentially unreliable. In addition, ambulatory biosensors are potentially not accurate for certain trait levels. For example, heart rate devices may be less accurate at high levels of physical activity. Ambulatory biosensors may not generalize well across time, devices, labs, and raters.

#### Other Examples

Other examples of psychophysiological measures include eyetracking, which is used to assess attention, and wearables, which are smart "textiles" or clothing that has embedded biosensors. 

### Using Psychophysiological Measures

Ideally, we would be able to combine psychophysiological measures as objective measures with other measures, e.g., a performance-based assessment or self-/informant-report, in a multi-method measurement of constructs. Psychophysiological measures as part of a multi-method assessment approach helps to address the bias of participants and the bias in clinical judgment of clinicians. Ambulatory measures can provide [incremental validity](#incrementalValidity) and increase the specificity of clinical judgment.

Challenges exist in selecting the granularity of analysis—how fine-grained to assess at each level of analysis—and how to combine measures across multiple levels of analysis because each may operate at different timescales. One option is to use multi-modal imaging, e.g., simultaneous recordings of EEG and fMRI or EEG and fNIRS, to cancel out weaknesses of each. For instance, EEG has stronger temporal resolution than fMRI—it provides a faster timecourse— but EEG has weaker spatial resolution than fMRI—it provides less accurate estimates of where in the brain the neural activity is occurring.

It is worth consider combining multiple measures in a latent variable framework using, for example, [factor analysis](#factorAnalysisOverview) or [structural equation modeling](#sem).

Given the costs, challenges, and time-consuming nature of psychophysiological data collection, science may be best positioned to address issues of [reliability](#reliability) and [validity](#validity) issues by sharing and combining data sets through consortia and [open science](#openScience). Consortia and data sharing can help ensure large enough sample sizes for adequate statistical power, especially for individual difference analyses. Larger samples would also allow cross-validation of analyses in independent samples using out-of-sample prediction.

## Conclusion
