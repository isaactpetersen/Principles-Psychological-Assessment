# Clinical Judgment vs. Algorithmic Prediction {#actuarial}

## Approaches to Prediction

There are two primary approaches to [prediction](#prediction): human (i.e., clinical) judgment and the actuarial (i.e., statistical) method.

### Human/Clinical Judgment

Using the clinical judgment method of [prediction](#prediction), all gathered information is collected and formulated into a diagnosis or [prediction](#prediction) in the clinician's mind. The clinician selects, measures, and combines risk factors and produces risk estimate solely according to clinical experience and judgment.

### Actuarial/Statistical Method

In the actuarial or statistical method of [prediction](#prediction) (i.e., statistical [prediction](#prediction) rules), information is gathered and combined systematically in an evidence-based statistical [prediction](#prediction) formula, and established cutoffs are used. The method is based on equations and data, so both are needed.

An example of a statistical method of [prediction](#prediction) is the Violence Risk Appraisal Guide [@Rice2013]. The Violence Risk Appraisal Guide is used in an attempt to predict violence and is used for parole decisions. For instance, the equation might be something like:

$$
\scriptsize
\text{Violence risk} = \beta \cdot \text{conduct disorder} + \beta \cdot \text{substance use} + \beta \cdot \text{suspended from school} + \beta \cdot \text{childhood aggression} + ...
$$

Then, based on their score and the established cutoffs, a person is given a "low risk," "medium risk," or "high risk" designation.

### Combining Human Judgment and Statistical Algorithms

There are numerous ways in which humans and statistical algorithms could be involved. On one extreme, humans make all judgments (consistent with the clinical judgment approach). On the other extreme, although humans may be involved in data collection, a statistical formula makes all decisions based on the input data, consistent with an actuarial approach. However, the clinical judgment and actuarial approaches can be combined in a hybrid way [@Dana2006]. For example, to save time and money, a clinician might use an actuarial approach in all cases, but might only use a clinical approach when the actuarial approach gives a "positive" test. Or, the clinician might use both human judgment and an actuarial approach independently to see whether they agree. That is, the clinician may make a [prediction](#prediction) based on their judgment and might also generate a [prediction](#prediction) from an actuarial approach.

The challenge is what to do when the human and the algorithm disagree. Hypothetically, humans reviewing and adjusting the results from the statistical algorithm could lead to more accurate [predictions](#prediction). However, human input also could lead to the possibility or exacerbation of [biased](#bias) [predictions](#prediction). In general, with very few exceptions, actuarial approaches are as accurate or more accurate than clinical judgment [@Dawes1989; @Aegisdottir2006; @Baird2000; @Grove1996; @Grove2000]. Moreover, the superiority of actuarial approaches to clinical judgment tends to hold even when the clinician is given more information than the actuarial approach [@Dawes1989]. Allowing clinicians to override actuarial [predictions](#prediction) consistently leads to lower predictive accuracy [@Garb2019].

In general, [validity](#validity) tends to increase with greater structure (e.g., [structured](#structuredInterview) or [semi-structured](#semiStructuredInterview) [interviews](#interview) as opposed to free-flowing, [unstructured interviews](#unstructuredInterview)). [Unstructured interviews](#unstructuredInterview) are susceptible to confirmatory [bias](#bias), such as exploring only the diagnoses that confirm the clinician's hypotheses rather than attempting to explore diagnoses that disconfirm the clinician's hypotheses, which leads to fewer diagnoses. [Unstructured interviews](#unstructuredInterview) are also susceptible to [bias](#bias) relating to race, gender, class, etc. [@Garb1997; @Garb2005].

There is sometimes a misconception that formulas cannot account for qualitative information. However, that is not true. Qualitative information can be scored or coded to be quantified so that it can be included in statistical formulas. That said, the quality of [predictions](#prediction) rests on the quality and relevance of the assessment information for the particular [prediction](#prediction)/judgment decision. If the assessment data are lousy, it is unlikely that a statistical algorithm (or a human for that matter) will make an accurate [prediction](#prediction). "Garbage in, garbage out." A statistical formula cannot rescue inaccurate assessment data.

## Errors in Clinical Judgment

Clinical judgment is naturally subject to errors. Below, I describe a few errors to which clinical judgment seems particularly prone.

When operating freely, clinicians tend to over-estimate exceptions to the established rules (i.e. the broken leg syndrome). @Meehl1957 acknowledged that there may be some situations where it is glaringly obvious that the statistical formula would be incorrect because it fails to account for an important factor. He called these special cases "broken leg" cases, in which the human should deviate from the formula (i.e., broken leg countervailing). The example goes like this:

> "If a sociologist were predicting whether Professor X would go to the movies on a certain night, he might have an equation involving age, academic specialty, and introversion score. The equation might yield a probability of .90 that Professor X goes to the movie tonight. But if the family doctor announced that Professor X had just broken his leg, no sensible sociologist would stick with the equation. Why didn't the factor of 'broken leg' appear in the formula? Because broken legs are very rare, and in the sociologist's entire sample of 500 criterion cases plus 250 cross-validating cases, he did not come upon a single instance of it. He uses the broken leg datum confidently, because 'broken leg' is a subclass of a larger class we may crudely denote as 'relatively immobilizing illness or injury,' and movie-attending is a subclass of a larger class of "actions requiring moderate mobility." [@Meehl1957]

However, people too often think that cases where they disagree with the statistical algorithm are broken leg cases. People too often think their case is an exception to the rule. As a result, they too often change the result of the statistical algorithm and are more likely to be wrong than right in doing so. Because actuarial methods are based on actual population levels (i.e., [base rates](#baseRate)), unique exceptions are not over-estimated.

Actuarial [predictions](#prediction) are perfectly [reliable](#reliability)—they will always return the same conclusion given an identical set of data. The human judge is likely to both disagree with others and with themself given the same set of symptoms.

A clinician's decision is likely to be influenced by past experiences, and given the sample of humanity that the clinician is exposed to, the clinician is likely (based on prior experience) to over-estimate the likelihood of occurrence of infrequent phenomena. Actuarial methods are based on objective algorithms, and past personal experience and personal [biases](#bias) do not factor into any decisions. Clinicians give weight to less relevant information, and often give too much weight to singular variables (e.g., Graduate Record Examination scores). Actuarial formulas do a better job of focusing on relevant variables. Computers are good at factoring in [base rates](#baseRate), [inverse conditional probabilities](#inverseFallacy), etc. Humans ignore [base rates](#baseRate) ([base rates](#baseRate) neglect), and tend to show [confusion of the inverse](#inverseFallacy).

Computers are better at accurately weighing risk factors and calculating [unbiased](#bias) risk estimates. In an actuarial formula, the relevant risk factors are weighted according to the predictive power. This stands in contrast to the [Diagnostic and Statistical Manual of Mental Disorders (DSM)](#dsm), in which each symptom is equally weighted.

Humans are typically given no feedback on their judgments. To improve accuracy of judgments, it is important for feedback to be clear, consistent, and timely. It is especially unlikely for feedback in clinical psychology to be timely because we will have to wait a long time to see the outcomes of [predictions](#prediction).

Clinicians are susceptible to representative schema [biases](#bias) [@Dawes1986]. Clinicians are exposed to a skewed sample of humanity, and make judgments based on a prototype from their ([biased](#bias)) experiences. This is known as the representativeness heuristic. Different clinicians may have different prototypes, leading to lower [inter-rater reliability](#interrater-reliability).

## Humans versus Computers

### Advantages of Computers

Here are some advantages of computers over humans:

- Computers can process lots of information simultaneously. So can humans. But computers can to an even greater degree.
- Computers are faster at making calculations.
- Computations by computers are error free (as long as the computations are programmed correctly).
- Computers' judgments will not be [biased](#bias) by fatigue or emotional responses.
- Computers' judgments will tend not to be [biased](#bias) in the way that humans' cognitive [biases](#bias) are, such as with anchoring [bias](#bias), representativeness [bias](#bias), confirmation [bias](#bias), or recency [bias](#bias) Computers are less likely to be over-confident in their judgments.
- Computers can more accurately weight the set of predictors based on large data sets. Humans tend to give too much weight to singular predictors.

### Advantages of Humans

Computers are bad at some things too. Here are some advantages of humans over computers (as of now):

- Humans can be better at identifying patterns in data (but also can mistakenly identify patterns where there are none).
- Humans can be flexible and take a different approach if a given approach is not working.
- Humans are better at tasks requiring creativity and imagination, such as developing theories that explain phenomena.
- Humans have the ability to reason, which is especially important when dealing with complex, abstract, or open-ended problems, or problems that have not been faced before (or for which we have insufficient data).
- Humans are better able to learn.
- Humans are better at holistic, gestalt processing, including facial and linguistic processing.

There *may* be situations in which a clinical judgment would do better than an actuarial judgment. One situation when clinical judgment would be important is when no actuarial method exists for the judgment or [prediction](#prediction). For instance, when no actuarial method exists for the diagnosis or disorder (e.g., suicide), it is up to the clinical judge. However, we could collect data on the outcomes or on clinicians' judgments to develop an actuarial method that will be more reliable than the clinicians' judgments. That is, an actuarial method developed based on clinicians' judgments will be more [accurate](#validity) than clinicians' judgments. In other words, we do not necessarily need clients' outcome data to develop an actuarial method. We could use the client's data as [predictors](#prediction) of the clinicians' judgments to develop a structured approach to [prediction](#prediction) that weighs factors similarly to clinicians, but with more [reliable](#reliability) [predictions](#prediction).

Another situation in which clinical judgment could outperform a statistical algorithm is in true "broken leg" cases, e.g., important and rare events (edge cases) that are not yet accounted for by the algorithm.

Another situation in which clinical judgment could be preferable is if advanced, complex theories exist. Computers have a difficult time adhering to complex theories, so clinicians may be better suited. However, we do not have any of these complex theories in psychology that are accurate. We would need strong theory informed by data regarding causal influences, and [accurate](#validity) measures to assess them. If theories alone were true and could explain everything, the psychoanalytic tradition would give us all the answers, as depicted in Figure \@ref(fig:psychoanalyticTradition). However, no theories in psychology are that good. Nevertheless, [predictive accuracy](#predictiveValidity) can be improved when considering theory [@Silver2012; @Garb2019].

```{r psychoanalyticTradition, out.width = "100%", fig.align = "center", fig.cap = "Depiction of the psychoanalytic tradition.", echo = FALSE}
knitr::include_graphics("./Images/psychoanalyticTradition.png")
```

If the diagnosis/[prediction](#prediction) requires complex configural relations that a computer will have a difficult time replicating, a clinician's judgment may be preferred. Although the likelihood that the clinician can accurately work through these complex relations is theoretically possible, it is highly unlikely. Holistic pattern recognition (such as language and faces) tends to be better by humans than computers But, computers are getting better with holistic pattern recognition through machine learning. An example of a simple decision (as opposed to a complex decision) would be whether to provide treatment A (e.g., cognitive behavior therapy) or treatment B (e.g., medication) for someone who shows melancholy.

In sum, the clinician seeks to integrate all information to make a decision, but is [biased](#bias).

### Comparison of Evidence

Hundreds of studies have examined clinical versus actuarial [prediction](#prediction) methods across many disciplines. Findings consistently show that actuarial methods are as [accurate](#validity) or more [accurate](#validity) than clinical [prediction](#prediction) methods. "There is no controversy in social science that shows such a large body of qualitatively diverse studies coming out so uniformly...as this one" [@Meehl1986, pp. 373–374].

Actuarial methods are particularly valuable for criterion-referenced assessment tasks, in which the aim is to [predict](#prediction) specific events or outcomes [@Garb2019]. For instance, actuarial methods have shown promise in [predicting](#prediction) violence, criminal recidivism, psychosis onset, course of mental disorders, treatment selection, treatment failure, suicide attempts, and suicide [@Garb2019]. Psychometric methods of scale construction such as [factor analysis](#factorAnalysis) may be preferred to statistical [prediction](#prediction) rules for [norm-referenced assessment](#norm) tasks such as describing personality and psychopathology [@Garb2019].

Moreover, actuarial methods are explicit; they can be transparent and lead to informed scientific criticism to improve them. By contrast, clinical judgment methods are not typically transparent; clinical judgment relies on mental processes that are often difficult to specify.

## Accuracy of Different Statistical Models {#modelAccuracy}

It is important to evaluate the [accuracy](#validity) of different types of statistical models to provide guidance on which types of models may be most [accurate](#validity) for a given [prediction](#prediction) problem. Simpler statistical formulas are more likely to generalize to new samples than complex statistical models due to model *over-fitting*. As described in Section \@ref(overfitting), [over-fitting](#overfitting) occurs when the statistical model accounts for error variance (an overly specific [prediction](#prediction)), which will not generalize to future samples.

@Youngstrom2018 discussed the benefits of the probability nomogram. The [probability nomogram](#bayesianUpdating) combines the risk ratio with the base rate (according to [Bayes theorem](#bayesTheorem)) to generate a [prediction](#prediction). The [risk ratio](#diagnosticLikelihoodRatio) ([diagnostic likelihood ratio](#diagnosticLikelihoodRatio)) is an index of the predictive validity of an instrument: it is the ratio of the probability that a test result is correct to the probability that the test result is incorrect. There are types of diagnostic likelihood ratios: the [positive likelihood ratio](#positiveLikelihoodRatio) and the [negative likelihood ratio](#negativeLikelihoodRatio). As described in Equation \@ref(eq:positiveLikelihoodRatio), the [positive likelihood ratio](#positiveLikelihoodRatio) is the probability that a person with the disease tested positive for the disease ([true positive rate](#sensitivity)) divided by the probability that a person without the disease tested positive for the disease ([false positive rate](#falsePositiveRate)). As described in Equation \@ref(eq:negativeLikelihoodRatio), the [negative likelihood ratio](#negativeLikelihoodRatio) is the probability that a person with the disease tested negative for the disease ([false negative rate](#falseNegativeRate)) divided by the probability that a person without the disease tested negative for the disease ([true negative rate](#specificity)). Using a [probability nomogram](#bayesianUpdating), you start with the [base rate](#baseRate), then plot the [diagnostic likelihood ratio](#diagnosticLikelihoodRatio) corresponding to a second source of information (e.g., positive test); then connect the [base rate](#baseRate) ([pretest probability](#pretestProbability)) to the [posttest probability](#posttestProbability) through the [likelihood ratio](#diagnosticLikelihoodRatio). These approaches are described in greater detail in Section \@ref(bayesianUpdating).

@Youngstrom2018 ordered actuarial approaches to diagnostic classification from less to more complex:

- [Predicting from the base rate](#predictingFromBaseRate)
- Take the best
- The [probability nomogram](#bayesianUpdating)
- Naïve Bayesian algorithms
- Logistic regression with one predictor
- Logistic regression with multiple predictors
- Least absolute shrinkage and selection option (LASSO)

As described in Section \@ref(predictingFromBaseRate), [predicting from the base rate](#predictingFromBaseRate) is selecting the most likely outcome in every case.	"Take the best" refers to focusing on the single variable with the largest [validity](#validity) coefficient, and making a decision based on it (based on some threshold). The [probability nomogram](#bayesianUpdating) combines the [base rate](#baseRate) ([prior probability](#pretestProbability)) with the information offered by an assessment finding to revise the probability. Naïve Bayesian algorithms use the [probability nomogram](#bayesianUpdating) with multiple assessment findings. Basically, you continue to calculate a new [posttest probability](#posttestProbability) based on each assessment finding, revising the [pretest probability](#pretestProbability) for the next assessment finding based on the [posttest probability](#posttestProbability) of the last assessment finding. However, this approach assumes that all [predictors](#prediction) are uncorrelated, which is probably not true in practice.

Logistic regression is useful for classification problems because it deals with a dichotomous dependent variable, but one could extrapolate it to a continuous dependent variable with multiple regression. The effect size from logistic regression is equivalent to [receiver operating characteristic (ROC) curve](#roc) analysis. Logistic regression with multiple predictors combines multiple [predictors](#prediction) into one regression model. Including multiple [predictors](#prediction) in the same model optimizes the weights of multiple [predictors](#prediction), but it is important for [predictors](#prediction) not to be collinear.

LASSO is a form of regression that can handle multiple [predictors](#prediction) (like multiple regression). However, it can handle more [predictors](#prediction) than multiple regression. For instance, LASSO can accommodate situations where there are more [predictors](#prediction) than there are participants. Moreover, unlike multiple regression, it can handle [collinear](#multiCollinearity) predictors. LASSO	uses internal cross-validation to avoid over-fitting.

@Youngstrom2018 found that model complexity improves [accuracy](#validity) but only to a point; some of the simpler models (Naïve Bayesian and Logistic regression) did just as well and in some cases better than LASSO models in classifying bipolar disorder.	Although LASSO models showed the highest [discrimination accuracy](#discrimination) in the internal sample, it showed the largest shrinkage in the external sample. Moreover, the LASSO models showed poor [calibration](#calibration) in the internal sample; they over-diagnosed bipolar disorder. By contrast, simpler models showed better [calibration](#calibration) in the internal sample. The [probability nomogram](#bayesianUpdating) with one or multiple assessment findings showed better [calibration](#calibration) than LASSO models because they accounted for the original [base rate](#baseRate), so they did not over-diagnose bipolar disorder.

In summary, the best models are those that are relatively simple (parsimonious), that can account for one or several of the most important [predictors](#prediction) and their optimal weightings; and that account for the [base rate](#baseRate) of the phenomenon. Even unit-weighted formulas (formulas whose [predictors](#prediction) are equally weighted with a weight of one) can sometimes generalize better to other samples than complex weightings [@Garb2019]. Differential weightings sometimes capture random variance and [over-fit](#overfitting) the model, thus leading to [predictive accuracy](#predictiveValidity) shrinkage in cross-validation samples [@Garb2019], as described below. The choice of [predictive](#prediction) variables often matters more than their weighting.

In general, there is often shrinkage of estimates from training data set to a test data set. *Shrinkage* is when variables with stronger predictive power in the original data set tend to show somewhat smaller predictive power (smaller validity coefficients) when applied to new groups. Shrinkage reflects a model [over-fitting](#overfitting) (i.e., fitting to error by capitalizing on chance). Shrinkage is especially likely when the original sample is small and/or unrepresentative and the number of variables considered for inclusion is large.	Cross-validation with large, representative samples can help evaluate the amount of shrinkage of estimates, particularly for more complex models such as machine learning models [@Ursenbach2019]. Ideally, cross-validation would be conducted with a separate sample (external cross-validation) to see the generalizability of estimates. However, you can also do internal cross-validation. For example, you can perform *k*-fold cross-validation, where you:

- split the dataset into *k* groups
- for each unique group:
  - take the group as a hold out or test data set
  - take the remaining groups as a training data set
  - fit a model on the training data set and evaluate it on the test data set
  - after all k-folds have been used as the test data set, and all models have been fit, you average the estimates across the models, which presumably yields more robust, generalizable estimates

An emerging technique that hold promise for increasing predictive accuracy of actuarial methods is machine learning [@Garb2019]. However, one challenge of some machine learning techniques is that they are like a "black box" and are not transparent, which raises [ethical](#ethics) issues [@Garb2019]. Machine learning may be most valuable when the data available are complex and there are many [predictive](#prediction) variables [@Garb2019].

## Why Clinical Judgment is More Widely Used Than Statistical Formulas

Despite actuarial methods being generally more accurate than clinical judgment, clinical judgment is much more widely used. There are several reasons why actuarial methods have not caught on. One reason why actuarial methods have not caught on is because of professional traditions. Experts in any field do not like to think that a computer could outperform them. Some practitioners argue that judgment/[prediction](#prediction) is an "art form" and that using a statistical formula is treating people like a number. However, using an approach (i.e., clinical judgment) that systematically leads to less [accurate](#validity) decisions and [predictions](#prediction) is an [ethical](#ethics) problem.

Some clinicians do not think that group means apply to an individual client. This invokes the distinction between nomothetic (group-level) inferences and idiographic (individual-level) inferences.	However, the scientific evidence and probability theory strongly favor the nomothetic approach to clinical [prediction](#prediction)—it is better to generalize from group-level evidence than throwing out all the evidence and taking the approach of "anything goes." Clinicians frequently believe the broken leg fallacy, i.e., thinking that your client is an exception to the algorithmic [prediction](#prediction). In most cases, deviating from the statistical formula will result in less [accurate](#validity) [predictions](#prediction). People tend to over-estimate the probability of low [base rate](#baseRate) conditions and events.

Another reason why actuarial methods have not caught on is the belief that receiving a treatment is the only thing that matters. But, it is an empirical question which treatment is most effective for whom. What if we could do better? For example, we could potentially use a formula to identify the most effective treatment for a client. Some treatments are no better than placebo; other treatments are actually harmful [@Lilienfeld2007; @Williams2021].

Another reason why clinical methods are more widely used than actuarial methods is the over-confidence in in clinicians' [predictions](#prediction)—clinicians think they are more accurate than they are. We see this when examining their [calibration](#calibration); their [predictions](#prediction) tend to be [mis-calibrated](#calibration). For example, things they report with 80% confidence occur less than 80% of the time. Clinicians will sometimes be correct by chance, and they tend to mis-attribute that to their assessment; clinicians tend to remember the successes and forget the failures. Note, however, that it is not just clinicians who are over-confident; humans in general tend to be over-confident in their [predictions](#prediction).

Another argument against using actuarial methods is that "no methods exist." In some cases, that is true—actuarial methods do not yet exist for some [prediction](#prediction) problems. However, one can always create an algorithm of the clinicians' judgments, even if one does not have access to the clients' outcome information. A model of clinicians' responses tends to be more [accurate](#validity) than clinicians' judgments themselves because the model gives the same outcome with the same input data—i.e., it is perfectly [reliable](#reliability).

Another argument from some clinicians is that "My job is to understand, not to predict." But what kind of understanding does not involve [predictions](#prediction)? [Accurate predictions](#predictiveValidity) help in understanding. Knowing how people would perform in different conditions is the same thing as good understanding.

## Conclusion

In general, it is better to develop and use structured, actuarial approaches than informal approaches that rely on human or clinical judgment.

## Suggested Readings

@Dawes1989; @Garb2019; @Grove2000
